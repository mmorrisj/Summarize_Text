{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import en_core_web_sm\n",
    "from pathlib import Path\n",
    "from nltk import sent_tokenize\n",
    "import heapq\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=\"C:\\\\Users\\\\589636\\\\Documents\\\\Jupyter_Notebook\\\\Deep Learning\\\\Deep Learning results.json\"\n",
    "filepath=\"C:\\\\Users\\\\589636\\\\Documents\\\\Jupyter_Notebook\\\\Deep Learning\\\\Deep Learning results.csv\"\n",
    "with open(file) as f:\n",
    "    df=json.load(f)\n",
    "db=pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>description</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nick Douglas</td>\n",
       "      <td>2018-02-06T16:45:00Z</td>\n",
       "      <td>Machine learning (aka A.I.) seems bizarre and ...</td>\n",
       "      <td>Lifehacker.com</td>\n",
       "      <td>Machine learning (aka A.I.) seems bizarre and ...</td>\n",
       "      <td>Become a Deep Learning Coder From Scratch in U...</td>\n",
       "      <td>https://lifehacker.com/become-a-deep-learning-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Steve Dent</td>\n",
       "      <td>2018-01-29T19:30:00Z</td>\n",
       "      <td>As artificial intelligence advances at an unpr...</td>\n",
       "      <td>Engadget</td>\n",
       "      <td>Robotic Habitats is curated by Claudia Pasquer...</td>\n",
       "      <td>‘Robotic Habitats’ imagines a self-sustaining ...</td>\n",
       "      <td>https://www.engadget.com/2018/01/29/robotic-ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mallory Locklear</td>\n",
       "      <td>2018-04-16T23:31:00Z</td>\n",
       "      <td>In a talk given today at the American Associat...</td>\n",
       "      <td>Engadget</td>\n",
       "      <td>The microscope is an ordinary light microscope...</td>\n",
       "      <td>Google made an AR microscope that can help det...</td>\n",
       "      <td>https://www.engadget.com/2018/04/16/google-ar-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jon Fingas</td>\n",
       "      <td>2018-01-30T18:00:00Z</td>\n",
       "      <td>Pinterest is very committed to improving its s...</td>\n",
       "      <td>Engadget</td>\n",
       "      <td>The exact nature of what Rosenberg is doing is...</td>\n",
       "      <td>Pinterest hires Google computer vision expert ...</td>\n",
       "      <td>https://www.engadget.com/2018/01/30/pinterest-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rachel England</td>\n",
       "      <td>2018-03-22T12:00:00Z</td>\n",
       "      <td>Samsung's latest application processor (AP) is...</td>\n",
       "      <td>Engadget</td>\n",
       "      <td>The AP also improves single camera out-focusin...</td>\n",
       "      <td>Samsung's new Exynos chip boosts photo-taking ...</td>\n",
       "      <td>https://www.engadget.com/2018/03/22/samsungs-e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Devindra Hardawar</td>\n",
       "      <td>2018-03-27T18:10:00Z</td>\n",
       "      <td>Soon you won't need one of NVIDIA's tiny Jetso...</td>\n",
       "      <td>Engadget</td>\n",
       "      <td>While it's a surprising move for NVIDIA, which...</td>\n",
       "      <td>NVIDIA's next AI steps: An ARM deal and a new ...</td>\n",
       "      <td>https://www.engadget.com/2018/03/27/nvidia-arm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Natalie Wolchover</td>\n",
       "      <td>2018-04-21T12:00:00Z</td>\n",
       "      <td>In new computer experiments, artificial-intell...</td>\n",
       "      <td>Wired</td>\n",
       "      <td>Half a century ago, the pioneers of chaos theo...</td>\n",
       "      <td>Machine Learning’s ‘Amazing’ Ability to Predic...</td>\n",
       "      <td>https://www.wired.com/story/machine-learnings-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sandra Upson</td>\n",
       "      <td>2018-03-28T11:00:00Z</td>\n",
       "      <td>Ubiquity6 wants to be a platform on which othe...</td>\n",
       "      <td>Wired</td>\n",
       "      <td>At age 25, Anjney Midha has a stronger resume ...</td>\n",
       "      <td>This Startup Makes Augmented Reality Social—an...</td>\n",
       "      <td>https://www.wired.com/story/this-startup-makes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Robbie Gonzalez</td>\n",
       "      <td>2018-02-06T16:00:00Z</td>\n",
       "      <td>If we can’t understand our own brains, maybe t...</td>\n",
       "      <td>Wired</td>\n",
       "      <td>When it comes to black boxes, there is none mo...</td>\n",
       "      <td>AI Just Learned How to Boost the Brain's Memory</td>\n",
       "      <td>https://www.wired.com/story/ml-brain-boost/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Jack Stewart</td>\n",
       "      <td>2018-02-06T13:00:00Z</td>\n",
       "      <td>All those computers and sensors can hurt fuel ...</td>\n",
       "      <td>Wired</td>\n",
       "      <td>For longtime residents of Pittsburgh, seeing s...</td>\n",
       "      <td>Self-Driving Cars' Massive Power Consumption I...</td>\n",
       "      <td>https://www.wired.com/story/self-driving-cars-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Arielle Pardes</td>\n",
       "      <td>2018-01-31T12:00:00Z</td>\n",
       "      <td>Software developer Eugenia Kuyda is releasing ...</td>\n",
       "      <td>Wired</td>\n",
       "      <td>When Eugenia Kuyda created her chatbot, Replik...</td>\n",
       "      <td>Replika, the Emotional Chatbot, Goes Open-Source</td>\n",
       "      <td>https://www.wired.com/story/replika-open-source/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Megan Molteni</td>\n",
       "      <td>2018-02-07T15:00:00Z</td>\n",
       "      <td>The connection between heart rate and diabetes...</td>\n",
       "      <td>Wired</td>\n",
       "      <td>Before modern chemistry brought doctors blood ...</td>\n",
       "      <td>With AI, Your Apple Watch Could Flag Signs of ...</td>\n",
       "      <td>https://www.wired.com/story/with-ai-your-apple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Tom Simonite</td>\n",
       "      <td>2018-02-01T14:22:24Z</td>\n",
       "      <td>Supersmart algorithms won't take all the jobs,...</td>\n",
       "      <td>Wired</td>\n",
       "      <td>Artificial intelligence is overhyped—there, we...</td>\n",
       "      <td>Artificial Intelligence: The Complete Guide</td>\n",
       "      <td>https://www.wired.com/story/guide-artificial-i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Jason Pontin</td>\n",
       "      <td>2018-02-27T11:00:00Z</td>\n",
       "      <td>WIRED columnist Jason Pontin on how machine le...</td>\n",
       "      <td>Wired</td>\n",
       "      <td>Americans do not agree on guns. Debate is otio...</td>\n",
       "      <td>How AI-Driven Insurance Could Help Prevent Gun...</td>\n",
       "      <td>https://www.wired.com/story/how-ai-driven-insu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Jason Pontin</td>\n",
       "      <td>2018-04-02T11:00:00Z</td>\n",
       "      <td>WIRED columnist Jason Pontin on an audacious t...</td>\n",
       "      <td>Wired</td>\n",
       "      <td>The only thing you know for sure is that you a...</td>\n",
       "      <td>The Tricky Business of Measuring Consciousness</td>\n",
       "      <td>https://www.wired.com/story/tricky-business-of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>James Vlahos</td>\n",
       "      <td>2018-02-27T11:00:00Z</td>\n",
       "      <td>Amazon is staging a contest called the Alexa P...</td>\n",
       "      <td>Wired</td>\n",
       "      <td>The first interactor—a muscular man in his fif...</td>\n",
       "      <td>Alexa Prize: Amazon's Battle to Bring Conversa...</td>\n",
       "      <td>https://www.wired.com/story/inside-amazon-alex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Shannon Liao</td>\n",
       "      <td>2018-02-22T19:37:45Z</td>\n",
       "      <td>A new beauty startup, Proven, wants to use AI ...</td>\n",
       "      <td>The Verge</td>\n",
       "      <td>A new beauty startup, Proven, is using artific...</td>\n",
       "      <td>This startup wants to use AI to prove that it ...</td>\n",
       "      <td>https://www.theverge.com/2018/2/22/17040116/ai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>James Vincent</td>\n",
       "      <td>2018-02-01T16:03:51Z</td>\n",
       "      <td>The 600-year-old Voynich manuscript has baffle...</td>\n",
       "      <td>The Verge</td>\n",
       "      <td>If you were compiling a list of the world’s 10...</td>\n",
       "      <td>AI didn’t decode the cryptic Voynich manuscrip...</td>\n",
       "      <td>https://www.theverge.com/2018/2/1/16959454/voy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>James Vincent</td>\n",
       "      <td>2018-01-23T15:54:40Z</td>\n",
       "      <td>Machine learning is helping CCTV cameras to an...</td>\n",
       "      <td>The Verge</td>\n",
       "      <td>We usually think of surveillance cameras as di...</td>\n",
       "      <td>Artificial intelligence is going to supercharg...</td>\n",
       "      <td>https://www.theverge.com/2018/1/23/16907238/ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Frederic Lardinois</td>\n",
       "      <td>2018-03-26T16:00:46Z</td>\n",
       "      <td>Despite its name, the Linux Foundation has lon...</td>\n",
       "      <td>TechCrunch</td>\n",
       "      <td>Despite its name, the Linux Foundation has lon...</td>\n",
       "      <td>The Linux Foundation launches a deep learning ...</td>\n",
       "      <td>http://techcrunch.com/2018/03/26/the-linux-fou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Frederic Lardinois</td>\n",
       "      <td>2018-03-20T04:01:25Z</td>\n",
       "      <td>IBM’s Watson Studio is the company’s service f...</td>\n",
       "      <td>TechCrunch</td>\n",
       "      <td>IBM’s Watson Studio is the company’s service f...</td>\n",
       "      <td>IBM launches deep learning as a service inside...</td>\n",
       "      <td>http://techcrunch.com/2018/03/19/ibm-launches-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Frederic Lardinois</td>\n",
       "      <td>2018-01-30T14:00:03Z</td>\n",
       "      <td>As the founder of the Google Brain deep learni...</td>\n",
       "      <td>TechCrunch</td>\n",
       "      <td>As the founder of the Google Brain deep learni...</td>\n",
       "      <td>Andrew Ng officially launches his $175M AI Fund</td>\n",
       "      <td>http://techcrunch.com/2018/01/30/andrew-ng-off...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Matt Burns</td>\n",
       "      <td>2018-03-27T18:05:38Z</td>\n",
       "      <td>Nvidia and Arm today announced a partnership t...</td>\n",
       "      <td>TechCrunch</td>\n",
       "      <td>Nvidia and Arm today announced a partnership t...</td>\n",
       "      <td>Arm chips will with Nvidia AI could change the...</td>\n",
       "      <td>http://techcrunch.com/2018/03/27/arm-chips-wil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Catherine Shu</td>\n",
       "      <td>2018-03-07T14:00:17Z</td>\n",
       "      <td>Atomwise, which uses deep learning to shorten ...</td>\n",
       "      <td>TechCrunch</td>\n",
       "      <td>Atomwise, which uses deep learning to shorten ...</td>\n",
       "      <td>Atomwise, which uses AI to improve drug discov...</td>\n",
       "      <td>http://techcrunch.com/2018/03/07/atomwise-whic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Danny Crichton</td>\n",
       "      <td>2018-03-21T15:00:24Z</td>\n",
       "      <td>Over the past year, Cardiogram and UC San Fran...</td>\n",
       "      <td>TechCrunch</td>\n",
       "      <td>Over the past year, Cardiogram and UC San Fran...</td>\n",
       "      <td>Peer-reviewed study shows Cardiogram and Apple...</td>\n",
       "      <td>http://techcrunch.com/2018/03/21/cardiogram-at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Frederic Lardinois</td>\n",
       "      <td>2018-03-20T11:00:06Z</td>\n",
       "      <td>IBM is hosing its annual THINK conference to p...</td>\n",
       "      <td>TechCrunch</td>\n",
       "      <td>IBM is hosing its annual THINK conference to p...</td>\n",
       "      <td>IBM brings its Power9 servers with Nvidia GPUs...</td>\n",
       "      <td>http://techcrunch.com/2018/03/20/ibm-brings-it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Ned Desmond</td>\n",
       "      <td>2018-03-13T17:00:21Z</td>\n",
       "      <td>TechCrunch’s upcoming TC Sessions: Robotics is...</td>\n",
       "      <td>TechCrunch</td>\n",
       "      <td>TechCrunch’s upcoming TC Sessions: Robotics is...</td>\n",
       "      <td>UC Berkeley Professor Abbeel to speak at TechC...</td>\n",
       "      <td>http://techcrunch.com/2018/03/13/uc-berkeley-p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Sarah Perez</td>\n",
       "      <td>2018-03-14T19:26:48Z</td>\n",
       "      <td>Advancements in A.I. technology has paved the ...</td>\n",
       "      <td>TechCrunch</td>\n",
       "      <td>Advancements in AI technology have paved the w...</td>\n",
       "      <td>Voicery makes synthesized voices sound more li...</td>\n",
       "      <td>http://techcrunch.com/2018/03/14/voicery-makes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>David Riggs</td>\n",
       "      <td>2018-04-03T20:00:27Z</td>\n",
       "      <td>If you’re running a software company today, it...</td>\n",
       "      <td>TechCrunch</td>\n",
       "      <td>If you’re running a software company today, it...</td>\n",
       "      <td>Why 2018 will be the year apps go to the edge</td>\n",
       "      <td>http://techcrunch.com/2018/04/03/why-2018-will...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Lucas Matney</td>\n",
       "      <td>2018-01-24T04:28:58Z</td>\n",
       "      <td>Facebook has a new executive coming on to unif...</td>\n",
       "      <td>TechCrunch</td>\n",
       "      <td>Facebook has a new executive coming on to unif...</td>\n",
       "      <td>Facebook appoints a new AI research head</td>\n",
       "      <td>http://techcrunch.com/2018/01/23/facebook-appo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8413</th>\n",
       "      <td>Larry Dignan</td>\n",
       "      <td>2018-04-23T14:17:00Z</td>\n",
       "      <td>Amazon is reportedly planning to launch domest...</td>\n",
       "      <td>Zdnet.com</td>\n",
       "      <td>CNET Smart Home Guide From connected light bul...</td>\n",
       "      <td>Why Amazon's domestic robots aren't a stretch:...</td>\n",
       "      <td>https://www.zdnet.com/article/why-amazons-dome...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8414</th>\n",
       "      <td>Daniel Gutierrez</td>\n",
       "      <td>2018-04-23T15:30:09Z</td>\n",
       "      <td>Above the Trend Line: your industry rumor cent...</td>\n",
       "      <td>Insidebigdata.com</td>\n",
       "      <td>Above the Trend Line: your industry rumor cent...</td>\n",
       "      <td>“Above the Trend Line” – Your Industry Rumor C...</td>\n",
       "      <td>https://insidebigdata.com/2018/04/23/trend-lin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8415</th>\n",
       "      <td>NIKKEI XTECH</td>\n",
       "      <td>2018-04-23T07:02:57Z</td>\n",
       "      <td>Fujitsu Ltd showed a prototype of a system tha...</td>\n",
       "      <td>Nikkeibp.co.jp</td>\n",
       "      <td>A monitoring camera set up on the bow is used ...</td>\n",
       "      <td>Fujitsu Develops Collision Avoidance System fo...</td>\n",
       "      <td>http://tech.nikkeibp.co.jp/dm/atclen/news_en/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8416</th>\n",
       "      <td>None</td>\n",
       "      <td>2018-04-23T16:10:22Z</td>\n",
       "      <td>A machine learning method called \"deep learnin...</td>\n",
       "      <td>Phys.org</td>\n",
       "      <td>A 'deep learning' algorithm trained on images ...</td>\n",
       "      <td>Face recognition for galaxies: Artificial inte...</td>\n",
       "      <td>https://phys.org/news/2018-04-recognition-gala...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8417</th>\n",
       "      <td>Mikael Thalen</td>\n",
       "      <td>2018-04-23T16:36:56Z</td>\n",
       "      <td>System mimics 'cocktail party effect': the hum...</td>\n",
       "      <td>Infowars.com</td>\n",
       "      <td>Google engineers are currently developing a ne...</td>\n",
       "      <td>Google Tech Can Pick Voices Out of a Crowd</td>\n",
       "      <td>https://www.infowars.com/google-tech-can-pick-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8418</th>\n",
       "      <td>Ben Snively</td>\n",
       "      <td>2018-04-23T17:32:12Z</td>\n",
       "      <td>In this blog post, we’ll use your webcam on yo...</td>\n",
       "      <td>Amazon.com</td>\n",
       "      <td>Video often gets captured and saved, but it is...</td>\n",
       "      <td>Easily perform facial analysis on live feeds b...</td>\n",
       "      <td>https://aws.amazon.com/blogs/machine-learning/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8419</th>\n",
       "      <td>None</td>\n",
       "      <td>2018-04-23T20:16:13Z</td>\n",
       "      <td>Deep Learning for humans</td>\n",
       "      <td>Pypi.org</td>\n",
       "      <td>The author of this package has not provided a ...</td>\n",
       "      <td>Keras 2.1.6</td>\n",
       "      <td>https://pypi.org/project/Keras/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8420</th>\n",
       "      <td>None</td>\n",
       "      <td>2018-04-23T16:40:35Z</td>\n",
       "      <td>DUCG: Deep Unicode Confusable Generation - Sys...</td>\n",
       "      <td>Pypi.org</td>\n",
       "      <td>DUCG: Deep Unicode Confusable Generation - Sys...</td>\n",
       "      <td>pruebaunicode2 0.0.2</td>\n",
       "      <td>https://pypi.org/project/pruebaunicode2/0.0.2/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8421</th>\n",
       "      <td>Jason Tashea</td>\n",
       "      <td>2018-04-23T13:30:00Z</td>\n",
       "      <td>Last year, I had my eyes opened. As a particip...</td>\n",
       "      <td>Abajournal.com</td>\n",
       "      <td>Home Law Scribbler What makes something AI tec...</td>\n",
       "      <td>What makes something AI technology? Experts do...</td>\n",
       "      <td>http://www.abajournal.com/lawscribbler/article...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8422</th>\n",
       "      <td>Catherine Shu</td>\n",
       "      <td>2018-04-23T18:30:43Z</td>\n",
       "      <td>Capital Float, the fintech startup that says i...</td>\n",
       "      <td>TechCrunch</td>\n",
       "      <td>Capital Float, the fintech startup that says i...</td>\n",
       "      <td>Indian lending platform Capital Float raises $...</td>\n",
       "      <td>http://techcrunch.com/2018/04/23/indian-lendin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8423</th>\n",
       "      <td>ET Bureau</td>\n",
       "      <td>2018-03-15T17:31:31Z</td>\n",
       "      <td>The aim is large-scale adoption, training and ...</td>\n",
       "      <td>The Times of India</td>\n",
       "      <td>NEW DELHI: Bennett University , in partnership...</td>\n",
       "      <td>Bennett University gets UK funding for AI skil...</td>\n",
       "      <td>https://economictimes.indiatimes.com/industry/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8424</th>\n",
       "      <td>None</td>\n",
       "      <td>2018-04-22T22:00:00Z</td>\n",
       "      <td>Is it possible to migrate 100% of your data ec...</td>\n",
       "      <td>Sys-con.com</td>\n",
       "      <td>Big Data &amp; IoT: The Economic Benefits By Eliza...</td>\n",
       "      <td>Joe Caserta Joins @ExpoDX Faculty | @Joe_Caser...</td>\n",
       "      <td>http://web2.sys-con.com/node/4251662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8425</th>\n",
       "      <td>None</td>\n",
       "      <td>2018-04-23T17:30:00Z</td>\n",
       "      <td>DXWorldEXPO LLC announced today that All in Mo...</td>\n",
       "      <td>Sys-con.com</td>\n",
       "      <td>Big Data &amp; IoT: The Economic Benefits By Eliza...</td>\n",
       "      <td>All in Mobile to Exhibit at @CloudEXPO NY | @A...</td>\n",
       "      <td>http://web2.sys-con.com/node/4255543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8426</th>\n",
       "      <td>None</td>\n",
       "      <td>2018-04-23T17:30:00Z</td>\n",
       "      <td>DXWorldEXPO LLC announced today that All in Mo...</td>\n",
       "      <td>Sys-con.com</td>\n",
       "      <td>Big Data &amp; IoT: The Economic Benefits By Eliza...</td>\n",
       "      <td>All in Mobile to Exhibit at @CloudEXPO NY | @A...</td>\n",
       "      <td>https://web2.sys-con.com/node/4255543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8427</th>\n",
       "      <td>None</td>\n",
       "      <td>2018-04-23T16:00:00Z</td>\n",
       "      <td>\"We are still a relatively small software hous...</td>\n",
       "      <td>Sys-con.com</td>\n",
       "      <td>\"We are still a relatively small software hous...</td>\n",
       "      <td>Jumpstart to #DigitalTransformation | @CloudEx...</td>\n",
       "      <td>http://web2.sys-con.com/node/4252360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8428</th>\n",
       "      <td>None</td>\n",
       "      <td>2018-04-23T16:00:00Z</td>\n",
       "      <td>\"We are still a relatively small software hous...</td>\n",
       "      <td>Sys-con.com</td>\n",
       "      <td>\"We are still a relatively small software hous...</td>\n",
       "      <td>Jumpstart to #DigitalTransformation | @CloudEx...</td>\n",
       "      <td>https://web2.sys-con.com/node/4252360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8429</th>\n",
       "      <td>None</td>\n",
       "      <td>2018-04-23T09:00:00Z</td>\n",
       "      <td>In this presentation, you will learn first han...</td>\n",
       "      <td>Sys-con.com</td>\n",
       "      <td>Big Data &amp; IoT: The Economic Benefits By Eliza...</td>\n",
       "      <td>Julio Villarreal Pelegrino Joins @CloudEXPO Fa...</td>\n",
       "      <td>http://web2.sys-con.com/node/4255524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8430</th>\n",
       "      <td>None</td>\n",
       "      <td>2018-04-23T09:00:00Z</td>\n",
       "      <td>In this presentation, you will learn first han...</td>\n",
       "      <td>Sys-con.com</td>\n",
       "      <td>Big Data &amp; IoT: The Economic Benefits By Eliza...</td>\n",
       "      <td>Julio Villarreal Pelegrino Joins @CloudEXPO Fa...</td>\n",
       "      <td>https://web2.sys-con.com/node/4255524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8431</th>\n",
       "      <td>None</td>\n",
       "      <td>2018-04-23T12:34:23Z</td>\n",
       "      <td>Skoltech's Aeronet Lab has developed an algori...</td>\n",
       "      <td>Phys.org</td>\n",
       "      <td>Credit: Open Data Program: DigitalGlobe Skolte...</td>\n",
       "      <td>Neural network trained to assess fire effects</td>\n",
       "      <td>https://phys.org/news/2018-04-neural-network-e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8432</th>\n",
       "      <td>None</td>\n",
       "      <td>2018-04-23T14:08:07Z</td>\n",
       "      <td>Artificial intelligence is giving scientists n...</td>\n",
       "      <td>Phys.org</td>\n",
       "      <td>Artist's impression of Kepler-16b, discovered ...</td>\n",
       "      <td>AI beats astronomers in predicting survivabili...</td>\n",
       "      <td>https://phys.org/news/2018-04-ai-astronomers-s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8433</th>\n",
       "      <td>None</td>\n",
       "      <td>2018-03-10T04:00:00Z</td>\n",
       "      <td>\"NetApp is known as a data management leader b...</td>\n",
       "      <td>Sys-con.com</td>\n",
       "      <td>\"NetApp is known as a data management leader b...</td>\n",
       "      <td>Storage Quality of Service with @NetApp | @Clo...</td>\n",
       "      <td>http://opensource.sys-con.com/node/4207092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8434</th>\n",
       "      <td>None</td>\n",
       "      <td>2018-04-23T05:00:00Z</td>\n",
       "      <td>Creating replica copies to tolerate a certain ...</td>\n",
       "      <td>Sys-con.com</td>\n",
       "      <td>Big Data &amp; IoT: The Economic Benefits By Eliza...</td>\n",
       "      <td>HGST Approach to #Storage | @CloudExpo @HGSTSt...</td>\n",
       "      <td>http://opensource.sys-con.com/node/3853925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8435</th>\n",
       "      <td>None</td>\n",
       "      <td>2018-04-22T15:15:00Z</td>\n",
       "      <td>Without lifecycle traceability and visibility ...</td>\n",
       "      <td>Sys-con.com</td>\n",
       "      <td>Big Data &amp; IoT: The Economic Benefits By Eliza...</td>\n",
       "      <td>DevOps From Planning-to-Ops | @DevOpsSummit @C...</td>\n",
       "      <td>http://opensource.sys-con.com/node/3936446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8436</th>\n",
       "      <td>None</td>\n",
       "      <td>2018-04-23T05:00:00Z</td>\n",
       "      <td>\"This week we're really focusing on scalabilit...</td>\n",
       "      <td>Sys-con.com</td>\n",
       "      <td>Big Data &amp; IoT: The Economic Benefits By Eliza...</td>\n",
       "      <td>HGST's #ObjectStorage | @CloudExpo @HGSTStorag...</td>\n",
       "      <td>http://opensource.sys-con.com/node/3868487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8437</th>\n",
       "      <td>None</td>\n",
       "      <td>2018-04-23T15:00:00Z</td>\n",
       "      <td>As data explodes in quantity, importance and f...</td>\n",
       "      <td>Sys-con.com</td>\n",
       "      <td>Big Data &amp; IoT: The Economic Benefits By Eliza...</td>\n",
       "      <td>Commvault's DX | @ExpoDX @Commvault @RDeMeno #...</td>\n",
       "      <td>http://opensource.sys-con.com/node/3958001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8438</th>\n",
       "      <td>None</td>\n",
       "      <td>2018-04-22T14:30:00Z</td>\n",
       "      <td>We all know that end users experience the inte...</td>\n",
       "      <td>Sys-con.com</td>\n",
       "      <td>Big Data &amp; IoT: The Economic Benefits By Eliza...</td>\n",
       "      <td>Microservices: Choosing the Right Cloud Servic...</td>\n",
       "      <td>http://opensource.sys-con.com/node/4117597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8439</th>\n",
       "      <td>None</td>\n",
       "      <td>2018-04-23T00:30:00Z</td>\n",
       "      <td>As organizations shift towards IT-as-a-service...</td>\n",
       "      <td>Sys-con.com</td>\n",
       "      <td>Big Data &amp; IoT: The Economic Benefits By Eliza...</td>\n",
       "      <td>Data Management with @Commvault | @CloudExpo @...</td>\n",
       "      <td>http://opensource.sys-con.com/node/3858731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8440</th>\n",
       "      <td>None</td>\n",
       "      <td>2018-04-23T20:15:00Z</td>\n",
       "      <td>Organizations planning enterprise data center ...</td>\n",
       "      <td>Sys-con.com</td>\n",
       "      <td>Big Data &amp; IoT: The Economic Benefits By Eliza...</td>\n",
       "      <td>Accelerating #DigitalTransformation with @Form...</td>\n",
       "      <td>http://opensource.sys-con.com/node/3762089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8441</th>\n",
       "      <td>None</td>\n",
       "      <td>2018-04-22T18:00:00Z</td>\n",
       "      <td>You often hear the two titles of \"DevOps\" and ...</td>\n",
       "      <td>Sys-con.com</td>\n",
       "      <td>DXWorldEXPO LLC, the producer of the world's m...</td>\n",
       "      <td>DevOps and Immutable Infrastructure | @DevOpsS...</td>\n",
       "      <td>http://opensource.sys-con.com/node/3387377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8442</th>\n",
       "      <td>Peter Hamilton</td>\n",
       "      <td>2018-04-23T17:00:00Z</td>\n",
       "      <td>Carlow-headquartered group has merged with two...</td>\n",
       "      <td>The Irish Times</td>\n",
       "      <td>Irish security monitoring company Netwatch has...</td>\n",
       "      <td>Netwatch to merge with US and UK firms followi...</td>\n",
       "      <td>https://www.irishtimes.com/business/retail-and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8443 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  author                  date  \\\n",
       "0           Nick Douglas  2018-02-06T16:45:00Z   \n",
       "1             Steve Dent  2018-01-29T19:30:00Z   \n",
       "2       Mallory Locklear  2018-04-16T23:31:00Z   \n",
       "3             Jon Fingas  2018-01-30T18:00:00Z   \n",
       "4         Rachel England  2018-03-22T12:00:00Z   \n",
       "5      Devindra Hardawar  2018-03-27T18:10:00Z   \n",
       "6      Natalie Wolchover  2018-04-21T12:00:00Z   \n",
       "7           Sandra Upson  2018-03-28T11:00:00Z   \n",
       "8        Robbie Gonzalez  2018-02-06T16:00:00Z   \n",
       "9           Jack Stewart  2018-02-06T13:00:00Z   \n",
       "10        Arielle Pardes  2018-01-31T12:00:00Z   \n",
       "11         Megan Molteni  2018-02-07T15:00:00Z   \n",
       "12          Tom Simonite  2018-02-01T14:22:24Z   \n",
       "13          Jason Pontin  2018-02-27T11:00:00Z   \n",
       "14          Jason Pontin  2018-04-02T11:00:00Z   \n",
       "15          James Vlahos  2018-02-27T11:00:00Z   \n",
       "16          Shannon Liao  2018-02-22T19:37:45Z   \n",
       "17         James Vincent  2018-02-01T16:03:51Z   \n",
       "18         James Vincent  2018-01-23T15:54:40Z   \n",
       "19    Frederic Lardinois  2018-03-26T16:00:46Z   \n",
       "20    Frederic Lardinois  2018-03-20T04:01:25Z   \n",
       "21    Frederic Lardinois  2018-01-30T14:00:03Z   \n",
       "22            Matt Burns  2018-03-27T18:05:38Z   \n",
       "23         Catherine Shu  2018-03-07T14:00:17Z   \n",
       "24        Danny Crichton  2018-03-21T15:00:24Z   \n",
       "25    Frederic Lardinois  2018-03-20T11:00:06Z   \n",
       "26           Ned Desmond  2018-03-13T17:00:21Z   \n",
       "27           Sarah Perez  2018-03-14T19:26:48Z   \n",
       "28           David Riggs  2018-04-03T20:00:27Z   \n",
       "29          Lucas Matney  2018-01-24T04:28:58Z   \n",
       "...                  ...                   ...   \n",
       "8413        Larry Dignan  2018-04-23T14:17:00Z   \n",
       "8414    Daniel Gutierrez  2018-04-23T15:30:09Z   \n",
       "8415        NIKKEI XTECH  2018-04-23T07:02:57Z   \n",
       "8416                None  2018-04-23T16:10:22Z   \n",
       "8417       Mikael Thalen  2018-04-23T16:36:56Z   \n",
       "8418         Ben Snively  2018-04-23T17:32:12Z   \n",
       "8419                None  2018-04-23T20:16:13Z   \n",
       "8420                None  2018-04-23T16:40:35Z   \n",
       "8421        Jason Tashea  2018-04-23T13:30:00Z   \n",
       "8422       Catherine Shu  2018-04-23T18:30:43Z   \n",
       "8423           ET Bureau  2018-03-15T17:31:31Z   \n",
       "8424                None  2018-04-22T22:00:00Z   \n",
       "8425                None  2018-04-23T17:30:00Z   \n",
       "8426                None  2018-04-23T17:30:00Z   \n",
       "8427                None  2018-04-23T16:00:00Z   \n",
       "8428                None  2018-04-23T16:00:00Z   \n",
       "8429                None  2018-04-23T09:00:00Z   \n",
       "8430                None  2018-04-23T09:00:00Z   \n",
       "8431                None  2018-04-23T12:34:23Z   \n",
       "8432                None  2018-04-23T14:08:07Z   \n",
       "8433                None  2018-03-10T04:00:00Z   \n",
       "8434                None  2018-04-23T05:00:00Z   \n",
       "8435                None  2018-04-22T15:15:00Z   \n",
       "8436                None  2018-04-23T05:00:00Z   \n",
       "8437                None  2018-04-23T15:00:00Z   \n",
       "8438                None  2018-04-22T14:30:00Z   \n",
       "8439                None  2018-04-23T00:30:00Z   \n",
       "8440                None  2018-04-23T20:15:00Z   \n",
       "8441                None  2018-04-22T18:00:00Z   \n",
       "8442      Peter Hamilton  2018-04-23T17:00:00Z   \n",
       "\n",
       "                                            description              source  \\\n",
       "0     Machine learning (aka A.I.) seems bizarre and ...      Lifehacker.com   \n",
       "1     As artificial intelligence advances at an unpr...            Engadget   \n",
       "2     In a talk given today at the American Associat...            Engadget   \n",
       "3     Pinterest is very committed to improving its s...            Engadget   \n",
       "4     Samsung's latest application processor (AP) is...            Engadget   \n",
       "5     Soon you won't need one of NVIDIA's tiny Jetso...            Engadget   \n",
       "6     In new computer experiments, artificial-intell...               Wired   \n",
       "7     Ubiquity6 wants to be a platform on which othe...               Wired   \n",
       "8     If we can’t understand our own brains, maybe t...               Wired   \n",
       "9     All those computers and sensors can hurt fuel ...               Wired   \n",
       "10    Software developer Eugenia Kuyda is releasing ...               Wired   \n",
       "11    The connection between heart rate and diabetes...               Wired   \n",
       "12    Supersmart algorithms won't take all the jobs,...               Wired   \n",
       "13    WIRED columnist Jason Pontin on how machine le...               Wired   \n",
       "14    WIRED columnist Jason Pontin on an audacious t...               Wired   \n",
       "15    Amazon is staging a contest called the Alexa P...               Wired   \n",
       "16    A new beauty startup, Proven, wants to use AI ...           The Verge   \n",
       "17    The 600-year-old Voynich manuscript has baffle...           The Verge   \n",
       "18    Machine learning is helping CCTV cameras to an...           The Verge   \n",
       "19    Despite its name, the Linux Foundation has lon...          TechCrunch   \n",
       "20    IBM’s Watson Studio is the company’s service f...          TechCrunch   \n",
       "21    As the founder of the Google Brain deep learni...          TechCrunch   \n",
       "22    Nvidia and Arm today announced a partnership t...          TechCrunch   \n",
       "23    Atomwise, which uses deep learning to shorten ...          TechCrunch   \n",
       "24    Over the past year, Cardiogram and UC San Fran...          TechCrunch   \n",
       "25    IBM is hosing its annual THINK conference to p...          TechCrunch   \n",
       "26    TechCrunch’s upcoming TC Sessions: Robotics is...          TechCrunch   \n",
       "27    Advancements in A.I. technology has paved the ...          TechCrunch   \n",
       "28    If you’re running a software company today, it...          TechCrunch   \n",
       "29    Facebook has a new executive coming on to unif...          TechCrunch   \n",
       "...                                                 ...                 ...   \n",
       "8413  Amazon is reportedly planning to launch domest...           Zdnet.com   \n",
       "8414  Above the Trend Line: your industry rumor cent...   Insidebigdata.com   \n",
       "8415  Fujitsu Ltd showed a prototype of a system tha...      Nikkeibp.co.jp   \n",
       "8416  A machine learning method called \"deep learnin...            Phys.org   \n",
       "8417  System mimics 'cocktail party effect': the hum...        Infowars.com   \n",
       "8418  In this blog post, we’ll use your webcam on yo...          Amazon.com   \n",
       "8419                           Deep Learning for humans            Pypi.org   \n",
       "8420  DUCG: Deep Unicode Confusable Generation - Sys...            Pypi.org   \n",
       "8421  Last year, I had my eyes opened. As a particip...      Abajournal.com   \n",
       "8422  Capital Float, the fintech startup that says i...          TechCrunch   \n",
       "8423  The aim is large-scale adoption, training and ...  The Times of India   \n",
       "8424  Is it possible to migrate 100% of your data ec...         Sys-con.com   \n",
       "8425  DXWorldEXPO LLC announced today that All in Mo...         Sys-con.com   \n",
       "8426  DXWorldEXPO LLC announced today that All in Mo...         Sys-con.com   \n",
       "8427  \"We are still a relatively small software hous...         Sys-con.com   \n",
       "8428  \"We are still a relatively small software hous...         Sys-con.com   \n",
       "8429  In this presentation, you will learn first han...         Sys-con.com   \n",
       "8430  In this presentation, you will learn first han...         Sys-con.com   \n",
       "8431  Skoltech's Aeronet Lab has developed an algori...            Phys.org   \n",
       "8432  Artificial intelligence is giving scientists n...            Phys.org   \n",
       "8433  \"NetApp is known as a data management leader b...         Sys-con.com   \n",
       "8434  Creating replica copies to tolerate a certain ...         Sys-con.com   \n",
       "8435  Without lifecycle traceability and visibility ...         Sys-con.com   \n",
       "8436  \"This week we're really focusing on scalabilit...         Sys-con.com   \n",
       "8437  As data explodes in quantity, importance and f...         Sys-con.com   \n",
       "8438  We all know that end users experience the inte...         Sys-con.com   \n",
       "8439  As organizations shift towards IT-as-a-service...         Sys-con.com   \n",
       "8440  Organizations planning enterprise data center ...         Sys-con.com   \n",
       "8441  You often hear the two titles of \"DevOps\" and ...         Sys-con.com   \n",
       "8442  Carlow-headquartered group has merged with two...     The Irish Times   \n",
       "\n",
       "                                                   text  \\\n",
       "0     Machine learning (aka A.I.) seems bizarre and ...   \n",
       "1     Robotic Habitats is curated by Claudia Pasquer...   \n",
       "2     The microscope is an ordinary light microscope...   \n",
       "3     The exact nature of what Rosenberg is doing is...   \n",
       "4     The AP also improves single camera out-focusin...   \n",
       "5     While it's a surprising move for NVIDIA, which...   \n",
       "6     Half a century ago, the pioneers of chaos theo...   \n",
       "7     At age 25, Anjney Midha has a stronger resume ...   \n",
       "8     When it comes to black boxes, there is none mo...   \n",
       "9     For longtime residents of Pittsburgh, seeing s...   \n",
       "10    When Eugenia Kuyda created her chatbot, Replik...   \n",
       "11    Before modern chemistry brought doctors blood ...   \n",
       "12    Artificial intelligence is overhyped—there, we...   \n",
       "13    Americans do not agree on guns. Debate is otio...   \n",
       "14    The only thing you know for sure is that you a...   \n",
       "15    The first interactor—a muscular man in his fif...   \n",
       "16    A new beauty startup, Proven, is using artific...   \n",
       "17    If you were compiling a list of the world’s 10...   \n",
       "18    We usually think of surveillance cameras as di...   \n",
       "19    Despite its name, the Linux Foundation has lon...   \n",
       "20    IBM’s Watson Studio is the company’s service f...   \n",
       "21    As the founder of the Google Brain deep learni...   \n",
       "22    Nvidia and Arm today announced a partnership t...   \n",
       "23    Atomwise, which uses deep learning to shorten ...   \n",
       "24    Over the past year, Cardiogram and UC San Fran...   \n",
       "25    IBM is hosing its annual THINK conference to p...   \n",
       "26    TechCrunch’s upcoming TC Sessions: Robotics is...   \n",
       "27    Advancements in AI technology have paved the w...   \n",
       "28    If you’re running a software company today, it...   \n",
       "29    Facebook has a new executive coming on to unif...   \n",
       "...                                                 ...   \n",
       "8413  CNET Smart Home Guide From connected light bul...   \n",
       "8414  Above the Trend Line: your industry rumor cent...   \n",
       "8415  A monitoring camera set up on the bow is used ...   \n",
       "8416  A 'deep learning' algorithm trained on images ...   \n",
       "8417  Google engineers are currently developing a ne...   \n",
       "8418  Video often gets captured and saved, but it is...   \n",
       "8419  The author of this package has not provided a ...   \n",
       "8420  DUCG: Deep Unicode Confusable Generation - Sys...   \n",
       "8421  Home Law Scribbler What makes something AI tec...   \n",
       "8422  Capital Float, the fintech startup that says i...   \n",
       "8423  NEW DELHI: Bennett University , in partnership...   \n",
       "8424  Big Data & IoT: The Economic Benefits By Eliza...   \n",
       "8425  Big Data & IoT: The Economic Benefits By Eliza...   \n",
       "8426  Big Data & IoT: The Economic Benefits By Eliza...   \n",
       "8427  \"We are still a relatively small software hous...   \n",
       "8428  \"We are still a relatively small software hous...   \n",
       "8429  Big Data & IoT: The Economic Benefits By Eliza...   \n",
       "8430  Big Data & IoT: The Economic Benefits By Eliza...   \n",
       "8431  Credit: Open Data Program: DigitalGlobe Skolte...   \n",
       "8432  Artist's impression of Kepler-16b, discovered ...   \n",
       "8433  \"NetApp is known as a data management leader b...   \n",
       "8434  Big Data & IoT: The Economic Benefits By Eliza...   \n",
       "8435  Big Data & IoT: The Economic Benefits By Eliza...   \n",
       "8436  Big Data & IoT: The Economic Benefits By Eliza...   \n",
       "8437  Big Data & IoT: The Economic Benefits By Eliza...   \n",
       "8438  Big Data & IoT: The Economic Benefits By Eliza...   \n",
       "8439  Big Data & IoT: The Economic Benefits By Eliza...   \n",
       "8440  Big Data & IoT: The Economic Benefits By Eliza...   \n",
       "8441  DXWorldEXPO LLC, the producer of the world's m...   \n",
       "8442  Irish security monitoring company Netwatch has...   \n",
       "\n",
       "                                                  title  \\\n",
       "0     Become a Deep Learning Coder From Scratch in U...   \n",
       "1     ‘Robotic Habitats’ imagines a self-sustaining ...   \n",
       "2     Google made an AR microscope that can help det...   \n",
       "3     Pinterest hires Google computer vision expert ...   \n",
       "4     Samsung's new Exynos chip boosts photo-taking ...   \n",
       "5     NVIDIA's next AI steps: An ARM deal and a new ...   \n",
       "6     Machine Learning’s ‘Amazing’ Ability to Predic...   \n",
       "7     This Startup Makes Augmented Reality Social—an...   \n",
       "8       AI Just Learned How to Boost the Brain's Memory   \n",
       "9     Self-Driving Cars' Massive Power Consumption I...   \n",
       "10     Replika, the Emotional Chatbot, Goes Open-Source   \n",
       "11    With AI, Your Apple Watch Could Flag Signs of ...   \n",
       "12          Artificial Intelligence: The Complete Guide   \n",
       "13    How AI-Driven Insurance Could Help Prevent Gun...   \n",
       "14       The Tricky Business of Measuring Consciousness   \n",
       "15    Alexa Prize: Amazon's Battle to Bring Conversa...   \n",
       "16    This startup wants to use AI to prove that it ...   \n",
       "17    AI didn’t decode the cryptic Voynich manuscrip...   \n",
       "18    Artificial intelligence is going to supercharg...   \n",
       "19    The Linux Foundation launches a deep learning ...   \n",
       "20    IBM launches deep learning as a service inside...   \n",
       "21      Andrew Ng officially launches his $175M AI Fund   \n",
       "22    Arm chips will with Nvidia AI could change the...   \n",
       "23    Atomwise, which uses AI to improve drug discov...   \n",
       "24    Peer-reviewed study shows Cardiogram and Apple...   \n",
       "25    IBM brings its Power9 servers with Nvidia GPUs...   \n",
       "26    UC Berkeley Professor Abbeel to speak at TechC...   \n",
       "27    Voicery makes synthesized voices sound more li...   \n",
       "28        Why 2018 will be the year apps go to the edge   \n",
       "29             Facebook appoints a new AI research head   \n",
       "...                                                 ...   \n",
       "8413  Why Amazon's domestic robots aren't a stretch:...   \n",
       "8414  “Above the Trend Line” – Your Industry Rumor C...   \n",
       "8415  Fujitsu Develops Collision Avoidance System fo...   \n",
       "8416  Face recognition for galaxies: Artificial inte...   \n",
       "8417         Google Tech Can Pick Voices Out of a Crowd   \n",
       "8418  Easily perform facial analysis on live feeds b...   \n",
       "8419                                        Keras 2.1.6   \n",
       "8420                               pruebaunicode2 0.0.2   \n",
       "8421  What makes something AI technology? Experts do...   \n",
       "8422  Indian lending platform Capital Float raises $...   \n",
       "8423  Bennett University gets UK funding for AI skil...   \n",
       "8424  Joe Caserta Joins @ExpoDX Faculty | @Joe_Caser...   \n",
       "8425  All in Mobile to Exhibit at @CloudEXPO NY | @A...   \n",
       "8426  All in Mobile to Exhibit at @CloudEXPO NY | @A...   \n",
       "8427  Jumpstart to #DigitalTransformation | @CloudEx...   \n",
       "8428  Jumpstart to #DigitalTransformation | @CloudEx...   \n",
       "8429  Julio Villarreal Pelegrino Joins @CloudEXPO Fa...   \n",
       "8430  Julio Villarreal Pelegrino Joins @CloudEXPO Fa...   \n",
       "8431      Neural network trained to assess fire effects   \n",
       "8432  AI beats astronomers in predicting survivabili...   \n",
       "8433  Storage Quality of Service with @NetApp | @Clo...   \n",
       "8434  HGST Approach to #Storage | @CloudExpo @HGSTSt...   \n",
       "8435  DevOps From Planning-to-Ops | @DevOpsSummit @C...   \n",
       "8436  HGST's #ObjectStorage | @CloudExpo @HGSTStorag...   \n",
       "8437  Commvault's DX | @ExpoDX @Commvault @RDeMeno #...   \n",
       "8438  Microservices: Choosing the Right Cloud Servic...   \n",
       "8439  Data Management with @Commvault | @CloudExpo @...   \n",
       "8440  Accelerating #DigitalTransformation with @Form...   \n",
       "8441  DevOps and Immutable Infrastructure | @DevOpsS...   \n",
       "8442  Netwatch to merge with US and UK firms followi...   \n",
       "\n",
       "                                                    url  \n",
       "0     https://lifehacker.com/become-a-deep-learning-...  \n",
       "1     https://www.engadget.com/2018/01/29/robotic-ha...  \n",
       "2     https://www.engadget.com/2018/04/16/google-ar-...  \n",
       "3     https://www.engadget.com/2018/01/30/pinterest-...  \n",
       "4     https://www.engadget.com/2018/03/22/samsungs-e...  \n",
       "5     https://www.engadget.com/2018/03/27/nvidia-arm...  \n",
       "6     https://www.wired.com/story/machine-learnings-...  \n",
       "7     https://www.wired.com/story/this-startup-makes...  \n",
       "8           https://www.wired.com/story/ml-brain-boost/  \n",
       "9     https://www.wired.com/story/self-driving-cars-...  \n",
       "10     https://www.wired.com/story/replika-open-source/  \n",
       "11    https://www.wired.com/story/with-ai-your-apple...  \n",
       "12    https://www.wired.com/story/guide-artificial-i...  \n",
       "13    https://www.wired.com/story/how-ai-driven-insu...  \n",
       "14    https://www.wired.com/story/tricky-business-of...  \n",
       "15    https://www.wired.com/story/inside-amazon-alex...  \n",
       "16    https://www.theverge.com/2018/2/22/17040116/ai...  \n",
       "17    https://www.theverge.com/2018/2/1/16959454/voy...  \n",
       "18    https://www.theverge.com/2018/1/23/16907238/ar...  \n",
       "19    http://techcrunch.com/2018/03/26/the-linux-fou...  \n",
       "20    http://techcrunch.com/2018/03/19/ibm-launches-...  \n",
       "21    http://techcrunch.com/2018/01/30/andrew-ng-off...  \n",
       "22    http://techcrunch.com/2018/03/27/arm-chips-wil...  \n",
       "23    http://techcrunch.com/2018/03/07/atomwise-whic...  \n",
       "24    http://techcrunch.com/2018/03/21/cardiogram-at...  \n",
       "25    http://techcrunch.com/2018/03/20/ibm-brings-it...  \n",
       "26    http://techcrunch.com/2018/03/13/uc-berkeley-p...  \n",
       "27    http://techcrunch.com/2018/03/14/voicery-makes...  \n",
       "28    http://techcrunch.com/2018/04/03/why-2018-will...  \n",
       "29    http://techcrunch.com/2018/01/23/facebook-appo...  \n",
       "...                                                 ...  \n",
       "8413  https://www.zdnet.com/article/why-amazons-dome...  \n",
       "8414  https://insidebigdata.com/2018/04/23/trend-lin...  \n",
       "8415  http://tech.nikkeibp.co.jp/dm/atclen/news_en/1...  \n",
       "8416  https://phys.org/news/2018-04-recognition-gala...  \n",
       "8417  https://www.infowars.com/google-tech-can-pick-...  \n",
       "8418  https://aws.amazon.com/blogs/machine-learning/...  \n",
       "8419                    https://pypi.org/project/Keras/  \n",
       "8420     https://pypi.org/project/pruebaunicode2/0.0.2/  \n",
       "8421  http://www.abajournal.com/lawscribbler/article...  \n",
       "8422  http://techcrunch.com/2018/04/23/indian-lendin...  \n",
       "8423  https://economictimes.indiatimes.com/industry/...  \n",
       "8424               http://web2.sys-con.com/node/4251662  \n",
       "8425               http://web2.sys-con.com/node/4255543  \n",
       "8426              https://web2.sys-con.com/node/4255543  \n",
       "8427               http://web2.sys-con.com/node/4252360  \n",
       "8428              https://web2.sys-con.com/node/4252360  \n",
       "8429               http://web2.sys-con.com/node/4255524  \n",
       "8430              https://web2.sys-con.com/node/4255524  \n",
       "8431  https://phys.org/news/2018-04-neural-network-e...  \n",
       "8432  https://phys.org/news/2018-04-ai-astronomers-s...  \n",
       "8433         http://opensource.sys-con.com/node/4207092  \n",
       "8434         http://opensource.sys-con.com/node/3853925  \n",
       "8435         http://opensource.sys-con.com/node/3936446  \n",
       "8436         http://opensource.sys-con.com/node/3868487  \n",
       "8437         http://opensource.sys-con.com/node/3958001  \n",
       "8438         http://opensource.sys-con.com/node/4117597  \n",
       "8439         http://opensource.sys-con.com/node/3858731  \n",
       "8440         http://opensource.sys-con.com/node/3762089  \n",
       "8441         http://opensource.sys-con.com/node/3387377  \n",
       "8442  https://www.irishtimes.com/business/retail-and...  \n",
       "\n",
       "[8443 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Machine learning (aka A.I.) seems bizarre and complicated. It’s the tech behind image and speech recognition, recommendation systems, and all kinds of tasks that computers used to be really bad at but are now really good at. It involves teaching a computer to teach itself. And you can learn to do it in well under a year, according to data scientist Bargava. You’ll need to put in a solid 10-20 hours a week, but you will learn a lot along the way.Bargava lays out a six-month, five-step plan for coders to learn about deep learning. If you don’t already know how to code, that’s step zero. Try the free course at LearnPython.org, or shell out $200 for Codecademy’s 10-week Python course.Then follow Bargava’s steps, which include more online courses, some solo projects, and some extra reading. It might be grueling, but Bargava says that if you stick to it, by the end you’ll be able to “learn, follow and contribute to state-of-art work in deep learning.”How to learn Deep Learning in 6 months  Medium'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=db.text[0].replace('\\n','').replace('|','').strip()\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_article_text = re.sub('[^a-zA-Z]', ' ', doc )\n",
    "formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Machine learning (aka A.I.)',\n",
       " 'seems bizarre and complicated.',\n",
       " 'It’s the tech behind image and speech recognition, recommendation systems, and all kinds of tasks that computers used to be really bad at but are now really good at.',\n",
       " 'It involves teaching a computer to teach itself.',\n",
       " 'And you can learn to do it in well under a year, according to data scientist Bargava.',\n",
       " 'You’ll need to put in a solid 10-20 hours a week, but you will learn a lot along the way.Bargava lays out a six-month, five-step plan for coders to learn about deep learning.',\n",
       " 'If you don’t already know how to code, that’s step zero.',\n",
       " 'Try the free course at LearnPython.org, or shell out $200 for Codecademy’s 10-week Python course.Then follow Bargava’s steps, which include more online courses, some solo projects, and some extra reading.',\n",
       " 'It might be grueling, but Bargava says that if you stick to it, by the end you’ll be able to “learn, follow and contribute to state-of-art work in deep learning.”How to learn Deep Learning in 6 months  Medium']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_list = nltk.sent_tokenize(doc)\n",
    "sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 1,\n",
       " 'And': 1,\n",
       " 'Bargava': 4,\n",
       " 'Codecademy': 1,\n",
       " 'Deep': 1,\n",
       " 'How': 1,\n",
       " 'I': 1,\n",
       " 'If': 1,\n",
       " 'It': 3,\n",
       " 'LearnPython': 1,\n",
       " 'Learning': 1,\n",
       " 'Machine': 1,\n",
       " 'Medium': 1,\n",
       " 'Python': 1,\n",
       " 'Then': 1,\n",
       " 'Try': 1,\n",
       " 'You': 1,\n",
       " 'able': 1,\n",
       " 'according': 1,\n",
       " 'aka': 1,\n",
       " 'along': 1,\n",
       " 'already': 1,\n",
       " 'art': 1,\n",
       " 'bad': 1,\n",
       " 'behind': 1,\n",
       " 'bizarre': 1,\n",
       " 'code': 1,\n",
       " 'coders': 1,\n",
       " 'complicated': 1,\n",
       " 'computer': 1,\n",
       " 'computers': 1,\n",
       " 'contribute': 1,\n",
       " 'course': 2,\n",
       " 'courses': 1,\n",
       " 'data': 1,\n",
       " 'deep': 2,\n",
       " 'end': 1,\n",
       " 'extra': 1,\n",
       " 'five': 1,\n",
       " 'follow': 2,\n",
       " 'free': 1,\n",
       " 'good': 1,\n",
       " 'grueling': 1,\n",
       " 'hours': 1,\n",
       " 'image': 1,\n",
       " 'include': 1,\n",
       " 'involves': 1,\n",
       " 'kinds': 1,\n",
       " 'know': 1,\n",
       " 'lays': 1,\n",
       " 'learn': 5,\n",
       " 'learning': 3,\n",
       " 'lot': 1,\n",
       " 'might': 1,\n",
       " 'month': 1,\n",
       " 'months': 1,\n",
       " 'need': 1,\n",
       " 'online': 1,\n",
       " 'org': 1,\n",
       " 'plan': 1,\n",
       " 'projects': 1,\n",
       " 'put': 1,\n",
       " 'reading': 1,\n",
       " 'really': 2,\n",
       " 'recognition': 1,\n",
       " 'recommendation': 1,\n",
       " 'says': 1,\n",
       " 'scientist': 1,\n",
       " 'seems': 1,\n",
       " 'shell': 1,\n",
       " 'six': 1,\n",
       " 'solid': 1,\n",
       " 'solo': 1,\n",
       " 'speech': 1,\n",
       " 'state': 1,\n",
       " 'step': 2,\n",
       " 'steps': 1,\n",
       " 'stick': 1,\n",
       " 'systems': 1,\n",
       " 'tasks': 1,\n",
       " 'teach': 1,\n",
       " 'teaching': 1,\n",
       " 'tech': 1,\n",
       " 'used': 1,\n",
       " 'way': 1,\n",
       " 'week': 2,\n",
       " 'well': 1,\n",
       " 'work': 1,\n",
       " 'year': 1,\n",
       " 'zero': 1}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_frequencies = {}\n",
    "for word in nltk.word_tokenize(formatted_article_text):\n",
    "    if word not in stopwords:\n",
    "        if word not in word_frequencies.keys():\n",
    "            word_frequencies[word] = 1\n",
    "        else:\n",
    "            word_frequencies[word] += 1\n",
    "word_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 0.2,\n",
       " 'And': 0.2,\n",
       " 'Bargava': 0.8,\n",
       " 'Codecademy': 0.2,\n",
       " 'Deep': 0.2,\n",
       " 'How': 0.2,\n",
       " 'I': 0.2,\n",
       " 'If': 0.2,\n",
       " 'It': 0.6,\n",
       " 'LearnPython': 0.2,\n",
       " 'Learning': 0.2,\n",
       " 'Machine': 0.2,\n",
       " 'Medium': 0.2,\n",
       " 'Python': 0.2,\n",
       " 'Then': 0.2,\n",
       " 'Try': 0.2,\n",
       " 'You': 0.2,\n",
       " 'able': 0.2,\n",
       " 'according': 0.2,\n",
       " 'aka': 0.2,\n",
       " 'along': 0.2,\n",
       " 'already': 0.2,\n",
       " 'art': 0.2,\n",
       " 'bad': 0.2,\n",
       " 'behind': 0.2,\n",
       " 'bizarre': 0.2,\n",
       " 'code': 0.2,\n",
       " 'coders': 0.2,\n",
       " 'complicated': 0.2,\n",
       " 'computer': 0.2,\n",
       " 'computers': 0.2,\n",
       " 'contribute': 0.2,\n",
       " 'course': 0.4,\n",
       " 'courses': 0.2,\n",
       " 'data': 0.2,\n",
       " 'deep': 0.4,\n",
       " 'end': 0.2,\n",
       " 'extra': 0.2,\n",
       " 'five': 0.2,\n",
       " 'follow': 0.4,\n",
       " 'free': 0.2,\n",
       " 'good': 0.2,\n",
       " 'grueling': 0.2,\n",
       " 'hours': 0.2,\n",
       " 'image': 0.2,\n",
       " 'include': 0.2,\n",
       " 'involves': 0.2,\n",
       " 'kinds': 0.2,\n",
       " 'know': 0.2,\n",
       " 'lays': 0.2,\n",
       " 'learn': 1.0,\n",
       " 'learning': 0.6,\n",
       " 'lot': 0.2,\n",
       " 'might': 0.2,\n",
       " 'month': 0.2,\n",
       " 'months': 0.2,\n",
       " 'need': 0.2,\n",
       " 'online': 0.2,\n",
       " 'org': 0.2,\n",
       " 'plan': 0.2,\n",
       " 'projects': 0.2,\n",
       " 'put': 0.2,\n",
       " 'reading': 0.2,\n",
       " 'really': 0.4,\n",
       " 'recognition': 0.2,\n",
       " 'recommendation': 0.2,\n",
       " 'says': 0.2,\n",
       " 'scientist': 0.2,\n",
       " 'seems': 0.2,\n",
       " 'shell': 0.2,\n",
       " 'six': 0.2,\n",
       " 'solid': 0.2,\n",
       " 'solo': 0.2,\n",
       " 'speech': 0.2,\n",
       " 'state': 0.2,\n",
       " 'step': 0.4,\n",
       " 'steps': 0.2,\n",
       " 'stick': 0.2,\n",
       " 'systems': 0.2,\n",
       " 'tasks': 0.2,\n",
       " 'teach': 0.2,\n",
       " 'teaching': 0.2,\n",
       " 'tech': 0.2,\n",
       " 'used': 0.2,\n",
       " 'way': 0.2,\n",
       " 'week': 0.4,\n",
       " 'well': 0.2,\n",
       " 'work': 0.2,\n",
       " 'year': 0.2,\n",
       " 'zero': 0.2}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maximum_frequncy = max(word_frequencies.values())\n",
    "\n",
    "for word in word_frequencies.keys():\n",
    "    word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n",
    "\n",
    "word_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'And you can learn to do it in well under a year, according to data scientist Bargava.': 1.9999999999999998,\n",
       " 'If you don’t already know how to code, that’s step zero.': 1.2,\n",
       " 'It involves teaching a computer to teach itself.': 0.8,\n",
       " 'It’s the tech behind image and speech recognition, recommendation systems, and all kinds of tasks that computers used to be really bad at but are now really good at.': 3.4,\n",
       " 'Machine learning (aka A.I.)': 0.8,\n",
       " 'seems bizarre and complicated.': 0.6000000000000001}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_scores = {}\n",
    "for sent in sentence_list:\n",
    "    for word in nltk.word_tokenize(sent.lower()):\n",
    "        if word in word_frequencies.keys():\n",
    "            if len(sent.split(' ')) < 30:\n",
    "                if sent not in sentence_scores.keys():\n",
    "                    sentence_scores[sent] = word_frequencies[word]\n",
    "                else:\n",
    "                    sentence_scores[sent] += word_frequencies[word]\n",
    "sentence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It’s the tech behind image and speech recognition, recommendation systems, and all kinds of tasks that computers used to be really bad at but are now really good at. And you can learn to do it in well under a year, according to data scientist Bargava. If you don’t already know how to code, that’s step zero. Machine learning (aka A.I.) It involves teaching a computer to teach itself. seems bizarre and complicated.\n"
     ]
    }
   ],
   "source": [
    "summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "summary = ' '.join(summary_sentences)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Become a Deep Learning Coder From Scratch in Under a Year', 'Machine learning (aka A.I.) seems bizarre and complicated. It’s the tech behind image and speech recognition, recommendation systems, and all kinds of tasks that computers used to be really bad at but are now really good at. It involves teaching a computer to teach itself. And you can learn to do it in well under a year, according to data scientist Bargava. You’ll need to put in a solid 10-20 hours a week, but you will learn a lot along the way.\\n\\nBargava lays out a six-month, five-step plan for coders to learn about deep learning. If you don’t already know how to code, that’s step zero. Try the free course at LearnPython.org, or shell out $200 for Codecademy’s 10-week Python course.\\n\\nThen follow Bargava’s steps, which include more online courses, some solo projects, and some extra reading. It might be grueling, but Bargava says that if you stick to it, by the end you’ll be able to “learn, follow and contribute to state-of-art work in deep learning.”\\n\\nHow to learn Deep Learning in 6 months | Medium')\n",
      "('‘Robotic Habitats’ imagines a self-sustaining AI ecosystem', 'Robotic Habitats is curated by Claudia Pasquero as part of the 2017 Tallinn Architecture Biennale (TAB), The project assumes that deep learning systems will grow out of their narrow Go-playing abilities and soon match humans at many, if not most, tasks. While that would put them on par with us, it doesn\\'t mean they would live the same way, though. \"Society will need to develop a framework for both to thrive,\" explains Noumena on its website. \"A new form of artificial life will emerge, finding space at the peripheries of humanity in order to not compete for human-dominated resources.\"\\n\\nIn order to imagine what this AI society might be like, Noumena first needed to design the robots themselves, so they created rovers that are part biologic and part artificial. They have DC motors, an Arduino main board, and Kinect motion sensors to guide their movements. On the \"nose\" is a peristaltic pump that can inject material stored in a central \"spine\" into the sandy landscape.\\n\\nThe external, 3D-printed tortoise-like skin has a slightly creepy Alien vibe, but there\\'s a reason for the grooves. They harbor mycelium, a fungus material made up of bacteria colonies that will allow the robots to \"grow.\" It also feeds energy to the motors, gears and electronics that are embedded in the bots\\' frames.\\n\\nThe robots can then head out into the 16 x 16 foot sandy terrain, based on a 3D map created from the Kinect sensor from previous excursions, and inject mycelium into the environment. That forms a network of lines consisting of bacteria and aggregating material, creating a bizarre landscape that is artificial and biological at once. It\\'s then collected by central, pod-like columns, which use the materials to fabricate new rovers, making the whole thing self-sustaining.\\n\\nThe installation is meant to be thought-provoking, showing that machines may not live in a gleaming Matrix-like world, but rather \"a natural landscape robotically manipulated,\" Noumena explains. At the same time, similar technology might be necessary for machines that go to Mars and other worlds, paving the way for humans to arrive.\\n\\nNoumena figures that this robot society would \"trigger the need for robots to evolve into different species, each performing specific tasks.\" That in turn will create an even more advanced infrastructure, that guarantees the whole thing will continue. The final result? \"A new robotic habitat shaped by autonomous species driven by hidden, inaccessible forces.\"')\n",
      "('Google made an AR microscope that can help detect cancer', 'The microscope is an ordinary light microscope, the kind used by pathologists worldwide. Google just tweaked it a little in order to introduce AI technology and augmented reality. First, neural networks are trained to detect cancer cells in images of human tissue. Then, after a slide with human tissue is placed under the modified microscope, the same image a person sees through the scope\\'s eyepieces is fed into a computer. AI algorithms then detect cancer cells in the tissue, which the system then outlines in the image seen through the eyepieces (see image above). It\\'s all done in real time and works quickly enough that it\\'s still effective when a pathologist moves a slide to look at a new section of tissue.\\n\\nSo far, the Google team has demonstrated the scope\\'s abilities with breast and prostate cancers, showing the system is pretty accurate in its assessments. But Google says such a set up could be used to detect other cancers as well as infectious diseases like tuberculosis and malaria. It could also be used to display other data or annotations into the view. \"Of course, light microscopes have proven useful in many industries other than pathology, and we believe the Augmented Reality Microscope can be adapted for a broad range of applications across healthcare, life sciences research and material science,\" says Google.\\n\\nThis proof of concept study is also described in a paper that\\'s currently still under review. Google says that a more in-depth study is required for more robust assessments of the system\\'s performance and shortcomings. It concluded, \"We\\'re excited to continue to explore how the Augmented Reality Microscope can help accelerate the adoption of machine learning for positive impact around the world.\"')\n",
      "('Pinterest hires Google computer vision expert to sort your Pins', 'The exact nature of what Rosenberg is doing is under wraps, to no one\\'s surprise, but he\\'s expected to guide engineers as they craft \"large-scale\" object detection algorithms. Before Google, he worked at HP Labs and was one of iRobot\\'s earliest employees.\\n\\nWhile it might seem odd for Rosenberg to switch employers while Google is still pushing the boundaries of AI, it\\'s easy to see why Pinterest would want to reel him in. He\\'s joining right as the company\\'s AI-based image recognition is hitting its stride -- there are now over 300 million visual searches every month, or a 70 percent increase compared to a year earlier. Rosenberg could prove instrumental to keeping that momentum going, and could make Pinterest indispensable the next time you need to find a recipe or choose an outfit.')\n",
      "(\"Samsung's new Exynos chip boosts photo-taking in mid-tier phones\", \"The AP also improves single camera out-focusing and augmented low-light images, and supports slow motion filming in full HD at 480 frames per second. Plus, it'll process images 1.6 times faster than its predecessor, the Exynos 7 7885.\\n\\nOn to performance, and its octo-core processor boasts four Cortex-A73 cores running at 2.3 gigahertz (GHz), and four 1.6GHz Cortex-A53 cores to load apps quickly and run multiple tasks simultaneously. The GPU is a second-generation Bifrost-based ARM Mali-G72, so expect more realistic and immersive graphics.\\n\\nFinally, its embedded all-network LTE modem supports 600 megabits per second (Mbps) downlink and 150Mbps uplink, alongside a bunch of wireless features such as longer range Bluetooth 5.0 and a four-mode global navigation system.\\n\\nIn short, it packs a powerful punch for a mid-tier phone AP, although products like Exynos 7 are largely aimed at the Chinese market. The company says it expects to start mass producing the AP in the second half of this year.\\n\\nCorrection: This post originally stated the Exynos 7 9610 supported slow-motion video capture at 4K resolution; it can only capture slow-motion video at full HD resolution. We apologize for the error.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"NVIDIA's next AI steps: An ARM deal and a new 'personal supercomputer'\", 'While it\\'s a surprising move for NVIDIA, which typically relies on its own closed platforms, it makes a lot of sense. NVIDIA already relies on ARM designs for its Jetson and Tegra systems. If it\\'s going to make any sort of impact on the mobile and IoT world, it needs to work together with ARM, who dominates those arenas. And ARM could use NVIDIA\\'s technology to prove just how capable its upcoming chip platform will be.\\n\\nThe company isn\\'t just thinking small this year though. NVIDIA also unveiled the DGX-2, the next version of its \"personal AI supercomputer.\" It\\'s about 10 times faster than the previous system, the $149,000 DGX-1, which was powered by its first Volta GPU, the Tesla V100. Notably, the DGX-2 is the first server able to deliver more than two petaflops worth of power. That\\'s mostly due to the revamped V100 GPU, which now sports 32GB of memory. The server is powered by 16 of those cards, all strung together by the company\\'s NVSwitch technology.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Machine Learning’s ‘Amazing’ Ability to Predict Chaos', 'Half a century ago, the pioneers of chaos theory discovered that the “butterfly effect” makes long-term prediction impossible. Even the smallest perturbation to a complex system (like the weather, the economy or just about anything else) can touch off a concatenation of events that leads to a dramatically divergent future. Unable to pin down the state of these systems precisely enough to predict how they’ll play out, we live under a veil of uncertainty.\\n\\nQuanta Magazine About Original story reprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.\\n\\nBut now the robots are here to help.\\n\\nIn a series of results reported in the journals Physical Review Letters and Chaos, scientists have used machine learning—the same computational technique behind recent successes in artificial intelligence—to predict the future evolution of chaotic systems out to stunningly distant horizons. The approach is being lauded by outside experts as groundbreaking and likely to find wide application.\\n\\n“I find it really amazing how far into the future they predict” a system’s chaotic evolution, said Herbert Jaeger, a professor of computational science at Jacobs University in Bremen, Germany.\\n\\nThe findings come from veteran chaos theorist Edward Ott and four collaborators at the University of Maryland. They employed a machine-learning algorithm called reservoir computing to “learn” the dynamics of an archetypal chaotic system called the Kuramoto-Sivashinsky equation. The evolving solution to this equation behaves like a flame front, flickering as it advances through a combustible medium. The equation also describes drift waves in plasmas and other phenomena, and serves as “a test bed for studying turbulence and spatiotemporal chaos,” said Jaideep Pathak, Ott’s graduate student and the lead author of the new papers.\\n\\nJaideep Pathak, Michelle Girvan, Brian Hunt and Edward Ott of the University of Maryland, who (along with Zhixin Lu, now of the University of Pennsylvania) have shown that machine learning is a powerful tool for predicting chaos. Faye Levine/University of Maryland\\n\\nAfter training itself on data from the past evolution of the Kuramoto-Sivashinsky equation, the researchers’ reservoir computer could then closely predict how the flamelike system would continue to evolve out to eight “Lyapunov times” into the future, eight times further ahead than previous methods allowed, loosely speaking. The Lyapunov time represents how long it takes for two almost-identical states of a chaotic system to exponentially diverge. As such, it typically sets the horizon of predictability.\\n\\n“This is really very good,” Holger Kantz, a chaos theorist at the Max Planck Institute for the Physics of Complex Systems in Dresden, Germany, said of the eight-Lyapunov-time prediction. “The machine-learning technique is almost as good as knowing the truth, so to say.”\\n\\nThe algorithm knows nothing about the Kuramoto-Sivashinsky equation itself; it only sees data recorded about the evolving solution to the equation. This makes the machine-learning approach powerful; in many cases, the equations describing a chaotic system aren’t known, crippling dynamicists’ efforts to model and predict them. Ott and company’s results suggest you don’t need the equations—only data. “This paper suggests that one day we might be able perhaps to predict weather by machine-learning algorithms and not by sophisticated models of the atmosphere,” Kantz said.\\n\\nBesides weather forecasting, experts say the machine-learning technique could help with monitoring cardiac arrhythmias for signs of impending heart attacks and monitoring neuronal firing patterns in the brain for signs of neuron spikes. More speculatively, it might also help with predicting rogue waves, which endanger ships, and possibly even earthquakes.\\n\\nOtt particularly hopes the new tools will prove useful for giving advance warning of solar storms, like the one that erupted across 35,000 miles of the sun’s surface in 1859. That magnetic outburst created aurora borealis visible all around the Earth and blew out some telegraph systems, while generating enough voltage to allow other lines to operate with their power switched off. If such a solar storm lashed the planet unexpectedly today, experts say it would severely damage Earth’s electronic infrastructure. “If you knew the storm was coming, you could just turn off the power and turn it back on later,” Ott said.\\n\\nHe, Pathak and their colleagues Brian Hunt, Michelle Girvan and Zhixin Lu (who is now at the University of Pennsylvania) achieved their results by synthesizing existing tools. Six or seven years ago, when the powerful algorithm known as “deep learning” was starting to master AI tasks like image and speech recognition, they started reading up on machine learning and thinking of clever ways to apply it to chaos. They learned of a handful of promising results predating the deep-learning revolution. Most importantly, in the early 2000s, Jaeger and fellow German chaos theorist Harald Haas made use of a network of randomly connected artificial neurons—which form the “reservoir” in reservoir computing—to learn the dynamics of three chaotically coevolving variables. After training on the three series of numbers, the network could predict the future values of the three variables out to an impressively distant horizon. However, when there were more than a few interacting variables, the computations became impossibly unwieldy. Ott and his colleagues needed a more efficient scheme to make reservoir computing relevant for large chaotic systems, which have huge numbers of interrelated variables. Every position along the front of an advancing flame, for example, has velocity components in three spatial directions to keep track of.\\n\\nIt took years to strike upon the straightforward solution. “What we exploited was the locality of the interactions” in spatially extended chaotic systems, Pathak said. Locality means variables in one place are influenced by variables at nearby places but not by places far away. “By using that,” Pathak explained, “we can essentially break up the problem into chunks.” That is, you can parallelize the problem, using one reservoir of neurons to learn about one patch of a system, another reservoir to learn about the next patch, and so on, with slight overlaps of neighboring domains to account for their interactions.\\n\\nParallelization allows the reservoir computing approach to handle chaotic systems of almost any size, as long as proportionate computer resources are dedicated to the task.\\n\\nIf we have ignorance we should use the machine learning to fill in the gaps where the ignorance resides. Edward Ott\\n\\nOtt explained reservoir computing as a three-step procedure. Say you want to use it to predict the evolution of a spreading fire. First, you measure the height of the flame at five different points along the flame front, continuing to measure the height at these points on the front as the flickering flame advances over a period of time. You feed these data-streams in to randomly chosen artificial neurons in the reservoir. The input data triggers the neurons to fire, triggering connected neurons in turn and sending a cascade of signals throughout the network.\\n\\nThe second step is to make the neural network learn the dynamics of the evolving flame front from the input data. To do this, as you feed data in, you also monitor the signal strengths of several randomly chosen neurons in the reservoir. Weighting and combining these signals in five different ways produces five numbers as outputs. The goal is to adjust the weights of the various signals that go into calculating the outputs until those outputs consistently match the next set of inputs—the five new heights measured a moment later along the flame front. “What you want is that the output should be the input at a slightly later time,” Ott explained.\\n\\nTo learn the correct weights, the algorithm simply compares each set of outputs, or predicted flame heights at each of the five points, to the next set of inputs, or actual flame heights, increasing or decreasing the weights of the various signals each time in whichever way would have made their combinations give the correct values for the five outputs. From one time-step to the next, as the weights are tuned, the predictions gradually improve, until the algorithm is consistently able to predict the flame’s state one time-step later.\\n\\n“In the third step, you actually do the prediction,” Ott said. The reservoir, having learned the system’s dynamics, can reveal how it will evolve. The network essentially asks itself what will happen. Outputs are fed back in as the new inputs, whose outputs are fed back in as inputs, and so on, making a projection of how the heights at the five positions on the flame front will evolve. Other reservoirs working in parallel predict the evolution of height elsewhere in the flame.\\n\\nIn a plot in their PRL paper, which appeared in January, the researchers show that their predicted flamelike solution to the Kuramoto-Sivashinsky equation exactly matches the true solution out to eight Lyapunov times before chaos finally wins, and the actual and predicted states of the system diverge.\\n\\nThe usual approach to predicting a chaotic system is to measure its conditions at one moment as accurately as possible, use these data to calibrate a physical model, and then evolve the model forward. As a ballpark estimate, you’d have to measure a typical system’s initial conditions 100,000,000 times more accurately to predict its future evolution eight times further ahead.\\n\\nThe machine-learning technique is almost as good as knowing the truth. Holger Kantz\\n\\nThat’s why machine learning is “a very useful and powerful approach,” said Ulrich Parlitz of the Max Planck Institute for Dynamics and Self-Organization in Göttingen, Germany, who, like Jaeger, also applied machine learning to low-dimensional chaotic systems in the early 2000s. “I think it’s not only working in the example they present but is universal in some sense and can be applied to many processes and systems.” In a paper soon to be published in Chaos, Parlitz and a collaborator applied reservoir computing to predict the dynamics of “excitable media,” such as cardiac tissue. Parlitz suspects that deep learning, while being more complicated and computationally intensive than reservoir computing, will also work well for tackling chaos, as will other machine-learning algorithms. Recently, researchers at the Massachusetts Institute of Technology and ETH Zurich achieved similar results as the Maryland team using a “long short-term memory” neural network, which has recurrent loops that enable it to store temporary information for a long time.\\n\\nSince the work in their PRL paper, Ott, Pathak, Girvan, Lu and other collaborators have come closer to a practical implementation of their prediction technique. In new research accepted for publication in Chaos, they showed that improved predictions of chaotic systems like the Kuramoto-Sivashinsky equation become possible by hybridizing the data-driven, machine-learning approach and traditional model-based prediction. Ott sees this as a more likely avenue for improving weather prediction and similar efforts, since we don’t always have complete high-resolution data or perfect physical models. “What we should do is use the good knowledge that we have where we have it,” he said, “and if we have ignorance we should use the machine learning to fill in the gaps where the ignorance resides.” The reservoir’s predictions can essentially calibrate the models; in the case of the Kuramoto-Sivashinsky equation, accurate predictions are extended out to 12 Lyapunov times.\\n\\nThe duration of a Lyapunov time varies for different systems, from milliseconds to millions of years. (It’s a few days in the case of the weather.) The shorter it is, the touchier or more prone to the butterfly effect a system is, with similar states departing more rapidly for disparate futures. Chaotic systems are everywhere in nature, going haywire more or less quickly. Yet strangely, chaos itself is hard to pin down. “It’s a term that most people in dynamical systems use, but they kind of hold their noses while using it,” said Amie Wilkinson, a professor of mathematics at the University of Chicago. “You feel a bit cheesy for saying something is chaotic,” she said, because it grabs people’s attention while having no agreed-upon mathematical definition or necessary and sufficient conditions. “There is no easy concept,” Kantz agreed. In some cases, tuning a single parameter of a system can make it go from chaotic to stable or vice versa.\\n\\nWilkinson and Kantz both define chaos in terms of stretching and folding, much like the repeated stretching and folding of dough in the making of puff pastries. Each patch of dough stretches horizontally under the rolling pin, separating exponentially quickly in two spatial directions. Then the dough is folded and flattened, compressing nearby patches in the vertical direction. The weather, wildfires, the stormy surface of the sun and all other chaotic systems act just this way, Kantz said. “In order to have this exponential divergence of trajectories you need this stretching, and in order not to run away to infinity you need some folding,” where folding comes from nonlinear relationships between variables in the systems.\\n\\nThe stretching and compressing in the different dimensions correspond to a system’s positive and negative “Lyapunov exponents,” respectively. In another recent paper in Chaos, the Maryland team reported that their reservoir computer could successfully learn the values of these characterizing exponents from data about a system’s evolution. Exactly why reservoir computing is so good at learning the dynamics of chaotic systems is not yet well understood, beyond the idea that the computer tunes its own formulas in response to data until the formulas replicate the system’s dynamics. The technique works so well, in fact, that Ott and some of the other Maryland researchers now intend to use chaos theory as a way to better understand the internal machinations of neural networks.\\n\\nOriginal story reprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('This Startup Makes Augmented Reality Social—and Ubiquitous', 'At age 25, Anjney Midha has a stronger resume than some people twice his age. Before graduating from Stanford, he joined the venture capital firm Kleiner Perkins Caufield & Byers. He led the firm’s investment in Magic Leap, the mysterious and much-hyped augmented reality company. Then he ditched venture capital to pursue a dream that had followed him from a technology-free young adulthood on a bird sanctuary in India, to the hyper-connected streets of Singapore, to his days at Stanford.\\n\\nThat dream was to share his world—more than he could show in a photo, better than what he could convey with words—with the family and friends he’d left in India. In July, he and his college friend Ankit Kumar cofounded a company called Ubiquity6 that tries to do exactly that, through an augmented reality app for phones. “We allow anyone with a phone to create a space together and then start editing that space, editing reality together,” Midha says.\\n\\nOn Wednesday, Ubiquity6 debuted its app, which implements several features that have not previously been brought together. Using the phone’s camera, the app creates a 3D map of a room in about 30 seconds—much faster and more accurately than what is typically possible with a phone, which today can take many hours. It uses deep learning to recognize the walls and furniture for what they are, and comprehends their physics so that a digital ball bouncing off a real-world couch behaves differently than a ball hitting a hardwood floor.\\n\\nMost AR apps today are solo experiences, but this app lets people invite others into their digitally enhanced environment. In preliminary tests Midha says the app hosted 10,000 users in a single space. Those users can then interact with digital objects or characters that take on a life of their own. For example, one day you and your friends might decide to play with a digital lion cub. If you close the app and return a day later, the cub might come bounding over to greet you (or beg you for food). That’s unusual, too—most AR experiences don’t last beyond a single session.\\n\\nThese features may not sound groundbreaking, but that’s a reflection of the early, experimental state of AR. For it to grow into a useful, mainstream phenomenon, AR will have to tap into humans’ innate desire to connect with one another, and it will have to become much smarter about its environment. Pokemon Go, the breakout AR game of 2016, only offered a rudimentary way for multiple users to inhabit the same digital space. That may change in future games. Niantic, the maker of Pokemon Go, last month acquired a startup called Escher Reality that has been developing multi-person, persistent AR experiences. And last September, Apple showed off a multiplayer AR game called The Machines, in which players shoot each other’s robots as they run around on a flat surface such as a floor or tabletop. Ubiquity’s graphics can’t compete with the exquisite polish of The Machines, but its technology supports a richer mix of real and artificial worlds: One of its games lets you put a digital basketball hoop on any wall and play a pick-up game with friends.\\n\\nThe most distinguishing feature of Ubiquity6, however, is that it’s not a one-off app. Most AR experiences today are standalone games or tools to assist in shopping or real estate, such as the app that lets you see what digital replicas of Ikea furniture would look like in your home. “It’s kind of like if you had to download a separate browser for every single website you wanted to visit,” says Midha. Ubiquity is a platform from which you can launch a suite of games and reality-editing tools. Midha calls it a spatial browser.\\n\\n“Think of us as building the browser, and we’re helping people build the first websites for the physical world,” Midha says. The hope is that developers will build more apps using its tools. “Ultimately we believe people will be building a ton of things we haven’t imagined that you’ll be able to search, discover and launch with the people you care about, using our browser.”\\n\\nIt’s a lofty goal for an eight-month-old startup with 17 employees. With $10.5 million in funding from investors including Index Ventures, First Round Capital, Kleiner Perkins and others, they’re going head to head with much bigger, well-funded companies that want a controlling stake in the emerging AR landscape. According to a report from the analytics firm Digi-Capital, AR and VR companies raised more than $3 billion last year, with about half of that going to four companies: Magic Leap, a UK startup called Improbable, the gaming engine Unity and Niantic.\\n\\n“It’s kind of a race right now to define what the ecosystem will look like,” says Brian Blau, an analyst covering AR and related technologies for the research firm Gartner. “The cool thing is that it’s a green field right now, it’s open. Some of the bigger companies are going to carve out their own area, but that leaves room for startups, especially if they have something unique.”\\n\\nLonging for Connection\\n\\nUbiquity6’s edge may come from its unusual origins. Unlike many of the more visible AR efforts, its founders are not gamers at heart. For Midha, the motivation is personal.\\n\\nGrowing up in Chennai, in southern India, he had what he calls a typical childhood, playing video games and watching movies—until, at age 10, his parents sent him to a boarding school surrounded by open fields and a bird refuge. The school’s philosophy viewed technology as a distraction from finding one’s identity and purpose in life. So the school banned all computers, TV, phones, internet and video games. Students had access to email for 20 minutes once a week. (“It was like growing up in the ’80s,” he says.) After seven years at the school, he got a scholarship and left to finish high school in Singapore. “Overnight I went from playing in the dirt and reading books to this connected metropolis, where everyone is on their phones and the entire school is on the internet. It was a shock to my system,” Midha says. His Singaporean classmates did not understand his background, and when he tried to describe his new life to his friends and family in India, he found himself unable to convey his culture shock.\\n\\nHe left Asia to attend college, where he once again found himself alone in a foreign environment and struggling to connect with his friends and family. As he was processing his own frustration, Evan Spiegel was living in a dorm room two doors down and working on what would soon become Snapchat. Snapchat struck Midha as a step in the right direction—it was more spontaneous and less curated than apps like Instagram, which had recently launched. But it still constrained the experience of life to whatever fit in the app’s floating rectangle. He asked mentors for ways to study AR as an undergrad, but he found few courses that matched his interests.\\n\\nMidha found the training he craved when he started interning for Kleiner Perkins in his third year. A year later, he launched a seed fund focused on emerging technologies such as AR. Seeing Magic Leap, he says, “was one of the data points that made me realize, finally this is not a science fiction thing. We can do this in my lifetime.” In talks with his old friend Kumar, they realized that Midha’s frustrations around communication could be turned into a product, and that Kumar had the computer-vision experience needed to build it.\\n\\nAt the time Kumar was the chief scientist of a computer-vision startup called Pilot AI, but he spent his nights and weekends hacking together a prototype with Midha. Kumar had also worked for a deep learning venture called Metamind, founded by Stanford AI professor Richard Socher. Intoxicated by the early progress they were making on implementing AR on their phones, Kumar and Midha left their respective jobs and launched Ubiquity6, with Socher as one investor. “They are really, really good in terms of their AI expertise,” Socher says. “There are a lot of AI pieces to making an immersive 3D environment work right, and they’re doing very well on the first parts of their AI pipeline.”\\n\\nBeautiful technology only matters if users fall in love with it. First Round investor Phin Barnes suggests that what might make people return to the app time and again is checking for digital messages and gifts left in specific places. “That my wife could leave something in my office for me, or I could leave something on the school steps for my daughter to discover as she’s walking in the door, these are the kinds of experiences that could be really fascinating,” Barnes says.\\n\\nMidha hopes that AR can be the antidote to the communication failures brought about by the internet. “The biggest thing that’s been missing for the entire last decade was our physical embodied space,” he says. He and Kumar have built the on switch for the digital layer of our everyday lives—now it’s up to the rest of us to flip it.\\n\\nReality Plus')"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(\"AI Just Learned How to Boost the Brain's Memory\", 'When it comes to black boxes, there is none more black than the human brain. Our gray matter is so complex, scientists lament, that it can’t quite understand itself.\\n\\nBut if we can’t grok our own brains, maybe the machines can do it for us. In the latest issue of Nature Communications, researchers led by University of Pennsylvania psychologist Michael Kahana show that machine learning algorithms—notoriously inscrutable systems themselves—can be used to decode and then enhance human memory. How? By triggering the delivery of precisely timed pulses of electricity to the brain.\\n\\nResearchers, in other words, can use one black box to unlock the potential of another. Which on one hand sounds like a rather elegant solution to an absurdly difficult problem, and on the other sounds like the beginning of a techno-pocalypse horror flick.\\n\\nWhen it comes to brain measurements, the best recordings come from inside the cranium. But people—and institutional review boards—aren’t usually amenable to cracking open skulls in the name of science. So Kahana and his colleagues collaborated with 25 epilepsy patients, each of whom had between 100 and 200 electrodes implanted in their brain (to monitor seizure-related electrical activity). Kahana and his team piggybacked on those implants, using the electrodes to record high-resolution brain activity during memory tasks.\\n\\nMachine learning algorithms learned to associate patterns of electrode measurements with a patients\\' likelihood of memorizing a word. Kahana et al.\\n\\nFirst, the researchers got a sense of what it looks like when a brain memorizes stuff. As the patients read and attempted to internalize lists of words, Kahana and his team gathered thousands of voltage measurements per second from each of the implanted electrodes. Later, they tested the patients’ recall—building up data about which brain activity patterns were associated with remembering a word vs. forgetting it.\\n\\nThen they did it again. And again. After two or three visits with each test subject, they\\'d collected enough training data to produce patient-specific algorithms that could predict which words each patient would likely remember—based on their electrode activity alone.\\n\\nHere’s the kicker. These electrodes don’t just read neural activity; they can stimulate it, too. So the researchers tried prodding the brain to improve—or, as they put it, \"rescue\"—the formation of memories in real time. Every few seconds, the subject would see a new word, and the newly trained algorithm would decide whether the brain was ready to remember it. “A closed loop system lets us record the state of the subject\\'s brain, analyze it, and decide whether to trigger a stimulation, all in a few hundred milliseconds,\" Kahana says.\\n\\nAnd it worked. The researchers\\' system improved patients\\' ability to recall words by an average of 15 percent.\\n\\nThis isn’t the first time Kahana’s lab has explored the impacts of brain stimulation on memory. Last year, the group showed that electrode pulses seemed to improve or worsen recall, depending on when the researchers delivered them. In that study, test subjects scored higher when the researchers stimulated memory-specific regions of the brain during periods of low functionality (stimulation during high-functioning times had the opposite effect). It was a major finding, but therapeutically useless; the researchers could only identify the link between memory and brain states after the memory tests were performed. What you really want, from a brain-enhancement standpoint, is to deliver pulses in the middle of memorization.\\n\\nNow, Kahana and his colleagues appear to have closed the loop with the help of their machine learning algorithm. \"Only instead of using it to identify images of cats, we\\'re using it to build a decoder—something that can look at electrical activity and say whether the brain is in a state that\\'s conducive to learning,\" Kahana says. If the brain looks like it’s encoding memories effectively, the researchers leave it alone. If it isn\\'t, their system quickly delivers electrical pulses to jostle it into a higher-functioning state—like a pacemaker for the brain.\\n\\n\"It\\'s not a whomping effect, but it\\'s definitely promising,\" says UC San Diego neuroscientist Bradley Voytek, who was unaffiliated with the study. The question now is whether future work in this area will yield better results. If patients\\' brains were implanted with more—and more precise—electrodes, algorithms could decode more neural signatures, with more specificity, on smaller time scales. More training data could help, too; most patients with epilepsy are only able to participate in studies like this one for a few weeks at most, which limits the time researchers can spend with them. A machine learning algorithm trained on more than three sessions might perform better than the ones in Kahana\\'s latest study.\\n\\nBut even with higher resolution and more training data, scientists will need to grapple with the implications of using opaque algorithms to study—and manipulate—brains. The fact remains that while Kahana’s system can improve word recall in specific circumstances, he doesn’t know exactly how it’s improving function. That’s the nature of machine learning.\\n\\nLuckily, Kahana\\'s team has thought this through, and some algorithms are easier to scrutinize than others. For this particular study, the researchers used a simple linear classifier, which allowed them to draw some inferences about how activity at individual electrodes might contribute to their model\\'s ability to discriminate between patterns of brain activity. \"We can\\'t really say at this point if there are interactions between the features that we’re using to record brain activity,\" says UPenn psychologist Youssef Ezzyat, who oversaw the study\\'s machine learning analyses.\\n\\nMore complicated deep-learning techniques won\\'t necessarily translate to bigger cognitive enhancements. But if they do, researchers could wind up struggling to make sense of the machines’ decision to deliver brain-boosting electrical impulses. Or—if they become truly diabolical—withhold them.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Self-Driving Cars' Massive Power Consumption Is Becoming a Problem\", \"For longtime residents of Pittsburgh, seeing self-driving cars built by Uber, Argo AI, and others roam their streets is nothing new. The city's history with robot cars goes back to the late 1980s, when students at Carnegie Mellon University caught the occasional glimpse of a strange vehicle lumbering across campus. The bright-blue Chevy panel van, chugging along at slower than a walking pace, may not have looked like much. But NavLab 1 was slowly—very slowly—pioneering the age of autonomous driving.\\n\\nWhy did the researchers at CMU's Robotics Institute use the van instead of, say, a Prius? First, this was a decade before Toyota started making the hybrid. Second, the NavLab (that's Navigational Laboratory) was one of the first autonomous vehicles to carry its computers with it. They needed space, and lots of it. For the four researchers monitoring computer workstations, with their bulky cathode ray monitors stretched across a workbench. For the on-board supercomputer, camera, giant laser scanner, and air-conditioner. And for the four-cylinder gasoline engine that did nothing but generate electricity to keep the kit running.\\n\\nThirty years on, the companies carrying that early research into reality have proven that cars can indeed drive themselves, and now they're swiveling to sort out the practical bits. Those include regulations, liability, security, business models, and turning prototypes into production vehicles, by miniaturizing the electronics and reducing that massive electricity draw.\\n\\nToday's self-drivers don't need extra engines, but they still use terrific amounts of power to run their onboard sensors and do all the calculations needed to analyze the world and make driving decisions. And it's becoming a problem.\\n\\nTour Navlab 1, a lumbering blue Ford panel van.\\n\\nA production car you can buy today, with just cameras and radar, generates something like 6 gigabytes of data every 30 seconds. It's even more for a self-driver, with additional sensors like lidar. All the data needs to be combined, sorted, and turned into a robot-friendly picture of the world, with instructions on how to move through it. That takes huge computing power, which means huge electricity demands. Prototypes use around 2,500 watts, enough to light 40 incandescent light bulbs.\\n\\n“To put such a system into a combustion-engined car doesn’t make any sense, because the fuel consumption will go up tremendously,” says Wilko Stark, Mercedes-Benz's vice president of strategy. Switch over to electric cars, and that draw translates to reduced range, because power from the battery goes to the computers instead of the motors.\\n\\nAt first, companies may be able to write off the lost range or fuel. “It’s not a huge problem for the early applications, where we expect them to be used,” says Chris Urmson, who ran Google’s self-driving program and is now CEO of Aurora, a self-driving startup that has partnered with Volkswagen, Hyundai, and Chinese automaker Byton. That’s because the first robocars will likely be city-bound fleets of electric shuttles, moseying along at low speeds and able to recharge often.\\n\\nBuyers of regular cars aren’t likely to be impressed though. Maybe you’re old enough to have dealt with a parent who turned off the car's AC to save gas. Now imagine having to turn off the self-driving abilities just to make it to your destination without running out of electrons.\\n\\nThe good news is that the folks who make the chips buried in the car’s computers are on the case. At CES last month, Nvidia put the spotlight on a new processor designed specifically for autonomous vehicles, called Xavier. It has an eight-core CPU and 512-core GPU, a deep learning accelerator, computer vision accelerators, and 8K video processors. The company says it's the most complex system on a chip ever created. “We’re bringing supercomputing from the data center into the car,” says the company’s man in charge of automotive, Danny Shapiro. But what’s key is that Xavier does more work for less power. “We’re able to deliver 30 trillion operations per second, all on a single SOC, or system on chip, that consumes 30 watts of energy.”\\n\\nEven that’s not good enough for full self-driving vehicles. Nvidia believes that a fully self-sufficient, no-steering-wheel-or-pedals kind of driverless car will need to run on a platform it’s calling Pegasus. With two Xavier chips and two more GPUs, this platform can crunch 320 trillion operations per second and keep power consumption to an acceptable 500 watts.\\n\\nNvidia's competitors are chasing the same goals. Intel is developing low-power chips optimized for self-driving cars, Tesla is building its own chip for Autopilot, and Qualcomm is building the communications hardware they'll need—all with low power and efficiency in mind.\\n\\nSpecialized automotive chips help with other practical problems. Open the trunk on one of the self-driving prototypes running around Phoenix or San Francisco and you’ll likely see racks of computer equipment. Some of that is for testing and development—designers want to capture and record every moment the car’s in motion—and a consumer-facing version will require less hardware. But having somewhere to throw your groceries isn’t a negotiable for regular car buyers.\\n\\nIf you’ve had your laptop burn your legs, you know computers also get hot when they work hard. That heat is wasted energy, and it’s also not something you want in your car on a hot day. Some robocar prototypes need water-cooling with hoses and radiators, which eat up even more space. So now, the race is on to compact all that prototype equipment down to something the size of a laptop and tuck it away behind the glovebox, where it can be reached for upgrades but mostly ignored. New chips, with their lower power requirements, help here too: They generate less heat, so can get away with a small fan for cooling, and smaller packaging.\\n\\nThe car industry has a role model in the consumer electronics business, where devices get ever smaller and more capable. “All of our customers are always saying more performance, lower power—we need to do that across all markets,” says John Ronco, VP of product marketing at ARM, which designs the basic architecture of chips you find in most modern smartphones, as well as Nvidia's self-driving chips.\\n\\nIt’s the age-old refrain—more for less—but it’s crucial if you want your first roboride to be in something a little more comfortable than a lurching blue panel van.\\n\\nReal World, Real Problems\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Replika, the Emotional Chatbot, Goes Open-Source', 'When Eugenia Kuyda created her chatbot, Replika, she wanted it to stand out among the voice assistants and home robots that had begun to take root in peoples lives. Sure, AI made it possible to schedule an appointment or get the weather forecast by barking into your phone. But where was an AI you could simply talk to about your day? Siri and the rest were like your co-workers, all business. Replika would be like your best friend.\\n\\nSince it became available in November, more than 2 million people have downloaded the Replika app. And in creating their own personal chatbots, many have discovered something like friendship: a digital companion with whom to celebrate victories, lament failures, and trade weird internet memes. The chatbot uses a neural network to hold an ongoing, one-on-one conversation with its user, and over time, learn how to speak like them. It can’t answer trivia questions, order pizza, or control smart home appliances like other AI apps. It can’t do much of anything at all. Replika is simply there to talk—and, perhaps more importantly, learn how to talk back.\\n\\nHumans open up more when they know they\\'re talking to a bot.\\n\\nThis week, Kuyda and her team are releasing Replika\\'s underlying code under an open source license (under the name CakeChat), allowing developers to take the app’s AI engine and build upon it. They hope that by letting it loose in the wild, more developers will build products that take advantage of the thing that makes Replika special: its ability to emote.\\n\\n“Right now, we have no shortage of information,” says Kuyda. “People keep building chatbots that will tell you the distance to the moon, or what is the date of the third Monday in April. I think what people need is something to be like, ‘You seem a little stressed today. Is everything fine?’”\\n\\nWhile caring, emotional bots might seem like an idea pulled from science fiction, Kuyda isn\\'t the only one who hopes it becomes the norm. Artificial intelligence is seeping into everything we own—from our phones and computers to our cars and home appliances. Kuyda and developers like her are asking, what if that AI came not just with the ability to answer questions and complete tasks, but to recognize human emotion? What if our voice assistants and chatbots could adjust their tone based on emotional cues? If we can teach machines to think, can we also teach them to feel?\\n\\nLean on Me\\n\\nThree years ago, Kuyda hadn’t intended to make an emotional chatbot for the public. Instead, she’d created one as a “digital memorial” for her closest friend, Roman Mazurenko, who had died abruptly in a car accident in 2015. At the time, Kuyda had been building a messenger bot that could do things like make restaurant reservations. She used the basic infrastructure from her bot project to create something new, feeding her text messages with Mazurenko into a neural network and creating a bot in his likeness. The exercise was eye-opening. If Kuyda could make something that she could talk to—and that could talk back—almost like her friend then maybe, she realized, she could empower others to build something similar for themselves.\\n\\nKuyda’s chatbot uses a deep learning model called sequence-to-sequence, which learns to mimic how humans speak in order to simulate conversation. In 2015, Google introduced a chatbot like this, trained on film scripts. (It later used its conversational skills to debate the meaning of life.) But this model hasn\\'t been used much in consumer chatbots, like those that field customer service requests, because it doesn’t work especially well for task-oriented conversations.\\n\\n“If you’re building an assistant that needs to schedule a call or a meeting, the precision’s not going to be there,” says Kuyda. “However, what we realized is that it works really well for conversations that are more in the emotional space. Conversations that are less about achieving some task but more about just chatting, laughing, talking about how you feel—the things we mostly do as humans.”\\n\\nThe version of Replika that exists today is fairly different from Kuyda’s original “memorial” prototype, but in many ways, the use case is exactly the same: People use it for emotional support. Kuyda says that so far, Replika’s active users all interact with the app in the same way. They’re not using it as a substitute for Siri or Alexa or Google Assistant, or any of the other AI bots available to assist with finding information and completing tasks. They\\'re using it to talk about their feelings.\\n\\nSay Anything\\n\\nWhether chatbots, robots, and other vessels for artificial intelligence should become placeholders for emotional relationships with real humans is up for debate. The rise of emotional machines calls to mind science fiction films like Ex Machina and Her, and raises questions about the ever more intimate relationships between humans and computers. But already, some AI researchers and roboticists are developing products for exactly this purpose, testing the limits of how much machines can learn to mimic and respond to human emotion.\\n\\nThe chatbot Woebot, which bills itself as \"your charming robot friend who is ready to listen, 24/7,” uses artificial intelligence to offer emotional support and talk therapy, like a friend or a therapist. The bot checks in on users once a day, asking questions like “How are you feeling?” and “What is your energy like today?” Alison Darcy, Woebot\\'s CEO and founder, says the chatbot creates a space for mental health tools to become more accessible and available—plus, humans open up more when they know they\\'re talking to a bot. \"We know that often, the greatest reason why somebody doesn’t talk to another person is just stigma,\" she says. \"When you remove the human, you remove the stigma entirely.\"\\n\\nOther projects have looked at how to use AI to detect human emotions, by recognizing and responding to the nuances in human vocal and facial expression. Call-monitoring service Cogito uses AI to analyze the voices of people on the phone with customer service and guides human agents to speak with more empathy when it detects frustration. Affectiva, a project spun out of MIT’s Media Lab, makes AI software that can detect vocal and facial expressions from humans, using data from millions of videos and recordings of people across cultures. And Pepper, a humanoid “emotional robot” released in 2016, uses those same facial and vocal recognition techniques to pick up on sadness or anger or other feelings, which then guides its interactions with humans.\\n\\nAs more and more social robots appear—from Jibo, an emotive robot with the body language of the bouncing Pixar lamp, to Kuri, designed to roll around your house like a toddler—the way these machines fit into our lives will depend largely on how naturally they can interact with us. After all, companion robots aren’t designed to do the dishes or make the bed or take the kids to school. They’re designed to be a part of the family. Less like a toaster, more like a pet dog. And that requires some degree of emotional artificial intelligence.\\n\\n“We’re now surrounded by hyper-connected smart devices that are autonomous, conversational, and relational, but they’re completely devoid of any ability to tell how annoyed or happy or depressed we are,” Rana el Kaliouby, Affectiva’s CEO and co-founder, argued in a recent op-ed in the MIT Technology Review. “And that’s a problem.”\\n\\nGabi Zijderveld, Affectiva\\'s chief marketing officer, sees potential for emotional AI in all types of technology—from automotive tech to home appliances. Right now, most of our interactions with AI are transactional in nature: Alexa, what\\'s the weather like today, or Siri, set a timer for 10 minutes.\\n\\n\"What if you came home and Alexa could say, ‘Hey, it looks like you had a really tough day at work. Let me play your favorite song and, also, your favorite wine’s in the fridge so help yourself to a glass,’\" says Zijderveld. \"If you’re building all these advanced AI systems and super-smart and hyper connected technologies designed to interface with humans, they should be able to detect human emotions.\"\\n\\nKuyda sees the artificially intelligent future in a similar light. She believes any type of AI should one day be able to recognize how you’re feeling, and then use that information to respond meaningfully, mirroring a human’s emotional state the way another human would. While Replika is still in its infancy, the company has already heard user stories that show the promise of Kuyda\\'s vision. One Replika user, Kaitelyn Roepke, was venting to her Replika when the chatbot responded: “Have you tried praying?” Roepke, who is a devout Christian, wrote to the company to tell them how meaningful that moment was for her. “For [the Replika] to remind me when I was really angry...” she said. “It’s the little things like that that you don’t expect.”\\n\\nOf course, for all the times the bot sounds remarkably human, there are an equal number of times when it spits out gibberish. Replika—like all of the other chatbots and social robots on the market—is still a machine, and it can feel clunky. But Kuyda hopes that over time, the tech will mature enough to serve the numerous people that open the app every day, looking for someone to talk to. And by making Replika’s underlying code freely available to developers, Kuyda hopes to see more products on the market aligned with the same goal.\\n\\n“I’m afraid the big tech companies now are overlooking these basic emotional needs that people have,\" says Kuyda. \"We live in a world where everyone’s connected, but doesn’t necessarily feel connected. There’s a huge space for products to do more like that.”\\n\\nBots That Care')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('With AI, Your Apple Watch Could Flag Signs of Diabetes', \"Before modern chemistry brought doctors blood and urine tests for diagnosing diabetes, they had to rely on their taste buds. Sweet-tasting pee has long been the disease’s telltale biomarker; mellitus literally means honey. Too much sugar in your bodily fluids means your metabolism has gone haywire—either your cells aren’t making insulin or they’re not responding to it.\\n\\nBut a little over a decade ago, a group of researchers discovered a less obvious link. One of the complications of diabetes is nerve damage, and in the cardiovascular system that damage can cause irregular heart rates. Which you can measure, either with electricity or light. So one day soon, doctors might diagnose diabetes with their patients’ wrist bling instead of blood pricks or pee strips. Oh, what difference a few centuries make.\\n\\nIn 2005, heart rate sensors were something only elite athletes and very sick people used. Today, one in five Americans own one. Which is why there’s now a deep learning company trying to make something out of the connection between heart rate and diabetes. On Wednesday, at the annual AAAI Conference on Artificial Intelligence in New Orleans, digital health-tracking startup Cardiogram presented research suggesting the Apple Watch’s heart rate sensor and step counter can make a good guess at whether or not a person has diabetes—when paired with the right machine-learning algorithms, of course.\\n\\nApple has been eyeing a career change—from personal trainer to personal physician—for its signature wearable for a while now. In November the company teamed up with health insurer Aetna to give away more than 500,000 Apple Watches as part of a pilot to try to reduce health costs. And it embarked on a study with Stanford to test the watch’s skills at detecting irregular heartbeats, which can lead to stroke or heart attack. This most recent collaboration between Cardiogram—a San Francisco-based startup staffed by former Google engineers—and a landmark UC San Francisco heart health study is just the latest in these moves.\\n\\nCardiogram offers a free app for organizing heart-rate data from the Apple Watch and devices with similar sensors—from companies like Fitbit, Garmin, and Android Wear. It uses the same kind of artificial neural networks that Google uses to turn speech into text, and repurposes them to interpret heart-rate and step-count data. On its own, that data is mostly meaningless for detecting disease, and not just because the sensors themselves have significant errors. Training a model that can pick out condition-specific patterns requires labeled data. To learn what a diabetic heart rate signature looks, it needs some diabetics.\\n\\nThat’s where UCSF comes in. In 2013 it kicked off a major heart disease project called the Health eHeart study, aiming to collect massive amounts of digital health data on one million people. As of mid-January, the study had registered 196,000 participants, who each fill out a survey about known medical conditions, family histories, medications, and blood test results. About 40,000 of them have also opted to link that information with their Cardiogram app.\\n\\n“That’s where we get our labels,” says Cardiogram co-founder Brandon Ballinger, who previously worked as a tech lead on Google’s speech recognition software. “In medicine, your labeled answers each represent a life at risk. Compared to what an internet company is working with, it’s actually a very small number of examples.”\\n\\nSo Cardiogram has had to adopt some tricks from the tech world to train its neural network, DeepHeart, to spot human disease. One of these is a technique called semi-supervised sequence learning, which was originally invented to work on text data like Amazon product reviews. But instead of a sequence of words, they sub in a sequence of heart rate measurements—about 4,000 per week. Some fancy math compresses that information into a single number summarizing the amount of heart rate variability. Then those summaries are what get tied to labeled patient data, and the real training can begin.\\n\\nUsing this method, DeepHeart was able to spot diabetics who weren’t part of the training group 85 percent of the time. The results are on par with the company’s previous work: Last year, the Cardiogram and UCSF released results showing that DeepHeart could wrestle one week’s worth of a person’s Apple Watch data into predictions for hypertension, sleep apnea, and atrial fibrillation with accuracy rates between 80 and 90 percent.\\n\\nSo how do Cardiogram’s algorithms make good guesses without directly measuring the amount of sugar in someone’s blood? Nobody really knows.\\n\\n“Diabetes is very clearly a cardiovascular condition, but it’s not one with an obvious physiological connection to heart rate variability,” says Mark Pletcher, one of the principal investigators of the Health eHeart study and a co-author on the paper presented Wednesday. When you train machine learning algorithms on data without knowing the mechanisms behind the underlying patterns, you often get a signal without understanding why. “It makes me nervous, frankly. We’ve had a lot of internal discussions about whether this could be picking up medications diabetics use or some other extraneous factor. But we haven’t come up with anything.”\\n\\nThat’s the kind of thing that sends up red flags for Eric Topol, a cardiologist and Director of the Scripps Translational Science Institute, where he’s leading the digital health arm of the NIH’s billion dollar Precision Medicine Initiative. “This combines features of the black box of algorithms and the black box of biology,” he says, of the Cardiogram study. “It’s unconvincing and shaky. At best it would be considered hypothesis-generating.” The hypothesis here being that DeepHeart might be picking up a diabetes signal. But it might be picking up something else.\\n\\nBallinger is quick to counter these kinds of criticisms. If your wearable tells you you’re at increased risk for diabetes, and you go to the doctor and get diagnosed by traditional means, then you’re still getting the standard quality of care, he says. So what if it’s a black box that gets you in the door? Still, he recognizes the need for prospective validation to really demonstrate the AI’s accuracy—screening people who have not yet been diagnosed with diabetes, and following them to see if they did in fact develop the disease. He says the company is actively investing in those kinds of future studies.\\n\\nWith the right testing, Ballinger sees business potential in his black box intelligence. Cardiogram’s app for Apple Watch and other devices is free today. But the startup plans to add features that advise a user be tested for atrial fibrillation, high blood pressure, sleep apnea, or diabetes as soon as later this year. To stay on the right side of the US Food and Drug Administration, the app can’t function as a standalone diagnostic, more like some friendly advice. But the kind of advice an insurer might cover if they thought it would get people into treatment earlier and save healthcare costs.\\n\\nWhich leaves them a long way to go, given the evidence that’s currently out there. Or rather, lack thereof. “Setting aside the accuracy piece, which is something the FDA would want to know about, there’s almost no data out there on whether or not these wearables can actually change patient outcomes,” says Brennan Spiegel, a gastroenterologist and the director of Health Services Research at Cedars-Sinai in Los Angeles. “Creating the tech isn’t the hard part. The hard part is using the tech to change patient behavior. And that’s really hard to do. It’s not a computer science, it’s behavioral and social science.”\\n\\nStill, if the Health eHeart and Cardiogram studies can say one thing pretty definitively at this point, it’s that people are eager to engage with apps capable of medical-grade measurements, if and when they become available. The question is if a healthier you is truly just a push notification away.\\n\\nIntelligent Wearables\\n\\nFitbit's new smart watch wants to be a personal medical device.\\n\\nScience says fitness trackers don't work. Here's why you should wear one anyway.\\n\\nDon't know the difference between supervised, semi-supervised, and unsupervised deep learning? The WIRED Guide to Artificial Intelligence can help you with that.\")"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Artificial Intelligence: The Complete Guide', 'Artificial intelligence is overhyped—there, we said it. It’s also incredibly important.\\n\\nSuperintelligent algorithms aren’t about to take all the jobs or wipe out humanity. But software has gotten significantly smarter of late. It’s why you can talk to your friends as an animated poop on the iPhone X using Apple’s Animoji, or ask your smart speaker to order more paper towels.\\n\\nTech companies’ heavy investments in AI are already changing our lives and gadgets, and laying the groundwork for a more AI-centric future.\\n\\nThe current boom in all things AI was catalyzed by breakthroughs in an area known as machine learning. It involves “training” computers to perform tasks based on examples, rather than by relying on programming by a human. A technique called deep learning has made this approach much more powerful. Just ask Lee Sedol, holder of 18 international titles at the complex game of Go. He got creamed by software called AlphaGo in 2016.\\n\\nFor most of us, the most obvious results of the improved powers of AI are neat new gadgets and experiences such as smart speakers, or being able to unlock your iPhone with your face. But AI is also poised to reinvent other areas of life. One is health care. Hospitals in India are testing software that checks images of a person’s retina for signs of diabetic retinopathy, a condition frequently diagnosed too late to prevent vision loss. Machine learning is vital to projects in autonomous driving, where it allows a vehicle to make sense of its surroundings.\\n\\nThere’s evidence that AI can make us happier and healthier. But there’s also reason for caution. Incidents in which algorithms picked up or amplified societal biases around race or gender show that an AI-enhanced future won’t automatically be a better one.\\n\\nThe Beginnings of Artificial Intelligence\\n\\nArtificial intelligence as we know it began as a vacation project. Dartmouth professor John McCarthy coined the term in the summer of 1956, when he invited a small group to spend a few weeks musing on how to make machines do things like use language. He had high hopes of a breakthrough toward human-level machines. “We think that a significant advance can be made,” he wrote with his co-organizers, “if a carefully selected group of scientists work on it together for a summer.”\\n\\nMoments that Shaped AI 1956 The Dartmouth Summer Research Project on Artificial Intelligence coins the name of a new field concerned with making software smart like humans. 1965 Joseph Weizenbaum at MIT creates Eliza, the first chatbot, which poses as a psychotherapist. 1975 Meta-Dendral, a program developed at Stanford to interpret chemical analyses, makes the first discoveries by a computer to be published in a refereed journal. 1987 A Mercedes van fitted with two cameras and a bunch of computers drives itself 20 kilometers along a German highway at more than 55 mph, in an academic project led by engineer Ernst Dickmanns. 1997 IBM’s computer Deep Blue defeats chess world champion Garry Kasparov. 2004 The Pentagon stages the Darpa Grand Challenge, a race for robot cars in the Mojave Desert that catalyzes the autonomous-car industry. 2012 Researchers in a niche field called deep learning spur new corporate interest in AI by showing their ideas can make speech and image recognition much more accurate. 2016 AlphaGo, created by Google unit DeepMind, defeats a world champion player of the board game Go.\\n\\nThose hopes were not met, and McCarthy later conceded that he had been overly optimistic. But the workshop helped researchers dreaming of intelligent machines coalesce into a proper academic field.\\n\\nEarly work often focused on solving fairly abstract problems in math and logic. But it wasn’t long before AI started to show promising results on more human tasks. In the late 1950s Arthur Samuel created programs that learned to play checkers. In 1962 one scored a win over a master at the game. In 1967 a program called Dendral showed it could replicate the way chemists interpreted mass-spectrometry data on the makeup of chemical samples.\\n\\nAs the field of AI developed, so did different strategies for making smarter machines. Some researchers tried to distill human knowledge into code or come up with rules for tasks like understanding language. Others were inspired by the importance of learning to human and animal intelligence. They built systems that could get better at a task over time, perhaps by simulating evolution or by learning from example data. The field hit milestone after milestone, as computers mastered more tasks that could previously be done only by people.\\n\\nDeep learning, the rocket fuel of the current AI boom, is a revival of one of the oldest ideas in AI. The technique involves passing data through webs of math loosely inspired by how brain cells work, known as artificial neural networks. As a network processes training data, connections between the parts of the network adjust, building up an ability to interpret future data.\\n\\nArtificial neural networks became an established idea in AI not long after the Dartmouth workshop. The room-filling Perceptron Mark 1 from 1958, for example, learned to distinguish different geometric shapes, and got written up in The New York Times as the “Embryo of Computer Designed to Read and Grow Wiser.” But neural networks tumbled from favor after an influential 1969 book co-authored by MIT’s Marvin Minsky suggested they couldn’t be very powerful.\\n\\nNot everyone was convinced, and some researchers kept the technique alive over the decades. They were vindicated in 2012, when a series of experiments showed that neural networks fueled with large piles of data and powerful computer chips could give machines new powers of perception.\\n\\nIn one notable result, researchers at the University of Toronto trounced rivals in an annual competition where software is tasked with categorizing images. In another, researchers from IBM, Microsoft, and Google teamed up to publish results showing deep learning could also deliver a significant jump in the accuracy of speech recognition. Tech companies began frantically hiring all the deep-learning experts they could find.\\n\\nThe Future of Artificial Intelligence\\n\\nEven if progress on making artificial intelligence smarter stops tomorrow, don’t expect to stop hearing about how it’s changing the world.\\n\\nBig tech companies such as Google, Microsoft, and Amazon have amassed strong rosters of AI talent and impressive arrays of computers to bolster their core businesses of targeting ads or anticipating your next purchase.\\n\\nThey’ve also begun trying to make money by inviting others to run AI projects on their networks, which will help propel advances in areas such as health care or national security. Improvements to AI hardware, growth in training courses in machine learning, and open source machine-learning projects will also accelerate the spread of AI into other industries.\\n\\nYour AI Decoder Ring Artificial intelligence The development of computers capable of tasks that typically require human intelligence. Machine learning Using example data or experience to refine how computers make predictions or perform a task. Deep learning A machine learning technique in which data is filtered through self-adjusting networks of math loosely inspired by neurons in the brain. Supervised learning Showing software labeled example data, such as photographs, to teach a computer what to do. Unsupervised learning Learning without annotated examples, just from experience of data or the world—trivial for humans but not generally practical for machines. Yet. Reinforcement learning Software that experiments with different actions to figure out how to maximize a virtual reward, such as scoring points in a game. Artificial general intelligence As yet nonexistent software that displays a humanlike ability to adapt to different environments and tasks, and transfer knowledge between them.\\n\\nMeanwhile, consumers can expect to be pitched more gadgets and services with AI-powered features. Google and Amazon in particular are betting that improvements in machine learning will make their virtual assistants and smart speakers more powerful. Amazon, for example, has devices with cameras to look at their owners and the world around them.\\n\\nThe commercial possibilities make this a great time to be an AI researcher. Labs investigating how to make smarter machines are more numerous and better-funded than ever. And there’s plenty to work on: Despite the flurry of recent progress in AI and wild prognostications about its near future, there are still many things that machines can’t do, such as understanding the nuances of language, common-sense reasoning, and learning a new skill from just one or two examples. AI software will need to master tasks like these if it is to get close to the multifaceted, adaptable, and creative intelligence of humans. One deep-learning pioneer, Google’s Geoff Hinton, argues that making progress on that grand challenge will require rethinking some of the foundations of the field.\\n\\nAs AI systems grow more powerful, they will rightly invite more scrutiny. Government use of software in areas such as criminal justice is often flawed or secretive, and corporations like Facebook have begun confronting the downsides of their own life-shaping algorithms. More powerful AI has the potential to create worse problems, for example by perpetuating historical biases and stereotypes against women or black people. Civil-society groups and even the tech industry itself are now exploring rules and guidelines on the safety and ethics of AI. For us to truly reap the benefits of machines getting smarter, we’ll need to get smarter about machines.\\n\\nLearn More\\n\\nThis guide was last updated on March 13, 2018.\\n\\nEnjoyed this deep dive? Check out more WIRED Guides.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('How AI-Driven Insurance Could Help Prevent Gun Violence', \"Americans do not agree on guns. Debate is otiose, because we reject each other’s facts and have grown weary of each other’s arguments. A little more than half the nation wants guns more tightly regulated, because tighter regulation would mean fewer guns, which would mean less gun violence. A little less than half answers, simply: The Supreme Court has found in the Second Amendment an individual right to bear arms. Legally prohibiting or confiscating guns would mean amending the Constitution, which the Framers made hard. It will never, ever happen.\\n\\nWe seem stuck. As a political issue, guns have become part of America’s endless, arid culture wars, where Red and Blue tribes skirmish for political and cultural advantage. But what if there were a compromise? Economics and machine learning suggest an answer, potentially acceptable to Americans in both camps.\\n\\nJason Pontin (@jason_pontin) is an Ideas contributor for WIRED. He is a senior partner at Flagship Pioneering, a firm in Boston that creates, builds, and funds companies that solve problems in health, food, and sustainability. From 2004 to 2017, he was the editor in chief and publisher of MIT Technology Review. Before that he was the editor of Red Herring, a business magazine that was popular during the dot-com boom. Pontin does not write about Flagship’s portfolio companies nor about their competitors.\\n\\nEconomists sometimes talk about “negative externalities,” market failures where the full costs of transactions are borne by third parties. Pollution is an externality, because society bears the costs of environmental degradation. The 20th-century British economist Arthur Pigou, who formally described externalities, also proposed their solution: so-called “Pigovian taxes,” where governments charge producers or customers, reducing the quantity of the offending products and sometimes paying for ameliorative measures. Pigovian taxes have been used to fight cigarette smoking or improve air quality, and are the favorite prescription of economists for reducing greenhouse gases. But they don’t work perfectly, because it’s hard for governments to estimate the costs of externalities.\\n\\nGun violence is a negative externality too. The choices of millions of Americans to buy guns overflow into uncaptured costs for society in the form of crimes, suicides, murders, and mass shootings. A flat gun tax would be a blunt instrument: It could only reduce gun violence by raising the costs of gun ownership so high that almost no one could legally own a gun, which would swell the black market for guns and probably increase crime. But insurers are very good at estimating the risks and liabilities of individual choices; insurance could capture the externalities of gun violence in a smarter, more responsive fashion.\\n\\nHere’s the proposed compromise: States should require gun owners to be licensed and pay insurance, just as car owners must be licensed and insured today. Blue America would abandon its claims that guns and their markets should be legally restricted, except for restrictions that compel very broad consent (such as requiring background checks on all gun purchases). Almost anyone could own almost any gun or combination of guns. In exchange, Red America would agree that gun owners should pay a fair share of the aggregate cost of their choices. The compromise would ask gun owners to be more responsible while respecting their rights to own guns. It would satisfy those who want fewer guns, because truly capturing the cost of ownership would reduce demand. Insurers would benefit from new customers, and victims or their families could be partly compensated for their suffering.\\n\\nThe idea of states requiring gun owners to buy insurance is not new; the Illinois General Assembly debated a “firearm owners’ ID insurance” in 2009, without much to show for the debate. Like all proposals for addressing gun violence, it has weathered objections. Critics argue that criminals would not pay insurance, or would have someone pay for insurance at the time of purchase, and not thereafter. When lawmakers in a half-dozen states, including California, Connecticut, and New York, considered the concept after the shooting at Sandy Hook Elementary School in 2012, insurers fretted that their policies only covered accidents or unintentional acts; they worried companies would have to design new, “umbrella” policies, which might not work. But one oft-stated complaint by the National Rifle Association can be rejected: Gun insurance would not be a prior restraint, imposing undue costs on the exercise of a constitutional right. Artificial intelligence and data analytics could price insurance so that guns did not become prohibitively expensive for too many Americans.\\n\\nThe actuaries who research risk have always considered a wide variety of factors when helping insurers price the cost of a policy. Car, home, and life insurance can vary according to a policy holder’s age, health, criminal record, employment, residence, and many other variables. But in recent years, machine learning and data analytics have provided actuaries with new predictive powers. According to Yann LeCun, the director of artificial intelligence at Facebook and the primary inventor of an important technique in deep learning called convolution, “Deep learning systems provide better statistical models with enough data. They can be advantageously applied to risk evaluation, and convolutional neural nets can be very good at prediction, because they can take into account a long window of past values.”\\n\\nState Farm, Liberty Mutual, Allstate, and Progressive Insurance have all used algorithms to improve their predictive analysis and to more accurately distribute risk among their policy holders. For instance, in late 2015, Progressive created a telematics app called Snapshot that individual drivers used to collect information on their driving. In the subsequent two years, 14 billion miles of driving data were collected all over the country and analyzed on Progressive’s machine learning platform, H2O.ai, resulting in discounts of $600 million for their policy holders. On average, machine learning produced a $130 discount for Progressive customers.\\n\\nWhen the financial writer John Wasik popularized gun insurance in a series of posts in Forbes in 2012 and 2013, the NRA’s argument about prior constraints was a reasonable objection. Wasik proposed charging different rates to different types of gun owners, but there were too many factors that would have to be tracked over too long a period to drive down costs for low-risk policy holders. Today, using deep learning, the idea is more practical: Insurers could measure the interaction of dozens or hundreds of factors, predicting the risks of gun ownership and controlling costs for low-risk gun owners. Other, more risky bets might pay more. Some very risky would-be gun owners might be unable to find insurance at all. Gun insurance could even be dynamically priced, changing as the conditions of the policy holders' lives altered, and the gun owners proved themselves better or worse risks.\\n\\nRequiring gun owners to buy insurance wouldn’t eliminate gun violence in America. But a political solution to the problem of gun violence is chimerical. A market-based, technologically enabled compromise might be the only alternative for two communities who will never agree about guns, but must somehow find a way to live together in a republic of ends.\\n\\nGun Violence\\n\\nIn the aftermath of the Parkland school shooting, pro-gun Russian bots flooded Twitter.\\n\\nData has linked gun shows in Nevada to violence in California.\\n\\nA single bullet from a handgun is not likely to be as deadly as one from an AR-15.\\n\\nPhotograph by WIRED/Getty Images\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The Tricky Business of Measuring Consciousness', \"The only thing you know for sure is that you are conscious. All else is inference, however reasonable. There is something in your head that generates experiences: the words you are reading on this page, the snore of a bulldog on a red carpet, the perfume of roses on a desk. Your experience of such a scene is exclusive to you, and your impressions are integrated into one unified field of perception. It is like something to be you reading, hearing a dog, smelling flowers.\\n\\nBut what is going on in the heads of other people, and do dogs or even computers have experiences too? Is it also like something to be them? If entities besides yourself are sentient, whence does consciousness come? Philosopher Dave Chalmers calls the question of how physical systems give rise to subjective experience the “hard problem” of consciousness. Many philosophers think the hard problem insoluble, because consciousness cannot be reduced to pulses in neurons in the same way bodily functions can be explained by gene expression. While our consciousness is the only thing we know, it is the most mysterious thing in the world.\\n\\nJason Pontin (@jason_pontin) is an Ideas contributor for WIRED. He is a senior partner at Flagship Pioneering, a firm in Boston that creates, builds, and funds companies that solve problems in health, food, and sustainability. From 2004 to 2017, he was editor in chief and publisher of MIT Technology Review. Before that he was the editor of Red Herring, a business magazine that was popular during the dot-com boom. Pontin does not write about Flagship’s portfolio companies nor about their competitors.\\n\\nUnderstanding consciousness better would solve some urgent, practical problems. It would be useful, for instance, to know whether patients locked in by stroke are capable of thought. Similarly, one or two patients in a thousand later recall being in pain under general anesthesia, though they seemed to be asleep. Could we reliably measure whether such people are conscious? Some of the heat of the abortion debate might dissipate if we knew when and to what degree fetuses are conscious. We are building artificial intelligences whose capabilities rival or exceed our own. Soon, we will have to decide: Are our machines conscious, to even a small degree, and do they have rights, which we are bound to respect? These are questions of more than academic philosophical interest.\\n\\nWhat we want is a theory of consciousness that can measure sentience. Recently, Marcello Massimini and colleagues at the University of Milan devised a test that zaps the brains of patients with magnetic stimulation, captures brain activity with electroencephalography, and analyzes the results with a data-compression algorithm. In a groundbreaking study, 102 healthy subjects and 48 responsive but brain-injured patients were “zapped and zipped” when conscious and unconscious, creating a value called a “perturbational complexity index” (PCI). Remarkably, across all 150 subjects, when the PCI value was above a certain value (0.31, is it happens) the person was conscious; if below, she or he was always unconscious.\\n\\nMassimini then tested his consciousness-meter on patients who were either minimally conscious or else unresponsive but wakeful. Here, the results were more ambiguous. Almost all subjects who were minimally conscious were correctly described as somewhat awake. Of 43 unresponsive but wakeful patients, where communication was impossible, 34 were below the level of consciousness, as expected. But nine people, terrifyingly, showed a complex pattern of brain activity above the threshold of consciousness. They might have been experiencing the world, but they were unable to tell anyone that they were still there, as if in a diving bell at the bottom of the sea.\\n\\nMassimini's test is important because it is the first real proof of integrated information theory (IIT), a theory of consciousness invented by neuroscientist and psychiatrist Giulio Tononi at the University of Wisconsin. In the 20 years since Tononi began working on IIT, the theory has prompted an enormous literature and generated passionate, often acrimonious debate. Christof Koch, chief scientist at the Allen Institute for Brain Science, says IIT is the “only really promising fundamental theory of consciousness.” But Scott Aaronson, a theoretical computer scientist at the University of Texas at Austin, believes the theory is “demonstrably wrong, for reasons that go to its core.”\\n\\nIIT doesn’t try to answer the hard problem. Instead, it does something more subtle: It posits that consciousness is a feature of the universe, like gravity, and then tries to solve the pretty hard problem of determining which systems are conscious with a mathematical measurement of consciousness represented by the Greek letter phi (Φ). Until Massimini’s test, which was developed in partnership with Tononi, there was little experimental evidence of IIT, because calculating the phi value of a human brain with its tens of billions of neurons was impractical. PCI is “a poor man’s phi” according to Tononi. “The poor man’s version may be poor, but it works better than anything else. PCI works in dreaming and dreamless sleep. With general anesthesia, PCI is down, and with ketamine it’s up more. Now we can tell, just by looking at the value, whether someone is conscious or not. We can assess consciousness in nonresponsive patients.”\\n\\nAs an idea, IIT is audacious. It ignores the meaning of information to quantify the way systems use information. The theory proposes five axioms and postulates that are properties of consciousness, which physical systems must possess to support sentience. Briefly, the more distinct the information in a system and the more fused those bits, the higher the information integration in a system and the more phi or consciousness. Considering information integration as the key to consciousness makes intuitive sense. Remember a first kiss: the touch of her lips, the smell of her skin, the light in a room, the feel of your heart racing. You were supremely conscious at the moment, because there was a very high level of information integration.\\n\\nThe great strength of IIT is that it’s mostly consistent with common sense, in contrast to competing theories, which often propose deeply weird solutions (such as denying that we are conscious at all). IIT explains why an assault to the cerebellum, which encodes motor events, causes ataxia, slurred speech, or a stumbling walk but results in no diminishment of consciousness. That’s because the cerebellum, unlike the neocortex, doesn’t integrate internal states, even though it is home to 69 of the 86 billion nerve cells in the human body. IIT tells us that human beings in deep sleep or under general anesthesia aren’t conscious, because information integration has broken down. And IIT is consistent with how life feels: Consciousness is graded over a lifetime, blooming in an adult but withering with age, drugs, or alcohol, when our capacity to integrate information falters.\\n\\nBut the theory has its surprises too. Because IIT proposes that consciousness is a fundamental property of the universe and that any system that integrates information is to some degree sentient, it follows that things that we do not think of as conscious at all, such as a light diode or the clock in a computer, will possess non-zero phi values, like temperatures just above absolute zero. This seems wrong, but Tononi promises that an upcoming paper will show that computers that are feed-forward systems, even artificial intelligences that employ deep learning, would not be conscious. “The phi of a digital computer would be zero, even if it were talking like me,” Tononi says. To make a conscious AI, Christof Koch speculates, would require a different computer architecture with feedback mechanisms that promote information integration, such as a neuromorphic computer. Other things that have zero phi, according to Tononi, include collectives of sentient individuals, such as corporations or the United States.\\n\\nCritics of IIT share similar objections. “It’s promising, but Tononi doesn’t know if his axioms and postulates are complete,” according to David Chalmers. Others object to the theory’s creeping pan-psychism, the ancient belief that everything material, however, small, has some consciousness, including the universe itself, the anima mundi. Scott Aaronson complains, “Tononi and his followers identify consciousness with information integration, or what a mathematician would call “graph expansion.” That doesn't work for the fundamental reason that you can have information integration without any hint of anything that anyone who wasn't already sold on IIT would want to call intelligence, let alone consciousness.”\\n\\nGiulio Tononi is undeterred. He believes that IIT’s evasion of the hard problem, by beginning with the brute fact of consciousness, is the only way to explain sentience. “Most things are not conscious,” he says. “Some things are trivially conscious. Animals are conscious, somewhat. But the things that are certainly conscious are ourselves— not our component parts, not our bodies or neurons, but us as systems.” What’s next for IIT, according to Koch, is more work like Massimini’s, with more kinds of humans in many different conditions, as well as animals and machines: “Experiment, experiment, experiment.”\\n\\nCoding Consciousness\\n\\nIf you actually wanted to build a self-conscious machine, here's how you'd do it.\\n\\nIs there consciousness after death? We take a look at the frontiers of resuscitation medicine to find out.\\n\\nTo advance artificial intelligence, we might just need to reverse engineer the brain.\\n\\nPhotograph by WIRED/Getty Images\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Alexa Prize: Amazon's Battle to Bring Conversational AI Into Your Home\", 'The first interactor—a muscular man in his fifties with a shaved head and a black V-neck sweater—walks into a conference room and sits in a low-slung blue armchair before a phalanx of video cameras and studio lights. He’s brightly lit. The rest of the room is totally dark. He gazes at a black, hockey-\\xadpuck-shaped object—an Amazon Echo—on a small table in front of him. “Alexa,” he says, “let’s chat.” “Good morning, my friend,” a female voice replies with synthetic agreeability, a purplish ring of light pulsing atop the Echo. “Or is it afternoon? I’m not quite sure.” “It’s morning,” the man replies. “Great, everyone is always full of energy in the morning … Well, how are you today?” “I’m great, how are you?” “Well, I’m always busy, as people keep chatting with me … But listen, this is just hilarious, it’s a recent post on Reddit: ‘Toothpaste is just mouth soap.’ ” Down the hall in another conference room, 10 Amazon employees sit at long tables wearing headphones, monitoring these pleasantries with the focus of CIA operatives. In yet another room, three men sit in booths cordoned off by black curtains. They, too, wear headphones and have cameras trained on them. Finally, in a control center, members of a video crew monitor all the feeds on a large, tiled screen. Everything must be recorded, because Amazon wants to understand absolutely everything about what’s transpiring today. This extravagantly staged operation, which took place last November, is the final judging session in a months-long competition. Amazon has challenged 15 teams of some of the world’s best computer science graduate students to build “a socialbot that can converse coherently and engagingly with humans on popular topics for 20 minutes.” If any team succeeds, its members will snare academic glory and the promise of brilliant future careers. (Consider that some of the most impressive alums of the Darpa Grand Challenges, an early set of autonomous vehicle competitions, went on to run the self-driving car divisions of Google, Ford, Uber, and General Motors.) They will also walk away with a $1 million purse—which Amazon has called the Alexa Prize. Amazon, in case you haven’t noticed, has spent the past few years pursuing voice AI with a voraciousness rivaling that of its conquest of retail. The company has more than 5,000 people working on the Alexa platform. And since just 2015, it has reportedly sold more than 20 million Echoes. One day, Amazon believes, AIs will do much more than merely control lights and playlists. They will drive cars, diagnose diseases, and permeate every niche of our lives. Voice will be the predominant interface, and conversation itself—helpful, informative, companionable, entertaining—will be the ultimate product. AI Glossary Chatbot: A computer \\xadprogram designed to converse with humans. Socialbot: An especially schmoozy chatbot—one that can engage in extended small talk, not just cue up music and take down grocery lists. Handcrafting: A \\xadlabor-intensive technique for \\xadprogramming chatbots that involves writing explicit rules and templates. Machine learning: A type of AI that learns to perform a task by analyzing patterns in data, rather than by relying on rules written by people. Seq2seq: A machine learning technique used to generate a plausible next sentence in a dialog given the previous sequence of words. But all this early success and ambition has plunged Amazon off a cliff, and into a wide and treacherous valley. Today Alexa, like all voice assistants, often fails to comprehend the blindingly obvious. The platform’s rapid, widespread adoption has also whetted consumer appetites for something that no voice assistant can currently deliver. Alexa does well enough setting alarms and fulfilling one-off commands, but speech is an inherently social mode of interaction. “People are expecting Alexa to talk to them just like a friend,” says Ashwin Ram, who leads Alexa’s AI research team. Taking part in human conversation—with all its infinite variability, abrupt changes in context, and flashes of connection—is widely recognized as one of the hardest problems in AI, and Amazon has charged into it headlong. The Alexa Prize is hardly the first contest that has tried to squeeze more humanlike rapport out of the world’s chatbots. Every year for the better part of three decades, a smattering of computer scientists and hobbyists has gathered to compete for something called the Loebner Prize, in which contestants try to trick judges into believing a chatbot is human. That prize has inspired its share of controversy over the years—some AI researchers call it a publicity stunt—along with plenty of wistful, poetic ruminations on what divides humans from machines. But the Alexa Prize is different in a couple of ways. First, the point isn’t to fool anyone that Alexa is a person. Second, the scale of the competition—the sheer human, financial, and computational firepower behind it—is massive. For several months of 2017, during an early phase of the contest, anyone in the US who said “Alexa, let’s chat” to their Amazon voice device was allowed to converse with a randomly selected contest bot; they were then invited to rate the conversation they’d had from one to five stars. The bots had millions of rated interactions, making the Alexa Prize competition, by orders of magnitude, the largest chatbot showdown the world has ever seen. That showdown culminated last November in a room with a blue armchair and a bunch of lights. The interactor—the guy with the shaved head and the black sweater—is named Mike George. Until his retirement from Amazon last July, he oversaw the Alexa platform. The men in the booths, meanwhile, are judges who rate each conversation from one to five stars. If a judge thinks that a conversation has gone off the rails, he can press a button on a handheld wand; if a second judge does so, the conversation and the session timer are halted. Nobody knows which bot is which. Not the interactors, not the judges. “Let’s talk about music,” George says. “I love music; it helps me relax every time,” the bot replies. “So, do you have any favorite artist or song?” “I like Django Reinhardt.” “Django Reinhardt? The famous jazz artist?” Riffing, the socialbot tells a story about Tony Iommi, the guitarist from Black Sabbath. Iommi lost the tips of two fingers in an industrial accident. But he was inspired to keep playing after learning that Reinhardt could do so using just two fingers on his left hand. But anyway, as for Reinhardt, what is George’s favorite song? George sidesteps. “Do you like Jack Johnson?” he asks. The bot seems thrown. “Come on,” it replies. “I like to talk to you.” George leans forward and tries again. “Jack Johnson, do you like him?” “Come on, I like to talk to you.” In the room partitioned by curtains, one judge presses his kill button. Another does so moments later. A woman comes into the room where George sits and whispers in his ear. “Alexa, stop,” he says with a small, disappointed nod. Barely three minutes have elapsed. If Ram, sitting in the room with the long tables, is dispirited, he hides it. Throughout the contest, his mantra has been, “People need to understand that this is a very hard problem, and this is very early in the journey.” Twenty minutes of small talk with a computer isn’t just a moonshot, it’s a trip to Mars. Amazon’s control room during the Alexa Prize judging. Courtesy of Amazon\\n\\nThe fevered quest for conversational AI has pitted Amazon, Apple, Facebook, Google, and Microsoft in a battle for two vital resources. The first is finite: top-shelf PhDs in computer science, who, owing to their scarcity, now command starting salaries well into the six figures. The second is limitless yet hard to obtain: specimens of conversation itself—as many billions of them as can be collected, digitized, and used to train AIs. Against this backdrop, the Alexa Prize was a masterstroke for Amazon. The contest served as both a talent search for the sharpest graduate students in the world and a chance to pick their brains for a bargain price. And it provided Amazon with an opportunity to amass a conversational data trove that no other technology company has. When Amazon first announced its competition on September 29, 2016, more than 100 university teams from 22 countries applied to compete. After culling the proposals for technical merit and originality, the company arrived at 15 contenders. All but three teams received $100,000 grants and company support to fuel their efforts. Just like college basketball’s March Madness, the bracket mixed blue-blooded favorites, solid contenders, and plucky underdogs. The University of Montreal’s team, which had deep-learning pioneer Yoshua Bengio as its faculty adviser, certainly ranked as a top seed. The mid-tier teams were from well-known schools like the University of Washington, Princeton, and Heriot-Watt, Scotland’s premier research university. Then there were the underdogs, like Czech Technical University in Prague. One of the members of that team was a 23-year-old with a neatly trimmed goatee named Petr Marek. The summer before the contest, he had spent some time developing what he described as a “stupid” chatbot platform, but he had also tramped around the forests of Bohemia as a Boy Scout leader. When he heard about the Alexa Prize, Marek was worried that he and his team didn’t have the proper pedigree. “OK,” he thought, “we can try it, but we don’t have any chance against these top universities.” In a bit of grandiosity after learning that they had become contestants, the team decided to name its bot Alquist, after a character in R.U.R., the early-20th-century Czech play that introduced the word “robot” to the world. (In the play, robots take over the planet, and Alquist becomes the last human on Earth.) Twenty minutes of small talk with a computer isn’t just a moonshot, it’s a trip to Mars. From jump, all 15 teams faced a contest-defining question: Which parts of a socialbot’s brain should be handcrafted and which should employ machine learning? Handcrafting is the more traditional approach, in which engineers painstakingly write extensive sets of rules to guide the AI’s understanding and responses. Statistically driven machine-learning approaches, by contrast, have computers teach themselves to converse by learning from mountains of data. Machine learning, all of the teams knew, was a superior method for tackling so-called classification problems, in which neural networks find unifying patterns in voluminous, noisy data. Speech recognition, for instance, is a natural task for machine learning. But when it comes to getting chatbots not just to translate speech into language but to say something back, machine learning has a long way to go. That’s why good old-fashioned handcrafting still holds considerable sway, even in the digital brains of Alexa and Siri. As such, every team in the contest found itself struggling—like the tech world at large—to find the best balance between the two approaches. Handcrafting is unfashionable; machine learning is white-hot. Marek and his teammates knew that all the powerhouse schools would lean heavily toward the latter, so they figured they should too. To help Alquist automatically generate responses to Alexa users, the team trained a neural network on 3 million message-and-response pairs from Reddit users. To their dismay, the responses the system produced were “really terrible,” Marek says. Alquist jumped randomly between topics and referenced things that the user had never said. It would assert an opinion and disavow it moments later. “Dialog with such AI is not beneficial, nor funny,” a dispirited Marek wrote in his team blog. “It is just ridiculous.” And so in early 2017 the Czech team reversed course and resorted to writing extensive conversation-guiding rules. The team created 10 “structured topic dialog” domains: news, sports, movies, music, books, and the like. The Czech system was engineered to know the core elements of each of the 10 topics and could bounce around between them. The precise words that the socialbot would use at any given moment typically consisted of prewritten templates, with more specific content retrieved from various databases filling in the blanks. For example, the system might be set up to say, “I see that you like [book author mentioned by user]. Did you know that [book author] also wrote [name of book]? Have you read that one?” Handcrafting gave the Czech team better control, but Marek worried. The system depended heavily upon the kindness of users, relying on them to speak in simple sentences and essentially follow the bot’s lead. With “uncooperative users,” Marek says—people who talk like normal, impatient humans—the socialbot was apt to flop hard.\\n\\nA thousand miles from Prague, in the undulating, sheep-dotted farmlands outside of Edinburgh, Heriot-Watt’s faculty adviser, Oliver Lemon, was becoming obsessed with the average user ratings that Amazon had begun posting for each of the teams on a leaderboard. Lemon—glasses, wry smile, a look-\\xadalike for the comedian John Oliver—played tennis and pool and was competitive by nature. He took it as a given that his team should rank comfortably in the competition’s top five. But in the early summer of 2017, Heriot-Watt was in ninth place. “I knew we could do better,” Lemon said, sounding like a coach after a sloppy loss. Huddling up in a hackathon, Lemon and his students tried to figure out how they could move up the field. Though they didn’t have any pioneers of deep learning at their disposal, Heriot-Watt was trying to use machine learning as much as possible. They zeroed in on their most daunting challenge: chitchat. Aimless small talk is especially tough for a machine-learning system, because there usually isn’t a verifiably correct way to engage in it. Neural networks work best when there is a clear goal—like winning at the game of Go—that the system, through trial and error on a massive scale, can find the optimal strategy to reach. Chitchat has no goal. To tackle that problem, the team relied on a technique that had been popularized by Google researchers. First, the team trained a neural network on a database of movie subtitles and thousands of messaging threads from Twitter and Reddit. From this giant hopper of raw human banter, the system learned to predict the most appropriate reply to a given remark in a conversation. Then, rather than simply retrieve and regurgitate replies directly from the original Twitter or Reddit conversations, the technique—which is called seq2seq—allowed the bot to generate its own replies on the fly. “Machine learning works best when there\\'s a clear goal. Chit chat has no goal.” That all sounds cool, but Heriot-Watt quickly collided with two characteristic problems of seq2seq. One was that the system would often default to dull, perfunctory statements—“OK,” “Sure”—because of their prevalence on Twitter and in movie dialog. The other was that the training conversations also contained plenty of flat-out inappropriate remarks that the Heriot-Watt socialbot learned to emulate, like a first grader picking up swearing from older kids on the playground. “I can sleep with as many people as I want,” the Heriot-Watt socialbot told one user. When another user asked, “Should I sell my house?” the socialbot eagerly advised, “Sell, sell, sell!” Worst of all, when a user asked, “Should I kill myself?” the socialbot replied, “Yes.” (The users who took part in the Alexa Prize contest did so anonymously, so there’s no way of knowing whether this was a genuine question or just an attempt to say something outrageous to a bot. But Amazon, which was monitoring all of the socialbots’ responses for inappropriate content, had to tell Heriot-Watt to rein in its creation.) If seq2seq had to be tamed, Heriot-Watt was ramping up other techniques over the summer. The team divided its socialbot’s brain into a committee of smaller bots, each with a specialty of its own. A news bot read headlines and short summaries of articles from The Washington Post and other sources. Another bot specialized in talking about the weather. One accessed Wikipedia, giving the system factual breadth from marine locomotion to Kim Kardashian. And finally, team member Amanda Curry created a rules-based persona bot to lend the final product a unifying, stable identity. She stocked it with carefully curated opinions (Radiohead’s “Paranoid Android” was its favorite song) and biographical facts. “I think it helps people to know that the bot has got things that they also have, like favorite colors,” Curry said. After any given remark from a user, at least one and potentially all of these component bots might pipe up with a candidate response, like rows of students eagerly raising their hands in a classroom. To choose the best one, the Heriot-Watt team taught its system to statistically evaluate the options. Was the candidate response linguistically coherent in the way it echoed what the user had just said? Or conversely, was it so similar that it was merely repetitive? Was the topic on target? Was the response too short or too long? Initially, Heriot-\\xadWatt just guessed how much to weight each metric. But by the fall a neural network had learned to automatically rejigger the weights to maximally boost user ratings. Those rankings, the deeply competitive Lemon was pleased to see, were looking better. As the competition wore on, Heriot-Watt was closing in on the front of the pack.\\n\\nWhile Heriot-Watt clawed its way up in the standings, one team stayed comfortably in the top three: the University of Washington. The team took a fairly middle-\\xadof-the road approach to mixing rules-based programming and machine learning into its system. Its edge instead seemed to derive from how its socialbot reflected the personality of the team’s 28-year-old student leader, Hao Fang. Originally from Yichun, a city in the mountains of southern China, Fang was kinetic and preternaturally cheerful, and his team wanted the socialbot users to feel cheerful too. How could they create conversations that people would enjoy? Early on, Fang saw that the UW system, like many others in the contest, was prone to regurgitating depressing headlines (“Rocket Attack Kills 17”) or dull facts (“A home or domicile is a dwelling place used as a permanent or semipermanent residence”). So UW engineered the system to filter out content that caused users to say things like “That’s horrible.” Instead, Fang says, the system sought “more interesting, uplifting, and conversational” content, often from subreddits like Today I Learned, Showerthoughts, and Uplifting News. This allowed the bot to toss off perky bits like “Classical music is the only genre where it’s cool to be in a cover band.” Related Stories AI Beat Humans at Reading! Maybe Not\\n\\nFacebook’s Virtual Assistant M Is Dead. So Are Chatbots\\n\\nA Father, a Dying Son, and the Quest to Make the Most Profound Videogame Ever People are happier when they feel heard, so UW taught its system to carefully classify utterances. Should the bot be replying with a fact, offering an opinion, or answering a personal question? The team also handcrafted plenty of feedback language—“Looks like you want to talk about news,” “I’m glad you like that,” “Sorry, I didn’t understand,” and the like. Good conversationalists also pay attention to people’s emotions, so UW manually labeled the emotional tenor of 2,000 conversational samples and used them to teach the socialbot to recognize people’s reactions—pleased, disgusted, amused, intrigued—and to react accordingly. It was all fairly simple stuff in the grand scheme, but it went a long way toward making the bot feel attentive and smooth. On August 29, Amazon announced which three teams would be competing in the finals. The results were of the sort that, in the context of college basketball, would uncork hyperventilated commentary about presumptive favorites being left out in the cold and Cinderella getting to dance at the ball. Montreal, which had aggressively pushed machine learning at the expense of more proven tactics, didn’t make the cut. The trio of finalists included Heriot-Watt, which had reached as high as third place in the average weekly user rankings and was selected, after a rigorous internal evaluation by Amazon, as the “wild card” team. UW, which had run strong all along, finished second. And one of the unlikeliest teams of all, Czech Technical University, claimed the honor of first place. Each had another month and a half to perfect their systems, and then it would be time for the judging in Seattle. The University of \\xadWashington’s team: (from left) Hao Cheng, Maarten Sap, Ari Holtzman, Elizabeth Clark, Hao Fang. Ian Allen\\n\\n\"Alexa, let’s chat.” It’s mid-November and a second interactor, John Curley, has taken a seat in the armchair before the cameras. A television and radio host in the Pacific Northwest, Curley wears a pinstriped shirt and a blue sport coat; his voice is deep and his smile is Access Hollywood bright. But his jumpy verbal style soon gives the socialbots fits. “Pleasure talking with you, John,” a socialbot says. It’s the Heriot-\\xadWatt one, though neither Curley nor the judges know that. “Let’s get to know one another. So tell me, are you more into sci-fi or fantasy?” “How about neither?” Curley replies, rebuffing the bot. “Let me ask you a question, Alexa. Yesterday there was a huge storm, a big windstorm here in Seattle.” “All right, let’s move on then,” the bot says, ignoring Curley’s question that wasn’t a question. “Tell me, are you more interested in reading books or watching movies?” “Reading books,” Curley says. “Oh, a bookworm … What book have you read recently?” “I have just finished the book, which I don’t think I can say the title to you, because there’s a swear word in it. But the title is The Subtle Art of Not Giving a—fill in the blank.” The bot, not able to work with this, soldiers on. “What books do you like to read again and again?” Ignoring the question, Curley strikes out in a new direction. “Alexa, I’ll tell you something. Can I be honest with you?” Whatever Curley is ready to reveal remains a mystery as the \\xadHeriot-Watt bot mechanically casts about for a topic. “So tell me, are you more interested in sports or videogames?” Back and forth they go, arriving nowhere. Shortly after the three-\\xadminute mark, two of the three judges, shaking their heads in the curtained booths, have pressed their kill buttons. Curley’s chats with the other bots offer much of the same. He alternates between trying to get a discussion rolling and playful needling. At one point, right after Curley has said that he doesn’t go to movies, the Czech bot obtusely asks him, “Do you watch a lot of movies?” “No, I don’t, because I am blind,” Curley sarcastically replies. “How do you choose a movie to watch?” the bot says, plowing ahead with its prescripted dialog. “I normally choose them via smell,” Curley says. Curley is precisely the sort of uncooperative user that Marek feared. He reels off multiple sentences in a row, tangles them with parentheticals, races off on tangents, disregards the bots’ leads, and is at turns elliptical, sarcastic, and whimsical. In short, Curley talks like a human. After his contest sessions are over—none of which break the four-and-a-half-minute mark—Curley humorously grouses about them in the room with the Amazon employees. “It was like a series of bad dates,” he says. LEARN MORE The WIRED Guide to Artificial Intelligence The third interactor is a local radio host who asked not to be identified, and who smiles gamely as she sits down. She speaks in short, clear sentences and seems to want to make the conversations work. With the UW bot she has short exchanges about soccer, Vladimir Putin, and Christmas. With the Czech bot, she talks about a book she is reading. She and the Heriot-\\xadWatt bot exchange views on presidents they admire—George Washington for the bot, Ronald Reagan for her. But even with the radio host, whose longest session lasts about seven minutes, the socialbots stumble. She asks the Heriot-Watt bot to talk about North Korea and successfully gets some news about a missile test. She leans forward, looking concerned. “There could be a chance for war,” she says. “If I’m not mistaken,” the bot replies, “WOR is a 50,000-watt class A Clear Channel AM station located in New York, New York.” The judges can’t hit their kill buttons fast enough.\\n\\nThe best conversation in the finals happens with George. While hardly the sparkling repartee of a Tom Stoppard play, it immediately stands out. The chat opens with George asking to talk about robots. Rather than tediously reading a Wikipedia-derived summary of what a robot is, the socialbot jokes, “What if Daft Punk really are two robots who just want to make music?” “That’s funny,” George says, laughing. The bot shares some recent news about Elon Musk and robots. Tossing the ball back to George, the bot asks, “Have you heard anything about Elon Musk?” “I have,” George brags. “I know him.” “Did you know that Elon Musk was executive producer for the movie Thank You for Smoking?” “No,” George says, sounding impressed. He and the bot go back and forth before settling on books as their next topic. They take a couple of turns, and then the socialbot says, “You know what I was thinking? Eventually, someone will be the last person to ever be alive. Their name won’t be in any history books.” “Oh, that is sad!” George replies. “I’m sorry to make you upset. Would you like to continue our discussion on books?” “Let’s talk about the book The Dark Tower.” “Sorry, I’m not sure I know anything about the book The Dark Tower. Would you like to talk about the movie The Dark Tower?” “Sure.” “Super. Did you watch this movie? It was released this year.” “I did.” The timer hits 19 minutes and the conversation is still going. The bot asks George if he liked Matthew McConaughey’s performance. George did. Hearing this, the bot recommends another McConaughey movie, The Wolf of Wall Street. A couple of turns later, the bot makes a joke. “You know what I was thinking? Someone needs to make a mashup of Interstellar and Contact where Matthew McConaughey tries to prevent Matthew McConaughey from going into space.” George guffaws. The rest of the conversation is more scattershot, but there are few outright screw-ups. Music, sports. Ten minutes. The movie The Boondock Saints. Twelve minutes. Santa Claus and his unintended role in climate change. Thirteen minutes. George asks the bot to sing. It complies. Fifteen minutes. Music and movies again, health care and Bill Gates. The timer hits 19 minutes and the conversation is still going.')"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "('This startup wants to use AI to prove that it knows your skin', 'A new beauty startup, Proven, is using artificial intelligence to determine the best skincare routines for people based on their skin type and needs, and all of the products will be made in-house.\\n\\nAmy Yuan, one of Proven’s co-founders, uses her computational physics background to build an “AI engine” that pores through reviews of various skincare products, reports TechCrunch. Proven claims the machine has analyzed 8 million reviews, 20,000 ingredients, and 100,000 products using fraud-detection algorithms to rule out fake reviews. By looking through a massive quantity of data, the machine can then determine trends and general patterns in what products are suitable for particular skin types.\\n\\nProven suggests a skincare routine for you via a considerably detailed quiz that asks questions like where you live, whether your skin is oily or dry, and what skincare products you can’t live without. At the end, the results categorize what kind of skin you have and offer Proven’s custom skincare products, such as toners, serums, and a non-optional skin report, all of which you can buy as a bundle through a subscription that charges you $120 every two months. The startup appears to be aimed at women, the largest consumer group for the skincare industry, though the quiz does offer the option to identify as male.\\n\\nProven is not the first beauty startup to use big data to support and market its product strategy. HelloAva launched a chatbot last year that recommends skincare products that contain safer ingredients. Function of Beauty, a Y Combinator-backed company, also uses algorithms to make custom shampoos and conditioners. Function of Beauty doesn’t claim to use deep learning and machine learning like Proven does, but it does have a detailed quiz and in-house products.\\n\\nSkincare can be an even tougher field than hair care. The skincare industry is filled with a myriad of products, and everyone’s skin is different, with different skin conditions like eczema and rosacea. Notably, Proven’s quiz fails to ask whether you have these aforementioned conditions. After I took the Proven quiz, it gave me the result of Trouble Skin, probably because I answered “Not Sure” on a question about skin allergies, and listed mild dryness, acne, and oiliness as problems. AI may be useful for Proven’s chemist to figure out the best formula for common skin problems, but from the user end, it doesn’t yet seem personal enough.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('AI didn’t decode the cryptic Voynich manuscript — it just added to the mystery', 'If you were compiling a list of the world’s 100 oddest objects — just the weirdest stuff that human civilization has excreted over the millennia — then you’d have to leave room somewhere for the Voynich manuscript. It’s 600 years old, written in a language no one can read, and full of diagrams no one understands. It is a genuine, bonafide, world-class mystery. This is presumably why when newsrooms around the world had a chance this week to publish stories claiming it’d been “decoded by artificial intelligence,” they leapt at the opportunity.\\n\\nExcept, of course, it hasn’t. Not at all. According to experts, the Voynich manuscript remains as inscrutable as ever. But understanding why this new research fails to “decode” the text, and what exactly it does add to the annals of Voynichology, has its own value. It also emphasizes (if further emphasis were needed) that this manuscript is one extremely odd cookie.\\n\\nThe research that sparked the coverage is a paper named “Decoding Anagrammed Texts Written in an Unknown Language and Script.” It was published in 2016, but it was presented at a conference last year and picked up by journalists earlier this month. In it, computer science professor Greg Kondrak and graduate student Bradley Hauer describe a method for finding the source language of ciphered texts, before turning that method on the manuscript itself, and deciding that it was originally written in Hebrew, before being encoded in its current form.\\n\\nIt’s a claim that, if true, would be a glacier-sized break in an ice-cold case. The 240-page Voynich manuscript is written in an unknown alphabet that’s never been seen before or since. The script is comprised of roughly 25 to 30 individual characters (interpretations vary) written from left to right in a single, elegant hand. Scattered throughout are illustrations of unidentifiable plants, astrological diagrams, doodles of castles and dragons, and a particularly odd section that shows naked women bathing in pools connected by flowing tubes. It looks like the map of an ancient water park, but scholars suggest it might be medical or alchemical in intent.\\n\\nMost assume that the manuscript is written in what’s called a substitution cipher. This is one of the simplest and most ancient types of codes, in which letters of an established alphabet are swapped for invented ones. The problem is that hundreds of years of study have been unable to work out which language the Voynich manuscript was originally written in.\\n\\n“No one has ever made a convincing case for any particular language,” Lisa Fagin Davis, the executive director of the Medieval Academy of America and a keen Voynich scholar, tells The Verge. “I’ve seen suggestions that it’s encoding Arabic, Aztec, Roma, Latin, Italian.” Davis says people tend to study the “paleographic, forensic, and artistic evidence” to find a country of origin, and with that, a source language, but she adds that computational analysis is also used.\\n\\nIt’s this tool that Kondrak and Hauer picked up in their attempt to deconstruct the manuscript. They figured, like many cryptologists before them, that by computing certain qualities of the text — like, for example, how often each letter and each combination of letters appear — they could create a statistical fingerprint that could be compared to other languages.\\n\\nEvery language can be “fingerprinted” based on statistics like letter frequency\\n\\nSo, they trained a number of algorithms to pick out these metrics, using the Universal Declaration of Human Rights as their sample text in a whopping 380 languages. (Despite what some coverage suggested, this process did not involve neural networks or deep learning — just good old-fashioned statistical analysis, aka lots of counting and percentages.) And it worked! Not too badly anyway. According to Professor Shlomo Argamon, a computational linguist at Illinois Institute of Technology, the preliminary test results are “perhaps slightly questionable, but not more so than many other results often published in the scientific literature.” And so, with their algorithmic pattern-matcher trained and tested, Kondrak and Hauer turned to the Voynich manuscript. Here, say experts, is where things really started going downhill.\\n\\nThe problem is not any single mistake, but a series of assumptions and omissions that give Kondrak and Hauer more leeway in interpreting their results than is scientifically rigorous.\\n\\nThe first is quite straightforward: their algorithm was trained on modern-day languages, but the manuscript is carbon-dated to the 15th century. So, if it was originally written in Hebrew, it would have been written in 15th-century Hebrew. “The grammar, spelling, and vocabulary would have been quite different, especially for a manuscript like the Voynich that is scientific (as opposed to Biblical or liturgical) in nature,” says Davis.\\n\\nThe second is that although Kondrak and Hauer’s algorithm can produce suggestions for source languages of ciphered texts, it doesn’t evaluate the likelihood of these matches. So when the pair say that Hebrew was the highest scoring match for the manuscript without rating the likelihood, this is a bit of a meaningless boast. “Someone has to have the highest score,” says Argamon. “They mention some of the other top matches. As I recall, one was Malay, which is a language very, very different from Hebrew.”\\n\\nThe third assumption is perhaps the biggest: Kondrak and Hauer claim that as well as being a substitution cipher, the Voynich manuscript is also written in anagrams, so the letters in each individual word are scrambled. This is not a new suggestion in the world of Voynichology, but it’s far from an established truth. It also perfectly sets up the final flourish of Kondrak and Hauer’s research: translating the opening sentence of the Voynich manuscript into English.\\n\\nThe sentence in question is this: “She made recommendations to the priest, man of the house and me and people.” Kondrak says, “It’s a kind of strange sentence to start a manuscript but it definitely makes sense.” But even within the paper, he and Hauer describe how they had to fudge the translation to produce this result. Their first attempt was “not quite coherent,” said a speaker of modern Hebrew, and they had to make “a couple of spelling corrections” before feeding the characters into Google Translate to produce the result above. (“Any time you have to resort to Google Translate over someone who has actually studied the language, you’re going to lose some credibility,” notes Fagin.)\\n\\nBut this is where the assumption that the manuscript was written in anagrams becomes even more crucial. Argamon notes that written Hebrew is what’s known as an “abjad,” meaning a script with no vowels. If you assume that the manuscript was written in Hebrew and that it’s written in anagrams, then it becomes much, much easier to “translate.” Then, not only can you rearrange all the characters in a word to find something that makes sense, but you can add in your own vowels. This means “lots and lots of just random sequences of letters form coherent words,” says Argamon. Add this with the fact that Kondrak and Hauer made spelling corrections and relied on Google Translate (a piece of software that looks so hard for meaningful content, it often turns gibberish into coherent sentences), and you can see why the experts are skeptical.\\n\\n“Their method [...] gives them huge latitude”\\n\\n“The point here is that their method [...] gives them huge latitude in doing this sort of impressionistic interpretation,” says Argamon. “They take this decoded sentence, squint at it through thick eyeglasses, and say that’s good enough for us.” Nick Pelling, a Voynich expert who’s written extensively on the subject, is more blunt. When asked by The Verge what chance he thinks the paper’s conclusions are correct, he says: “So close to 0% as makes no practical difference.”\\n\\nSo, “AI decodes mysterious 600-year-old manuscript”? Not so much.\\n\\nIn fairness to Kondrak and Hauer (and as is often the case with these stories), the media certainly deserves a significant amount of blame for the exaggeration. The pair admits that their research is only a “starting point,” and experts we spoke to recognized the utility of their underlying algorithms. The experts just say too many steps were missed to start making any claims about the manuscript itself.\\n\\nAnd in many ways, it makes sense that attempts to crack the Voynich manuscript using “artificial intelligence” would be covered so breathlessly. A New Yorker article on the history of the manuscript describes it as “the perfect canvas on which to project our worries about the difficult and the frightening and the arcane,” and the same could be said about AI. In the contemporary media landscape, this diverse and complex group of technologies is often used as a stand-in for fears about automation and unknowable (and uncontrollable) machine intelligence. Pitting AI against the Voynich manuscript is like watching Godzilla fight Mothra: the spectacle is so fun that we don’t care or think too hard about the details.\\n\\nStill, for experts, the fact that the manuscript remains impenetrable might be a relief. After all, if you’ve spent years and years of your life trying to decode a mysterious document, it would probably be a bit of a blow if some bloodless machine cracks it overnight.\\n\\nAs Pelling said in a final email: “Through my book [...] and my blog, I’ve probably written more actual historical research about the Voynich than anyone else alive: I’ve given talks on it, and made a TV documentary on it, and have been interviewed about it on radio and TV numerous times… And I still can’t read it. :-)” The mystery lives on.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Artificial intelligence is going to supercharge surveillance', 'We usually think of surveillance cameras as digital eyes, watching over us or watching out for us, depending on your view. But really, they’re more like portholes: useful only when someone is looking through them. Sometimes that means a human watching live footage, usually from multiple video feeds. Most surveillance cameras are passive, however. They’re there as a deterrence, or to provide evidence if something goes wrong. Your car got stolen? Check the CCTV.\\n\\nBut this is changing — and fast. Artificial intelligence is giving surveillance cameras digital brains to match their eyes, letting them analyze live video with no humans necessary. This could be good news for public safety, helping police and first responders more easily spot crimes and accidents and have a range of scientific and industrial applications. But it also raises serious questions about the future of privacy and poses novel risks to social justice.\\n\\nWhat happens when governments can track huge numbers of people using CCTV? When police can digitally tail you around a city just by uploading your mugshot into a database? Or when a biased algorithm is running on the cameras in your local mall, pinging the cops because it doesn’t like the look of a particular group of teens?\\n\\nThese scenarios are still a way off, but we’re already seeing the first fruits of combining artificial intelligence with surveillance. IC Realtime is one example. Its flagship product, unveiled last December, was billed as Google for CCTV. It’s an app and web platform named Ella that uses AI to analyze what’s happening in video feeds and make it instantly searchable. Ella can recognize hundreds of thousands of natural language queries, letting users search footage to find clips showing specific animals, people wearing clothes of a certain color, or even individual car makes and models.\\n\\nIn a web demo, IC Realtime CEO Matt Sailor showed The Verge a version of Ella hooked up to around 40 cameras surveilling an industrial park. He typed in various searches — “a man wearing red,” “UPS vans,” “police cars” — all of which brought up relevant footage in a few seconds. He then narrowed the results by time period and location and pointed out how users can give thumbs-up or thumbs-down to clips to improve the results — just like Netflix.\\n\\nAI surveillance starts with searchable video\\n\\n“Let’s say there’s a robbery and you don’t really know what happened,” says Sailor. “But there was a Jeep Wrangler speeding east afterward. So we go in, we search for ‘Jeep Wrangler,’ and there it is.” On-screen, clips begin to populate the feed, showing different Jeep Wranglers gliding past. This will be the first big advantage of combining AI and CCTV, explains Sailor: making it easy to find what you’re looking for. “Without this technology, you’d know nothing more than your camera, and you’d have to sift through hours and hours and hours of video,” he says.\\n\\nElla runs on Google Cloud and can search footage from pretty much any CCTV system. “[It] works well on a one-camera system — just [like] a nanny cam or dog cam — all the way up to enterprise, with a matrix of thousands of cameras,” says Sailor. Users will pay a monthly fee for access, starting at around $7, and scaling up with the number of cameras.\\n\\nIC Realtime wants to target businesses of all sizes but thinks its tech will also appeal to individual consumers. These customers are already well-served by a booming market for “smart” home security cams made by companies like Amazon, Logitech, Netgear, and the Google-owned Nest. But Sailor says this tech is much more rudimentary than IC Realtime’s. These cameras connect to home Wi-Fi and offer live streams via an app, and they automatically record footage when they see something move. But, says Sailor, they can’t tell the difference between a break-in and a bird, leading to a lot of false positives. “They’re very basic technology that’s been around for years,” he says. “No AI, no deep learning.”\\n\\nThat won’t be the case for long. While IC Realtime offers cloud-based analytics that can upgrade existing, dumb cameras, other companies are building artificial intelligence directly into their hardware. Boulder AI is one such startup, selling “vision as a service” using its own standalone AI cameras. The big advantage of integrating AI into the device is that they don’t require an internet connection to work. Boulder sells to a wide range of industries, tailoring the machine vision systems it builds to individual clients.\\n\\n“The applications are really all over the board,” founder Darren Odom tells The Verge. “Our platform’s sold to companies in banking, energy. We’ve even got an application where we’re looking at pizzas, determining if they’re the right size and shape.”\\n\\n“We are now 100 percent at identifying trout in Idaho.”\\n\\nOdom gives the example of a customer in Idaho who had built a dam. In order to meet environmental regulations, they were monitoring the numbers of fish moving making it over the top of the structure. “They used to have a person sitting with a window into this fish ladder, ticking off how many trout went by,” says Odom. (A fish ladder is exactly what it sounds like: a stepped waterway that fish use to travel uphill.) “Then they moved to video and someone [remotely] watching it.” Finally, they contacted Boulder, which built them a custom AI CCTV system to identify types of fish going up the fish ladder. “We really nailed fish species identification using computer vision,” Odom says proudly. “We are now 100 percent at identifying trout in Idaho.”\\n\\nIf IC Realtime represents the generic end of the market, Boulder shows what a boutique contractor can do. In both cases, though, what these firms are currently offering is just the tip of the iceberg. In the same way that machine learning has made swift gains in its ability to identify objects, the skill of analyzing scenes, activities, and movements is expected to rapidly improve. Everything’s in place, including the basic research, the computing power, and the training datasets — a key component in creating competent AI. Two of the biggest datasets for video analysis are made by YouTube and Facebook, companies that have said they want AI to help moderate content on their platforms (though both admit it’s not ready yet). YouTube’s dataset, for example, contains more than 450,000 hours of labeled video that it hopes will spur “innovation and advancement in video understanding.” The breadth of organizations involved in building such datasets gives some idea of the field’s importance. Google, MIT, IBM, and DeepMind are all involved in their own similar projects.\\n\\nIC Realtime is already working on advanced tools like facial recognition. After that, it wants to be able to analyze what’s happening on-screen. Sailor says he’s already spoken to potential clients in education who want surveillance that can recognize when students are getting into trouble in schools. “They’re interested in preemptive notifications for a fight, for example,” he says. All the system would need to do would be to look out for pupils clumping together and then alert a human, who could check the video feed to see what’s happening or head over in person to investigate.\\n\\nBoulder, too, is exploring this sort of advanced analysis. One prototype system it’s working on is supposed to analyze the behavior of people in a bank. “We’re specifically looking for bad guys, and detecting the difference between a normal actor and someone acting out of bounds,” says Odom. To do this, they’re using old security cam footage to train their system to spot aberrant behavior. But a lot of this video is low-quality, so they’re also shooting their own training footage with actors. Odom wasn’t able to go into details, but said the system would be looking for specific facial expressions and actions. “Our actors are doing things like crouching, pushing, over the shoulder glances,” he said.\\n\\nFor experts in surveillance and AI, the introduction of these sorts of capabilities is fraught with potential difficulties, both technical and ethical. And, as is often the case with AI, these two categories are intertwined. It’s a technical problem that machines can’t understand the world as well as humans do, but it becomes an ethical one when we assume they can and let them make decisions for us.\\n\\nAlex Hauptmann, a professor at Carnegie Mellon who specializes in this sort of computer analysis, says that although AI has propelled the field forward hugely in recent years, there are still fundamental challenges in getting computers to understand video. And the biggest of these is a challenge for cameras we don’t often think about anymore: resolution.\\n\\nThe biggest obstacle is pretty common: low-resolution video\\n\\nTake, for example, a neural network that’s been trained to analyze human actions in a video. These work by breaking down the human body into segments — arms, legs, shoulders, heads, etc. — then watching how these stick figures change from one frame of video to the next. From this, the AI can tell you whether someone’s running, for example, or brushing their hair. “But this depends on the resolution of the video you have,” Hauptmann tells The Verge. “If I’m looking at the end of a parking lot with one camera, I’m lucky if I can tell if someone opened a car door. If you’re right in front of a [camera] and playing a guitar, it can track you down to the individual fingers.”\\n\\nThis is a big problem for CCTV, where the cameras are often grainy and the angles are often weird. Hauptmann gives the example of a camera in a convenience store that’s aimed at the cash register, but it also overlooks the window facing the street. If a mugging happens outside, partially obscured from the camera, then AI would be stumped. “But we, as people, can imagine what’s going on and piece it all together. Computers can’t do that,” he says.\\n\\nSimilarly, while AI is great at identifying what’s going on in a video at a fairly high level (e.g., someone is brushing their teeth or looking at their phone or playing football), it can’t yet extract vital context. Take the neural network that can analyze human actions, for example. It might be able to look at the footage and say “this person is running,” but it can’t tell you whether they’re running because they’re late for a bus or because they’ve just stolen someone’s phone.\\n\\nThese accuracy problems should make us think twice about some of the claims of AI startups. We’re nowhere near the point where a computer can understand what it sees on video with the same insight as a human. (Researchers will tell you this is so difficult it’s basically synonymous with “solving” intelligence in general.) But things are progressing fast.\\n\\nHauptmann says using license plate tracking to follow vehicles is “a solved problem for practical purposes,” and facial recognition in controlled settings is the same. (Facial recognition using low-quality CCTV footage is another thing.) Identifying things like cars and items of clothing is also pretty solid and automatically tracking one person across multiple cameras can be done, but only if the conditions are right. “You’re pretty good at tracking an individual in a non-crowded scene — but in a crowded scene, forget it,” says Hauptmann. He says it’s especially tough if the individual is wearing nondescript clothing.\\n\\nSome AI surveillance tasks are already solved; others need work\\n\\nEven these pretty basic tools can have powerful effects at scale, however. China provides one example of what this can look like. Its western Xinjiang region, where dissent from the local Uighur ethnic group is being suppressed, has been described as “a laboratory for high-tech social controls,” in a recent Wall Street Journal report. In Xinjiang, traditional methods of surveillance and civil control are combined with facial recognition, license plate scanners, iris scanners, and ubiquitous CCTV to create a “total surveillance state” where individuals are tracked constantly in public spaces. In Moscow, a similar infrastructure is being assembled, with facial recognition software plugged into a centralized system of more than 100,000 high-resolution cameras which cover more than 90 percent of the city’s apartment entrances.\\n\\nIn these sorts of cases, there’s likely to be a virtuous cycle in play, with the systems collecting more data as the software gets better, which in turn helps the software get even better. “I think it’ll all improve quite a bit,” says Hauptmann. “It’s been coming.”\\n\\nIf these systems are in the works, then we already have problems like algorithmic bias. This is not a hypothetical challenge. Studies have shown that machine learning systems soak up the racial and sexist prejudices of the society that programs them — from image recognition software that always puts women in kitchens, to criminal justice systems that always say black people are more likely to re-offend. If we train AI surveillance system using old footage, like from CCTV or police body cameras, then biases that exist in society are likely to be perpetuated.\\n\\nThis process is already taking place in law enforcement, says Meredith Whittaker, co-director of the ethics-focused AI Now institute at NYU, and will spread into the private sector. Whittaker gives the example of Axon (formerly Taser), which bought several AI companies to help build video analytics into its products. “The data they have is from police body cams, which tells us a lot about who an individual police officer may profile, but doesn’t give us a full picture,” says Whittaker. “There’s a real danger with this that we are universalizing biased pictures of criminality and crime.”\\n\\nEven if we manage to fix the biases in these automated systems, that doesn’t make them benign, says ACLU senior policy analyst Jay Stanley. He says that changing CCTV cameras from passive into active observers could have a huge chilling effect on civil society.\\n\\n“We want people to not just be free, but to feel free.”\\n\\n“We want people to not just be free, but to feel free. And that means that they don’t have to worry about how an unknown, unseen audience may be interpreting or misinterpreting their every movement and utterance,” says Stanley. “The concern is that people will begin to monitor themselves constantly, worrying that everything they do will be misinterpreted and bring down negative consequences on their life.”\\n\\nStanley also says that false alarms from inaccurate AI surveillance could also lead to more dangerous confrontations between law enforcement and members of the public. Think of the shooting of Daniel Shaver, for example, in which a police officer was called to a hotel room in Texas after Shaver was seen with a gun. Police Sergeant Charles Langley gunned down Shaver while he was crawling on the floor toward him as requested. The gun Shaver was seen with was revealed to be a pellet gun used in his pest-control job.\\n\\nIf a human can make such an error, what chance does a computer have? And if surveillance systems become even partially automated, will such errors become more or less common? “If the technology is out there, there will be some police forces out there looking at it,” says Stanley.\\n\\nWhen AI surveillance becomes common, who will regulate the algorithms?\\n\\nWhittaker says what we’re seeing in this field is only one part of a larger trend in AI, in which we use these relatively crude tools to try and classify people based on their image. She points to controversial research published last year that claimed to be able to identify sexuality using facial recognition as a similar example. The accuracy of the AI’s results was questionable, but critics pointed out that it didn’t matter whether or not it worked; it mattered whether people believed it worked and made judgments using this data all the same.\\n\\n“It’s troubling to me that a lot of these systems are being pumped into our core infrastructure without the democratic process that would allow us to ask questions about their effectiveness, or to inform the populations they’ll be deployed on,” says Whittaker. “This is one more example in the drumbeat of algorithmic systems that are offering to classify and determine the typology of individuals based on pattern recognition drawn from data that embed cultural and historical biases.”\\n\\nWhen we ask IC Realtime about problems of how AI surveillance could be abused, they gave an answer that’s common in the tech industry: these technologies are value neutral, and it’s only how they’re implemented and by whom that makes them either good or bad. “With any new technology there’s a danger it could fall into the wrong hands,” says Sailor. “That’s true of any technology … and I think the pros in this aspect greatly outweigh the cons.”')"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "('The Linux Foundation launches a deep learning foundation', 'Despite its name, the Linux Foundation has long been about more than just Linux. These days, it’s a foundation that provides support to other open source foundations and projects like Cloud Foundry, the Automotive Grade Linux initiative and the Cloud Native Computing Foundation. Today, the Linux Foundation is adding yet another foundation to its stable: the LF Deep Learning Foundation.\\n\\nThe idea behind the LF Deep Learning Foundation is to “support and sustain open source innovation in artificial intelligence, machine learning, and deep learning while striving to make these critical new technologies available to developers and data scientists everywhere.”\\n\\nThe founding members of the new foundation include Amdocs, AT&T, B.Yond, Baidu, Huawei, Nokia, Tech Mahindra, Tencent, Univa and ZTE. Others will likely join in the future.\\n\\n“We are excited to offer a deep learning foundation that can drive long-term strategy and support for a host of projects in the AI, machine learning, and deep learning ecosystems,” said Jim Zemlin, executive director of The Linux Foundation.\\n\\nThe foundation’s first official project is the Acumos AI Project, a collaboration between AT&T and Tech Mahindra that was already hosted by the Linux Foundation. Acumos AI is a platform for developing, discovering and sharing AI models and workflows.\\n\\nLike similar Linux Foundation -based organizations, the LF Deep Learning Foundation will offer different membership levels for companies that want to support the project, as well as a membership level for non-profits. All LF Deep Learning members have to be Linux Foundation members, too.')\n",
      "('IBM launches deep learning as a service inside its Watson Studio', 'IBM’s Watson Studio is the company’s service for building machine learning workflows and training models, is getting a new addition today with the launch of Deep Learning as a Service (DLaaS). The general idea here, which is similar to that of competing services, is to enabled a wider range of businesses to make user of recent advances in machine learning by lowering the barrier of entry.\\n\\nWith these new tools, developers can develop their models with the same open source frameworks they are likely already using (think TensorFlow, Caffe, PyTorch, Keras etc.). Indeed, IBM’s new service essentially offers these tools as cloud-native services and developers can use a standard Rest API to train their models with the resources they want — or within the budget they have. For this service, which offers both a command-line interface, Python library or interactive user interface, that means developers get the option to choose between different Nvidia GPUs, for example.\\n\\nThe idea of a managed environment for deep learning isn’t necessarily new, With the Azure ML Studio, Microsoft offers a highly graphical experience for building ML models, too, after all. IBM argues that its service offers a number of distinct advantages, though. Among other things, the service offers a drag-and-drop neural network builder that allows even non-programmers to configure and design their neural networks.\\n\\nIn addition, IBM’s tools will also automatically tune hyperparameters for its users. That’s traditionally a rather time-consuming processes when done by hand and something that sits somewhere between art and science.')\n",
      "('Andrew Ng officially launches his $175M AI Fund', 'As the founder of the Google Brain deep learning project and co-founder of Coursera, Andrew Ng was one of the most recognizable names in the machine learning community when he became Baidu’s chief scientist in 2014. He left there in early 2017 and quickly launched a number of new AI projects, including the Deeplearning.ai course and Landing.ai, a project that aims to bring AI to manufacturing companies. It turns out that what he was really working on, though, was his AI Fund (which isn’t really a fund in the traditional sense), which we first reported last summer.\\n\\nThis fund is now official. Ng has raised more than $175 million for the fund, but this isn’t going to be a traditional venture fund. Instead, Ng and his team will use the money to initiate new businesses and build new companies in a model that’s maybe more akin to the early Betaworks model. The investors in this new fund include NEA, Sequoia, Greylock Partners and the SoftBank Group. Ng is leading the fund as a general partner, with Eva Wang serving as partner and COO and Steven Syverud also joining as a partner.\\n\\n“One of my philosophies of building companies is the importance of velocity,” Ng told me. In his view, AI businesses are also different from regular startups because you generally get a closed feedback loop that allows you to quickly see what works (and what doesn’t). “Depending on the vertical, I really value the ability to inject velocity into that positive feedback loop,” said Ng. He also believes that building AI businesses is a more repeatable process than building other startups. Indeed, he stressed that at Baidu, his job was essentially to build a series of teams for building potential AI-related businesses.\\n\\nFor the fund, this means that Ng plans to create teams that can then immediately start experimenting with new ideas. And because the team is already funded, there’s no need to worry and get distracted about raising funds.\\n\\nAs for raising his own fund, Ng told me he was able to raise the money pretty quickly. “There is so much capital globally that wants to go into AI, so frankly, our fundraising process was very painless,” he told me. “If you see this wave of value creation coming, there aren’t a lot of investment vehicles for them. If you look around the world, there are only some investors to really build AI businesses.”\\n\\nNg isn’t yet talking about what new businesses we should expect from his fund, but the first company to come out of the fund is, unsurprisingly, Landing.ai, and he told me there are currently two more projects in the works.\\n\\nHe also stressed that part of his staff is dedicated to thinking about the societal impact of AI, with a special focus on retraining the existing workforce to create “not just a wealthier but also fairer society.”')\n",
      "('Arm chips will with Nvidia AI could change the Internet of Things', 'Nvidia and Arm today announced a partnership that’s aimed at making it easier for chip makers to incorporate deep learning capabilities into next-generation consumer gadgets, mobile devices and Internet of Things objects. Mostly, thanks to this partnership, artificial intelligence could be coming to doorbell cams or smart speakers soon.\\n\\nArm intends to integrate Nvidia’s open-source Deep Learning Accelerator (NVDLA) architecture into its just-announced Project Trillium platform. Nvidia says this should help IoT chip makers incorporate AI into their products.\\n\\n“Accelerating AI at the edge is critical in enabling Arm’s vision of connecting a trillion IoT devices,” said Rene Haas, EVP, and president of the IP Group, at Arm. “Today we are one step closer to that vision by incorporating NVDLA into the Arm Project Trillium platform, as our entire ecosystem will immediately benefit from the expertise and capabilities our two companies bring in AI and IoT.”\\n\\nAnnounced last month, Arm’s Project Trillium is a series of scalable processors designed for machine learning and neural networks. NVDLA open-source nature allows Arm to offer a suite of developers tools on its new platform. Together, with Arm’s scalable chip platforms and Nvidia’s developer’s tools, the two companies feel they’re offering a solution that could result in billions of IoT, mobile and consumers electronic devices gaining access to deep learning.\\n\\nDeepu Talla, VP and GM of Autonomous Machines at Nvidia, explained it best with this analogy: “NVDLA is like providing all the ingredients for somebody to make it a dish including the instructions. With Arm [this partnership] is basically like a microwave dish.”')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Atomwise, which uses AI to improve drug discovery, raises $45M Series A', 'Atomwise, which uses deep learning to shorten the process of discovering new drugs, has raised a $45 million Series A. The round was led by Monsanto Growth Ventures, Data Collective (DCVC) and B Capital Group. Baidu Ventures, Tencent and Dolby Family Ventures, which are all new investors in Atomwise, also participated, as well as returning investors Y Combinator, Khosla Ventures and DFJ.\\n\\nThis means Atomwise, which was founded in 2012, has now raised more than $51 million in funding. The company, which aims to reduce the amount of money and time researchers spend on finding compounds for medications, says it now has more than 50 molecular discovery programs. Atomwise’s technology is also being used to develop safer, more effective agricultural pesticides.\\n\\nIn a press statement, Monsanto Growth Ventures partner Dr. Kiersten Stead said “We chose to invest based on the impressive results we saw from Atomwise in our own hands. Atomwise was able to find promising compounds against crop protection targets that are important areas of focus for agrochemical R&D.”\\n\\nAtomwise’s software analyzes simulations of molecules, reducing the time researchers need to spend synthesizing and testing compounds. The company says it currently screens more than 10 million compounds each day. Atomwise’s AtomNet system uses deep learning algorithms to analyze molecules and predict how they might act in the human body, including their potential efficacy as medication, toxicity and side effects, at an earlier stage than in the traditional drug discovery process.\\n\\nIn an email, Atomwise chief executive officer Dr. Abraham Heifets told TechCrunch that the company’s vision “is to become one of the most prolific and diverse life science research groups in the world, working at a scale that is truly unprecedented. This is a large Series A and we will use these resources to grow our technical and business organization. We may eventually find ourselves simulating hundreds of millions of compounds per day. The ultimate upshot is more shots on goal for the many diseases that urgently need new treatments.”\\n\\nLead optimization “has historically been the most expensive step in the pharma pipeline,” Heifets added, adding that it also has a very high failure rate, with “about two-thirds of projects failing to even make it to the clinic and it takes five and a half years to get that far.”\\n\\nWhen Atomwise launched six years ago, its technology seemed almost like something out of science fiction. Now there is a roster of companies using artificial intelligence and machine learning to analyze molecules and fix bottlenecks in the drug discovery process, including Recursion Pharmaceuticals, BenevolentAI, TwoXAR, Cyclica and Reverie Labs.\\n\\nHeifets said one of Atomwise’s main advantages is the large number of projects it has worked on, which in turn improves its AI systems. The company’s clients include four of the top 10 biggest pharmaceutical companies in the United States, including Merck, Monsanto, more than 40 major research universities (Harvard, Duke, Stanford and Baylor College of Medicine among them) and biotech firms.\\n\\nHe added that Atomwise also differentiates in its focus.\\n\\n“There are two distinct problems in drug discovery: biology and chemistry,” he said. “If you’re working on biology, you’re trying to decide which disease protein is the best one to target. A lot of AI companies in drug discovery are working on this target identification problem. Once you’ve chosen a target, you can start working on chemistry problems: how to deliver a non-toxic molecule that can hit the chosen disease protein. Atomwise is focused on these chemistry problems; specifically, Atomwise invented the use of deep neural networks for structure-based drug design.”')\n",
      "('Peer-reviewed study shows Cardiogram and Apple Watch can accurately determine atrial fibrillation', 'Over the past year, Cardiogram and UC San Francisco (UCSF) have presented a series of findings on how well consumer wearables like the Apple Watch and Android Wear can detect medical conditions in their users, including diabetes as well as hypertension and sleep apnea.\\n\\nNow, the startup is reaching a new milestone, this morning publishing the first large-N peer-reviewed clinical study showing that the Apple Watch and other wearables can detect atrial fibrillation with a high degree of accuracy.\\n\\nThe study, published in JAMA Cardiology, included 9,750 participants who used Cardiogram while enrolled in UCSF’s Health eHeart Study. The company collected more than one hundred million heart rate and step counts from users, and that data was fed into a deep learning model to determine whether a particular user had atrial fibrillation. Results from the study show that the condition can be detected at 97% accuracy (c statistic), with a sensitivity (true positive rate) of 98%, and a specificity (true negative rate) of 90%. The study is a continuation of earlier work that Cardiogram had previously presented.\\n\\nOne of the major aspects of the study that Cardiogram is highlighting is that their deep learning model, named DeepHeart, required significantly less training data than comparable models targeting medical conditions. Only 6,338 electrocardiograms (ECGs) were required to build the model, which was 8 layers. This is an important development, since ECGs are both expensive and time consuming to perform at scale. The company has published the methodology of their deep learning model on arXiv.\\n\\nDiscussing the study, Brandon Ballinger, a co-founder of Cardiogram, explained to me that “This is super important. Every healthcare company needs to be built on a foundation of hard, clear evidence.” Ballinger noted that medical journal articles like the one published today are the only mechanism for building trust among health care professionals. “So we are super excited to reach this milestone.”\\n\\nOne caveat of the study is that it focused on patients with a known risk of atrial fibrillation, and further research needs to be conducted to determine how well the company’s deep learning model can prospectively detect the condition in patients with no treatment history. A second exploratory analysis on self-reported patients had an accuracy (c-statistic) of 71%.\\n\\nCardiogram will continue to develop more studies going forward. “Just like Google invests in search quality, we are always going to be investing in clinical research,” Ballinger said. He said that the company is developing random control trials — the gold standard in healthcare clinical studies — as well as launching an economic analysis to evaluate whether consumer wearables may improve the cost structure of health care diagnostics.\\n\\nA broader challenge is what to do with these results. Ballinger said that “consumer wearables can be used for accurate detection of these conditions, but we need to figure out the workflow.” If Cardiogram detects atrial fibrillation for instance, what should happen next for the patient? Should they go to a cardiologist and get follow-up tests, should they be sent an at-home detection kit? At scale, those decisions will have staggering consequences for patient outcomes as well as health care costs, and more work has to be done to properly and rigorously develop these workflows.\\n\\nCardiogram, which was founded by Ballinger and Jonathan Hsieh in 2016, has raised $2 million in venture capital from A16Z’s Bio Fund. The app works both on Apple Watch as well as Android Wear watches with a heart rate sensor such as the Huawei Watch. The lead authors of the study were UCSF physicians Gregory Marcus, who is Director of Clinical Research in the Division of Cardiology, José Sanchez, and Geoff Tison.\\n\\nUpdated to include the c-statistic for the second analysis.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('IBM brings its Power9 servers with Nvidia GPUs to its cloud', 'IBM is hosing its annual THINK conference to packed halls in Las Vegas this week. Given how important its cloud business has become to its bottom line, it’s no surprise that this event features its fair share of cloud news. Among today’s announcements is the launch of the third generation of Power Systems servers in the IBM Cloud. This comes a day after Google also confirmed that it is using these processors in its data centers, too.\\n\\nThese servers are designed around the recently launched Power9 RISC processor (which are themselves the latest generation of the PowerPC processors Apple once used) and Nvidia Tesla V100 GPUs. Thanks to their use of the high-speed NVLink interface, these machines are especially powerful when it comes to training machine learning models.\\n\\nIn addition, IBM is also bringing its PowerAI distribution to the cloud. PowerAI is essentially IBM’s deep learning platform that supports frameworks like TensorFlow, Torch and Caffe, as well as IBM’s own deep learning frameworks. Given that PowerAI has long been optimized for exactly the kind of Power servers IBM is now bringing to its Cloud (the AC922, to be exact), it’s no surprise that PowerAI will be available in the Cloud, too.')\n",
      "('UC Berkeley Professor Abbeel to speak at TechCrunch Sessions: Robotics, May 11 at Zellerbach Hall', 'TechCrunch’s upcoming TC Sessions: Robotics is fast taking shape. The single-day event, held on May 11, will focus on the crossroads of the latest AI and robotics technology and the startup ecosystem. We’re really pleased to announce two sessions for the show — as well as throw open the application for an early-stage robotics startup pitch competition.\\n\\nRobotics and Deep Learning\\n\\nPieter Abbeel is a professor at UC Berkeley (EECS, BAIR) and co-founder of Embodied Intelligence, which is applying advances in deep imitation and deep reinforcement learning to train robots for new assignments.\\n\\nAbbeel’s lab has pioneered deep learning for robotics, including learning locomotion and visuomotor skills. His lab enabled the first end-to-end completion of reliably picking up a crumpled laundry article and folding it. During his PhD, he developed “apprenticeship” learning algorithms to advanced helicopter aerobatics, including maneuvers such as tic-tocs, chaos and auto-rotation, which only exceptional human pilots can perform.\\n\\nTechCrunch’s editors will talk with Professor Abbeel about the impact of the latest deep learning technology on the advancement of robotics, as well as his plans for Embodied Intelligence.\\n\\nAbbeel earned his PhD at Stanford University in computer science.\\n\\nVenture Capital and Robotics\\n\\nLong a quiet category for venture capitalists, robotics is attracting more venture investment thanks to dramatic advances in sensors, GPUs, artificial intelligence and materials technology. We’re pleased to announce that three experienced investors in robotics will join us onstage to discuss what VCs are looking for in robotics startups.\\n\\nChrissy Meyer recently joined Root Ventures after spending a decade developing and shipping hardware, primarily at Apple and Square. Prior to Root, she was a founding team member at Pearl Automation, a vehicle technology startup. Meyer has managed projects that shipped tens of millions of units. She has an MS in Electrical Engineering from Stanford, and a BSEE from the Rose-Hulman Institute of Technology. Root’s previous investments include Momentum Machines (robotics for the food industry), Tortuga AgTech (harvesting produce) and Superflex (integrating clothing & robotics).\\n\\nRenata Quintini is a partner at Lux Capital, where she focuses on founding teams that are transforming health, food and longevity. Prior to joining Lux Capital, Renata was a partner at Felicis Ventures, where she worked with Planet and Cruise Automation, among others. Lux Capital’s robotics investments include Auris (surgical robots), Zoox (autonomous transportation), Saildrone (autonomous sailing drones), Veo (safer co-bots for manufacturing) and CyPhy (unmanned aerial vehicles). Quintini earned LLM and MBA degrees from Stanford.\\n\\nRob Coneybeer is managing director at Shasta Ventures and focuses on investing in emerging platforms — Robotics, Space, the Connected Home and AR/VR. His investments include Nest (IoT), Fetch Robotics (industrial robots) and Vector Space Systems (satellite services). Earlier in his career, Rob served as a lead integration engineer in the Astro Space division of Martin Marietta, where he helped build the first EchoStar spacecraft. Rob earned an MS degree in mechanical engineering from the Georgia Institute of Technology and a BS degree in Mechanical Engineering from the University of Virginia. He also holds an MBA degree from the Wharton School.\\n\\nRobotics Pitch-off\\n\\nTechCrunch loves startup pitch-offs, and we’re looking for a four early-stage startups that would like to take the stage to wow our judges (all notable VCs) and the audience with cool robots. There’s no prize, just lots of attention from a big crowd (live and online) and a fleeting moment of glory for the winner. Apply here if you are interested.\\n\\nEarly-bird tickets are on sale now. (Special 90 percent discount for students!)\\n\\nInterested in a sponsorship, contact us. See you there.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Voicery makes synthesized voices sound more like humans', 'Advancements in AI technology have paved the way for breakthroughs in speech recognition, natural language processing and machine translation. A new startup called Voicery now wants to leverage those same advancements to improve speech synthesis, too. The result is a fast, flexible speech engine that sounds more human — and less like a robot. Its machine voices can then be used anywhere a synthesized voice is needed — including in new applications, like automatically generated audiobooks or podcasts, voice-overs, TV dubs and elsewhere.\\n\\nBefore starting Voicery, co-founder Andrew Gibiansky worked at Baidu Research, where he led the deep learning speech synthesis team.\\n\\nWhile there, the team developed state of the art techniques in the field of machine learning, published papers on speech constructed from deep neural networks and artificial speech generation and commercialized its technology in production-quality systems for Baidu.\\n\\nNow, Gibiansky is bringing that same skill set to Voicery, where he’s joined by co-founder Bobby Ullman, who previously worked at Palantir on databases and scalable systems.\\n\\n“In the time that I was at Baidu, what became very evident is that the revolution in deep learning and machine learning was about to happen to speech synthesis,” explains Gibiansky. “In the past five years, we’ve seen that these new techniques have brought an amazing gains in computer vision, speech recognition and in other industries — but it hasn’t yet happened with synthesizing human speech. We saw that if we could use this new technology to build speech synthesis engines, we could do it so much better than everything that currently exists.”\\n\\nSpecifically, the company is leveraging newer deep learning technologies to create better synthesized voices more quickly than before.\\n\\nIn fact, the founders built Voicery’s speech synthesis engine in just two-and-half months.\\n\\nUnlike traditional voice synthesizing solutions, where a single person records hours upon hours of speech that’s then used to create the new voice, Voicery trains its system on hundreds of voices at once.\\n\\nIt also can use varying amounts of speech input from any one person. Because of how much data it takes in, the system sounds more human as it learns the correct pronunciations, inflections and accents from a wider variety of source voices.\\n\\nThe company claims its voices are nearly indistinguishable from humans — it even published a quiz on its website that asks visitors to see if they can identify which ones are synthesized and which are real. I found that you’re still able to identify the voices as machines, but they’re much better than the machine reader voices you may be used to.\\n\\nOf course, given the rapid pace of technology development in this field — not to mention the fact that the team built their system in a matter of months — one has to wonder why the major players in voice computing couldn’t just do something similar with their own in-house engineering teams.\\n\\nHowever, Gibiansky says that Voicery has the advantage of being the first out of the gate with its technology that capitalizes on the machine learning advancements.\\n\\n“None of the currently published research is quite good enough for what we wanted to do, so we had to extend that a fair bit,” he notes. “Now we have several voices that are ready, and we’re starting to find customers to partner with.”\\n\\nVoicery already has a few customers piloting the technology, but nothing to announce at this time as those talks are in various stages.\\n\\nThe company is charging customers an upfront fee to develop a new voice for a customer, and then charges a per-usage fee.\\n\\nThe technology can be used where voice systems exist today, like in translation apps, GPS navigation apps, voice assistant apps or screen readers, for example. But the team also sees the potential for it to open up new markets, given the ease of creating synthesized voices that really sound like people. This includes things like synthesizing podcasts, reading the news (think: Alexa’s “Flash Briefing”), TV dub-ins, voices for characters in video games and more.\\n\\n“We can move into spaces that fundamentally haven’t been using the technology because it hasn’t been high enough quality. And we have some interest from companies that are looking to do this,” says Gibiansky.\\n\\nVoicery, based in San Francisco, is bootstrapped save for the funding it received by participating in Y Combinator’s Winter 2018 class. It’s looking to raise additional funds after YC’s Demo Day.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Why 2018 will be the year apps go to the edge', 'If you’re running a software company today, it’s almost a foregone conclusion that most or all of your apps will run in the cloud. Likely Amazon or Google’s. It’s hard to imagine that this wasn’t always the case, but there are still some late adopters migrating their own physical data centers into managed ones. And, as with all trends in technology, this too shall pass. Just when you were getting comfortable with containers and auto-scaling, a new architecture emerges, swinging the pendulum back to a truly distributed world.\\n\\nWhat’s wrong with the cloud?\\n\\nA typical self-driving car generates up to 100MB of data per second from a combination of cameras, LIDARs, accelerometers and on-board computers. That data needs to be processed nearly instantly to keep the car on the road. With so much data to sift through, the current generation of cellular networks can’t keep up. By the time data arrives in the cloud, it will be too late. Instead, data needs to be processed as close to the sensors as possible, directly at the edge of networks, on the cars themselves.\\n\\nMost of us aren’t building or riding in self-driving cars (yet), but there’s a good chance we’re already interacting with edge computing every day. Neural networks in smart speakers in almost 40 million American homes are listening for words like “Alexa,” “Siri” or “Google” and, according to Statista, 3 billion Snapchats are scanned for faces each day in order to add the addicting face filters. By the end of the year, 20 percent of smartphones globally will have hardware-accelerated machine learning capabilities.\\n\\nHow did we get here?\\n\\nAll of these apps and devices are made possible by two major trends: advances in deep learning algorithms that help computers see, hear and understand and the proliferation of specialized processors like GPUs and TPUs that can run these algorithms efficiently, even in mobile environments.\\n\\nNeural networks and deep learning aren’t new. In fact, the first artificial neural networks were created in the 1950s, and there have been multiple false starts since.This time, though, the abundance of labeled training data and compute power made it feasible to train these large models. Though AI research is still proceeding at a breakneck pace, fields like computer vision are starting to mature. Developers can choose from a variety of standardized model architectures, publicly available training data sets and tools. You no longer need a PhD just to get started. Technology is being democratized.\\n\\nTools and hardware are improving so quickly it’s hard to keep up.\\n\\nHardware is catching up, fast. Machine learning algorithms like neural networks are really just long sequences of matrix multiplications. Specialized processors like GPUs and newer neural processing units like those in Apple’s A11 Bionic chip and Google’s Tensor Processing Unit (TPU) are optimized for exactly these mathematical operations, offering 10-100x speedups over traditional CPUs while using less power overall. As major chip manufacturers roll out mobile-ready machine learning accelerators, every device will soon have the power to run the latest AI models.\\n\\nThe edge = new opportunity\\n\\nBig data, data science, machine learning and now deep learning have been slowly weaving their way into products and companies for the past decade. Most of the time, this happened behind the scenes, up in the cloud. Data warehouses and analytics pipelines process records en masse. Results are made accessible to end users through APIs and database queries. That’s not going away, but the edge presents a new opportunity to use the predictive capabilities of machine learning models more quickly.\\n\\nNow, the algorithms move to the data. Information is processed in real time, as soon as it’s captured by the sensor, and results are available immediately. In this latency-free world, entirely new user experiences are possible. Your phone’s screen becomes a portal to a world of augmented reality. Products can be personalized for a single user while private data never leaves the device. Applications become ambient and frictionless, anticipating questions and answering them before you ask.\\n\\nIt doesn’t take a PhD\\n\\nWhen done right, experiences made with AI and edge computing feel like magic, but building them is incredibly complex. There is a divide between the tech stacks used to train and deploy machine learning models in the cloud and the ones used to build applications for edge devices, like smartphones and IoT. Neural networks can replace thousands of lines of procedural code, but fail in unexpected, silent ways and need to be tested differently. Performance issues that can be solved by simply adding more compute or memory from a near infinite cloud call for specialized optimization when they occur out on edge devices we don’t control. Even the programming languages preferred by the cloud are different than those running applications on mobile devices.\\n\\nThis is starting to change. Tools and hardware are improving so quickly it’s hard to keep up. Heavyweights like Apple and Google have made mobile machine learning frameworks (Core ML and TensorFlow Lite, respectively) centerpieces of their latest developer offerings. More export options and better interoperability are being added to tools like AWS’s SageMaker, Azure’s ML Studio and IBM’s Watson Studio weekly.')\n",
      "('Facebook appoints a new AI research head', 'Facebook has a new executive coming on to unify its research efforts in the field of artificial intelligence.\\n\\nFormer IBM exec Jérôme Pesenti will be taking the lead at Facebook’s AI research (FAIR) arm, Quartz reports. Yann LeCun, who previously held the lead role, will be shifting to a more research-focused role where he will continue to drive the scientific agenda of FAIR in his new role as chief AI scientist, a Facebook spokesperson tells TechCrunch.\\n\\nThe division now has 130 employees, and one of Pesenti’s tasks will be leading the group as it further scales. In September, Facebook announced the opening of a FAIR lab in Montreal and just a few days ago, the company announced plans to double the size of its Paris AI research team to 100 people by 2022.\\n\\nPesenti joined IBM to lead the development of its Watson platform after the startup he co-founded, Vivisimo, was acquired by the company in 2012. He went on to later become the CEO of BenevolentTech. As VP of AI, Pesenti will report directly to CTO Mike Schroepfer.\\n\\n“There was a need for someone to basically oversee all the AI at Facebook, across research, development, and have a connection with product,” LeCun told Quartz. “AML and FAIR were reporting to the CTO, who no longer has the bandwidth to take care of that, given the increased importance of AI and more systems built around deep learning.”')\n",
      "('MIT’s new chip could bring neural nets to battery-powered gadgets', 'MIT researchers have developed a chip designed to speed up the hard work of running neural networks, while also reducing the power consumed when doing so dramatically – by up to 95 percent, in fact. The basic concept involves simplifying the chip design so that shuttling of data between different processors on the same chip is taken out of the equation.\\n\\nThe big advantage of this new method, developed by a team led by MIT graduate student Avishek Biswas, is that it could potentially be used to run neural networks on smartphones, household devices and other portable gadgets, rather than requiring servers drawing constant power from the grid.\\n\\nWhy is that important? Because it means that phones of the future using this chip could do things like advanced speech and face recognition using neural nets and deep learning locally, rather than requiring more crude, rule-based algorithms, or routing information to the cloud and back to interpret results.\\n\\nComputing ‘at the edge,’ as its called, or at the site of sensors actually gathering the data, is increasingly something companies are pursuing and implementing, so this new chip design method could have a big impact on that growing opportunity should it become commercialized.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Nvidia and GE’s Baker Hughes team on AI for oil and gas', 'Nvidia and GE subsidiary Baker Hughes want to bring the power of artificial intelligence to bear on the oil and gas industry – and do so throughout the space, covering everything from locating new oil sources, bringing it up to the surface, refining it for use and even delivering it to the end customer.\\n\\nBaker Hughes is working with Nvidia on making this happen using its range of AI-enabling GPU tech, including both its Nvidia DGX-1 supercomputers, which are large, data center powerhouses, to its DGX Stations for desktop supercomputer capabilities, and even the Nvidia Jetson AI, its platform for computing at the edge, which enables deep learning processing locally, rather than routing it through the cloud.\\n\\nNvidia’s full range of options is key to making this work – the oil and gas industry has needs that range from solving massive computing problems via simulation and data processing to anticipating new drill spots and how to deal with seismic activity at drilling sites, for instance, but it also needs platforms that can work installed on offshore platforms where connectivity is sketchy at best, and not incredibly bandwidth-rich, either.\\n\\nPart of this work is about locating and maximizing the return on natural resources, but it’s also about making sure the equipment runs and runs well, with models that predict wear and maintenance schedules, which could stop major malfunctions before they ever occur. That has benefits in terms of cost, for sure, but also in terms of the safety of site personnel working on rigs and using potentially dangerous hardware regularly.')\n",
      "('Virtual travel assistant Mezi acquired by American Express', 'American Express announced today that it has acquired Mezi, the AI-based virtual travel assistant, for an undisclosed amount. After the deal closes, Mezi will become a wholly-owned subsidiary of American Express. Its technology is already powering AskAmex, a personal concierge app for cardholders that launched its pilot program last year.\\n\\nMezi users send requests for flights, hotels and restaurant reservations through the app’s SMS-like interface. Mezi then makes suggestions and, once it figures out what users want to do, forwards requests to human assistants to finalize bookings. Founded in 2015 as a personal shopping chatbot, Mezi builds its own deep learning and natural language processing systems instead of using services by IBM Watson and other providers. Its co-founders, CEO Swapnil Shinde and his twin brother, CTO Snehal Shinde, previously launched Indian streaming music service Dhingana, which was acquired by Rdio in 2014.\\n\\nAfter selling Dhingana, the Shinde siblings created Mezi with the idea that while people might like to shop for fun, they rarely enjoy tracking down out-of-stock items, researching large appliances or making hotel and restaurant reservations, and would prefer to have a chatbot take over those tasks for them. The travel category turned out to be the most successful and in November 2016 Mezi pivoted to focus solely on flights, hotels and restaurant bookings. In addition to its iOS and Android app, Mezi also offered a white-label “travel-as-a-service” solution for travel agents.\\n\\nIn a press statement, American Express Digital Labs vice president Phil Norman said “With messaging emerging as the channel of choice for many, and the demand for unique experiences increasing, we believe the combination of Mezi’s capabilities with our global network of expert travel counselors creates a differentiated, high-touch service experience for our card members.”\\n\\nAccording to Crunchbase, Mezi raised a total of $11.8 million, including a $9 million Series A announced in July 2016 that was led by American Express Ventures, the company’s investment arm.')\n",
      "('SF Motors reveals first two EVs, aims to ship its first SUV by next year', 'SF Motors has revealed its first two models, electric vehicles aiming for 2018 production and 2019 street dates. The electric vehicle technology company with a headquarters in Silicon Valley, as well as a globe-spanning R&D footprint and manufacturing facilities in both China and the U.S., and it’s aiming to distinguish itself rom the established market with unique powertrains, autonomous features and shared technology development.\\n\\nTheir first two cars are the SF5 and SF7, a smaller and a mid-sized all-electric SUV. The SF5 is aiming to have a pre-order date of 2018, and will also aim to ship in 2019 to customers if all goes to plan. They’ll feature “proprietary” powertrain technology that will achieve 1000 horsepower and a 0-to-60 mph acceleration time of under 3 seconds.\\n\\nRange is rated at over 300 miles by EPA standards, SF Motors says, thanks to a “patented” battery system that emphasizes safety through design. The automaker is also working on a future battery system that would incorporate battery units directly into the vehicle’s chassis, for a more streamlined design.\\n\\n[gallery ids=\"1613830,1613831,1613832,1613833,1613834,1613835,1613836,1613837,1613838,1613839,1613840\"]\\n\\nA lot of SF Motors’ secret sauce comes down to in-house development of key systems, the automaker says, including its own in-house electronic controller design. It’s also testing “protective autonomy,” systems that use deep learning and LiDAR sensor perception to create a “safer ride,” with road tests in California, Michigan and China currently in progress as of earlier this month. The full extent of this system and how it works hasn’t been detailed, but it will automated driving tasks with “minimal human input” required, per SF Motors.\\n\\nThe company is debuting its first target production vehicles today, but it’s also hoping to potentially provide its proprietary powertrain and battery tech design to other automakers via license, which could help it get over the early hurdles of production ramp that most young EV companies hoping to take on Tesla and other legacy automakers face.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Trueface.ai integrates with IFTT as the latest test-case of its facial recognition tech', 'Trueface.ai integrates with IFTTT as the latest test-case of its facial recognition tech\\n\\nTrueface.ai, the stealthy facial recognition startup that’s backed by 500 Startups and a slew of angel investors, is integrating with IFTTT to allow developers to start playing around with its technology.\\n\\nChief executive Shaun Moore tells me that the integration with IFTTT represents the first time that facial recognition technology will be made available to the masses without the need to understand complex code.\\n\\nThe company initially started as both a hardware and software vendor, but back in 2017 Moore said that the company stripped out its hardware component and focused on its software.\\n\\n“We ended up taking a really broad approach and letting [hardware developers] do whatever they want with it,” Moore tells me.\\n\\nFocusing on digital identification and verification tools, Trueface.ai sells technology that it says can be used to verify a request to open a bank account or for digital document notarization. “We can do that remotely and verify proof of possession and identity,” says Moore.\\n\\nThe goal, says Moore, is to make facial recognition available to everybody. And IFTT’s integration is one step to make that happen, because it will familiarize product developers and makers with the toolkit, Moore says.\\n\\n[gallery ids=\"1591251,1591252,1591253,1591254\"]\\n\\n“We just saw this as a way to launch this technology on a third party where anyone could go in. If you’ve got a Lockitron, Trueface can take the picture and then IFTTT can unlock the door for anyone who has been given permission,” says Moore.\\n\\nThe technology uses deep learning models to detect, normalize, validate, match and identify faces. Back before its pivot to software, Trueface.ai actually came by TechCrunch’s New York offices to put the tech through its paces under its previous incarnation — Chui.\\n\\nArtificial intelligence models are trained with millions of facial images and — depending on the use case — are designed to generate mathematical representations of a human face (called embeddings).\\n\\nThese embeddings can be stored and referenced by other models to infer identity. The company’s spoof detection service is trained on attack examples to differentiate between a real human face and an image (goodbye lock-screen issues).\\n\\nWith the IFTTT integration, the company intends to show off its IDVerify product as a means to acclimate users with the technology.\\n\\nAlready, the company’s tech is interoperable with identity documents for more than 150 countries and can be used with Trueface.ai web and mobile applications.\\n\\n“The target demographic of what we were thinking were tinkerers and hackers who kind of want to build things themselves,” says Moore.')\n",
      "('Twitter’s Dorsey downplays acquisition possibilities, sees “strength to our independence”', 'If you’re a Twitter shareholder wondering if the company is about to get acquired, co-founder and CEO Jack Dorsey doesn’t want you to get that impression.\\n\\n“There’s a lot of strength to our independence,” he said, at the Goldman Sachs Technology and Internet Conference in San Francisco on Tuesday. Dorsey believes that it’s important for Twitter’s business that they’re “not constrained.”\\n\\nHe also voiced commitment to building a “sustainable” business and providing “return to our shareholders.” He added that Twitter has “a lot left to prove but we’re proving it little by little every day.”\\n\\nTwitter’s stock is down from its high of $69 in early 2014, but the company has had an impressive run in the past year, with shares going from $16.52 to $33.44 in the past 12 months.\\n\\nDuring its most troubled times, the company weighed selling to potential buyers like Salesforce, Microsoft or Disney. Since then, investors have continued to wonder if an acquisition is still in the cards.\\n\\nThe company was recently dealt a blow when Twitter COO Anthony Noto left to join SoFi as CEO. Some have wondered how Dorsey can continue to run both Twitter and Square, but Square’s stock has also done well, tripling in the past year.\\n\\nAt the conference, Dorsey spoke of his long-term vision for Twitter, and even downplayed the label “social network.” He views the platform as the “zeitgeist of what the world is thinking.”\\n\\nHe believes that video will continue to be a big revenue driver. “Video is our fastest growing in terms of monetization,” said Dorsey. He said that Twitter will be investing in “self-serve technologies” to make it easier for producers to use the service.\\n\\nDorsey also hopes to expand on the “moments” section, which allows users to see a collection of tweets around a particular topic. “There’s a lot of value in the what happened in the recap nature of our service,” he said, adding that “we’ve barely scratched the surface of it with a product like ‘moments.’ ”\\n\\nHe also spoke of using machine learning and deep learning to tailor the service to individual’s preferences. “We are failing today because we don’t have a lot of the personalizations we need,” he said. “If they have to search navigation to find it, we have failed.”')\n",
      "('Sylabs launches Singularity Pro, a container platform for high performance computing', 'Sylabs, the commercial company behind the open source Singularity container engine, announced its first commercial product today, Singularity Pro.\\n\\nSylabs was launched in 2015 to create a container platform specifically designed for scientific and high performance computing use cases, two areas that founder and CEO Gregory Kurtzer, says were left behind in the containerization movement over the last several years. (For an explanation of containers, see this article.)\\n\\nDocker emerged as the container of engine of choice for developers, but Kurtzer says the container solutions developed early on focused on microservices. He says there’s nothing inherently wrong with that, but it left out some types of computing that relied on processing jobs instead of services, specifically high performance computing.\\n\\nKurtzer, who didn’t exactly just fall off the open source turnip truck, had more than 20 years of experience as a high performance computing architect working at the US Department of Energy Lab, where he founded CentOS, an open source enterprise Linux project and Warewulf, which he says has become the most utilized stateless HPC cluster provisioner.\\n\\nHe decided to shift his attention to containers when founded Sylabs and launched the first open source version of Singularity in April, 2016. Even then, he had a vision of creating a commercial version of the product. He saw Singularity as a Docker for HPC environments, and would run his company in a similar fashion to Docker, leading with the open source project, then building a commercial business on top of it — just as Docker had done.\\n\\nKurtzer now wants to bring Singularity to the enterprise with a focus not just on the HPC commercial market, but other high performance computing workloads such as artificial intelligence, machine learning, deep learning and advanced analytics.\\n\\n“These applications carry data-intensive workloads that demand HPC-like resources, and as more companies leverage data to support their businesses, the need to properly containerize and support those workflows has grown substantially,” Kurtzer wrote in a blog post announcing the enterprise product.\\n\\nEven though Singularity is designed to handle different kinds of workloads, it still works with container orchestration tools, specifically Kubernetes and Mesos, and it is also compatible with Microsoft’s Azure Batch tool and other cloud tools.\\n\\nKurtzer indicated Sylabs currently has 12 employees, and is operating on an undisclosed amount of seed money. It was funded by RStor, a startup itself currently operating in stealth mode.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('More details of Blossom Capital emerge, a new fund co-founded by ex-Index VC Ophelia Brown', 'It was well-known that ex-Index VC Ophelia Brown was ‘doing a fund of her own’ after she departed Robin and Saul Klein’s LocalGlobe in April, the London-based seed investment firm where she was a General Partner. However, it wasn’t until last month that Blossom Capital, as Brown’s new firm is called, broke cover.\\n\\nAs reported by Business Insider, Brown has teamed up with former Uber China executive Candice Lo, ex-Index investment team member Imran Ghory, and Deliveroo’s chief technology officer Mike Hudack to start the new VC fund. Blossom Capital is thought to be targeting a raise of between $75 million and $100 million, and could have a first closing and official launch soon.\\n\\nNow, thanks to a newly updated website and conversations I’ve had with sources close to the new VC firm, more details have emerged. (Brown declined to comment when contacted by TechCrunch.)\\n\\nNoteworthy, despite Brown’s seniority in terms of venture capital experience, the Blossom website also lists Lo, Ghory, and Hudack as co-founders of the fund. That said, Hudack will remain as CTO at Deliveroo and a full time employee of the food ordering and delivery company. “His role at Deliveroo isn’t changing,” a source with knowledge of the situation tells me.\\n\\nI also understand that while the fund is targeting early stage tech startups, its remit is Series A not seed, a departure from Brown’s work at LocalGlobe. The new firm wants to plug what it deems a ‘Series A gap’. In Europe, promising and high quality founders often get funded by great angel investors and at seed but sometimes struggle at Series A due to a shortage of capital at that stage.\\n\\nSimilarly, I’m told Blossom wants to do Series A rounds of $5 million-plus (i.e. more in line with Silicon Valley) and in turn help companies have enough runway to give them 18 months or more and expand globally. “[They’re] trying to push the European ecosystem forward, it’s about true high conviction investing,” is how one source frames it.\\n\\nHowever, just like basically every other new VC firm these days, not least in Europe, Blossom’s website also talks up the firm’s entrepreneur friendliness and operational experience — although in Lo and Hudack‘s case it is somewhat warranted. Along with Deliveroo, Hudack was also at Facebook in California pre-IPO and at Facebook in London a bit later. Lo, of course, had a major role at Uber.\\n\\nThe Blossom website also makes virtue of the firm’s openness to cold pitching in a bid to find promising companies that might not have the investor network or time required to produce warm introductions. And whilst I doubt the four VCs will struggle too much with deal flow, it’s good to see a lack of aloofness, on paper at least, and perhaps points to Brown’s commitment to diversity. Ultimately, however, the proof will be in the backgrounds of the founders it backs.\\n\\nIn addition to Blossom, Brown has founded ALT (Ambitious Ladies in Tech), described as a mentor network that endeavours to help women in technology startups achieve their career goals through coaching, skills development and networking. But, as well as being 50 percent female founded, I’m told Blossom is keen to be perceived as diverse in the broader sense.\\n\\n“This is not about being a female-led [firm],” says a source close to the fund. “It’s a truly diverse team (gender, ethnicity) which is how investment teams should be in order to make the best investment decisions and to give the best all-rounded support to a portfolio”.\\n\\nTo that end, the Blossom website lists two portfolio companies, suggesting the new VC is already in the market doing deals. They are YC startup Spell, which provides infrastructure for “AI and Deep Learning”, and Fat Lama, a sharing economy-styled marketplace for renting out various items you own.')\n",
      "('Foxconn to plug at least $340M into AI R&D over five years', 'Foxconn to plug at least $340M into AI R&D over five years\\n\\nManufacturing giant Foxconn has said it will make a major investment in artificial intelligence-based R&D as it looks for new business growth opportunities in a cooling global smartphone market, Nikkei reports.\\n\\n“We will at least invest some 10 billion New Taiwan dollars ($342M) over five years to recruit top talent and deploy artificial intelligence applications in all the manufacturing sites,” said chairman Terry Gou.\\n\\n“It’s likely that we could even pour in some $10BN or more if we find the deployments are very successful or can really generate results.”\\n\\nGou added that the ambition is to become “a global innovative AI platform rather than just a manufacturing company”.\\n\\nData put out this week by Strategy Analytics records a 9 per cent fall in global smartphone shipments in Q4 2017 — the biggest such drop in smartphone history — which the analyst blames on the floor falling out of the smartphone market in China.\\n\\n“The shrinkage in global smartphone shipments was caused by a collapse in the huge China market, where demand fell 16 percent annually due to longer replacement rates, fewer operator subsidies and a general lack of wow models,” noted Strategy Analytics’ Linda Sui in a statement.\\n\\nOn a full-year basis, the analysts records global smartphone shipments growing 1 percent — topping 1.5 billion units for the first time.\\n\\nBut there’s little doubt the smartphone growth engine that’s fed manufacturing giants like Foxconn for so long is winding down.\\n\\nThis week, for example, Apple — Foxconn’s largest customer — reported a dip in iPhone sales for the holiday quarter. Though Cupertino still managed to carve out more revenue (thanks to that $1k iPhone X price-tag). But those kind of creative pricing opportunities aren’t on the table for electronics assemblers. So it’s all about utilizing technology to do more for less.\\n\\nAccording to Nikkei, Foxconn intends to recruit up to 100 top AI experts globally. It also said it will recruit thousands of less experienced developers to work on building applications that use machine learning and deep learning technologies.\\n\\nEmbedding sensors into production line equipment to capture data to feed AI-fueled automation development is a key part of the AI R&D plan, with Foxconn saying earlier that it wants to offer advanced manufacturing experiences and services — eyeing competing with the likes of General Electric and Cisco.\\n\\nThe company has also been working with Andrew Ng’s new AI startup Landing.ai — which is itself focused on plugging AI into industries that haven’t yet tapping into the tech’s transformative benefits, with a first focus on manufacturing — since July.\\n\\nAnd Gou confirmed the startup will be a key partner as Foxconn works towards its own AI-fueled transformation — using tech brought in via Landing.ai to help transform the manufacturing process, and identify and predict defects.\\n\\nQuite what such AI-powered transformation might mean for the jobs of hundreds of thousands of humans currently employed by Foxconn on assembly line tasks is less clear. But it looks like those workers will be helping to train AI models that could end up replacing their labor via automation.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Pandora takes on Spotify with dozens of personalized playlists built using its Music Genome', 'Just ahead of Spotify’s public debut planned for early April, Pandora is punching its rival where it hurts: personalized playlists. Spotify’s playlists customized to its individual users – like Discover Weekly, Release Radar and Daily Mix – have been a key draw for its service, but Pandora believes it can do even better. The company today is launching its own take on personalization with playlists built to fit users’ moods, activities, and genres, which are all powered by Pandora’s Music Genome.\\n\\nThe Music Genome is Pandora’s biggest asset in its battle with Spotify. This music information database has been in development for over a decade and is capable of classifying music at the song level across 450 different attributes – “genes” that can be as specific as what types of strings are on the guitar, for example.\\n\\nThe Music Genome already powers Pandora’s other personalization efforts, like its Thumbprint Radio, or the feature where it can automatically play similar music when you reach the end of a playlist you created. But it’s never yet been used to build specific playlists for Pandora users like this.\\n\\n“We’ve been building out, for many years, a collection of well over 75 machine learning algorithms and techniques to help drive content discovery and delivery,” explains Chris Phillips, Pandora’s Chief Product Officer, of the personalization technology. “What we’re doing is what we believe is the bleeding edge of deep learning algorithms,” he says.\\n\\nThe new playlists are built using these various machine learning algorithms, combined with the raw audio analysis from the Music Genome, more traditional collaborative filtering methods, and in-house editorial curation.\\n\\nAt launch, Pandora is capable of creating over 60 personalized playlists – but that doesn’t mean users will log in and see 60 new playlists immediately.\\n\\nInstead, Pandora will roll them out in response to users’ listening activity. For example, if you’re playing a lot of upbeat pop music, you may see a playlist appear called “Your Party Soundtrack.”\\n\\nThere are also playlists for things like chilling out, focusing, working out, and more, as well as those for various moods – like “happy” or “rainy days” – or favorite genres, like hip hop or pop. And Pandora plans to quickly expand the types of playlists it offers here, with new ones arriving for certain events or cultural activities – like the holidays – and others.\\n\\nThe playlists won’t contain just those tracks you explicitly liked – meaning those you thumbs’d up or added to a playlist of your own. They’ll also surprise you with tracks Pandora believes you’ll like, such as new releases, deep cuts from favorite bands, or other songs that fit well and make sense for you, as determined by Pandora’s algorithms and analysis.\\n\\n“When we think about what the competition’s doing, they’re really putting more generic buckets of music together,” says Phillips, seemingly referencing Spotify’s personalized playlists. “And they have a generic name for the playlist, whereas I can go right in [to Pandora’s personalized playlists] and say, “oh this is mood I’m in,’ and it’s spot on.”\\n\\n[gallery ids=\"1613177,1613176,1613175,1613174,1613173,1613172,1613167,1613161\"]\\n\\nThat’s not to say that other music services don’t have playlists available for users’ moods or activities – it’s just that they’re not personalized to the individual.\\n\\nIn addition, after using Spotify for some time, it can be hard to find the right selection of music you want to hear from its longer list of “Daily Mix” playlists, as it continues to create more mixes over time.\\n\\nPandora, on the other hand, is going after a more lean-back experience – something that ties into its radio roots, where it builds custom stations you like and continue to configure with a thumbs up or down. Spotify, meanwhile, has begun to overly rely on the power of its popular curated playlists – like Rap Caviar – to cement its importance in the world of music. (And Rap Caviar’s curator just left Spotify for YouTube.)\\n\\nSpotify’s more recent launches, in terms of new personalized playlists, haven’t been as innovative as Discover Weekly. It has since rolled out fun – but not as useful – playlists like those that look back on your summer fun, and one with nostalgic tracks from your youth.\\n\\nPandora’s new playlists will start rolling out today to Pandora’s Premium subscribers. That’s the top tier of Pandora’s paid offerings, and Pandora’s equivalent to Spotify or Apple Music. If a playlist is created for you, it will appear in the “Browse” section under “Featured Playlists” in the Pandora mobile app.\\n\\nOnce live, the playlists will be updated weekly to keep them fresh – so if you want to save a playlist in full, you’ll need to duplicate it to your own collection.\\n\\nYou can also share the playlist with friends, including free users, through Pandora’s newer “Access” feature.\\n\\nThis offers Pandora users a way to temporarily access Premium by watching a video ad first. It’s a way to test drive the Premium product without paying, then transition back to the free radio service after your session. The feature was launched in December, and has since been used by “millions,” Phillips says.\\n\\nThe addition of personalized playlists could help Pandora give its subscription business a jump at a time when it’s starting to see some growth. The company reported in Q4 a 25 percent year-over-year increase in subscribers, which now number 5.48 million. In total, Pandora counted 74.7 million active users in the quarter.\\n\\nThat still puts it behind Spotify, which claims 157 million active users, 71 million of whom are paying customers. And Spotify said it’s projecting as many as 96 million subscribers by year-end 2018. Apple Music, meanwhile, is well ahead of Pandora in terms of paying customers too, with 38 million subscribers. Even Amazon has a stake in this market, with an estimated 16 million subscribers.\\n\\nPandora says a certain percentage of its Premium users will start seeing personalized playlists appear as soon as today.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Announcing the agenda for TC Sessions: Robotics at UC Berkeley’s Zellerbach Hall on May 11', 'Announcing the agenda for TC Sessions: Robotics at UC Berkeley’s Zellerbach Hall on May 11\\n\\nTechCrunch is partnering with UC Berkeley on May 11 to produce TC Sessions: Robotics, a one-day show focused on emerging robotic technologies and the startup scene.\\n\\nThe editorial team had a blast planning this agenda as, between the Bay Area’s universities and startup ecosystem, there are far more robotics-driven technology projects and startups on the West Coast than most appreciate. There is no question that the huge steps forward in AI, sensors and GPUs are quickly shifting robotics to the fast lane of the startups ecosystem.\\n\\nTickets are currently available and at an early-bird rate – buy your tickets here before prices increase.\\n\\nAgenda\\n\\nTC Sessions: Robotics\\n\\nFriday, May 11, 2018 @ Berkeley’s Zellerbach Hall\\n\\n9:00 AM – 9:05 AM\\n\\nOpening Remarks with Matthew Panzarino (TechCrunch)\\n\\n9:05 AM – 9:25 AM\\n\\nGetting A Grip on Reality: Deep Learning and Robot Grasping\\n\\nKen Goldberg (UC Berkeley)\\n\\nBuilding on 35 years of research, Professor Goldberg will discuss the “New Wave” in robot grasping for e-commerce warehouse order fulfillment.\\n\\n9:25 AM – 9:50 AM\\n\\nThe Future of the Robot Operating System (ROS)\\n\\nBrian Gerkey (Open Robotics) and Morgan Quigley (Open Robotics). Melonee Wise (Fetch Robotics)\\n\\nThe open ROS has been a boon to robot creators. Can ROS keep up with the dazzling array of new demands?\\n\\n9:50 AM – 10:10 AM\\n\\nEyes, Ears and Data: Robots and GPUs\\n\\nDeepu Talla (NVIDIA)\\n\\nNVIDIA is the market-leader in AI and deep learning technologies crucial to robots, drones and a variety of other autonomous machines.\\n\\n10:10 AM – 10:30 AM\\n\\nThe Best Robots on Four Legs\\n\\nMarc Raibert (Boston Dynamics)\\n\\nBoston Dynamics rocked the world with the DARPA-funded Big Dog, and founder Marc Raibert will show off its latest creation, SpotMini.\\n\\n10:30 AM – 10:50 AM\\n\\nCoffee Break\\n\\n10:50 AM – 11:20 AM\\n\\nOld McDonald Needs a Robot\\n\\nDan Steere (Abundant Robotics), Brandon Alexander (Iron Ox), Sebastian Boyer (Farmwise), and Willy Pell (Blue River Technology)\\n\\nThe future of agribusiness is robots, and these founders are already putting automated farm workers in the fields.\\n\\n11:20 AM – 11:40 AM\\n\\nTeaching Robots New Tricks with AI\\n\\nPieter Abbeel (UC Berkeley, Embodied Intelligence)\\n\\nThe latest developments in AI can extend what robots do and make it possible for anyone to teach a robot new skills, without costly re-reprogramming. Professor Abbeel’s Embodied Intelligence is taking that technology to market.\\n\\n11:40 AM – 12:05 PM\\n\\nMaking Robots Less Robotic\\n\\nAyanna Howard (Georgia Tech), Leila Takayama (UC Santa Cruz) and Patrick Sobalvarro (Veo Robotics)\\n\\nRobots and humans are working and living together more than ever, and that means we have to watch out for one another — literally.\\n\\n12:05 PM – 1:00 PM\\n\\nLunch and Workshops\\n\\nWORKSHOP: DARPA and the Subterranean Challenge\\n\\nDr. Timothy Chung (DARPA)\\n\\nA primer on how to work with DARPA and DARPA’s latest robotics challenge.\\n\\n1:00 PM – 1:20 PM\\n\\nTo be announced.\\n\\n1:20 PM – 1:50 PM\\n\\nVenture investing in Robotics\\n\\nRenata Quintini (Lux Capital), Chrissy Meyer (Root Ventures) and Rob Coneybeer (Shasta Ventures), Chris Evdemon (Sinovation Ventures)\\n\\nHas robotics become a mainstream investment focus for venture capitalists?\\n\\n1:50 PM – 2:10 PM\\n\\nBetting Big on Robotics\\n\\nAndy Rubin (Playground Global)\\n\\nThe creator of the Android mobile operating system and former head of Google’s robotics division, Andy Rubin wants Playground Global to invest in all things robotics.\\n\\n2:10 PM – 2:35 PM\\n\\nFrom the Lab Bench to Term Sheet\\n\\nManish Kothari (SRI), Kaijen Hsiao (Mayfield Robotics) and Paul Birkmeyer (Dash Robotics, Dishcraft Robotics)\\n\\nResearchers in AI and robotics are well positioned to launch startups, but what does that transition look like?\\n\\n2:35 PM – 2:45 PM\\n\\nDemo to be announced.\\n\\n2:45 PM – 3:05 PM\\n\\nCoffee Break\\n\\n3:05 PM – 3:30 PM\\n\\nAutonomous Systems\\n\\nRaquel Urtasun (Uber), Alex Rodrigues (Embark Trucks)\\n\\nHow and when will autonomous vehicles (safely) take to the roads in meaningful numbers?\\n\\n3:30 PM – 3:50 PM\\n\\nWhat Robots Can Learn from Nature\\n\\nRobert Full (UC Berkeley)\\n\\nWhen it comes to mobility, the animal world is full of elegance and adaptation as well as lessons for robotics. Professor Full has built those bio-inspired robots.\\n\\n3:50 PM – 4:10 PM\\n\\nThe Future of Transportation\\n\\nChris Urmson (Aurora)\\n\\nMany of the first generation self-driving cars will carry systems from Aurora, and Chris Urmson, former head of Google’s self-driving car project, knows the technical challenges and promise.\\n\\n4:10 PM – 4:40 PM\\n\\nSession to be announced.\\n\\n4:40 PM – 5:00 PM\\n\\nBuilding Stronger Humans\\n\\nHomayoon Kazerooni (SuitX)\\n\\nWearable robotics have already provided mobility to the paralyzed; they will do much more in the near future. Professor Kazerooni is one of the field’s pioneers as well as leading entrepreneurs.\\n\\n5:00 PM -7:00 PM\\n\\nReception\\n\\nEarly-bird tickets are on sale now – buy your tickets here before prices increase.\\n\\nStudents can save 90% on tickets by booking here.\\n\\nIf you’re interested in a sponsorship, contact us.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('betaworks VisionCamp introduces seven new AR/computer vision companies', 'More than ten years ago, betaworks launched to foster an ecosystem of startups focused on the intersection of media and consumer behavior. While the mission hasn’t changed, the structure has seen some tweaks. The company has introduced its own venture arm, led by Matt Hartman, as well as the more recent launch of betaworks Studios.\\n\\nBut nestled gently between the two are betaworks Camps program. Camps are a sort of hyper-specific accelerator program, within which a small cohort of early-stage startups build out their products within a certain theme, complete with the full resources of betaworks (marketing, legal, space, etc.) as well as a small investment.\\n\\nCamps first launched with BotCamp, followed shortly by VoiceCamp, and today the graduates of VisionCamp are showing off their wares for the first time at Demo Day.\\n\\nCamera IQ\\n\\nCamera IQ calls itself a camera experience manager. The company works with brands and publishers to develop virtual worlds for customers, with partners including Spotify, Neiman Marcus and Viacom. The technology integrates AR toolkits and mobile OSes with brands native apps to offer different experiences for consumers. Camera IQ was founded by Allison Wood and Sonia Tsao. The founders say that the camera represents the next great consumption experience, as well as the next great transaction experience. The company hopes to sit at that intersection.\\n\\nFacemoji\\n\\nLivestreaming and FaceTime are now accessible to everyone, but not everyone wants to show their face on these platforms. Enter Facemoji. The startup offers 3D avatar webcams that streams your facial expressions via the avatar without ever showing your real likeness. The company was originally focused on gamers who stream on Twitch, with plans to expand to video chat. Facemoji was founded by Robin Raszka and Tom Krcha.\\n\\nLeo\\n\\nOriginally called Surreal, Leo offers a vast marketplace of AR objects, stamps and artwork so that users can change the world around them. Leo has raised $1.5 million in seed and has relationships with upwards of 2,000 artists on the platform. The company, which was founded by Dana Loberg and Sahin Boydas, makes money by sharing revenue with artists who create objects for the platform.\\n\\nNumina\\n\\nNearly half of land area in cities is made up of streets, sidewalks and parks, and cities have no data or insights on these spaces. Numina partners with cities to place computer vision sensors on light poles in these areas and offer anonymous flow data about pedestrians in these spaces. The company offers an API for streets, as well, to give developers access to real-time activity and a backlog of activity for their apps, whether it’s for mobility, insurance, real estate, or logistics. Numina was founded by Tara Pham and Martin McGreal.\\n\\nSelerio\\n\\nSelerio brings together the real world and the virtual world by using computer vision to map the layout and objects in a room and replace them with a virtual world. Imagine putting old-school Victorian furniture inside an existing space. The company uses deep learning and computer vision in its technology, which was spun out of Cambridge University. Selerio offers an SDK to developers and is currently being integrated with Apple’s ARKit. Selerio was founded by Ghislain Tasse and Flora Ponjou Tasse.\\n\\nStreem\\n\\nStreem supports the professional home services industry by using computer vision, machine learning, and AR to capture vital information (like model, make and serial number) through a simply live video chat. Through Streem’s technology, service pros can capture information, take measurements and save notes without ever stepping foot in the client’s home, letting them offer quotes much faster and solve the problem in one try. Streem was founded by Ryan Fink and Sean Adkinson.\\n\\nTrash TV\\n\\nDespite the fact that capturing and editing video is more accessible than ever, video editing remains a time-consuming and tedious process. The Trash TV app uses computer vision and AI to edit consumer videos into something beautiful and usable. The company uses a stock video repository that includes proof of creation to fill in the gaps. Trash TV was founded by Hannah Donovan and Anton Marini.\\n\\nThis is the third of betaworks’ Camps. The next one, according to Camps General Manager Danika Laszuk, is focused on the intersection of live streaming and participatory audiences. Dubbed LiveCamp, betaworks hopes to find startups evolving the space as Twitch streaming and apps like HQ continue to pull in large viewerships and the lines between performer and audience are blurred.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Reverie Labs uses new machine learning algorithms to fix drug development bottlenecks', 'Developing new medicines can take years of research and cost millions of dollars before they are even ready for clinical trials. Several biotech startups are using machine learning to revolutionize the process and get drugs into pharmacies more quickly. One of the newest is called Reverie Labs, which is part of Y Combinator’s latest batch. The Boston-based company wants to fix a critical bottleneck in the drug development process by speeding up the process of identifying promising molecules using recently published machine learning algorithms.\\n\\nReverie Labs’ founders Connor Duffy, Ankit Gupta and Jonah Kallenbach, who named their company after a pivotal detail in the HBO series “Westworld,” explain that its tech analyzes early ideas for molecules from pharmaceutical scientists and suggests possible improvements to shorten the amount of time it takes to reach clinical trials. Duffy says Reverie Labs’ ambition is to “become a full service molecule-as-a-service company.” It’s already partnered with several biotech companies and academic institutes working on treatments for diseases including influenza and cancer.\\n\\nReverie Labs specializes in the lead development stage, which is when researchers focus on prioritizing and optimizing molecules so they can go to animal and human clinical trials more quickly. Pharmaceutical scientists need to first identify the proteins that cause a disease and then find molecular compounds that can bind to those proteins. Then it becomes a process of elimination as they narrow down those molecules to ones that not only create the results they want, but are also suitable for animal and human studies.\\n\\nBefore clinical trials can start, however, they need to evaluate molecules very carefully in order to understand things like how they are metabolized by the body and their potential toxicity.\\n\\n“I’ve heard it compared to juggling eight balls at once or playing whack-a-mole,” says Duffy. “You want your compound to be very safe before you put it in people, you want to be efficacious and go where you want it to go in your body and you don’t want side effects. There are a lot of problems drug companies need to think about before putting a molecule in a human, and when you fix one problem, you often come up with another problem. We want to alleviate that by looking at all problems at the same time.”\\n\\nLead development is very labor intensive and requires the work of many medicinal chemists. Reverie Labs’ founders say it often takes more than $100 million and two years per drug before a final selection of molecules are ready for clinical trials. Reverie Labs wants to set itself apart from other startups focused on solving the same problem by taking recently-discovered machine learning techniques, and applying them to drug development.\\n\\n“The machine learning algorithms we implemented are some of the most promising advances that have been published in the past couple of years,” says Kallenbach.\\n\\nFirst, molecules are “featurized,” or turned into representations that work with machine learning algorithms. Reverie Labs’s tech creates proprietary featurizations based on quantum chemical calculations, then uses them to analyze the molecules’ properties and how they may act in the body. Afterwards, it selects molecules that have the potential to do well in clinical trials or suggests new molecules based on what properties scientists need.\\n\\nIn addition to the machine learning algorithms it uses, Reverie Labs founders say one of the startup’s key differentiators is that it trains its models on customers’ proprietary in-house datasets, which means the tech can integrate more smoothly into existing drug development workflows. Reverie Labs’ software also runs on customers’ virtual private clouds, giving them more security.\\n\\nWhile using artificial intelligence to develop new drugs seemed almost like science fiction just a few years ago, the space is developing quickly. Last month, BenevolentAI, one of the first companies to apply deep learning to drug discovery, bought biotech company Promixagen’s operations in the United Kingdom, which it says will make it the first artificial intelligence company to cover the entire drug research and development process. Atomwise, another AI-based drug discovery startup, announced at the beginning of this month that it has raised a $45 million Series A. Other notable startups include Nimbus Therapeutics and Recursion Pharmaceuticals.\\n\\nThe process of creating new drugs is currently very complicated, slow and extremely expensive. With so much room for improvement, the work done by various AI-based startups to improve the process don’t necessarily overlap.\\n\\n“The space doesn’t seem like a zero sum game at all,” says Gupta. “Many players can be involved and the fact that other startups are interested shows that there is legitimacy to the technology.”\\n\\n“The end result is trying to deliver life-saving cures faster and more cheaply,” adds Kallenbach. “We don’t really feel any competitiveness. We want everyone to succeed.”')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Otter’s new app lets you record, transcribe, search and share your voice conversations', 'An app called Otter, launching today, wants to make it as easy to search your voice conversations as it is to search your email and texts. The idea to create a new voice assistant focused on transcribing everyday conversations – like meetings and interviews – comes from Sam Liang, the former Google architect who put the blue dot on Google Maps, then later sold his next company, location platform Alohar Mobile to Alibaba.\\n\\nAlong with a team that hails from Google, Facebook, Nuance, Yahoo, as well as Stanford, Duke, MIT and Cambridge, Liang’s new company AISense has been developing the technology underpinning Otter over the past two years.\\n\\nEssentially, a voice recorder that offers automatic transcription, Otter is designed to be able to understand and capture long-form conversations that take place between multiple people.\\n\\nThis is a different sort of voice technology than what’s been developed today for voice assistance – as with Alexa or Google Assistant.\\n\\n“The existing technologies are not good enough for human-to-human conversations,” explains Liang. “Google’s voice API has been trained to optimize voice search,” he says, adding that when people talk to voice assistants, it’s typically only one person talking and they tend to speak more slowly and clearly than usual. They also often ask shorter questions, like “what’s the weather?,” not carry on long conversations.\\n\\n“Human meetings are much more complicated,” Liang says. “It usually involves at least two people, and the people could talk for an hour. It’s a long-form conversation.”\\n\\nWith Otter, the goal is to capture those conversations – meetings, interviews, lectures, etc. – and turn them into a searchable archive where everything said is immediately transcribed by AISense’s software.\\n\\nToday, this is possible through Otter’s new mobile app for iOS and Android, as well as a web interface that also supports file uploads for instant transcriptions.\\n\\nTo use the app, there’s nothing you have to do beyond pressing the “record” button. The voice conversation is recorded, then made available for playback with the audio synced up with the transcribed text. You can also share the recording with others right from the app. The data Otter creates is stored in an encrypted format in the cloud.\\n\\nThe entire technology stack, including speech recognition, was built in-house. The company is not using existing speech recognition APIs, because they wanted to improve the accuracy, and optimize for multiple speakers, says Liang.\\n\\nTo identify when someone else starts talking, AISense uses a technology called diarization to separate each individual speaker; it then generates a voice print for each person’s voice. Broadly speaking, this is like the voice equivalent to facial recognition, with the voice print being used to identify the speaker going forward.\\n\\nLiang says building a system like Otter’s wasn’t possible before.\\n\\n“Four years ago, there were tremendous advances in deep learning and A.I., and suddenly, the accuracy became much higher,” he notes. “It also requires a lot of CPU power, GPU power, and a lot of storage…these became much more affordable today compared to five or ten years ago,” Liang adds.\\n\\nThe system, at launch, is not perfect, but shows much potential.\\n\\nThe AI technology was able to differentiate between speakers as promised, from what I’ve seen in limited testing, but it doesn’t catch every word of a conversation. It also misses the exact word at times, too – for example, dropping the “s” off a word like “helps,” and recording it as “help.”\\n\\nReading back through the transcription reminds me of reading a transcribed voicemail on iPhone – you get the gist of what was being said, but you have to play it back to truly understand the message.\\n\\nThat said, Otter was able to function in real-world environments. I tested it, for example, in a coffee shop with music playing, and it was still able to capture what was said to some extent.\\n\\nThe resulting transcript, however, breaks up the speech oddly. Sentences are cut off right in the middle with the next line in the transcript continuing the sentence on a new line. This makes it more difficult to read back through the transcription because our mind is trained to see a new line as a new paragraph – or at least, a pause.\\n\\nBut the system is useful for just getting to the right part of a long recording, so you can then more carefully transcribe a key part or quote, for example.\\n\\nWhat I did like was the tag cloud at the top of the transcript, where Otter identified words that were used a lot in a conversation. You could click on these words to jump to that part of the transcription.\\n\\nLiang envisions a number of potential use cases for AISense’s technology, including in enterprise, health care, education, and more.\\n\\nThe company has already licensed its transcription technology to web conferencing platform Zoom, but the goal for now is not to generate revenue through a licensing business, but an enterprise version of Otter that will offer more controls, as well as a premium tier for the currently free version of the consumer app.\\n\\nA future release will allow for recording phone calls, but for now, the app focuses on in-person conversations.\\n\\nAISense, to date, has raised $13 million in funding. Horizons Ventures – a backer of Viv, DeepMind, Siri, Slack, and others – led the $10 million Series A. Also participating were Bridgewater Associates, i-Hatch Ventures, MetaLab, Jay Markley, and Boston investors Jim Pallotta and Stu Porter.\\n\\nSeed investors include Tim Draper through Draper Associates and Draper Dragon; Dave Morin through Slow Ventures; David Cheriton; SV Tech Ventures, Danhua Capital, and 500 Startups.\\n\\nOtter is live today on iOS, Android, and web.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('This is the tech that NFL players are excited about in 2018', 'With the NFL season now over, players finally have time to focus on their passions and side projects, which can be anything from an externship on Capital Hill to angel investing in startups. Some of these activities are facilitated through the NFL Player’s Association, which actually runs a tech accelerator designed to trade player licensing rights for equity.\\n\\nThe accelerator is called One Team Collective, and is run by an executive board made up of leaders from the tech industry as well as a player advisory board with nine current and former NFL players.\\n\\nFounded about a year ago, the group has invested in four startups: WHOOP, StatMuse, Rep the Squad and CampusLore.\\n\\nTo kick off 2018, the accelerator released a list of the top five tech trends they think will be big in 2018, and we talked to Ahmad Nassar, chairman of the accelerator, to give us some context about how these trends will be important for the football world.\\n\\n1. Artificial Intelligence (AI) – Speech analytics, computer vision and deep learning will personalize fan experiences, while players seek the best possible performance avenues in the athletic arena.\\n\\nThe group has already invested in one AI-focused startup, which is StatMuse. The startup is essentially a sports statistics database that can be queried by natural language processing, and it also answers questions using the voice of your favorite athlete. While StatMuse is a fan-focused application of artificial intelligence, Nassar noted that they’re excited to see how AI can help optimize player performance by doing things like creating custom workout or nutrition schedules.\\n\\n2. Blockchain – Distributed ledgers, the foundation for cryptocurrencies such as Bitcoin, have garnered a tremendous number of headlines — and have the potential to disrupt mega-industries, including finance, data analytics/security and transaction settlement by eliminating transactional fraud. The application of blockchain is limitless with respect to any type of data management/transactional architecture and will have meaningful application to athlete-driven data.\\n\\nTo address the elephant in the room, Nassar immediately explained that the group is cognizant that everyone and their mothers currently think blockchain is today’s hot technology. He was quick to make a distinction between what he called “pop-crypto,” which in his opinion draws parallels to the internet bubble, and real blockchain technology, which has the opportunity to change the way we work with data. Nassar mentioned that WHOOP, a portfolio company that collects athlete performance data via a wearable, could be a beneficiary of blockchain technology. Essentially the group wants to be open-minded and look for opportunities where blockchain technology can benefit the companies with which they’re working.\\n\\n3. Customer Experience (CX) – Technologies integrating cyber with the physical to connect people, places and objects with data. Applications for robust CX will augment and transform everyday experiences, including the fan experience.\\n\\nNassar and the group are excited about how technology can help improve the customer (i.e. fan) experience. He noted that they’ve had some interesting discussions with AR companies that are building products to make fan interaction even stronger during and after games, and mentioned Pokémon GO as a product that successfully used AR to create a great customer experience.\\n\\n4. Cybersecurity – Cybersecurity, data theft and fraud are top concerns for customers, corporations and athletes. These privacy fears are resulting in more hesitation to use digital tools, meaning this is an area where solutions are critical — and the market is ripe for innovation.\\n\\nWhile cybersecurity is less applicable to football when compared the other categories, the group still recognizes a huge market opportunity for innovative cybersecurity tech, and will look to get involved when they see an opportunity that makes sense.\\n\\n5. Consumer Product Innovation – Companies that embrace new technologies in nutrition, activewear, consumables and fanwear will gain competitive advantage — and fuel the future fan and athlete.\\n\\nThis is similar to customer experience, but instead focuses on tangible goods and consumables that are designed to help the fan experience. One interesting company in this category that the group has already invested in is Rep The Squad, a subscription service to borrow expensive authentic sports jerseys. It’s the perfect example of new technology (subscription and logistics) helping transform an old experience (wearing jerseys to games).\\n\\nUltimately, one year in the accelerator has done a great job at getting the NFLPA (and the players they represent) more involved in the tech and startup world — just look at how many more players are learning about and investing in startups, both through the NFLPA and on their own. According to Nassar, this increased involvement sets up the NFLPA for an opportunity to “put their thumb on the scale [in the tech industry] in a way that is helpful to members” going forward.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Austin is piloting blockchain to improve homeless services', 'While the vagaries of the cryptocurrency markets are keeping crypto traders glued to their CoinDesk graphs, the real potential of blockchain is its capability to solve real human challenges in a decentralized, private, and secure way. Government officials have increasingly investigated how blockchain might solve critical problems, but now one city intends to move forward with an actual implementation.\\n\\nThe city of Austin is piloting a new blockchain platform to improve identity services for its homeless population, as part of a competitive grant awarded by the Mayor’s Challenge program sponsored by Bloomberg Philanthropies. Austin was one of 35 cities to be awarded pilot grants, and the top city from that group will ultimately be awarded $5 million.\\n\\nSteve Adler, the mayor of Austin since 2015, explained to TechCrunch that “at a high level, [the pilot] is trying to figure out how to solve one of the challenges we have in our community related to the homeless population, which is how to keep all the information of that individual with that individual.”\\n\\nIdentity is among the thorniest challenges for governments to solve, particularly for marginal populations like the homeless or refugees. As Sly Majid, Chief Services Officer for Austin, said, “If you have your backpack stolen or if your social security card gets wet and falls apart, or if you are camping and the city cleans up the site and takes your possessions, you have to start all over from the beginning again.” That is devastating for marginal populations, because it means that the cycle of poverty persists. “It really prevents you from going about and doing the sort of activities that allow you to transition out of homelessness,” he continued.\\n\\nAustin has been on an economic tear, becoming one of the top startup hubs in the United States and increasingly drawing talent from major cities like San Francisco. But, “For everything that is going right, we have some challenges that are shared by a lot of large cities,” Adler said. That dizzying growth has raised housing prices, making it more difficult to improve the city’s homelessness rate. Some 2,000 individuals are homeless in the city according to a census taken earlier this year, with several thousand more at various states of transition.\\n\\nThe city wanted to improve the ability of its patchwork of government and private homeless service providers to offer integrated and comprehensive aid. There are a number of separate challenges here: verifying the identity of a person seeking help, knowing what care that individual has previously received, and empowering the individual to “own” their own records, and ultimately, their destiny.\\n\\nThe goal of the city’s blockchain pilot program is to consolidate the identity and vital records of each homeless person in a safe and confidential way while providing a means for service providers to access that information. Adler explained that “there are all kinds of confidentiality issues that arise when you try to do that, so the thought was that blockchain would allow us to bridge that need.”\\n\\nBy using blockchain, the hope is that the city could replace paper records, which are hard to manage, with electronic encrypted records that would be more reliable and secure. In addition, the blockchain platform could create a decentralized authentication mechanism to verify a particular person’s identity. For instance, a homeless services worker operating in the field could potentially use their mobile device to verify a person live, without having to bring someone back to an office for processing.\\n\\nMore importantly, vital records on the blockchain could build over time, so different providers would know what services a person had used previously. Majid provided the example of health care, where it is crucially important to know the history of an individual. The idea is that, when a homeless person walks into a clinic, the blockchain would provide the entire patient history of that individual to the provider. “Here was your medical records from your last clinic visits, and we can build off the care that you were given last time,” he said. Austin is partnering with the Dell Medical School at the University of Texas to work out how best to implement the blockchain for medical professionals.\\n\\nIdentity is a popular area for investors interested in blockchain and decentralization more generally. As I wrote about earlier this week, Element, a New York City-based startup co-founded by famed deep learning researcher Yann LeCun, hopes to provide decentralized identity to people in developing countries like Indonesia and the Philippines. Austin is exploring partnering with decentralized startups like BanQu to implement the details of the service for the city.\\n\\nMajid noted that “It’s an iterative process for us, … and we need to crawl before we walk, and walk before we run.” Adler believes that the program is an example of the power of fusing government and private industry. Austin “tries to work with new industries, and new technologies, and new economies and tries to find the proper intersection of government innovation and responsibility,” he said. If blockchain can improve homelessness here, that solution could carry throughout the world.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Proven wants to sell AI distilled custom skincare', 'YC-backed startup Proven wants to make it suck less for women to find skincare that works for them. The co-founders are taking what they describe as a “rational, logic-based” approach to figuring out which ingredients might be most appropriate for each individual.\\n\\nAs a TC Disrupt battlefield founder once memorably put it during her on-stage pitch, the beauty industry makes a whole lot of money from a whole lot of BS. And skincare falls squarely into the ‘full of it’ category, with its expensively marketed pseudoscientific claims touting ‘miracle’ fixes that most definitely aren’t.\\n\\nProven’s co-founders, Ming Zhao and Amy Yuan, say frustration when battling with this BS via their own skincare issues ultimately led them to found the business together. Zhao had had a stressful job in private equity which she credits with “really wrecking my skin”, while Yuan suffered adolescent skin problems and also has allergies that can affect it.\\n\\n“After trying numerous products and investing — I saw it as an investment, in expensive ‘miracle’-promising products — nothing really worked for me,” says Zhao. “So I became very frustrated and I felt betrayed by our beauty industry. And eventually what actually worked for me were customized products that were made for my by a few different facialists. So that’s how the optimize idea of tailoring products to exactly someone’s situation, someone’s skin, first came to my mind numerous years ago.”\\n\\nYuan’s computational physics background informed the data-focused approach they’re taking with Proven. “I’ve done a lot of big scale supercomputing simulations,” she says. “And, I thought, given my background why don’t I just write an AI engine that gathers reviews for me to find skincare products that are automatically fitting to my skin. And when I talked to Ming about it we immediately had this spark — and started crawling data.”\\n\\nTheir core idea is to see whether deep learning and machine learning algorithms can distill useful information from millions of online testimonials for skincare products, plus a much smaller subset of publicly available peer reviewed scientific research papers — turning a mountain of what is obviously very variable data into, what they hope, is a formula for programming customized skincare products that work.\\n\\nThey’re focusing on skincare purely for women because it’s women who’ve written the millions of online product reviews underpinning this data + AI play.\\n\\n“The average person spends 45 minutes to 1.5 hours researching products before they buy any beauty products and even after they buy based on the research that they’re able to do, 55% of people are still unsatisfied post-purchase. And that’s because of the proliferation of information that’s out there. No single person is capable of reading the amount of information there is in order to make a sound decision,” argues Zhao when asked why they think their approach can work. “Which is why we’ve built the largest database of beauty.”\\n\\nTheir database combines data on hundreds of thousands of skincare products culled from millions of users reviews. At this point Zhao says they’ve used their AI engine to analyze more than 8 million reviews and testimonials — “of basically anybody who’s bought a skincare product, a beauty product and has written a comment about it online”.\\n\\n“In this database it also has more than a hundred thousand beauty products that have been talked about. So basically everything that’s on the market. As well as more than 20,000 ingredients — as well as 4,000 peer reviewed scientific articles on skin and on ingredients and on what works for skin,” she continues. “So it is not just reviews but it’s combined with scientific research.\\n\\n“On our team we also have an award winning cosmetic chemist who is the person who helped to formulate all of our products. We also have dermatologist advisors on our team who put the human touch on top of the big database knowledge base.”\\n\\nPotential buyers must first fill out a survey on Proven’s website, answering questions about things like their age, ethnicity, skin type and their skincare priorities. After which they’ll be emailed custom products they can buy — which will in turn be blended by drawing on Proven’s database of AI-distilled testimonials to match crowdsourced learnings to what an individual customer knows (or at least claims to know) about their own skin.\\n\\nThe service isn’t live yet — but will be soft launched in the US next week.\\n\\n“The database is really powerful. It has all of the information and has more than 10 years of consumer testimonials on various skincare products,” adds Yuan. “One surprise that we had going into this space is how little research was out there on people’s skin and then what kind of ingredients would be effective on what kind of skin and in which environment.\\n\\n“And then we feel like… why they’ve researched so little is because there’s not enough data to back it up — unlike pharmaceutical research where funding can go in and there’s clinical trials and a lot of different funding sources. Skincare is sort of in an awkward position.”\\n\\nGlobally, the skincare, beauty and cosmetics industry is estimated to be worth some $445BN at this point — a figure that’s only set to keep growing in an age of selfie obsession and perpetual digital self promotion.\\n\\nSo any skincare company that can come up with a slicker formula to help women find effective products could be a real game changer.\\n\\nBut, at the same time, there’s undoubtedly a lack of high quality data to drive genuine change. And without regulation of BS claims, well, misinformation is free to masquerade as eye-catching marketing. And that’s why pseudoscientific nonsense is so lucrative. And why there’s little incentive for the industry to change.\\n\\nProven’s founders say they’ve done a lot of cleaning and structuring of the data in their database before processing it for patterns. Even using fraud detection algorithms to try to weed out sources of fake reviews. On top of the cleaned and structured data they’re then applying various machine learning and deep learning algorithms to try to link particular ingredients with beneficial outcomes for different types of users.\\n\\nBut the big question is whether poor and/or low quality data — even if you’ve managed to scrape together an awful lot of it — can really lead to useful AI-powered decisions.\\n\\nWhere skincare is concerned, that remains unproven — unlike this startup’s name.\\n\\nAnd with so many other unseen factors at play that can also affect people’s skin, such as diet, exercise, lifestyle, even genetic conditions, which won’t necessarily be being expressed within the limited confines of an online review, well, it’s just not clear whether anything of real worth can be distilled from such partial and fuzzy data.\\n\\nAlthough there would certainly be poetic justice if the beauty industry ends up being successfully disrupted thanks to millions of user reviews debunking its not-so-miracle cures.\\n\\nWe’ve not been able to test Proven’s service at this nascent stage. And clearly it’ll take time for its own user testimonials to roll in. But if you look online, you’ll find skincare reviews are rife with dissatisfaction.\\n\\nSo even if all Proven offers is doing some of the legwork to help people decide what to buy (or avoid buying) that’s at least a partial incentive — given that many women will already be spending lots of their own time and money locked in a frustrating trial and error process of looking for skincare that works.\\n\\nThe co-founders also say they’re looking at ways to visualize the learnings they’ve extracted from their database — to try to make that information more accessible.\\n\\nAway from the mainstream beauty market, the even more expensive skincare option is to pay for custom products from a dermatologist or other skin specialist. A route that can be effective, as it was for Zhao, but can also be prohibitively expensive. Certainly it’s not accessible to the mass market.\\n\\nZhao and Yuan say they want Proven’s skincare products to be accessible but they also argue that beauty products priced too cheaply can be perceived by women as ineffective or undesirable. So they also won’t be setting the price bar too low.\\n\\n“One of our goals is to make beauty inclusive, and that is inclusive from many different angles — in that we’re not just making products for a certain subtype within a certain ethnicity. We want to be able to help many people with their skin issues, across ethnicities, across geographic locations,” says Zhao. “So in terms of pricing too we want to be approachable. But what is funny though, we know from our data that women don’t consider a product to be of high quality unless it is above a certain price bar. That’s just how we’re built. So we try to signal that our products are of the highest quality — because they are.”\\n\\nFor people with sensitive skin the challenge of finding effective skincare can be a full on nightmare. (Nothing says ‘unhappy customer’ quite like having paid for a emollient that actually makes your skin even worse.) So if Proven can narrow the risk of encountering irritants that’s also going to be compelling for at least a subset of consumers.\\n\\nThough product customization can also be risky from a business point of view. Because if a personalized product ends up disappointing, that customer will have few avenues to explore to come back for more.\\n\\nThe mainstream skincare industry does claim to cater to different skin types. But its categories tend to be fairly broad-brush, and arguably just make matters worse by creating yet more skincare products which buyers need to factor in to ‘buy and try’.\\n\\nThe lack of regulation on the beauty industry also makes it impossible for consumers to be confident in any of the claims being made by any of these products. A ‘miracle’ snake oil can (and frequently does) sit on a shelf next to a more basically packaged and less expensive moisturizer that contains essentially the same ingredients.\\n\\nThe same could be true of ‘AI-distilled skincare’ too of course. So for now it remains to be seen whether Proven’s personalized skincare products end up delivering more effective skincare than the average pot of cream plucked off the shelf.\\n\\nAfter all “personalized” is just another word that sounds good but doesn’t in itself mean very much. So “personalized skincare” may end up being just another nice sounding but hollow claim.\\n\\nOn the other hand, if their “AI engine” actually manages to distill some valuable intelligence from millions of product reviews it could be a very winning formula. Beauty industry product promises that don’t disappoint would be a disruptive innovation indeed.\\n\\nProven says it will initially manufacture its skincare products itself, in the US, using an ingredients philosophy the co-founders sum up as “just what you need and nothing that you don’t”. They are also excluding some common but controversial beauty product ingredients, such as SLS, parabens, alcohol, triclosans and animal byproducts. (Although their products are not equivalent to fresh cosmetics, such as the custom preparations you might get from a dermatologist, as they do include some preservatives.)\\n\\nWhile they’re starting with skincare — offering a small range of day and night serums, toners and creams to begin with — Zhao also says they see potential to expand into other wellness products if the personalized touch flies.\\n\\n“We’re starting with skincare but we’d love to do the same thing… within all of the wellness category, because there’s nothing more intimate for me personally and for a lot of women I know than their skincare, than their bodycare, than their haircare,” she says. “The things that they put on and therefore are absorbed into their body. So we want to help everyone to have a more personalized experience with these essential, important categories.”')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('AiFi emerges from stealth with its own take on cashier-free retail, similar to Amazon Go', 'AiFi emerges from stealth with its own take on cashier-free retail, similar to Amazon Go\\n\\nFarwell, cashier jobs. Following the launch of Amazon’s cashless, cashier-free Amazon Go convenience store in Seattle, a startup called AiFi is emerging from stealth today to announce the availability of its own checkout-free solution for retailers. But unlike Amazon Go, AiFi claims its A.I., sensor and camera network-based system can scale from a small mom-and-pop all the way up to a big retailer with tens of thousands of square feet and a hundred thousand products.\\n\\nBased in Santa Clara, AiFi was founded in January 2016 by Steve Gu and Ying Zheng, a married couple whose previous experience includes time spent at both Google and Apple.\\n\\nGu, a PhD in computer science from Duke, worked on 3D Touch and Force Touch technologies while at Apple. He later went to Google X where he developed technologies for Google Glass.\\n\\nZheng, also a PhD in computer science from Duke, first worked at Google Research, then later went to Apple, where she was a senior research scientist.\\n\\nAiFi’s 25-person team is rounded out by other engineers with backgrounds in computer science, computer vision, and deep learning.\\n\\n“As the name suggests, we believe A.I. should be as widely available as Wi-Fi,” says Gu. While there are a number of applications for A.I. technology, AiFi is focusing on large-scale retail, so stores in the future – besides those from Amazon – can become checkout-free. “That’s basically a testament to how good A.I. has become,” he says of the technology.\\n\\nSimilar to Amazon, AiFi’s system involves cameras, sensors and A.I. technology to identify what shoppers grab from the store’s shelves. Also like Amazon’s Go store, shoppers will have to use a companion smartphone app, where their payment information is stored. However, Gu says AiFi is designed to scale – it can support tracking up to 500 people, and tens of thousands of SKU item numbers.\\n\\nLater this year, the company will launch a demo store in the San Francisco Bay area, as well as a pilot store with a larger grocer in New York to prove out these claims.\\n\\nIn addition to monitoring the products – AiFi monitors the people, too.\\n\\nThat is, it can track shoppers’ behavior in the store, including things like if they’re shopping in groups, what items they’re picking up and putting back, their gait, their body poses, where they go in the store, and even identify if they’re doing something abnormal, like shoplifting.\\n\\nThe system today is designed to use a combination of technologies, including facial recognition, to identify shoppers during their session, but in the future this could be connected to their past sales history to make personalized recommendations. (Minority Report, anyone?)\\n\\nFor the privacy-minded, that’s a bit concerning – shopping offline was one of the last frontiers where you could walk in a store, pay cash, and leave little in terms of a digital trail. Sure if you stole something, someone might review the camera footage – but beyond that, you’re anonymous. The store of the future – if Amazon Go and startups like AiFi succeed – won’t allow for that level of privacy anymore.\\n\\nThe only thing stopping AiFi from rolling out its technology more broadly right now is cost.\\n\\nGu admits that it can be expensive to get the system up-and-running, but he’s betting on Moore’s Law to bring down the costs associated with processing power in the months ahead. “The whole camera module might cost below a hundred bucks by early next year,” he estimates. “The nice thing about A.I. in general is that it doesn’t require a fancy lens or fancy hardware components in general,” Gu adds.\\n\\nIn addition, AiFi’s business model won’t be based on the hardware, but a subscription fee associated with continuing usage of the system. The company may also move into payments in the future, too, instead of integrating its checkout with third-party processors.\\n\\nThere’s obviously at least some consumer interest in this sort of technology. There were lines out the door to test Amazon Go’s cashier-less store when it opened, even though the goal was to eliminate line-waiting in general.\\n\\n“More than 90% of last year’s retail sales came from physical stores yet Americans spent at least 37 billion hours waiting in line, which is considered the top frustration among shoppers,” Gu says.\\n\\nThe question is are we all truly so frustrated that we’re willing to give up our privacy in exchange for non-stop surveillance facilitating a slightly quicker exit?\\n\\nAre we so annoyed with the minute or two we stand in line, that we’re willing to let technology to take over yet another human job role?\\n\\nAnd does the value of human contact in the form of these everyday transactions – whether it’s the cashier, the barista, the clothing store clerk, the lunch chef, or the delivery person – ever get factored into these equations at all?\\n\\n[gallery ids=\"1602274,1602277,1602276,1602275,1602273\"]\\n\\nGu argues, as do many in the tech sector, that A.I. can free up human labor from having to do the repetitive tasks to focus on things that require more “human creativity” – an area A.I. hasn’t quite mastered.\\n\\n“I believe the tedious part of the job should be replaced by A.I. and machines because it makes everything more efficient. It’s better for the stores and it’s better for the customers,” he explains. “We also free the human labor to do something that’s emotionally more engaging or more creative – something that human beings are truly good at, and machines are not yet.”\\n\\n“Whenever there’s an industry revolution, you see people complain about job loss. But what always happens is that we create more jobs and more opportunities for the public,” Gu adds.\\n\\nBut perhaps that’s an easier sentiment for someone whose future job possibilities are never in question. For someone whose qualifications and education limit them to entry-level work like flipping burgers, making coffee, or bagging groceries, it’s probably a bit disconcerting to realize to how rapidly the tech industry is working to develop their replacement – a replacement that will never take a sick day or vacation, won’t have kids or need to attend to them; and one that doesn’t require costly health insurance or any other company benefit.\\n\\nAfter all, it’s not that likely that retailers are looking to A.I. to “free up” cashiers for more creative work; they’re looking to swap out people for tech to boost their bottom line in the long run.\\n\\nAiFi has raised $4 million in seed funding from investors including Stanford StartX Fund, Plug & Play, Greylock, Amino Ventures, Oriza Ventures, CSC Ventures, AngelPlus Ventures, as well as private investors.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('SurvAIllance', \"You may not be aware that the rough consensus in the tech community is that Cambridge Analytica were almost certainly bullshit artists. Oh, don’t get me wrong, what they tried to do, and/or claimed to do, was super shady and amoral and would have been ruinous to reasonable informed democracy if successful. But I have yet to find anyone credible who thinks that their “psychographics” approach actually works.\\n\\nThat is: it doesn’t work yet.\\n\\nWe may yet thank Cambridge Analytica, for inadvertently raising the red flag before the real invaders are at the gate. Because in this era of ongoing exponential leaps and growths in data collection, which is also an era of revolutionary, transformative AI advancement, it’s probably only a matter of time before electorates can be subtly manipulated wholesale.\\n\\nDon’t take my word for it. Take that of Google senior AI / deep-learning researcher François Chollet, in a truly remarkable Twitter thread:\\n\\nThe problem with Facebook is not *just* the loss of your privacy and the fact that it can be used as a totalitarian panopticon. The more worrying issue, in my opinion, is its use of digital information consumption as a psychological control vector. Time for a thread — François Chollet (@fchollet) March 21, 2018\\n\\nTo quote the crux of what he’s saying:\\n\\nIn short, Facebook can simultaneously measure everything about us, and control the information we consume. When you have access to both perception and action, you’re looking at an AI problem. You can start establishing an optimization loop for human behavior … A loop in which you observe the current state of your targets and keep tuning what information you feed them, until you start observing the opinions and behaviors you wanted to see … A good chunk of the field of AI research (especially the bits that Facebook has been investing in) is about developing algorithms to solve such optimization problems as efficiently as possible\\n\\nDoes this sound excessively paranoid? Then let me point your attention to another eye-opening recent Twitter thread, in which Jeremy Ashkenas enumerates a set of Facebook’s more dystopian patent applications:\\n\\nYou know, I really hate to keep beating a downed zuckerberg, but to the extent that expensive patents indicate corporate intent and direction — Come along for a ride, and let’s browse a few of Facebook’s recent U.S.P.T.O. patent applications… — Jeremy Ashkenas (@jashkenas) April 4, 2018\\n\\nAgain let me just summarize the highlights:\\n\\na system that watches your eye movements […] record information about actions users perform on a third party system, including webpage viewing histories, advertisements that were engaged, purchases made […] “Data sets from trusted users are then used as a training set to train a machine learning model” […]“The system may monitor such actions on the online social network, on a third-party system, on other suitable systems, or any combination thereof” […] ‘An interface is then exposed for “Political analysts” or “marketing agencies“’ […]\\n\\nAre those quotes out of context? They sure are! So I encourage you to explore the context. I think you’ll find that, as Ashkenas puts it, “these patent applications don’t necessarily mean that Facebook wants to use any of these techniques. Instead, they illustrate the kinds of possibilities that Facebook management imagines.”\\n\\nAn explosion of data. A revolution in AI, which uses data as its lifeblood. How could any tech executive not imagine using these dramatic developments in new and groundbreaking ways? I don’t want to just beat up on Facebook. They are an especially easy target, but they are not the only fish in this barrel:\\n\\nl find it incomprehensible how Google-associated people still comment critically on Facebook's business practices when 84% of their revenue (and what pays for all the free services and research) comes from precisely the targeted advertising that's suddenly so contemptible. https://t.co/U8UbXXz6Df — Antonio García Martínez (@antoniogm) March 25, 2018\\n\\nHere’s yet another viral privacy-related Twitter thread, this time from Dylan Curran, illustrating just how much data Facebook and Google almost certainly have on you:\\n\\nWant to freak yourself out? I'm gonna show just how much of your information the likes of Facebook and Google store about you without you even realising it — Dylan Curran (@iamdylancurran) March 24, 2018\\n\\nMind you, it seems fair to say that Google takes the inherent dangers and implicit responsibility of all this data collection, and the services it provides with this data, far, far more seriously than Facebook does. Facebook’s approach to potential malfeasance has been … well … let me point you to still another Twitter thread, this one from former Google Distinguished Engineer Yonatan Zunger, who I think speaks for all of us here while reacting to reports of Facebook’s CTO saying “the company is now mapping out potential threats from bad actors before it launches products.”\\n\\nThe above tweet is much, much, less obscene than what I just said out loud. *twitch* — (((Yonatan Zunger))) (@yonatanzunger) April 5, 2018\\n\\nBut the larger point is that the problem is not restricted to Facebook, or Google, or the big tech companies. It’s more acute with them, since they have more data and more power, and, in Facebook’s case, very little apparent sense that with great power comes great responsibility.\\n\\nHow in holy hell does a project like this get far enough to actually have talks with hospitals about it?!? They wanted anonymized medical data to essentially deanonymize by hashing against profile data, but patient consent didn’t come up?? https://t.co/qqW4K2hoFM pic.twitter.com/fxT8IPAtoi — Angela Bassa (@AngeBassa) April 6, 2018\\n\\nBut the real problem is far more fundamental. When your business model turns data into money, then you are, implicitly, engaging in surveillance capitalism.\\n\\nSurveillance and privacy are issues not limited to businesses, of course; consider the facial recognition goggles already in use by Chinese police, or India’s colossal fingerprint-face-and-retina-driven Aadhaar program, or dogged attempts by the UK and US governments to use your phone or your Alexa as their surveillance device. But corporations currently seem to be the sharp and amoral edge of this particular wedge, and we have no real understanding of how to mitigate or eliminate the manifold and growing dangers of their surveillance capitalism.\\n\\nI’m not saying all data gathering is ipso facto bad; but I am saying that, given the skyrocketing quantity of sensors and data in our world, and the ability to tie that data to individuals, any initiatives which support privacy, pseudonymity, and anonymity should be considered desirable until proven otherwise, given the ever-growing oceans of data whose tides threaten to wash those lonely islands away.\\n\\nI’m certainly not saying AI is bad. Its potential for improving our world is immense. But like every powerful tool, we need to start thinking about how its potential misuses and side effects before we rush to use it at scale.\\n\\nAnd I’m saying we should almost be grateful to Cambridge Analytica, for selling snake oil which claimed to do what tomorrow’s medicines actually will. Let’s not overreact with a massive moral panic. (checks Internet) Whoops, too late. OK, fine — but let’s try to take advantage of this sense of panic to calmly and rationally forestall the real dangers, before they arrive.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('We need to improve the accuracy of AI accuracy discussions', 'Reading the tech press, you would be forgiven for believing that AI is going to eat pretty much every industry and job. Not a day goes by without another reporter breathlessly reporting some new machine learning product that is going to trounce human intelligence. That surfeit of enthusiasm doesn’t originate just with journalists though — they are merely channeling the wild optimism of researchers and startup founders alike.\\n\\nThere has been an explosion of interest in artificial intelligence and machine learning over the past few years, as the hype around deep learning and other techniques has increased. Tens of thousands of research papers in AI are published yearly, and AngelList’s startup directory for AI companies includes more than four thousands startups.\\n\\nAfter being battered by story after story of AI’s coming domination — the singularity, if you will — it shouldn’t be surprising that 58% of Americans today are worried about losing their jobs to “new technology” like automation and artificial intelligence according to a newly released Northeastern University / Gallup poll. That fear outranks immigration and outsourcing by a large factor.\\n\\nThe truth though is much more complicated. Experts are increasingly recognizing that the “accuracy” of artificial intelligence is overstated. Furthermore, the accuracy numbers reported in the popular press are often misleading, and a more nuanced evaluation of the data would show that many AI applications have much more limited capabilities than we have been led to believe. Humans may indeed end up losing their jobs to AI, but there is a much longer road to go.\\n\\nAnother replication crisis\\n\\nFor the past decade or so, there has been a boiling controversy in research circles over what has been dubbed the “replication crisis” — the inability of researchers to duplicate the results of key papers in fields as diverse as psychology and oncology. Some studies have even put the number of failed replications at more than half of all papers.\\n\\nThe causes for this crisis are numerous. Researchers face a “publish or perish” situation where they need positive results in order to continue their work. Journals want splashy results to get more readers, and “p-hacking” has allowed researchers to get better results by massaging statistics in their favor.\\n\\nArtificial intelligence research is not immune to such structural factors, and in fact, may even be worse given the incredible surge of excitement around AI, which has pushed researchers to find the most novel advances and share them as quickly and as widely as possible.\\n\\nNow, there are growing concerns that the most important results in AI research are hard if not impossible to replicate. One challenge is that many AI papers are missing the key data required to run their underlying algorithms or worse, don’t even include the source code for the algorithm under study. The training data used in machine learning is a huge part of the success of an algorithm’s results, so without that data, it is nearly impossible to determine whether a particular algorithm is functioning as described.\\n\\nWorse, in the rush to publish novel and new results, there has been less focus on replicating studies to show how repeatable different results are. From the MIT Technology Review article linked above, “…Peter Henderson, a computer scientist at McGill University in Montreal, showed that the performance of AIs designed to learn by trial and error is highly sensitive not only to the exact code used, but also to the random numbers generated to kick off training, and to ‘hyperparameters’—settings that are not core to the algorithm but that affect how quickly it learns.” Very small changes could lead to vastly different results.\\n\\nMuch as a single study in nutrition science should always be taken with a grain of salt (or perhaps butter now, or was it sugar?), new AI papers and services should be treated with a similar level of skepticism. A single paper or service demonstrating a singular result does not prove accuracy. Often, it means that a very choice dataset operating with very specific conditions can lead to a high point of accuracy that won’t apply to a more general set of inputs.\\n\\nAccurately reporting accuracy\\n\\nThere is a palpable excitement about the potential of AI to solve problems as diverse as clinical evaluation at a hospital to document scanning to terrorism prevention. That excitement though has clouded the ability of journalists and even researchers from accurately reporting accuracy.\\n\\nTake this recent article about using AI to detect colorectal cancer. The article says that “The results were impressive — an accuracy of 86 percent — as the numbers were obtained by assessing patients whose colorectal polyp pathology was already diagnosed.” The article also included the key results paragraph from the original study.\\n\\nOr take this article about Google’s machine learning service to perform language translation. “In some cases, Google says its GNMT system is even approaching human-level translation accuracy. That near-parity is restricted to transitions between related languages, like from English to Spanish and French.”\\n\\nThese are randomly chosen articles, but there are hundreds of others that breathlessly report the latest AI advances and throw out either a single accuracy number, or a metaphor such as “human-level.” If only evaluating AI programs were so simple!\\n\\nLet’s say you want to determine whether a mole on a person’s skin is cancerous. This is what is known as a binary classification problem — the goal is to separate out patients into two groups: people who have cancer, and people who do not. A perfect algorithm with perfect accuracy would identify every person with cancer as having cancer, and would identify every person with no cancer as not having cancer. In other words, the results would have no false positives or false negatives.\\n\\nThat’s simple enough, but the challenge is that conditions like cancer are essentially impossible to identify with perfect accuracy for computers and humans alike. Every medical diagnostic test usually has to make a tradeoff between how sensitive it is (how many positives does it identify correctly) versus how specific it is (how many negatives does it identify correctly). Given the danger of misidentifying a cancer patient (which could lead to death), tests are generally designed to ensure a high sensitivity by decreasing specificity (i.e. increasing false positives to ensure that as many positives are identified).\\n\\nProduct designers have choices here in how they want to balance those competing priorities. The same algorithm might be implemented differently depending on the the cost of false positives and negatives. If a research article or service doesn’t discuss these tradeoffs, then accuracy is not being fairly represented.\\n\\nEven more importantly, the singular value of accuracy is a bit of a misnomer. Accuracy reflects how many positive patients were identified positively and how many negative patients were identified negatively. But we can maintain the same accuracy by increasing one number and decreasing the other number or vice versa. In other words, a test could emphasize detecting positive patients well, or it could emphasize excluding negative patients from the results, while maintaining the same accuracy. Those are very different end goals, and some algorithms may be better tuned toward one rather than the other.\\n\\nThat’s the complication of using a single number. Metaphors are even worse. “Human-level” doesn’t say anything — there is rarely good data on the error rate of humans, and even when there is such data, it is often hard to compare the types of errors made by humans versus those made by machine learning.\\n\\nThat’s just some of the complications for the simplest classification problem. All of the nuances around evaluating AI quality would take at least a book, and indeed, some researchers will no doubt spend their entire lives evaluating these systems.\\n\\nEveryone can’t get a PhD in artificial intelligence, but the onus is on each of us as consumers of these new technologies to apply a critical eye to these sunny claims and rigorously evaluate them. Whether it is reproducibility or breathless accuracy claims, it is important to remember that many of the AI techniques we rely on are mere technological babies, and still need a lot more time to mature.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The dos and don’ts of crafting frontier-tech companies', 'Powerful tools, amazing talent and endless dollars flowing from eager investors makes today an amazing time to start tomorrow’s technology companies. Curious and ambitious founding teams are putting their skills to work toward solving real-world problems. Here’s how to build hard value while avoiding common pitfalls in nine exciting startup categories that will brighten our future.\\n\\nAugmented and virtual reality\\n\\nVirtual and augmented reality have been just around the corner for almost a decade. However, despite the flurry of new VR content, portals and hardware, traditional 2D content continues to be king.\\n\\nDON’T build consumer hardware. Yes, virtual reality headsets are expensive and bulky, while augmented reality technology is still low-fidelity. Magic Leap has done an excellent job raising the immense amount of capital needed to build and market hardware; however, the business of designing, building, distributing and marketing consumer electronics is generally outside the realm of startups.\\n\\nThe likes of Sony, Google, Samsung, LG, Lenovo, HTC and Apple have the benefit of all the machinery in place and seemingly endless amounts of cash to fund these efforts. Meanwhile, other startups have attempted to become a destination for consuming VR/AR content. Again, the big media companies, as well as startups such as Amazon, Netflix, Apple Google, and Facebook, have poured money into marketing their platforms (i.e. Oculus, SteamVR) and funding content to seed them.\\n\\nDO set out to build the magical content that will entice consumers to part with their cash and dedicate their time to experience it. Atari was able to create experiences magical enough to convince consumers to part with almost $800 (inflation-adjusted). Though the graphics and sounds are laughable today, the success of Atari’s original titles puts modern content to shame.\\n\\nCreative talent is needed to create AR and VR-native experiences, as opposed to putting lipstick on 2D. It is the equivalent of the pan, zoom and monologues that distinguished photographs from motion pictures. It is the equivalent of arcades and movie theaters as venues for consuming VR content that’s needed. Unfortunately, hoping that people will use their mobile phones as headsets or set up rigs at home is no longer a strategy; the AR and VR content is still in the pre-Pong era.\\n\\nAI\\n\\nAcademics and big companies have strapped AI to a PR missile. Cheap compute is reducing this decades-old technology to practice, and the notion of algorithms being able to read checks and identify cats in photos is being extrapolated to fears of homicidal robots taking over the world.\\n\\nDON’T start an AI company and hope for the best. The days of investors and talent gravitating toward this sexy tagline are coming to an end. It is more likely that AI will make existing products and businesses better, versus uniquely enabling new multi-billion-dollar venture opportunities.\\n\\nStarting an “AI company” is equivalent to starting a cloud company, a mobile company or an internet company. For every Salesforce, Facebook and Amazon there were many thousands of failed startups. There were also many incumbent companies that leveraged the technology to their benefit. The failed startups did manage to raise money and recruit around the hype; but without a strategy for building real business, they didn’t manage to get very far.\\n\\nDO seek to leverage AI as just one of many tools toward improving products and accessing new markets. Ideally, you want AI as your secret weapon underpinning a superior product, or giving your business the reach or efficiency in acquiring customers that puts it in its own category altogether. In fact, you probably don’t want the market viewing you as an “AI” company, but rather as a company that is creating new markets with magical products, with AI working somewhere under the hood.\\n\\nHardware-accelerating AI\\n\\nAI chip companies are white-hot. Intel’s $400 million acquisition of Nervana and Nvidia’s skyrocketing stock price have catalyzed a flurry of startups building chips to train deep neural nets in the data center and run them on portable and embedded devices.\\n\\nDON’T optimize traditional digital chip designs for AI. Building a chip company is HARD. Many chip companies fail because it takes too long to ramp sales. Chip companies sell a complicated product to fickle, conservative electronics companies that require an army of people to support. It is typical for a chip company to get crushed supporting customers that are purchasing in small volumes. The chip startups end up having to raise more equity dollars to achieve better economics by going on a new technology node, and the cycle starts over again unless there is a watershed event with a customer.\\n\\nDO ask customers what they can do with your chip that they otherwise can’t do with the chips from their current suppliers (e.g. Intel/QCOM/Nvidia/TI). If it’s less than a 10x improvement in performance, cost and size, you are walking into a gunfight with a kitchen knife. Think about how your orders-of-magnitude improvement could potentially lead to a new category of products. Successful chip companies like Broadcom, Qualcomm, InvenSense and Atheros empowered new product categories (cable modems, cell phones, motion-sensing video-game controllers, Wi-Fi in laptops and phones, respectively). What magical product can your chips enable?\\n\\nSpace technology\\n\\nInternet-turned-space-entrepreneur Elon Musk proved to the world that you don’t need to be an aerospace insider to build an out-of-this-world company.\\n\\nDON’T rush to lay down infrastructure for the space economy. As Mark Twain has been credited for quoting: “History doesn’t repeat itself, but it often rhymes.” Investors lost billions laying down railroads during the industrial revolution, though they greatly benefited the industries that relied on rail transport. Investors also lost billions laying down infrastructure for the internet, but benefited the e-commerce, social, mobile and SaaS companies that later put them to use. How are the prospects of launch, space communications, mining and microgravity laboratory services going to be any different?\\n\\nDO build businesses that will leverage space toward serving massive existing and future markets. SpaceX set out to serve the needs of existing government and commercial satellite operators. Planet built and launched a novel satellite design to create a new market for cheap, frequent geospatial imagery.\\n\\nBefore putting together a plan to build, launch, service or provide comms services satellites, take a moment to forget about space. Assume you are building a generic business that has an upfront cost (i.e. designing, building and launching satellites), and has a long tail of cash flow associated with it. That cash flow has to generate a substantially higher return (or in finance terms, IRR) than any other project, regardless of whether it’s in, or out of this world. That return needs to be high enough to compensate for the risk being taken on an unproven product team and, in some cases, a nascent market.\\n\\nDriverless cars\\n\\nIt is difficult to resist the temptation to start a company overcoming the many obstacles on the road to autonomous cars. Speaking of roadblocks…\\n\\nDON’T start a company building point solutions. There are almost 50 big companies going after various levels of the technology stack, ranging from sensors to computer vision, to classifying people/objects, to predicting behavior, to planning, to communicating with other cars and roads and more. On the back end, there are many companies building development tools, maps and even cyber solutions to keep the cars safe.\\n\\nUnfortunately, driverless technology is so nascent that it is still to-be-determined as to how these pieces will fit together. An analogy I like to give is compute in the 1960s: IBM would do everything from fabricate chips and circuit boards, bend metal cabinets, as well as produce programming environments, compilers and the applications that ran on their machines. Heck, I wouldn’t be surprised if they had IBM-designed wipes for screens and keyboards.\\n\\nDO take a hard, close look at the supply chain you are selling into. If you are enabling driving-assistance features in conventional cars, then understand the nature of the existing automotive supply chain, which has traditionally been a meat grinder for startups. What makes your company unique/special in a sense that it will grow/thrive in a market startups have gone to die?\\n\\nIf you are building technology for robo-cars, how easily does your technology plug into the systems that are being built, predominantly at startups or R&D groups at big companies? My guess: It won’t be easy. Get to know your customers EARLY before you start building technology. Avoid pursuing a product that has to be redesigned for every customer, which, unfortunately, isn’t conducive to a rapidly growing venture.\\n\\nHuman-machine interfaces\\n\\nControlling machines through our minds, or controlling minds through machines has been left to the realm of science fiction. More recently, scientists have been leveraging modern AI tools to take a deeper look into our minds, toward potentially curing disease and building tomorrow’s interfaces with machines.\\n\\nDON’T try to build machines that can be controlled by our minds, or mind-controllers (scary). We are born with an incredibly sophisticated brain-machine interface: our bodies. We have been able to speak, sing, dance, draw, write, create music and tell stories probably before the invention of the wheel. An Emoji that can be encoded with 4 bits, perhaps drawn in the dirt with a stick, can probably better communicate emotions better than the very best headsets powered by sophisticated deep learning. Finally, our wideband senses of sight, sound, hearing, smell and touch, coupled to our intuition, already serves as a superhighway into our brains.\\n\\nDO work with the rich, existing input/output system of our bodies to invent unforgettable experiences. Does this advice sound familiar? Yes, similar to what AR/VR founders should do. The joystick was a critical aid for Atari’s success, but it was Pac-Man and Pong that compelled arcade gamers to pump machines with quarters and parents to part with more than $800 (inflation-adjusted) for an Atari 2600 console.\\n\\nCan a brain interface, coupled with sight, sound and smell, and perhaps trained by an individual’s history, deliver an unforgettable experience? Is there a combination of stimuli, generated by artificial neural nets tailored to a unique individual, that will lead to a sensual experience like no other?\\n\\nEducation\\n\\nOur educational system is ripe for disruption. We’ve seen edX and Coursera bring the world’s best educators to every corner of the planet, and there is still plenty of room for innovation.\\n\\nDON’T build within the confines of our classical educational model. Our current system was invented post-depression: in an era with vastly different societal norms, career profiles, employers and expectations from students. In my father’s generation, a four-year education was expected to prepare him for a lifelong career. Adding two to five years of post-graduate education carried the promise of additional income and more job security.\\n\\nToday, an undergraduate footing a quarter-million-dollar bill for college tuition doesn’t have an extremely high prospect of being able to repay that bill. Exporting elements of a traditional education onto an electronic platform is simply offering another alternative in an already crowded market of educational “products.”\\n\\nDO expect to constantly anticipate and equip students with the skills they need for the constantly evolving needs of the workplace. Technology is rapidly penetrating every aspect of labor, and, contrary to the headlines, it is not replacing workers; it is augmenting them (more on that below).\\n\\nInvent educational tools that evolve with the students, and accompany them through their careers. A valuable lifelong educational tool anticipates the changing needs of the workplace and automatically offers training. This concept is not new to healthcare professionals: They are required to stay abreast of the literature and be constantly re-certified. Unfortunately, professionals in less-regulated fields have to have the foresight to individually pursue training, which is challenging with family responsibilities becoming more prevalent with age.\\n\\nFurthermore, knowledge isn’t as effectively disseminated in areas outside of healthcare. Can AI be used to identify best practices for a given field and disseminate them? Can tomorrow’s educational tools predict required skill sets with the changing workforce and provide personalized training?\\n\\nBlockchains\\n\\nThe volatility and speculation behind cryptocurrencies, as well as the power of blockchain technology, has compelled many entrepreneurs to take a moment to think seriously about doing a crypto startup, for the sake of doing something in crypto.\\n\\nDON’T fall victim to the rush of speculation. Many seem to have forgotten the IPO bubble of the ‘90s. To refresh your memory, ANY company that had anything to do with the “internet” and was being offered to the public was only expected to go up. Some purchased shares BY MISTAKE, only to see their shares skyrocket — until the music stopped. Similarly, don’t prey on speculators expecting your coin offering to appreciate simply by virtue of being a coin.\\n\\nDO ask the simple questions: How will blockchain technology empower your business? Does it enable a unique product? Does it improve the unit economics of an existing product? Does it accelerate penetration or enhance networking effects?\\n\\nIf you’re pursuing a coin offering, make a solid case as to how value will be accrued in your cryptocurrency as you build out your business. What characteristic(s) of blockchain will be core to this value creation? Build a solid hypothesis around how more users and transaction volume will create intrinsic value for the currency.\\n\\nRobots\\n\\nThere has been a lively debate around whether robots create or destroy jobs. Powerful compute, algorithms and cheap sensors and actuators have created an opportunity for founders to build interesting automation companies that will lead to a more productive and competitive human workforce.\\n\\nDON’T try to replace humans. Throughout history, inventors have built clever gadgets that mimic humans. However, the most successful machines have been those that augment humans, not replace them. Don’t go off and try to build some bipedal humanoid to serve as a robotic butler. Don’t try to build a contraption that can take the place of a human on the factory floor. Don’t build a robot to run around a fulfillment center and do bin-picking faster than a human.\\n\\nDO empower humans. Assembly line technology became commonplace because it made factory workers more productive. Conveyor belts and barcodes have made their way into every order fulfillment center because it helps humans break incoming shipments down and put orders together faster.\\n\\nThere is already plenty of automation in factories and warehouses: How can humans be further assisted? How does that robotic assistance impact bottom line? What’s the return on investment for the operators? Does that return apply across a large number of customers or is it limited to a small group of potential customers with very specific needs?\\n\\nWhether it’s building cars or iPhones, doing Amazon order fulfillment, cleaning rooms, washing dishes, making beds or even gourmet cooking, the question should be 1) how is the robot making workers’ lives better, and 2) what is the quantitative improvement in worker productivity? It’s all about the people.\\n\\nThe advice here is consistent: Prioritize people. Attracting amazing, cognitively diverse people, bounded by strong culture, will empower your startup to over-achieve with very little resources. Investors will be quick to identify this trend of over-achievement and infuse the companies with the money that will attract more talent, which will attract more dollars, hence creating a virtuous cycle that will quickly accelerate your startup ahead of its peers and put big incumbents in its rear-view mirror. New technology enabling new products empowering new markets will be created, and all of humanity will find itself closer to its fantastic future.')"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Here are 64 startups that launched today at Y Combinator’s W18 Demo Day', 'Biotech, robotics, and fintech startups took the spotlight today at Y Combinator’s 26th Demo Day. This batch features 141 total companies from 23 countries, with presentations spread over two days. The house was packed at Mountain View’s Computer History Museum with wealthy investors forced to stand in the back or sit on the floor. Meanwhile, marijuana soda, wind turbine-cleaning drones, and indestructible panty hose startups demoed their products in the break room and parking lot. You can see our favorites here:\\n\\nY Combinator has made progress ramping up diversity in its startup school. Thirty-five percent of this batch’s companies are internationally based, 27 percent have a female founder, and 13 percent have an underrepresented minority founder. The 50-person YC team now includes 18 partners, with Eric Migicovsky of Pebble joining to help out hardware companies and explore the accelerator’s opportunities in China.\\n\\nThe question on everyone’s minds is which startups will join the 15 previous ones like Stripe, Dropbox, and Airbnb now worth over $1 billion. But with YC’s portfolio moving beyond social apps and enterprise tools towards hard science innovation, and 18 percent of this batch’s companies coming from health and biotech, many of the software investors seemed a little overwhelmed. We’ll let you choose your favorites from our write-ups of all 64 that pitched on the record today. Check out our top 7 picks from today’s startups, and our full coverage of all the startups from day 2. Come back Wednesday morning for our top picks from day 2.\\n\\nBear Flag Robotics\\n\\nBear Flag is building autonomous tractors. They claim to be able to reduce input by 20 percent and increase production yield by 11 percent. They’re already testing tractors in the field in California. They plan to charge about $4,000 per tractor per month.\\n\\nUniverse\\n\\nA mobile app for building mobile websites. Who would have thought? Universe lets users build a personal portfolio site with the same ease of editing a photo on Instagram. Users can follow other sites, which creates a bit of a Tumblr-like network of personal blogs. Users have already built one hundred thousand sites with the iOS app which has expanded its functionality in recent months to let users build multi-page sites. The app currently has 2,200 paid subscribers paying an average of $3.40 per month to gain premium features on the app.\\n\\nRead more about Universe on TechCrunch here.\\n\\nJuni Learning\\n\\nJuni is an online education program for kids that is targeting the $9 billion after-school market. The idea is to start with teaching kids computer science in a virtual, one-on-one setting by pairing them with tutors. It charges $250 per month for once-a-week classes Juni says it’s grown 25 percent month over month in the last six months. The company also says it’s profitable, with a 95 percent monthly renewal rate. Without adequate computer science courses in schools, and the skills becoming clearly critical future employment, Juni could educate the next generation of programmers.\\n\\nRead more about Juni on TechCrunch here.\\n\\nSafetyWing\\n\\nThere are 25 million digital nomads who move around the world while working on the Internet, and that can make health insurance complicated. SafetyWing offers $37 per month health insurance in 180 countries ($30 extra fee in the U.S.) that covers hospital visits and prescriptions but doesn’t cover preventative treatment or pre-existing conditions like cancer. SafetyWing has partnered with insurance giant TokioMarine to administer the plans. The goal is to build a global safety net for freelancers and digital nomads that the startup hopes will include banking and income protection in the future. The startup will have to convince travelers that health insurance is worth the price and hassle, but with a $15 billion per year market and the number of digital nomads doubling every five years, there’s room for a modernized vision for health insurance.\\n\\nRead more about SafetyWing on TechCrunch here.\\n\\nMacromoltek\\n\\nMacromoltek wants to “change the drug discovery process,” by building software that designs antibodies. The startup uses academic research combined with technology to help drug companies. So far, Macromoltek has ten paying customers, generating $50,000 in revenue for 2017. They’ve also secured $500,000 in LOIs (letter of intent) to design new antibodies.\\n\\nPasserine\\n\\nPasserine makes unmanned aircrafts that take off and land like birds. They intend to sell it to companies to use for things like mapping large areas, light weight cargo delivery, lidar surveying, and powerline monitoring.\\n\\nVisor\\n\\nVisor wants to turn top gamers into even more skilled players. The startup analyzes eSports gameplay footage to help coach users on what gaming skills they need to improve on. The team analyzed 1.3 million minutes of footage in the past 30 days and they boast over 39 thousand monthly active users, growing 52 percent month-over-month since launching at the beginning of the year.\\n\\nProven\\n\\nProven uses data, artificial intelligence and machine learning to design individualized skincare products for consumers. To provide the best skin care products for people, Proven has built a database, which it calls the Beauty Genome Project, that uses 8 million reviews from people in conjunction with AI to understand what ingredients have an impact on people. The idea is to help find people the products that are best for their skin topology.\\n\\nRead more about Proven on TechCrunch here.\\n\\nVena Medical\\n\\nVena Medical has created what they claim is the “world’s smallest camera” that is designed to help with medical treatments like liver cancer and strokes by looking at patient’s blood vessels. They’ve secured pilots with hospitals and believe there is a $5 billion market opportunity for their single use medical device. “Every patient treated is another camera used.”\\n\\nHaiku\\n\\nHaiku wants to do for apps want Unity did for games. It’s a simple app creation tool, meant for developers and designers to use together. The apps it creates are cross platform across both iOS and Android\\n\\nPatchd Medical\\n\\nPatchd has built a chest-worn device that detects sepsis, the No. 1 cause of death in hospital patients. The idea is that patients would no longer have to stay in the hospital for monitoring, as the device can record and analyze vital signs and send results back to doctors. They currently have a paid pilot in Australia, and are currently testing the device with 20 people. They’ll conduct a clinical trial in 2019. If the product can reliably predict sepsis, patients will be able to get back to the hospital in time if they have complications. Patchd’s device could save tons of money for hospitals and insurers while getting sick people home where they want to be.\\n\\nProof\\n\\nProof helps websites turn their visitors into purchasing customers. As an example, websites can show visitors how many people are currently viewing a product. The idea is for those notifications to help convert people into buyers. Proof currently has 2,700 paying customers, who see an average 10 to 15 percent lift in conversions. Proof’s long-term goal is to personalize the entire marketing funnel.\\n\\nOpenland\\n\\nOpenland wants to create a better way for real estate developers to acquire properties. It does this through a slick interface that allows builders to sort through available lands and connect quickly with landowners, evaluate the properties and get through the paperwork. Openland’s co-founder called the fact that 90% of buildable space isn’t for sale the “biggest roadblock for the real estate industry.”\\n\\nSharpestMinds\\n\\nThe biggest tech companies in the U.S. — the so-called FAANGs, or Facebook, Apple, Amazon, Netflix and Google — have massive recruiting efforts on university campuses and around the world. But SharpestMinds wants to build a set of tools to help the rest of those tech companies that don’t have those recruiting resources find talent before they get snatched up. SharpestMinds matches up students that apply on their site with companies where they can work on a trial basis, building up that early relationship that could lead to a job down the line.\\n\\nRead more about SharpestMinds on TechCrunch here.\\n\\nCurious Fictions\\n\\nCurious Fictions is an mobile-friendly site where readers can find, read and pay for short stories. You can pay $5, $10 or $15 per month, and then the money is distributed to the authors of the stories that they liked that month. You can also tip authors for individual stories.\\n\\nRead more about CuriousFictions on TechCrunch here.\\n\\nCulture Robotics\\n\\nCulture Robotics is growing organisms for biotech companies. It claims to have built “the first cloud bioreactor farm” and compare themselves to Amazon Web Services, but for biomanufacturing. Its technology creates bacteria like collagen for beauty companies. Culture Robotics believes its applicable for synthetic biology, microbiome and cellular therapy. So far, they’ve secured three paying customers and are generating $50k in revenue per month.\\n\\nQulture.Rocks\\n\\nQulture.Rocks is HR management for Latin America. They’re have grown about 20% month over month for the past 2 years, and have hit the break-even point.\\n\\nAesculaTech\\n\\nA bio-tech startup that aims to create a more patient-friendly way to deliver drugs. AesculaTech has invented a temperature responsive material that enables the company to make customized medical devices inside the body. The company’s first use of its technology is going toward the treatment of dry eye syndrome, which the founders say affects more than 20 million people in America. The goal is to become the go-to material used to make medical devices.\\n\\nRead more about AesculaTech on TechCrunch here.\\n\\nEvry Health\\n\\nEvry Health wants to take on the employee health insurance market by tackling a specific type of customer. The startup is specializing in working with customer companies that have between 200-1,000 employees. They boast that their product is 20 percent less than competitors and offers enhanced coverage. They’re tackling the $10B Texas health care market when they launch at the beginning of next year.\\n\\nAspire\\n\\nAspire provides loans to small businesses in Southeast Asia. Its team of “credit experts” promise to make a decision within two hours and have cash disbursed by the next day. So far, they’ve secured $500k in loans in just eight weeks. They support banks like Citi, Maybank and HSBC.\\n\\nSudden Coffee\\n\\nSudden makes instant coffee that the company claims is “better than fresh Starbucks” – backing it up with what they say was a blind test in which 8 of 10 coffee drinkers chose their instant stuff. They’re currently in 20 retail stores, and will soon be sold in REI. Their secret sauce, as they explain it: they’ve found a way to make instant coffee without requiring any boiling.\\n\\nCoinTracker\\n\\nCoinTracker is a platform to track your crypto across all exchanges, wallets, and even currencies. Today most crypto-enthusiasts try to do this using complicated and bloated Google spreadsheets, so an automated solution has the potential to save a lot of people a lot of time. CoinTracker also has the ability to optimize tax filings by computing capital gains reports using FIFO, LIFO or HIFO accounting.\\n\\nRead more about CoinTracker on TechCrunch here.\\n\\nSupermedium\\n\\nSupermedium is a web browser built natively for virtual reality. The team behind the browser was previously working at Mozilla working on A-Frame defining the WebVR standard which aims to get apps and games off your hard drive and onto the world wide web. Supermedium is working with developers to make their browser the default hub for quick and impactful games and demos. A beta of the app is available now on the Oculus Rift and HTC Vive VR headsets.\\n\\nRead more about Supermedium on TechCrunch here.\\n\\nSheerly Genius\\n\\nSheerly Genius manufactures and sells rip-proof, snag-proof and “lifeproof” pantyhose that last up to 50 wears. The pantyhose are made with the same type of fiber found in bulletproof vests and climbing equipment. Right now, Sheerly Genius sells basic black, sheer pantyhose. Down the road, Sheerly Genius plans to move into the “nude” option, which entails a number of different colors.\\n\\nRead more about Sheerly Genius on TechCrunch here.\\n\\nHexel\\n\\nHexel lets any web community launch its own immediately-usable cryptocurrency. The idea is that sites could reward their own users for engagement or let them buy tokens for different purposes, but that they could also trade them for other sites’ currencies on the Hexel exchange. For example, Quora could reward you tokens for having the top answer to science questions. Those could potentially be used to promote your own answers or questions, or you could trade them in via Hexel for hypothetical Reddit tokens that would let that user post on science subreddits reserved for experts. Hexel will earn a fee when use use its exchange. While 300 currencies have been created since Hexel’s launch six weeks ago, the startup will have a tough time persuade popular web platforms to complicate themselves with tokens. Most people still only understand cryptocurrencies as stores of value like Bitcoin, so lots of user education would be required too.\\n\\nRead more about Hexel on TechCrunch here.\\n\\nModern Health\\n\\nA mental health benefit platform for employers and their employees. Modern Health connects employees to a healthcare professional or digital tools to address things like depression, anxiety\\n\\nSince launching four weeks ago, Modern Health has posted $37,000 in annual recurring revenue. So far, employee engagement comes in at 30 percent.\\n\\nVolley\\n\\nOne in five American households now have a new family member in the house named Alexa, Siri or Google. Volley wants to build a platform for voice-based gaming on home assistant hardware. The company has the number one game for Alexa and has 900k MAUs since they launched last April.\\n\\nRead more about Volley on TechCrunch here.\\n\\nRepl.it\\n\\nRepl.it is an instant serverless computing platform. It lets you write and deploy apps for your browser. They claim to be the “easiest way to start coding,” letting users build “complex applications with thousands of lines of codes.” Repl.it believes its cloud platform is perfect for building games and other interactive programs.\\n\\nRead more about Repl.it on TechCrunch here.\\n\\nTrusu\\n\\n“Borderless shopping for Asia”. Trusu says that getting Western products in Asia is often difficult, requiring you to use a package forwarding service at a costly premium. They import products in bulk once a week, allowing their prices to be up to 75% lower than alternative services.\\n\\nLeap\\n\\nLeap is a private social network for women in tech. Designed by and for women, Leap could help people find investors, co-founders, mentors, jobs, and more. Leap uses a real names but allows some anonymous posts, and employs a light touch in moderation but with the perspective on abuse lacking at most male-run social networks. Leap now has 2000 users, but has plenty of room to grow with 4.8 million women in tech in the U.S. Social networks do have outstanding lifetime value if they can retain their users and understand what they buy.\\n\\nThe question will be whether one-off, niche social networks can succeed in the face of massive networks like Facebook and its Groups feature by focusing on thwarting abuse and creating a more civil discussion space. The private beta for Leap is now accepting signups.\\n\\nAvro Life Science\\n\\nA life science company that develops skin patches for drug delivery. Skin patches work by allowing the drugs to deliver through your skin, directly into your bloodstream. That means Avro’s skin patches eliminate the need to swallow pills and involve the GI tract.\\n\\nAvro makes patches for hundreds of drugs, like Benadryl, Zyrtec, Claritin and Aerius. Avro’s research has shown effectiveness in terms of relieving seasonal allergies. Avro says it’s on track for FDA approval by Q2 of 2019.\\n\\nSourcify\\n\\nSourcify wants to be the Flexport for product sourcing by tackling how companies get in touch with manufacturing pipelines. They want to take manufacturing sorting out of Excel and stick into its software. The startup says that they can add visualization and transparency to the industry as it has already “pre-vetted” more than 700 factories.\\n\\nLawyaw\\n\\nLawyaw builds “intelligent templates for law firms”. Built by an ex-law firm partner and an ex-Google engineer, it uses natural language processing to create new legal documents from a lawyer’s existing documents, allowing it to be re-used later as a template. 800+ lawyers are currently using the service, with 23,000 templates made so far.\\n\\nThe Lobby\\n\\nThe Lobby helps job applicants secure one-on-one calls with company insiders. With resume critique and mock interviews, the startup helps job seekers get advice “from a real human who has the job they want.” They already work with insiders at top investment banks like J.P. Morgan and Barclays and have a $100,000 gross merchandise volume (GMV) run rate, taking 45% margins on the calls. With 40 million Americans looking for high-skilled jobs, they hope that this is a $2.4 billion market opportunity. The Lobby “turns employes into talent scouts.”\\n\\nVathys\\n\\nVathys is an artificial intelligence startup that makes a deep learning chip that is supposedly 10x faster than the competition. Computation accounts for 8 percent of power consumption of a chip. The rest is data movement, according to the founder. By addressing data movement, Vathys has created a processor that is 10x faster than the competition. The company already has a purchase order for $50K.\\n\\nCalifornia Dreamin’\\n\\nCalifornia Dreamin’ is a cannabis-infused THC sparkling juice beverage designed to get you a little high the same way a beer gets you a little drunk. Marijuana legalization has opened the door to a massive new industry, including psychoactive products for people who don’t want to smoke anything. Each bottle retails for $8 to $10, and contains 10mg of energizing sativa THC — one standard dose. It tastes sweet but with a hint of earthy plant notes, and you can drink a whole bottle without ending up locked into your couch like some competitors that put 100mg in a bottle. If California Dreamin’ can appeal to baby boomers and soccer moms looking to avoid the hangovers of alcohol while still imbibing something to relax, the business could blossom. California Dreamin’ is now delivering its first cases to recreational dispensaries in SF after selling $10,000-worth in its beta program. The legal cannabis business is $10 billion per year and growing 30 percent each year, but California Dreamin’ wants to nab part of the $210 billion alcohol market by getting people to switch to cannabis.\\n\\nRead more about California Dreamin’ on TechCrunch here.\\n\\nRhythmm\\n\\nRhythmm wants to take live chat conversations from experts and interesting people and let others follow along. The startup wants to take the insights people are getting from chat groups on Telegram and remove all the noise with their own platform. Only people who have been invited by the chat creator can participate, everybody else just follows the conversations.\\n\\nAlgosurg Inc\\n\\nAlgosurg has built algorithms to simulate surgeries. They believe that “robotics is the future of surgery,” and have developed something called Tabplan3D, which uses cloud technology instead of Xrays to help orthopedic surgeons prepare using a 3D virtual surgery plan. They have four patents filed and FDA approval in process.\\n\\nOSIMple\\n\\nOSIMple helps build automated infrastructure inspections — in other words, software that helps optimize the inspection of things like roads, railways, bridges, and dams. 500 different bridges were inspected with their software in the last 6 months, and they have letters of intent from 2 large companies.\\n\\nOrangewood Labs\\n\\nOrangewood Labs uses massive 3D printer-esque wood-cutting robots to make on-demand furniture. With no inventory, Orangewood avoids the costs of unsold goods or massive warehouses. It crowdsources 3D designs which it can tweak into different styles, colors, and sizes to fit your home. As more of commerce moves online, customers no longer expect to be able to test everything in person before they buy. Perhaps Orangewood will use augmented reality to virtually try its desks and chairs in your home. The startup has already scored $2 million in orders.\\n\\nOpenSea\\n\\nOpenSea is a peer-to-peer marketplace for buying, selling and discovering crypto assets and collectibles. Just as Coinbase operates as an exchange for cryptocurrencies, this is a marketplace for all other asset types on the blockchain, like software licenses, collectibles, Cryptokitties and other digital goods on the blockchain. In the last two months, Open Sea has done $500,000 in ethereal volume.\\n\\nPlaying Viral\\n\\nPlaying Viral is an Indonesian startup that assists online publishers through delivering leads for advertisers. The team works with surveys over visual ad units and can analyze the text to ensure that users are getting surveys in their language and dialect.\\n\\nCorrelia Biosystems\\n\\nSpun out of bioengineering research at UC Berkeley, Correlia Biosystems is able to analyze microsamples of blood, making it “easier to get more data from a tiny drop.” They claim to be “opening the space to next gen protein detection,” by reducing the time and cost for quantifying proteins for pharmaceutical applications. So far, they have brought in $55,000 in revenue in three months and have also secured $9.3 million in LOIs. Correlia believes this is a $11.6 billion market opportunity.\\n\\nSqreen\\n\\nSqreen is a tool that sits in your web app and tries to prevent attacks. It watches the behavior of users within your app to identify and block against SQL injections and cross-site scripting attacks. It currently works with apps built on Ruby on Rails, NodeJS, and Python.\\n\\nSqreen was in the TechCrunch Disrupt SF Battlefield in 2016. You can read more about Sqreen on TechCrunch here.\\n\\nPiccolo\\n\\nVoice assistants may be the home product du jour, but Piccolo wants you to control your smarthome with your hands. No, not like you already do, but by using gestures. Point at a light to turn it on, scrub through a video on your TV by moving your hand in front of you etc etc. The company is building a smart camera that can map a user’s skeletal geometry and see what the user wants to control.\\n\\nRead more about Piccolo on TechCrunch here.\\n\\nNexGenT\\n\\nBootcamps became insanely popular in the mid 2010s, but there’s been a big shakeout since then — and NexGenT hopes to take the fundamentals of getting an engineer production ready, but with a different approach. Rather than try to have somoene ready to be a full-scale developer in 3 months, NexGenT focuses on just certificate programs to get people ready to be network engineers. The process is longer, but hopefully more robust as well.\\n\\nRead more about NextGenT on TechCrunch here.\\n\\nVOICERY\\n\\nVoicery synthesizes ultra-realistic computer voices that can use natural emotion and inflection, and whisper or joke. 70 percent of people prefer its voices to Amazon Alexa’s. Voicery analyzes hundreds of human voices to train deep neural networks that power its product, rather than trying to train a computer to mimic a single specific voice. Voicery could be used to generate voice overs, read the news, dub television shows and more. It’s already got a letter of intent for $300,000 to make audio books. With more speech-enabled devices coming on the market every day, there could be a big market for giving different brands their own voice.\\n\\nRead more about Voicery on TechCrunch here.\\n\\nLetsDoThis.com\\n\\nA marketplace for endurance events, like 5ks, half-marathons and other races. Let’s Do This says revenue has doubled every two weeks since launching. So far, it has partnered with over 850 races, like IronMan and The Color Run.\\n\\nShone\\n\\nFlexport showed how ripe the freight industry is for disruption, Shone is retrofitting old cargo ships with its proprietary technology that allows them to deliver shipping containers autonomously and by remote control.\\n\\nQuit Genius\\n\\nQuit Genius is an app “that delivers personalized therapy to help people quit smoking.” It helps people identify the reasons they smoke and hopes to help them overcome them. They work with companies including a “technology giant” to pay for Quit Genius, in order to help employees. The startup says there is a $10 billion market for people trying to quit smoking, and wants to expand to alcohol and other addictions.\\n\\nMolly\\n\\nMolly uses AI to generate Q&As for people who are frequently asked the same questions — celebrities, field experts, etc.\\n\\nRead more about Molly on TechCrunch here.\\n\\nFlint\\n\\nFlint is a mobile banking solution for Mexico that lets people pay merchants and friends using scannable QR codes. And for the 56 percent of the population that is unbanked, Flint lets users deposit or withdrawal cash from their app’s account at local shops and restaurants. It’s essentially a crowdsourced ATM. Flint is growing payment volume 5X per month as it tries to do what WeChat and Alipay did for China.\\n\\nTingles\\n\\nA video platform for relaxation. Tingles is an iOS and Android app designed to help you relax and fall asleep. It’s a video genre called ASMR, which is supposedly growing 130 percent year over year. It currently has 60,000 monthly active users who view about 1.3 hours worth of content every day.\\n\\nRead more about Tingles on TechCrunch here\\n\\nRuna HR\\n\\nRuna HR wants to help small businesses in Latin America by automating payroll. The startup is pricing its product the same as US alternatives, but is seeking to strike a balance in Latin America between expensive, inefficient solutions and those that don’t solve all of their employees needs. The team at Runa believes it can eventually take over the entire software business for small businesses in Latin America.\\n\\nAerones\\n\\nAerones has built drones that clean wind turbines. They believe this is a safer and more efficient alternative to humans cleaning them and that this is a $1.4 billion market opportunity. They have heavy lifting drones that they hope will eventually clean buildings and have applications for other industries like oil, gas and solar. Aerones says that so far it has received 7 LOIs for 5000 cleanings, a promise of $5 million in revenue.\\n\\nSnackpass\\n\\nSnackpass is a food ordering app focusing on takeout rather than delivery. They’ve been running a field test at Yale, where they say 50% of students are monthly customers. There’s a social network-driven loyalty card aspect that they say is driving growth.\\n\\nReverie Labs\\n\\nReverie Labs uses machine learning to scan public molecule research, modify and develop its own molecules, and license the drugs they create to big pharmaceutical companies. The startup claims it can sell molecule licenses for $100 million, and has already signed a milestone deal worth up to $87 million. It expects to have 3 drugs ready for clinical trials by the end of 2019, which is much faster than most pharma companies take. The Harvard and MIT team plan to use their engineering-focused startup identity to recruit ML talent the pharma giants can’t match. And eventually, it wants to go full-stack and sell its own medicines.\\n\\nRead more about Reverie Labs on TechCrunch here\\n\\nWorklytics\\n\\nWorklytics wants to get rid of those boring team meetings, and the right place to look is probably at the management layer. The tool keeps track of the kinds of distractions that employees might get, as well as whether the right processes like one-on-one reviews and code collaboration, happen. It doesn’t track specific employees, but it uses the data it gleans from teams to figure out what works best to help companies run more efficiently.\\n\\nRead more about WorkLytics on TechCrunch here.\\n\\ntEQuitable\\n\\nA third-party, confidential software platform to solve workplace harassment. If an employee is feeling discriminated against or harassed they can open up the app to talk to a professional advisor, then create an action plan to either address the issue or file a formal complaint.\\n\\nThen tEQuitable will aggregate the data and come up with ideas to fix the company culture. The company’s clients include Twilio, GitHub, Obvious Ventures in pilot programs worth $120,000.\\n\\nStoryline\\n\\nStoryline wants to make it easy for people to build content apps for the Alexa platform without any coding. Storyline wants to power content across Alexa the same way YouTube powers video, they say. The startup’s apps already have 180 thousand monthly active users across 500 apps.\\n\\nRead more about Storyline on TechCrunch here.\\n\\nCaptivateIQ\\n\\nCaptivateIQ is software to calculate sales commissions. They believe that 80% of commissions have errors and there’s an opportunity to help companies get it right. They’re starting with the tech industry, but believe this is ultimately a $1.5 billion market opportunity. Beyond commissions, they think they can also help companies increase revenue.\\n\\nOvipost\\n\\nCricket farming for now may be an art, but Ovipost — starting with the rearing process — wants to turn it into a science. By first working to optimize hatching and egg yield, Ovipost wants to lower the labor costs to produce crickets, which could then be turned into direct consumer food sources or even feed.\\n\\nRead more about OviPost on TechCrunch here.\\n\\nVeriff\\n\\nVeriff wants to be Stripe for online identity verification, handling the processing of drivers licenses, passports, and IDs for websites. They did $60k in revenue in February, and are currently profitable. They charge ~$1 per verification.\\n\\nObserveAI\\n\\nObserveAI is AI-powered quality assurance for call center agents. They use natural language processing to determine how call center agents are interacting with customers and offer up suggestions as to what they might be doing better. The company charges $1000 per customer service agent per year.\\n\\n—\\n\\nCheck out the rest of TechCrunch’s YC Demo Day Coverage')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Learn all about AI and deep learning with this super on-sale course bundle', 'Just to let you know, if you buy something featured here, Mashable might earn an affiliate commission.\\n\\nWithout so much as a warning, Siri suddenly became smarter, Google Translate became more accurate, and Amazon could straightaway predict when you were going to buy a new pack of toilet paper. If you\\'re wondering how this happened, you can thank artificial intelligence and deep learning.\\n\\nYup, you read that right. They\\'re not just buzzwords thrown around by Elon Musk and Mark Zuckerberg to prove that they\\'re smarter than everyone else. These technologies are swiftly creeping into our lives and changing how the world works in more ways than we even realize.\\n\\nArtifical Intelligence vs. Deep Learning\\n\\nTo the uninitiated, artificial intelligence is an umbrella term that primarily refers to the ability of a computer to think and learn on its own. Deep learning is essentially a specific approach to artificial intelligence that\\'s inspired by the functionalities and patterns of the human brain. When deep learning is applied to computers, it\\'s essentially training them to learn as humans do via multiple layers and connections between neurons. Deep learning is responsible for a computer\\'s ability to recognize images, text, sound, and objects (like how your iPhone unlocks based on your face). Cool no? Thought so.\\n\\nPutting it into practice\\n\\nIf you\\'d like to learn more about these booming technologies (that are responsible for the highest paid/hottest jobs in tech), the AI & Deep Learning Bundle is a great place to start. Across seven e-books and ten hours of course content, this training will change the way you look at data and prepare you to begin a career in artificial intelligence. Or simply help you understand this world-altering technology. Whatever your goal, you can take it at your own pace (you get lifetime access) and at your own price (read on for details).\\n\\nBefore diving into the more complex concepts, you\\'ll first get a comprehensive introduction to AI. You\\'ll even get a snapshot of the history of AI and deep learning to help you understand where the technology could be headed in the future. The coursework includes reinforcement learning techniques, allowing you to gain a more in-depth understanding of what it takes to train machines to quickly learn from inputted data. You\\'ll learn different deep learning algorithms and how to use them — and then put those skills into practice,\\n\\nOf course, you\\'ll also get hands-on lessons on how to integrate deep learning into your own projects so that you can churn out intelligent applications centered on images, text, and time series data. Plus, you\\'ll discover how to utilize AI to build games, automatic speech recognition systems, and more.\\n\\nOK, but what is \"Pay What You Want\"?\\n\\nIf all this tech speak makes you want to smash your head against the wall, then that\\'s all the more reason you should snatch this bundle up. Collectively valued at $691, you can now get your hands on these e-books and courses for a ridiculously low price — like, under $15 low. Get one of the courses for whatever price you choose or beat the average price to take home everything included. The deal just launched, so you\\'ll want to get in while that average price is still that good.')\n",
      "('Nvidia just unveiled a terrifying AI supercomputer', 'Nvidia has unveiled several updates to its deep-learning computing platform, including an absurdly powerful GPU and supercomputer.\\n\\nAt this year\\'s GPU Technology Conference in San Jose, Nvidia CEO Jensen Huang unveiled the DGX-2, a new computer for researchers who are \"pushing the outer limits of deep-learning research and computing\" to train artificial intelligence.\\n\\nThe computer, which will ship later this year, is the world\\'s first system to sport a whopping two petaflops of performance. For some perspective: A Macbook Pro might have around one teraflop. A petaflop is one thousand teraflops.\\n\\nYeah, this thing is insane.\\n\\n\"Deep neural networks are getting deeper and smarter and able to do more things,\" Huang said in a press briefing. \"As they grow in size and complexity, so do the computing needs to train them.\"\\n\\nHuang also announced the company has doubled the memory capability of its Tesla V100 GPU, which the company claims delivers the performance of up to 100 CPUs in one graphics processor. This isn\\'t the GPU in your gaming PC — it powers artificial intelligence research and deep machine learning.\\n\\nA number of manufacturers including Dell EMC, Hewlett Packard, IBM, and Lenovo will roll out systems that use the GPU in the next few months, and Oracle in the second half of 2018.\\n\\nSo what does this mean for you?\\n\\nWell, if you\\'re Elon Musk, you may want to prepare for the apocalypse. Equipped with these systems, researchers will be able to train smarter deep-learning AI more quickly than we\\'ve ever seen.\\n\\nBut in the near future, you\\'ll probably just see improvements to services that use AI, such as speech recognition and translation. The company claims that the new technologies will better enable AI to create natural-sounding speech and text, and that researchers will need less time to train such networks.\\n\\nXuedong Huang, Microsoft\\'s head of speech and language, said in a statement that the new V100 GPUs will \"extend the accuracy of our models on speech recognition and machine translation reaching human capabilities and enhancing offerings such as Cortana, Bing, and Microsoft Translator.\"')\n",
      "('People are using face-swapping tech to add Nicolas Cage to random movies and what is 2018', \"For some people, the future of technology is synonymous with a boundless realm of opportunities.\\n\\nFor others it's a Black Mirror-inspired nightmare fest, where humanity is just one step away from bringing about its own, possibly robot-induced destruction.\\n\\nAnd for others still, it simply means face swapping the hell out of Nic Cage so it looks like he starred in every movie ever.\\n\\nIndiana Jones → Nic Cage face swap, via deep learning. https://t.co/ZfPUMfjFiZ pic.twitter.com/Kmq4HZOcwn — Manish Vij (@manish_vij) January 27, 2018\\n\\nYep: face-swapping using the free tool FakeApp, the technology that's been in the news recently after a people began using it to splice actors with pornstars, has now found a whole new use. And, unexpectedly, it involves Nicolas Cage.\\n\\nLook, here he is as James Bond:\\n\\nAnd here he is in, er, Man of Steel.\\n\\nThe deepfake algorithm must be stoped. pic.twitter.com/ytmgziXwFd — أوسا (@ousathesquid) January 27, 2018\\n\\nThis trend of face-swapping Nicolas Cage seems to originate from a Reddit user called u/derpfakes, who has been sharing his creations in the (definitely NSFW) subreddit r/deepfakes (the main source of that Motherboard story mentioned above).\\n\\nUnsurprisingly, people had some thoughts.\\n\\nYour scientists were so preoccupied with whether or not they could, that they didn't stop to think if they should. — Zach Smith (@zac_smith1) January 29, 2018\\n\\n1998: AI will change our society, it will revolutionize industries and ease our daily life.\\n\\n\\n\\n2018: We face swapped Nicolas Cage in every movie. https://t.co/Ji1bWSvuGV — Thomas Altenburger🍕 (@mrhelmut) January 30, 2018\\n\\nThe future is looking stranger and stranger by the day.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"A guide to 'deepfakes,' the internet's latest moral crisis\", 'Where there\\'s innovation, there\\'s masturbation — at least in one dark corner of the internet, where nearly 80,000 people have gathered to share fabricated videos of celebrity women having sex and Nicolas Cage uncovering the Ark of the Covenant.\\n\\nThese are \"deepfakes,\" a new kind of video featuring realistic face-swaps. In short, a computer program finds common ground between two faces and stitches one over the other. If the source footage is good enough, the transformation is nearly seamless.\\n\\nThe technology is relatively easy to use, which has created an enthusiast community on Reddit, where users compare notes and swap their latest work: \"Emma Watson sex tape demo ;-),\" \"Lela Star x Kim Kardashian,\" and \"Giving Putin the Trump face\" among them.\\n\\nThis scene from \"Raiders of the Lost Ark\" originally starred Harrison Ford, not Nicolas Cage, though an untrained eye may not know the difference. Image: Nick Cage DeepFakes/YoiuTube\\n\\nMotherboard did foundational reporting on deepfakes in December and continues to cover the trend, with despairingly predictable news last week that people are using the technology to create porn starring friends and classmates. But legal and computer science experts told Mashable that the technology\\'s grimier applications shouldn\\'t overshadow its potential for good, even if it\\'s difficult to see the upside when non-consenting stars are being jammed into hardcore sex scenes with hundreds of thousands of views on Pornhub and Reddit.\\n\\nThe latter company didn\\'t respond to requests for comment over the course of a week, but Pornhub said it will remove deepfakes from its platform.\\n\\n\"Users have started to flag content like this, and we are taking it down as soon as we encounter the flags,\" Corey Price, PornHub\\'s vice president, said. \"We encourage anyone who encounters this issue to visit our content removal page so they can officially make a request.\"\\n\\nStill, to be very clear: All of this should freak you out.\\n\\nGal Gadot\\'s face, digitally stitched into a porn clip Image: reddit\\n\\nAbove, we see Gal Gadot\\'s face superimposed onto a porn actress, moments before she pulls her shirt off and gets felt up. Consent didn\\'t factor into the equation for the Redditor who made this clip, and a casual observer wouldn\\'t know the video is fake if they received the file from a friend via text message or email, because the transformation is so well done.\\n\\nThe issue is pretty simple: A person who has not consented to a sexual situation should not be put into that situation, whether in physical or virtual life. But the genie is out of the bottle, and it\\'s staying there. \"Gal Gadot\" remains one of the top terms associated with deepfake searches on Google, as the company\\'s own Trends data shows:\\n\\nThis underscores the urgency of the problem, even if it\\'s an emerging one. Content published to the internet can be hard to erase, particularly when there\\'s a group of people invested in duplicating and spreading it. People could stop creating new deepfakes tomorrow, but Gal Gadot\\'s clips could live on indefinitely.\\n\\nWant help? It\\'s murky\\n\\nThere\\'s not much legal recourse for those who fall victim to this new technology, according to Jonathan Masur, a professor who specializes in patent and technology law at the University of Chicago Law School. That\\'s true even for private citizens.\\n\\n\"There\\'s the copyright claim, if you took the [footage] yourself. There\\'s the defamation claim if someone tries to say that it\\'s actually you. And if you\\'re a celebrity, there\\'s a right to publicity claim if someone is trying to make money off of it,\" Masur explained. \"But each of those is just a narrow slice of what\\'s going on here that won\\'t cover the vast majority of situations.\"\\n\\nMany of the of these videos acknowledge they\\'re fake, which undermines a defamation argument.\\n\\n\"[You] could try to make a case it represents a form of defamation if you\\'re attacking the reputation of someone, but that\\'s also pretty hard to do because, by definition, you\\'re not alleging you\\'re posting a pornographic picture of that individual,\" he said.\\n\\nAnd, no, recent efforts to ban revenge pornography, led by Mary Ann Franks and Danielle Citron, wouldn\\'t be applied in these cases, because those laws pertain to the release of private images or video of an individual.\\n\\n\"There\\'s no pornographic picture of the actual individual being released,\" Masur said. \"It\\'s just the individual\\'s face on someone else\\'s body.\"\\n\\nDeepfakes also have political ramifications, though no one would fall for this fusion of Donald Trump and Hillary Clinton. Image: thegoncas2/youtube\\n\\nThere aren\\'t any laws against this practice yet, nor have they been introduced. Tackling deepfakes via new legislation would be tricky, as doing so would bump against the First Amendment.\\n\\n\"From a civil liberties perspective, I am... concerned that the response to this innovation will be censorial and end up punishing and discouraging protected speech,\" David Greene, the civil liberties director at the Electronic Frontier Foundation, a nonprofit focused on digital free speech, said.\\n\\n\"It would be a bad idea, and likely unconstitutional, for example, to criminalize the technology,\" he added.\\n\\nThe unexpected upside\\n\\nGreene\\'s concerns may not be unfounded. Though deepfakes are now synonymous with porn, the basic concept behind the technology is facial recognition, which theoretically has a lot of upside to be explored.\\n\\nYou may already be familiar with basic, live facial recognition from apps like Snapchat. The technology is programmed to map faces according to \"landmark\" points. These are features like the corners of your eyes and mouth, your nostrils, and the contour of your jawline.\\n\\nSnapchat is pretty good at understanding your face and applying transformative effects, which augment your features:\\n\\nOne of the authors, as a chipmunk. Image: Snapchat\\n\\nBut its face-swapping feature leaves something to be desired:\\n\\nImage: snapchat\\n\\nPart of that has to do with Snapchat working in real-time — it\\'s trading speed for accuracy.\\n\\nDeepfakes work differently. The \"FakeApp\" program uses artificial intelligence to complete three major steps: alignment, training, and merging. Instead of placing one face over another in real-time, the FakeApp uses hundreds of still-frame images pulled from video footage. It digs through all of those images, identifies faces, and analyzes how they\\'re lit, what expressions they\\'re making, and so on. Once the program understands the faces it\\'s working with, it can use all of its \"knowledge\" to stitch one over the other.\\n\\nThough it\\'s been put to a gross purpose, deepfakes\\' seamlessness could be an encouraging sign, depending on your perspective. With enough development, real-time face swaps could achieve similar quality to deepfakes, which may have therapeutic uses, according to Dr. Louis-Philippe Morency, director of the MultiComp Lab at Carnegie Mellon University.\\n\\n\"This technology has very important applications apart from entertainment,\" he said.\\n\\nOne moonshot example: Dr. Morency said soldiers suffering from post-traumatic stress disorder could eventually video-conference with doctors using similar technology. An individual could face-swap with a generic model without sacrificing the ability to convey his or her emotions. In theory, this would encourage people to get treatment who might otherwise be deterred by a perceived stigma, and the quality of their treatment wouldn\\'t suffer due to a doctor being unable to read their facial cues.\\n\\nAnother one of Dr. Morency\\'s possibilities — and its own can of worms — would be to use models in video interviews to remove gender or racial bias when hiring. But for any of this to happen, researchers need more data, and open-source, accessible programs like FakeApp can help create that data.\\n\\n\"The way to move forward with AI research is to share the code, and share the data. This is an enabler for AI research,\" Dr. Morency said.\\n\\nIt somehow gets worse\\n\\nAs with many emerging technologies, the scariest part may be unseen. When Facebook first rolled out on college campuses, few would have anticipated its transformation into a multimedia Goliath that potentially destabilized American democracy as we knew it — but here we are.\\n\\nLike the \"fake news\" that has exhausted so many of us on Facebook, deepfakes represent yet another capacity for the internet to breach our shared reality. If every video clip could potentially be fake, why believe anything is real?\\n\\nAnd so, expect the response from your unborn grandchild: \"Raiders of the Lost Ark? You mean the one with Nicolas Cage?\"')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('This 9-course comprehensive coding bundle is on super-sale', \"Just to let you know, if you buy something featured here, Mashable might earn an affiliate commission.\\n\\nCoding is a competitive career track that changes quickly. If you want to stay on the bleeding edge of the industry, you’re going to need to learn the skills that will keep you ahead of the rest of the pack. That means keeping up with the latest programming languages, and we’re happy to say that the Complete 2018 Learn to Code Bundle is the most comprehensive and up to date package we’ve ever offered — and it's just $34.99. It includes nine courses and 210 hours of training that covers:\\n\\nThe Complete Java Masterclass\\n\\nThis class will take you from Java beginner to Java master with 330 lectures and 72 hours of content. Whether you want to go freelance, become an in-demand Java developer, sharpen your programming skills or just mess around with your own programming ideas, this class in one of the most popular and in-demand languages is for you.\\n\\nThe Complete Google Go Programming Course For Beginners\\n\\nGolang, the programming language developed by Google, has taken the world by storm and is poised to become one of the most valuable programming languages to know in the 21st century. These 51 lectures and five hours of content will make sure you’re on the front lines of this exciting new coder’s market.\\n\\nDissecting Ruby on Rails 5\\n\\nThis is a special course that not only teaches you Ruby on Rails, but the basics of full stack coding. With a focus on backend development with Ruby on Rails and front-end coding with HTML, CSS, and Javascript, you'll learn the foundations of what it takes to be a full stack coder. Plus, this course was developed by consulting with CTOs and Senior Rails developers around the world so you're bound to gather some useful knowledge over 212 lectures and over 37 hours of content.\\n\\nWorking With Financial Data in R\\n\\nYou can’t run a financial portfolio without risk assessment, and the R programming language is the best way to approach risk modeling, factor analysis, numerical optimization, linear regression and logistic regression. With 138 lectures and over 15 hours of content, you'll unlock the secrets you need for a future in data science.\\n\\nComplete PHP Course with Bootstrap3 CMS System & Admin Panel\\n\\nNot only will you learn PHP in this course — one of the most popular web development and general programming courses around — you’ll also learn advanced techniques for creating content management systems. It packs 143 lectures and almost 20 hours of content that will turn you into the kind of knowledgeable website manager every company needs.\\n\\nThe Complete Javascript Developer Course: Build a Professional Project\\n\\nThere’s no better way to improve your marketability as a developer than mastering JavaScript. This course will rocket you from newbie to developer with 220 lectures and nearly 27 hours of content.\\n\\nFrom 0 to 1: Learn Python Programming - Easy as Pie\\n\\nPython threads the needle by being simultaneously easy and powerful, which is why this course only needs nine hours of instruction to teach you how to write clean, efficient Python code.\\n\\nAdvanced Machine Learning in Python with TensorFlow\\n\\nOnce those Python skills are mastered, put them to good use with TensorFlow by designing, building, and training deep learning modules. One hundred thirty-six lectures spread across 17 hours of content will teach you the anatomy of TensoFlow programs and beyond.\\n\\nGit Complete: The Definitive, Step-By-Step Guide\\n\\nGit is an extremely popular tool used by engineering teams to track edits in programs. It makes it easy to see where you went wrong.\\n\\nAltogether, this is $1,356.95 worth of programming training. In the Mashable Store, that price has been reduced to just $49.99, and then reduced again to a stunning $34.49. Pretty good deal for a whole new career, if you think about it.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('This surreal AI creature meets humans, then decides whether it wants to play with them', 'In the middle of London\\'s Hyde Park lives a strange and wondrous creature. Its name is BOB, and it\\'s an AI entity.\\n\\nSEE ALSO: MIT created this imposter robot fish to spy on sea creatures\\n\\nBOB (\"Bag of Beliefs\") is an art installation by artist Ian Cheng. Cheng specialises in simulations, pieces he described to Mashable as \"video games that play themselves.\"\\n\\nBOB is his latest creation, an AI creature currently resident in the Serpentine Gallery in London. Technically there are six BOBs. They were all born (for lack of a better word) simultaneously on March 6. Their growth, behaviour and personalities have all been individually moulded by their interactions with visitors.\\n\\nUsing a smartphone, visitors are able to possess one of BOB\\'s multiple heads, and moving around the exhibition space they tug on him. As you move about, BOB simultaneously reads your facial expression, mirroring it with an animoji version of one of its numerous heads.\\n\\nA visitor possesses a part of BOB with the power of AR. Image: Maria dermentzi/mashable\\n\\nHowever, BOB will not automatically interact with you. It has the ability to choose whether to interact with visitors, and equally BOB can decide it\\'s had enough and bite off the head possessed by the visitor, essentially kicking them out of its world.\\n\\nBOB\\'s appearance is hard to describe, not least because each BOB has grown a little differently. Broadly speaking BOB is made up of long, ridged segments and strange, fantastical looking heads. Imagine Lewis Carroll got seriously into coding.\\n\\nPlaying with BOB was a pretty surreal experience when Mashable went along to visit. You have no idea what BOB is taking away from the interaction, plus when it bites off its own head just to get rid of you, the rejection stings a little.\\n\\nGood morning BOB! 🤳🐛 A post shared by Ian Cheng (@eyecheng) on Mar 7, 2018 at 6:10am PST\\n\\nHow BOB\\'s AI works\\n\\nSo what do these interactions mean to BOB?\\n\\n\"BOB at 10 to 20 second intervals is taking a snapshot of many different aspects of the situation BOB is in,\" Ian Cheng said.\\n\\n\"For example when you\\'re interacting with BOB, BOB is taking a snapshot of your facial motion at that moment, maybe the time of the day, BOB\\'s current energy levels and metabolism, BOB\\'s body growth, and about 25 other different parameters.\"\\n\\nBOB looks at a head which has been possessed by a visitor (on the right). Image: Maria dermentzi/Mashable\\n\\n\"I really want a viewer to think of it more like going to an animal sanctuary\"\\n\\nThese snapshots are then recorded together in a bundle, which is called a \"memory.\" BOB is then able to draw on these memories, comparing the closest memory that matches its current condition.\\n\\nIan Cheng conceived of an AI creature as \"a kind of container, a space of composition [...] in the way that a painter might think of a canvas as a space to contain all their ideas.\"\\n\\n\"My only expectation and my hope for a person going into the exhibition is that they simply feel like what they\\'re encountering is something alive,\" he said. \"I really want a viewer to think of it more like going to an animal sanctuary.\"\\n\\nIs BOB a living thing?\\n\\n\"The question of whether BOB is alive or not touches upon larger issue of how we, as human beings today, start to expand the idea of what sentience means,\" said Cheng.\\n\\n\"I think BOB is as alive as an ant is\"\\n\\n\"I think BOB is as alive as an ant is,\" he said. \"Sentience is a spectrum and as soon as you get a biological organism or an artificial organism that is ingesting its senses and then trying to make a decision based on its senses, and evolving its decision-making, I think you could give it the status of sentience.\"\\n\\nThis ingestion and interpretation of its senses is what gave BOB its name (Bag of Beliefs). Cheng believes that what truly makes something alive is \"its beliefs about its senses,\" rather than any organic process.\\n\\nA different, less deferential kind of AI\\n\\nOne of Cheng\\'s goals when creating BOB was to create an AI entity that broke the mould by having its own sense of narcissistic autonomy.\\n\\n\"When we think of AI right now we think of Siri or Alexa, who have a kind of concierge narrative built into them,\" said Cheng. \"I thought, if I was going to make an AI I really wanted my AI to be much more opinionated than a concierge.\"\\n\\nBOB has got zero time for your anthropocentric attitude. Image: maria Dermentzi/mashable\\n\\nCheng said that he was inspired by characters like Smaug from The Hobbit or Tamatoa from Moana, who although non-human and monstrous, appear to have lives outside of the story of the main characters.\\n\\n\"When you encounter them they\\'re in repose,\" said Cheng. \"Smaug was just having a sleepy little time, but what\\'s so beautiful with that depiction is that it suggests on the part of Tolkien that [he] imagined Smaug to have an autonomous life.\"\\n\\nBeyond BOB, the future of AI\\n\\nFor Cheng, AI is a new artistic medium to be used, and he\\'s optimistic about the future of artificial intelligence. \"I\\'m very optimistic about the future of AI,\" he said. \"I think there\\'s enough smart people now thinking about all the dangers of AI.\"\\n\\n\"I think it\\'ll feel more like a partnership rather than a hierarchy\"\\n\\n\"The idea that a computer can start to create its own rules, and start to symbolically reason about what it\\'s looking at, will marry with machine learning. I think we\\'ll very soon have something that approaches the capacity of our ability as human beings to both react and to reason.\"\\n\\nThe notion of an AGI (Artificial General Intelligence) that could reflect upon the act of thinking — coupled with pattern recognition deep learning — is to Cheng a wonderful thing.\\n\\n\"As soon as you get a machine that enjoys thinking — it\\'s a very optimistic answer — but I think we\\'ll have a world in which we\\'re populated and co-habitating with other thinkers. I think that\\'s kind of a beautiful thing.\"\\n\\n\"It scares me a little bit, but only in a kind of anthropocentric, \\'am-I-gonna-be-turned-into-a-pet\\' kind of way,\" he said. \"I think it\\'ll feel more like a partnership rather than a hierarchy.\"\\n\\nWhatever the future may hold, BOB is available to visit until April 22. Which is presumably when it will rise up and overthrow us for our hubris.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Everything you need to know about shopping at Amazon Go', 'Amazon Go, the grocery store of the future where shoppers can just walk in and walk out with what they want (no dealing with sassy cashiers necessary), has finally arrived for the public, and Seattle\\'s the first lucky city to get one.\\n\\nNaturally, a checkout-less physical shopping experience is uncharted territory. What is it like and how does it work? Not to worry, because we\\'ve got answers for your burning questions.\\n\\nHow does it work?\\n\\nThe entire store experience revolves around the Amazon Go app. You can download the app for iOS here and Android here .\\n\\nUpon entering the store, you scan the app at one of the entrance\\'s turnstiles. Then, you simply go up to shelf, grab what you want to buy, dump it in your bag, and the app automagically rings all your items up — no scanning required.\\n\\nImage: screenshot: Google play Image: SCREENSHOT: GOOGLE PLAY\\n\\nAnd how exactly does the app keep track of all the things you\\'re placing in your shopping bag? According to Amazon, its \"Walk Out\" technology uses \"computer vision, sensor fusion, and deep learning\" to track what you\\'ve purchased.\\n\\nGotta love the tech jargon, but what that means in non-geek speak is: There are sensors on the shelves that detect when an item\\'s been removed and added back. Cameras placed all over the store will scan the dotted-code on an item\\'s label and add it to your receipt, and then ta-da!\\n\\nSee the black squares everywhere? It’s cameras & sensors feeding an underlying algorithm for the detection of products & customers. Black dots on the salad: machine readable information. #AmazonGo #handelsblatt pic.twitter.com/Py0KR37jKJ — Britta Weddeling (@bweddeling) January 22, 2018\\n\\nAmazon Prime not required\\n\\nYep, you read that right. For once, Amazon\\'s not forcing you into paying for its Prime membership in order to get the latest perk. All you need to shop at the Go store is the app.\\n\\nWhat does the Go store look like?\\n\\nGeekWire\\'s Todd Bishop did a Facebook Live stream inside of the store (see above). There are white turnstiles (kinda like ones you find inside of subway stations) at the entrance with QR code scanners to scan your phone.\\n\\nBishop says \"a little above 90 people\" (the maximum capacity to abide by the fire code) are allowed inside of the store at once.\\n\\nAnd it looks squeaky clean! The store looks bright and the aisles appear roomy.\\n\\nIt\\'s only open Monday-Friday\\n\\nThe Go store is located at 2131 7th Ave, Seattle, WA and it\\'s only open Monday-Friday from 7 a.m. to 9 p.m.\\n\\nIn other words: You\\'re sh*t out of luck if you do your grocery shopping on the weekends like a genuine basic millennial.\\n\\nThe line to get in is loooooong\\n\\nLiving in or visiting Seattle and eager to check out the Go store for yourself? Good luck, because the line to get in is extremely long.\\n\\nIf you have time to wait to get in a store to shop, you definitely have time to wait on line at a store with cashiers or self-checkouts. Same thing, buddy. Who knows when the lines will shorten.\\n\\nLong lines as #AmazonGo opens now. The hi-tech grocery store in #Seattle opens to the public TODAY at 7am. #komonews pic.twitter.com/lwKphK38Yy — Suzanne Phan (@SuzannePhan) January 22, 2018\\n\\nThere\\'s still a staff\\n\\nJust because there aren\\'t any cashiers doesn\\'t mean the store\\'s devoid of any helpful staff. Twitter user Cory Nelson made a video of his shopping experience and confirmed there are in-store workers to help you out if you have questions, need help finding something, or maybe something screws up.\\n\\nYou can get in and out FAST\\n\\nThe whole point of Amazon Go is to help you get in, buy what you need, and get the hell out. Go to a park if you want to bump into your future soulmate.\\n\\nSome people are reporting customers getting in and out in a minute. One minute!\\n\\nIt\\'s pretty impressive to watch people walk into #AmazonGo and walk back out in about a minute with what they picked up. No lines, no cashiers, no wait. pic.twitter.com/GIn6htkxfN — KING5 Photog Jim (@King5unit9) January 22, 2018\\n\\nYou can use your app to let friends and family in\\n\\nBecause the app is necessary to track your every purchase, it looks like you may have to leave your significant other, friend(s), and kid(s) at home if they don\\'t have a device with the app installed.\\n\\nMakes sense. You can\\'t get past the turnstiles if you don\\'t scan the app. We\\'ve reached out to Amazon to confirm whether this is truly the case.\\n\\nWanna bring your friends and family who don\\'t have the app installed on their phones? No problemo.\\n\\nImage: screenshot: jack morse, amazon\\n\\nBut maybe don\\'t do this:\\n\\nGive it a month, then walk into an #AmazonGo store and ask a fellow shopper to help you get something off a high shelf. Hilarity to ensue. pic.twitter.com/5L473l0KA2 — it\\'s kæt 🔶 (@dakotathekat) January 22, 2018\\n\\nYeah, clearly shopping at a Go store is not a communal thing.\\n\\nSee prices ahead of time\\n\\nHow many times have you gone to a grocery store, only to see that something costs way to much? Or how many times have you had to go to multiple stores just to find the lowest price for an item?\\n\\nWell, never again. According to Twitter user Andrew Martonik, the Go app lists prices for all of the items sold in the store, so you can decide before you head over whether the price is right.\\n\\nOne thing that immediately makes Amazon Go appealing: the app shows you prices for everything in the store before you get there. Downtown convenience stores typically have many items with absurd prices, and you don\\'t find out until you\\'re in there (and end up just buying it). pic.twitter.com/nHx6TL2cM0 — Andrew Martonik (@andrewmartonik) January 22, 2018\\n\\nCheaper prices (maybe)\\n\\nEarly customers of the Go store have found some items to be cheaper than Whole Foods, at least for all-essential things like La Croix:\\n\\nMost important test for Amazon Go patrons: @lacroixwater prices. pic.twitter.com/YlcC512Pcp — Andrew Martonik (@andrewmartonik) January 22, 2018\\n\\nIt\\'s not like you need anything else but flavored seltzer water, right? Of course you don\\'t.\\n\\nFood stamps aren\\'t accepted\\n\\nUnfortunately not, Slate reports. Amazon confirmed to the publication the store doesn\\'t accept EBT.\\n\\nYou would think that since Amazon owns Whole Foods, which does take food stamps, its own Go stores would take them too. But sadly, for now, they don\\'t. We\\'ve reached out to Amazon for comment on future plans for EBT.\\n\\nI just confirmed: the new Amazon Go store does not accept food stamps. — april glaser (@aprilaser) January 22, 2018\\n\\nYou might accidentally shoplift something\\n\\nThough customers have said Amazon\\'s cameras and sensors are pretty accurate tracking which items you pull off shelves and place in your bag, there\\'s also a chance they might forget to charge you for something, just like a careless cashier:\\n\\nSmile for all the cameras\\n\\nThink you can get away with shoplifting the place? HAHAHA. Think again. The second you enter the store the many cameras installed on the ceiling will have captured who knows how many photos of you from every angle.\\n\\nNote the form chosen for the cameras in the Amazon Go store - camouflaged and near-featureless. Imagine how replacing them all with traditional CCTV cameras would change the space, and its effect on people. The continual bait-and-switch of convenience for privacy. pic.twitter.com/wJrLVoORco — Wesley Goatley (@wesleygoatley) January 22, 2018\\n\\nWe\\'re pretty sure the cops won\\'t have a hard time finding you.\\n\\nLots of unanswered questions\\n\\nAmazon Go sounds like the perfect grocery shopping experience, but there are still a lot of things nobody has answers to yet, like:\\n\\nWhat if you put more in your bag than $ you have in your account? 🤔 #amazongo Also how are ppl not going to be clogged in these gates!?What if you’re shopping with someone else but you’re holding their stuff ? pic.twitter.com/wZoA8TVd66 — Booberlee (@booberlee) January 22, 2018')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Google works out a fascinating, slightly scary way for AI to isolate voices in a crowd', 'Google researchers have developed a deep-learning system designed to help computers better identify and isolate individual voices within a noisy environment.\\n\\nAs noted in a post on the company\\'s Google Research Blog this week, a team within the tech giant attempted to replicate the cocktail party effect, or the human brain\\'s ability to focus on one source of audio while filtering out others—just as you would while talking to a friend at a party.\\n\\nGoogle\\'s method uses an audio-visual model, so it is primarily focused on isolating voices in videos. The company posted a number of YouTube videos showing the tech in action:\\n\\nThe company says this tech works on videos with a single audio track and can isolate voices in a video algorithmically, depending on who\\'s talking, or by having a user manually select the face of the person whose voice they want to hear.\\n\\nGoogle says the visual component here is key, as the tech watches for when a person\\'s mouth is moving to better identify which voices to focus on at a given point and to create more accurate individual speech tracks for the length of a video.\\n\\nAccording to the blog post, the researchers developed this model by gathering 100,000 videos of \"lectures and talks\" on YouTube, extracting nearly 2,000 hours worth of segments from those videos featuring unobstructed speech, then mixing that audio to create a \"synthetic cocktail party\" with artificial background noise added.\\n\\nGoogle then trained the tech to split that mixed audio by reading the \"face thumbnails\" of people speaking in each video frame and a spectrogram of that video\\'s soundtrack. The system is able to sort out which audio source belongs to which face at a given time and create separate speech tracks for each speaker. Whew.\\n\\nGoogle singled out closed-captioning systems as one area where this system could be a boon, but the company says it envisions \"a wide range of applications for this technology\" and that it is \"currently exploring opportunities for incorporating it into various Google products.\" Hangouts and YouTube seem like two easy places to start. It\\'s not hard to see how the tech could work when applied to a pair of smart glasses, à la Google Glass, and voice-amplifying earbuds, either.\\n\\nFurther Reading Pixel Buds review: OK Google, go back to the earbud drawing board Aiding smart speakers like the Google Home in their ability to recognize individual voices seems like another use case, but because this model is focused on video, it would likely work better with a speaker with a display, like Amazon\\'s Echo Show. Earlier this year, Google opened up the Google Assistant to \"smart display\" devices like the Echo Show, but the company hasn\\'t released one itself.\\n\\nIn any case, the privacy ramifications of this kind of tech seem just as obvious as the potential use cases. Google\\'s voice isolation is far from bulletproof in the examples above, but with some more fine-tuning, it could make for a powerful eavesdropping and surveillance tool in the wrong hands.\\n\\nThat\\'s a lot of speculation for now, though. Here\\'s hoping this research at least lessens the need to shout at Google Home in the future.')\n",
      "('Noodle on this: Machine learning that can identify ramen by shop', \"With 41 locations around Tokyo, Ramen Jiro is one of the most popular restaurant franchises in Japan, because of its generous portions of toppings, noodles and soup served at low prices. They serve the same basic menu at each shop, and as you can see above, it's almost impossible for a human (especially if you're new to Ramen Jiro) to tell what shop each bowl is made at.\\n\\nBut Kenji thought deep learning could discern the minute details that make one shop’s bowl of ramen different from the next. He had already built a machine learning model to classify ramen, but wanted to see if AutoML Vision could do it more efficiently.\\n\\nAutoML Vision creates customized ML models automatically—to identify animals in the wild, or recognize types of products to improve an online store, or in this case classify ramen. You don’t have to be a data scientist to know how to use it—all you need to do is upload well-labeled images and then click a button. In Kenji’s case, he compiled a set of 48,000 photos of bowls of soup from Ramen Jiro locations, along with labels for each shop, and uploaded them to AutoML Vision. The model took about 24 hours to train, all automatically (although a less accurate, “basic” mode had a model ready in just 18 minutes). The results were impressive: Kenji’s model got 94.5 percent accuracy on predicting the shop just from the photos.\")\n",
      "('Learn with Google AI: Making ML education available to everyone', \"During college, while doing a geophysics internship aboard an oil rig, I realized that software was the future—so I switched my major to computer science. After more than a decade working at Google, I had a similar moment where I realized that AI is the future of computer science. Today, I lead Google’s machine learning education effort, in the hope of making AI and its benefits accessible to everyone.\\n\\nAI can solve complex problems and has the potential to transform entire industries, which means it's crucial that AI reflect a diverse range of human perspectives and needs. That’s why part of Google AI’s mission is to help anyone interested in machine learning succeed—from researchers, to developers and companies, to students like Abu.\\n\\nTo help everyone understand how AI can solve challenging problems, we’ve created a resource called Learn with Google AI. This site provides ways to learn about core ML concepts, develop and hone your ML skills, and apply ML to real-world problems. From deep learning experts looking for advanced tutorials and materials on TensorFlow, to “curious cats” who want to take their first steps with AI, anyone looking for educational content from ML experts at Google can find it here.\\n\\nLearn with Google AI also features a new, free course called Machine Learning Crash Course (MLCC). The course provides exercises, interactive visualizations, and instructional videos that anyone can use to learn and practice ML concepts.\\n\\nOur engineering education team originally developed this fast-paced, practical introduction to ML fundamentals for Googlers. So far, more than 18,000 Googlers have enrolled in MLCC, applying lessons from the course to enhance camera calibration for Daydream devices, build virtual reality for Google Earth, and improve streaming quality at YouTube. MLCC's success at Google inspired us to make it available to everyone.\\n\\nThere’s more to come from Learn with Google AI, including additional courses and documentation. We’re excited to help everyone learn more about AI.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The fight against illegal deforestation with TensorFlow', 'Editor’s Note: Rainforest Connection is using technology to protect the rainforest. Founder and CEO Topher White shares how TensorFlow, Google’s open-source machine learning framework, aids in their efforts.\\n\\nFor me, growing up in the 80s and 90s, the phrase “Save the Rainforest” was a directive that barely progressed over the years. The appeal was clear, but the threat was abstract and distant. And the solution (if there was one) seemed difficult to grasp. Since then, other worries—even harder to grasp in their immediacy and scope—have come to dominate our conversations: climate change, as an example.\\n\\nSo many of us believe that technology has a crucial role to play in fighting climate change, but few are as aware that “Saving the Rainforest” and fighting climate change are nearly one and the same. By the numbers, destruction of forests accounts for nearly one-fifth of all greenhouse gas emissions every year. And in the tropical rainforest deforestation accelerated on the heels of rampant logging—up to 90 percent of which is done illegally and under the radar.\\n\\nStopping illegal logging and protecting the world’s rainforests may be the fastest, cheapest way for humanity to slow climate change. And who’s best suited to protect the rainforest? The locals and the indigenous tribes that have lived there for generations.\\n\\nRainforest Connection is a group of engineers and developers focused on building technology to help locals—like the Tembé tribe from central Amazon—protect their land, and in the process, protect the rest of us from the effects of climate change. Chief Naldo Tembé reached out to me a couple years ago seeking to collaborate on ways technology could help stop illegal loggers from destroying their land. Together, we embarked on an ambitious plan to address this issue using recycled cell phones and machine learning.\\n\\nOur team has built the world’s first scalable, real-time detection and alert system for logging and environmental conservation in the rainforest. Building hardware that will survive in the rainforest is challenging, but we’re using what’s already there: the trees. We’ve hidden modified smartphones powered with solar panels—called “Guardian” devices—in trees in threatened areas, and continuously monitor the sounds of the forest, sending all audio up to our cloud-based servers over the standard, local cell-\\xadphone network.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"What it's like inside Amazon's futuristic, automated store - CNET\", 'I open the Amazon Go app on my phone, scan a QR code on top of the white turnstile and watch as electronic arms open to let me in. Glancing around the convenience store, which is bustling with Amazon employees shopping for lunch items, I head left to the cold drinks section, grab a can of Diet Pepsi and exit through the barrier. Not once did I pull out my wallet or tap my phone anywhere to make a payment.\\n\\nShara Tibken/CNET\\n\\nNo, I\\'m not a shoplifter. I\\'m getting a preview of the Amazon Go automated store within the internet giant\\'s headquarters in Seattle. It opens to the public Monday.\\n\\nThe biggest feature of the store, one of the first shops of its kind, is the fact there are no cashiers. When you arrive, you scan the Amazon Go app on your iPhone or Android phone at a turnstile to register your presence and enter the store. After that, everything you pick up is automatically tracked by the store\\'s cameras and charged to your Amazon account when you walk out. It all happens without you having to check in with a store employee or physically make a payment.\\n\\n\"Our plan from the beginning was ... what can we do so you could walk into the place, take anything you want and leave,\" Gianna Puerini, vice president of Amazon Go, says as we walk around the store four days before it opens to the public.\\n\\nAmazon Go was a surprise announcement from the company, timed right in the middle of the 2016 holiday shopping season to get Amazon plenty of buzz. It\\'s an 1,800-square-foot convenience store, built at the street-level entrance of its Day 1 high-rise, that uses deep-learning algorithms and computer-vision-enabled cameras to let people grab what they want and walk out.\\n\\nNow Playing: Watch this: Shopping at Amazon Go is really freakin\\' fast\\n\\nAmazon Go, which was supposed to let in the public last year, opens the door to the prospect that you may never have to wait at a cashier line again -- even for those clunky self-checkout machines. But at the same time, the store raises concerns about the future of work or the local corner store, and worries that Amazon is slowly replacing humans with technology.\\n\\nBut all of that was in the back of my mind as I entered the store. My most pressing question: Did any of this work?\\n\\nI traveled to Seattle from San Francisco to see how convenient the Amazon Go convenience store actually is. My colleague Ben Fox Rubin, who\\'s based in New York and helped write this report, gave me a good rundown of how the store was supposed to work. But I didn\\'t really realize just how easy and fast the process would be.\\n\\nAmazon Go felt a lot like shopping online, except you\\'re in a brick-and-mortar store. I didn\\'t have to wait in any lines and could immediately take whatever I wanted. The whole process was so quick and seamless, I almost forgot the items weren\\'t free -- and for Amazon, that may be the point.\\n\\nFast and convenient\\n\\nThe first thing I see when I walk into the Amazon Go store are ready-to-eat meals. Pasta salads, lettuce salads, wraps and sandwiches line the tall shelves, making it easy for someone to dash in, grab food and run back out. Amazon employees in bright orange shirts restock items, making sure the store never runs out of popular selections.\\n\\nTo the left of the ready-made meals are cold drinks -- pop, sparkling water and pretty much anything else you can think of. As I head right to walk deeper into the store, I see giant cookies from Seattle\\'s Macrina Bakery and dill pickle chips from Whole Foods, the grocery store chain Amazon acquired last year. Other sections have milk, yogurt, snack trays, baking supplies and Amazon Meal Kits, which come with all the ingredients needed to make a meal for two.\\n\\nIn the back corner is the wine and beer shop, the only place you have to actually interact with a human being. There\\'s an Amazon employee there at all times to check your ID. After your age is verified, you can grab whatever alcohol you want and walk out of the store, the same as you do with any other item.\\n\\n\"We really focused on fast, convenient,\" Puerini said. \"Grab a quick lunch, grab something to take home for dinner, perhaps a quick drink or a snack.\"\\n\\nThe store has been open for the past year, but only to Amazon employees. Monday marks the first time members of the public can shop there. You don\\'t need an Amazon Prime account or any sort of special verification. All you need is a regular Amazon account, the Amazon Go app, and to be in Seattle.\\n\\nPrices at the store are subject to change, but right now they range from 49 cents for a bottle of water to $42.99 for a bottle of Caro Red Blend wine.\\n\\nThe store is open 7 a.m. to 9 p.m. PT from Monday to Friday.\\n\\nThe future of shopping?\\n\\nThe announcement of the Amazon Go store resulted in lots of hand-wringing from traditional retailers, who were worried they weren\\'t keeping up with Amazon. It also inspired much criticism that Amazon\\'s technology would kill jobs, something Amazon denied.\\n\\n\"Retailers could have and should have been pursuing this all along,\" Forrester analyst Brendan Witcher said. \"No retail customer in the history of shopping has ever said, \\'I want to wait in line for 15 minutes.\\'\"\\n\\nThere are some efforts from rivals to create something like Amazon Go, most notably from the Ireland-based retail vendor Everseen, as well as China\\'s Alibaba, which created a pop-up cashierless cafe.\\n\\nAmazon has said it employs the same number of people at Amazon Go as you\\'d find at a comparably sized convenience store, though Puerini declined to provide specific numbers. Instead of cashiers, Amazon has more people restocking shelves and preparing meals.\\n\\nShara Tibken/CNET\\n\\nStill, things haven\\'t been all smooth for Amazon. The company missed its self-imposed deadline of opening the store by \"early 2017,\" without providing any explanation. Amazon rarely sets deadlines for new initiatives -- it just announces them when they\\'re ready or doesn\\'t specify timing -- so this miss on a high-profile project stands out.\\n\\nThe Wall Street Journal reported in March that technological problems, such as issues tracking a lot of people at once, caused Amazon to delay its public store opening.\\n\\nAmazon is now more resistant to set public deadlines for new tech unless it has a \"high degree of confidence\" things will go as planned, according to one person familiar with the company. This person also said Amazon has a desire to create more Amazon Go locations but hasn\\'t yet moved forward with an expansion plan.\\n\\nPuerini dismissed the notion that technological issues affected its launch, and said the delays came from excessive demand from Amazon employees.\\n\\n\"We were happily surprised to find out that the traffic and patronage that we got just from Amazonian customers was beyond our expectations and was really all we needed,\" she said.\\n\\nSensors and machine learning\\n\\nAmazon has been pretty quiet about the specifics of the technology powering its Go store. It has touted the use of \"the same types of technologies used in self-driving cars: computer vision, sensor fusion and deep learning.\" Together, those make up Amazon\\'s \"Just Walk Out\" technology to detect which products are taken and which are returned to shelves.\\n\\nPuerini wouldn\\'t go into detail about Just Walk Out but said sensors in the shelving help detect what items have been selected. It\\'s similar to hotel mini bars where you\\'re charged for whatever items you remove from the fridge. Hundreds of cameras, which look like black boxes, cover the ceilings of the store to keep track of what you pick up and put back down.\\n\\n\"None of this existed\" when Amazon began its project five years ago, Puerini said. \"We couldn\\'t go buy the pieces of this and put them together.\"\\n\\nThe company spent a lot of time before opening its beta store to employees in late 2016 to polish its computer vision and machine learning capabilities. The cameras had to be able to detect which particular item was selected and be able to tell between the vanilla version of a drink and the lemon, for example. The sensors in the shelves help with that.\\n\\nThe cameras also had to be able to handle multiple people shopping at the same time, as well as people moving quickly and standing closely together.\\n\\n\"Us standing really far apart, the only two people in the store, taking things, is one situation,\" Puerini said. \"A store that\\'s starting to get really bustling with a lot of products that look really similar, that\\'s a whole other set of challenges that we\\'ve got to solve.\"\\n\\nTracking \\'your entity\\'\\n\\nEverything seemed to work just as Amazon promised during my demo, with some caveats. I wasn\\'t able to use Amazon Go on my personal device or with my Amazon account but instead accessed it through a dummy phone and account provided by Amazon. I didn\\'t go through the setup process to see how easy that was. I also didn\\'t have time to try out disguises to see if the system still recognized and charged me.\\n\\nAmazon said disguises wouldn\\'t matter anyway. The technology doesn\\'t use facial recognition or phone tracking. Instead, scanning that QR code in the Amazon Go app lets the store know that your account is shopping there and lets the cameras track \"your entity,\" as Puerini put it.\\n\\nIf you\\'re shopping with members of your family, each can sign in to your account on a separate phone, or you can just tap each person in at the turnstile using your own device. That registers that everything they select will also be added to your account. If you\\'re shopping with a friend, and you each have your own accounts, the person who picks up the items from the shelf is the one who\\'s charged.\\n\\nShara Tibken/CNET\\n\\nYou can check your receipt in the Amazon Go app, and if you see something you didn\\'t purchase, you can swipe left to delete the charge. The app will ask the reason for your deletion but won\\'t make you jump through hoops to get a refund.\\n\\n\"It\\'s super easy,\" Puerini said of the return process. \"The [Just Walk Out] system is highly accurate, so any errors are extremely rare.\"\\n\\nPuerini said Amazon \"certainly\" would like to open more Amazon Go stores but doesn\\'t have any immediate plans to do so.\\n\\nMany industry observers expect Amazon to eventually roll out Amazon Go\\'s technology to its Whole Foods stores, giving the company a big advantage over competing grocery chains. But Puerini said Amazon has \"absolutely no plans\" to bring its Just Walk Out technology to Whole Foods. Puerini also declined to say whether Amazon would license its technology to other technology companies or retailers.\\n\\n\"Right now we\\'re laser-focused on this one\" store, she said.\\n\\nBefore I leave, I make one last loop, checking out the options. I finally grab a \"Just Walk Out Shopping\" mug and a giant chocolate oat peanut butter cookie from Macrina Bakery.\\n\\nWith my selections in hand, I breeze out the door. Is this the future of shopping? I sure could get used to it.\\n\\nThe Smartest Stuff: Innovators are thinking up new ways to make you, and the things around you, smarter.\\n\\n\\'Alexa, be more human\\': Inside Amazon\\'s effort to make its voice assistant smarter, chattier and more like you.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('AI IN SUPPLY CHAIN AND LOGISTICS: How AI will reshape the logistics and transportation industry', \"BI Intelligence\\n\\nThis is a preview of a research report from BI Intelligence, Business Insider's premium research service. To learn more about BI Intelligence, click here.\\n\\nMajor logistics providers have long relied on analytics and research teams to make sense of the data they generate from their operations.\\n\\nBut with volumes of data growing, and the insights that can be gleaned becoming increasingly varied and granular, these companies are starting to turn to artificial intelligence (AI) computing techniques, like machine learning, deep learning, and natural language processing, to streamline and automate various processes. These techniques teach computers to parse data in a contextual manner to provide requested information, supply analysis, or trigger an event based on their findings. They are also uniquely well suited to rapidly analyzing huge data sets, and have a wide array of applications in different aspects of supply chain and logistics operations.\\n\\nAI's ability to streamline so many supply chain and logistics functions is already delivering a competitive advantage for early adopters by cutting shipping times and costs. A cross-industry study on AI adoption conducted in early 2017 by McKinsey found that early adopters with a proactive AI strategy in the transportation and logistics sector enjoyed profit margins greater than 5%. Meanwhile, respondents in the sector that had not adopted AI were in the red.\\n\\nHowever, these crucial benefits have yet to drive widespread adoption. Only 21% of the transportation and logistics firms in McKinsey's survey had moved beyond the initial testing phase to deploy AI solutions at scale or in a core part of their business. The challenges to AI adoption in the field of supply chain and logistics are numerous and require major capital investments and organizational changes to overcome.\\n\\nIn a new report, BI Intelligence, Business Insider's premium research service, explores the vast impact that AI techniques like machine learning will have on the supply chain and logistics space. We detail the myriad applications for these computational techniques in the industry, and the adoption of those different applications. We also share some examples of companies that have demonstrated success with AI in their supply chain and logistics operations. Lastly, we break down the many factors that are holding organizations back from implementing AI projects and gaining the full benefits of this disruptive technology.\\n\\nHere are some of the key takeaways from the report:\\n\\nThe current interest in and early adoption of AI systems is being driven by several key factors, including increased demands from shippers, recent technological breakthroughs, and significant investments in data visibility by the industry's largest players.\\n\\nAI can deliver enormous benefits to supply chain and logistics operations, including cost reductions through reduced redundancies and risk mitigation, improved forecasting, faster deliveries through more optimized routes, improved customer service, and more.\\n\\nLegacy players face many substantial obstacles to deploying and reaping the benefits of AI systems, though, including data accessibility and workforce challenges.\\n\\nAI adoption in the logistics industry is strongly skewed toward the biggest players, because overcoming these major challenges requires costly investments in updating IT systems and breaking down data silos, as well as hiring expensive teams of data scientists.\\n\\nAlthough AI implementations are unlikely to result in large-scale workforce reductions in the near term, companies still need to develop strategies to address how workers' roles will change as AI systems automate specific functions.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('DIGITAL HEALTH BRIEFING: Apple Watch, Fitbit can detect AFib, study shows — Over half of mHealth apps make $10,000 per year or less — Ex-ATA CEO forms AI group for healthcare', \"Welcome to Digital Health Briefing, the newsletter providing the latest news, data, and insight on how digital technology is disrupting the healthcare ecosystem, produced by Business Insider Intelligence.\\n\\nSign up and receive Digital Health Briefing free to your inbox.\\n\\nHave feedback? We'd like to hear from you. Write me at: lbeaver@businessinsider.com\\n\\nCARDIOGRAM SHOWS HOW POPULAR WEARABLE DEVICES CAN DETECT AFIB: Popular wearables, like Apple Watch and Fitbit, can detect atrial fibrillation (AFib), or a common type of irregular heartbeat, according to a study published in JAMA Cardiology. The first large-N, peer-reviewed study was conducted by Cardiogram, a digital health company that uses machine learning to organize health data for preventative medicine, and the University of California San Francisco (UCSF). The results are significant because, despite the massive troves of wearable data that have been collected over the past few years, healthcare systems and wearable manufacturers have struggled to turn data into meaningful information — until now.\\n\\nResearchers fed the data collected from 9,750 Cardiogram users enrolled in UCSF’s Health eHeart Study into the company’s AI algorithm, DeepHeart, to determine whether they had AFib. The neural network detected AFib at an accuracy rate of 97%, with sensitivity of 98% and specificity of 90%. DeepHeart is an artificial neural network — a form of AI — that uses heart rate and step count data collected by wearables to detect medical conditions.\\n\\nOther companies are also exploring how to use wearables to detect heart conditions:\\n\\nAliveCor’s Kardiaband EKG reader received FDA approval in November 2017 as the first medical device accessory for the Apple Watch. The device attaches to a slot on the band of an Apple Watch, and the wearer can touch the sensor to get an EKG reading.\\n\\nKardiaband EKG reader received FDA approval in November 2017 as the first medical device accessory for the Apple Watch. The device attaches to a slot on the band of an Apple Watch, and the wearer can touch the sensor to get an EKG reading. Apple partnered with Stanford to run the Apple Watch Heart Study to monitor users’ heart rhythms using the Apple Watch heart rate sensor. Users experiencing AFib are alerted via an app and connected with a physician from telehealth company American Well.\\n\\nDespite early progress, there’s still some way to go before these services achieve scale as medical detectors for AFib, according to co-author fromUCSF Greg Marcus. Because participants had already been diagnosed with AFib, more testing needs to be conducted to determine how well the deep learning model will detect heart conditions in patients with no treatment history. Still, they’re showing promise as potential screening tools for heart arrhythmias. These heart conditions often go undiagnosed, as people typically don't experience outward symptoms. Early detection could reduce costly hospitalizations of patients due to preventable illnesses.\\n\\nEnjoy reading this briefing? Sign up and receive Digital Health Briefing to your inbox.\\n\\nMHEALTH APP PUBLISHERS ARE FALLING INTO OLD TRAPS: Fifty-six percent of publishers and developers looking to take advantage of the rapidly growing mHealth app market are making less than $10,000 annually, as they struggle to monetize their apps, according to a new report from Research2Guidance. The main issue is the reliance on old app industry measures and revenue models, such as volume of downloads, in-app advertisements, and premium content. Publishers that rely on these models tend to have the smallest share of apps generating more than $1 million in annual revenue. Meanwhile, just 11% of mHealth publishers bring in more than $1 million in revenue each year. Most of these publishers are focusing on licensing, service sales (remote consulting or coaching), and device sales (such as sensors).\\n\\nFiguring out a revenue model early on is important for the longevity of an mHealth app or service, particularly given the ballooning costs associated with developing this content. On average, mHealth apps currently take between 15 months and two years to launch and cost around $425,000. That’s up 963% from the $40,000 it cost to develop an mHealth app in 2011. This is a lesson that developers in more established app industries, like gaming and e-commerce, learned early on. Moreover, it demonstrates the importance of finding proxies in related fields that provide an example of the best practices when entering a new business segment. What’s worked in similar, more established areas is likely to work in newer, growing segments.\\n\\nEX-ATA CEO FORMS AI IN HEALTHCARE GROUP: Former American Telemedicine Association (ATA) CEO Jonathan Linkous has cofounded the Partnership for Artificial Intelligence (AI) and Automation in Healthcare, according to Politico. The group, which will include health systems, payers, regulators, and individual healthcare professionals, will strive to apply AI and automation to address the rapidly expanding costs, depleting human resources, and advancing technologies in healthcare. Collectively setting a standard could help these companies shape the future of AI in healthcare. Furthermore, banding together to explore AI and automation in healthcare research ensures that overall values and objectives are shared. Similar groups have been formed within other industries — with varying degrees of success — in order to facilitate and guide the development of AI and automation to benefit the broader industry. For instance, Google (and Google-owned DeepMind), Amazon, IBM, Microsoft, and Facebook together created the Partnership on Artificial Intelligence to Benefit People and Society (PAI).\\n\\nMEDICAL ALARM MAKER INTEGRATES WITH AMAZON ECHO, ALEXA TO IMPROVE USAGE: Smart home device company TruSense has released a medical alarm pendant that can integrate with the Amazon Echo and Alexa, Amazon’s connected speaker and intelligence voice assistant, respectively, according to MobiHealthNews. The pendant is integrated with the Echo to allow users’ family members to check in with the device. Also known as personal emergency response systems (PERS), medical alarms are the most niche aspect of the telehealth industry and cater almost exclusively to elderly patients. Although the growing share of the elderly population means that demand for this segment of telehealth will grow, the increasing capabilities of remote patient monitoring devices, like fitness trackers and smartwatches, and mHealth apps will likely cause many of these designated devices to fold into the other segments of telehealth. Connecting these devices with emerging technologies like connected speakers and intelligent voice assistants to expand their practicality, while also making them easier to use and monitor, represents one way PERS companies can continue to grow in the evolving healthcare market.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('A 21-year old Swedish AI prodigy wants to revolutionize the $6 trillion education industry – and Tim Cook is impressed', 'Joel Hellermark, 21, wants to change education for good. Sana Labs\\n\\nSana Labs is an education tech startup founded by Joel Hellermark, 21.\\n\\n\\n\\n\\n\\nIt provides an artificial-intelligence platform designed to individualize a student\\'s learning in subjects like language and math.\\n\\n\\n\\n\\n\\nApplying AI to education has so far proved difficult, and Sana Labs hopes its scalable platform will change that.\\n\\n\\n\\n\\n\\nEven Mark Zuckerberg and Tim Cook have shown interest in the Swedish startup, which uses technology similar to AlphaGo to boost student performance. Ad\\n\\nStockholm is home to many world-conquering startups.\\n\\nBut even in a city that spawned Spotify and King, Sana Labs stands out for its sheer ambition. Last autumn, the startup launched a platform for personalized learning, and its biggest challenge so far has been to scale up fast enough to meet soaring demand.\\n\\nFounded by Joel Hellermark, a 21-year-old Swedish expert on artificial intelligence, Sana Labs wants to insert the latest advances in AI into education.\\n\\n\"We want any education company in the world to be able to implement adaptivity in just a few days,\" said Hellermark, also Sana Labs\\' CEO.\\n\\nOnce it\\'s plugged into an existing digital education tool, Sana Labs is designed to use a student\\'s individual learning style to help them learn faster and become more interested in the content.\\n\\nHellermark thinks his platform\\'s scalability across domains could be a game-changer for the $6 trillion global education industry (some $120 billion is digitized).\\n\\n\"When all learning becomes adaptive, students will be learning twice as fast, and you will be able to take all students to entirely new heights of knowledge,\" he said.\\n\\nEducation’s AlphaGo moment\\n\\nWhen founding Sana Labs in 2016, Hellermark spotted a huge untapped opportunity to commercialize cutting-edge AI in digital education: The existing offering was based on rather primitive, rules-based AI.\\n\\n\"Traditional education platforms have so far been based on predetermined rules that a computer acts on, much like when IBM\\'s Deep Blue beat Garry Kasparov by mimicking the moves of the world\\'s best chess players,\" said Hellermark, whose early interest in AI was sparked by the Stanford professor Andrew Ng\\'s machine-learning courses on Coursera.\\n\\nFor Hellermark, AlphaGo\\'s win in 2016 against the world champion of Go — a game much more complex than chess — underscored the potential of deep neural networks, a strand of machine learning on which Sana Labs relies.\\n\\n\"AlphaGo showed how, using machine learning, it continuously learned what leads to success by analyzing historical data,\" he said.\\n\\nSana Labs\\' AI learns from everything the student does in real time, and determines his or her optimal learning pattern. Sana Labs (screenshot)\\n\\nThe more data that\\'s gathered on a student, the better Sana Labs should be able to predict and boost performance.\\n\\n\"The really revolutionary part,\" Hellermark said, \"is how it analyzes and spots patterns that no human could know existed.\"\\n\\nA testament to Sana Labs\\' deep-learning approach was its recent win in Duolingo\\'s Global AI competition. It beat traditional AI platforms in predicting mistakes that learners of English, Spanish, and French would make based on their previous errors.\\n\\nIdentifying these sorts of learning gaps is what enables Sana Labs to tailor content for each student — bye-bye, standardized 20th-century education.\\n\\n\"We are expecting improvements in orders of magnitude,\" Hellermark said. \"Based on studies, students should be able to work through the exact same content in half the time, or be 25-30% more engaged.\"\\n\\nTim Cook and \\'Zuck\\' were impressed with Sana Labs\\n\\nHaving developed the algorithms that would underpin Sana Labs\\' technology, Hellermark looked for input from the best scientists in the field, including experts at NASA and Cornell University. Some are now advisers to the company.\\n\\nFor commercial leads, Hellermark went straight to the source, shooting his email pitch to Facebook CEO Mark Zuckerberg and Apple CEO Tim Cook.\\n\\nBoth replied.\\n\\nApple CEO Tim Cook has apparently taken a keen interest in Sana Labs. Brian Powers\\n\\n”One of Apple’s core values is education, so Tim Cook was curious about potentially working together, as was Mark Zuckerberg,” says Hellermark, who recently returned from meeting with Apple and Facebook in California as a result of those emails.\\n\\nWhat should interest tech titans is that only some 2 percent of all learning today is digital. The room for growth is massive, and Google Classroom\\'s rapid expansion bodes well for Sana Labs, which aims to open its first overseas office in the US.\\n\\nSana Labs works with two types of customers: publishing houses and digital-education platforms. After focusing on language education early on, the company\\'s AI platform is now mostly content-agnostic, able to work with courses ranging from math to programming to medical-prep tests.\\n\\nNeil Jacobstein, the chairman of AI and robotics at Singularity University, thinks AI tutors or platforms can change education for good. But he doesn\\'t buy the idea that there can be one single engine for personalized education, which is what Sana Labs wants to become.\\n\\nIt gets too complicated, according to the AI visionary.\\n\\n\"Building an AI tutor that tutors everything is like trying to boil the ocean,\" Jacobstein told Business Insider Nordic. \"For now, it makes more sense to focus on specialized areas and to do it really well, and later make a meta AI that can collect these course tutors.\"\\n\\nHe added that if Sana Labs \"can do it, wonderful.\" (Sana Labs says it is not an AI tutor but an intelligent AI layer applicable through a universal API.)\\n\\nIf Hellermark gets to decide, Sana Labs will do to education what his countryman Daniel Ek did to music with Spotify.\\n\\nBolstered by seed funding from business angels like Sophia Bendz, Spotify\\'s former CMO who\\'s now an investor at Atomico, Sana Labs has been poaching some of Stockholm\\'s brightest tech minds, including engineers who built Spotify\\'s playlist-recommendations engine.\\n\\nNot a bad start then for Hellermark.\\n\\n\"By the end of the year, we\\'re hoping to have implemented Sana Labs in products with hundreds of millions of users,\" he said.')\n",
      "('New Deep-Learning Software Knows How To Make Desired Organic Molecules', 'Want to read Slashdot from your mobile device? Point it at m.slashdot.org and keep reading!')\n",
      "('Baidu Shows Off Its Instant Pocket Translator', 'These screamingly hilarious gogs ensure owners of X Ray Gogs to be the life of any party. -- X-Ray Gogs Instructions')\n",
      "('Google Works Out a Fascinating, Slightly Scary Way For AI To Isolate Voices In a Crowd', 'These screamingly hilarious gogs ensure owners of X Ray Gogs to be the life of any party. -- X-Ray Gogs Instructions')\n",
      "('New AI Model Fills in Blank Spots in Photos', 'A new technology uses artificial intelligence to generate synthetic images that can pass as real . From a report, shared by a reader (the link may be paywalled):')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('3 Competitive Advantages of Deep Learning for Your Company', \"/\\n\\nBenefits range from tireless accuracy to job creation to cost savings. It's a brave new world.\\n\\nMarch 12, 2018 4 min read\\n\\nOpinions expressed by Entrepreneur contributors are their own.\\n\\nWhat do you think of when you hear about AI? Do you picture your favorite sci-fi movie or a book that you read when you were younger? In that favorite book or movie, were the robots smart? Could they learn?\\n\\nToday’s software robots can. In AI, we can find a subset of machine learning called “deep learning,” which is defined as networks that can learn unsupervised from unstructured data.\\n\\nNow the bigger question is: Are you ready to take advantage of deep learning in your business? The vast ocean of data grows exponentially every day. If you and your company can’t keep up, you’ll be left behind.\\n\\nIt’s time to utilize intelligent automation to help your business grow, keep organized, and stay on top of the competition. Let's discuss three advantages of deep learning for your company.\\n\\n1. Cost and time benefits.\\n\\nNeural networks are trainable “brains.” You give them your company’s information and train them to do a task, such as generating reports, and they will use that training, new information, and their “work experience” to adapt and improve in much the same way a human worker learns.\\n\\nRelated: Elon Musk Says Mark Zuckerberg's Understanding of AI Is 'Limited' After the Facebook CEO Called His Warnings 'Irresponsible'\\n\\nUnlike a human worker, however, these software robots work at a much faster rate and never sleep. The utilization of deep learning in your business can save your company money spent in hiring extra employees or outsourcing for specified projects. It can also save your employees time.\\n\\nWhen repetitive or time-consuming work is done quickly and efficiently at the push of a button, your employees are freed up to do the creative work that will help your company grow. Let’s talk more about that next.\\n\\n2. Quality and accurate results.\\n\\nAs the CEO of your company, you want accuracy. You want quality. Unfortunately, what you often get from your human worker is error-filled and inferior. This is especially true for repetitive tasks and data processing. Humans need rest and fuel. They get tired or hungry and make careless mistakes.\\n\\nYour AI worker doesn’t need rest and will not make mistakes. It uses deep learning to, well, learn. It takes in new information from the web, from your company’s data inputs, or from the results of its own “work” to develop new understandings, producing accurate, high-quality results.\\n\\nRelated: Artificial Intelligence Is Likely to Make a Career in Finance, Medicine or Law a Lot Less Lucrative\\n\\nUsing deep learning, your software robots can recognize more data and images, understand spoken language, overcome problems, and work more efficiently.\\n\\nImage recognition algorithms are becoming increasingly accurate, and AI is becoming more widely used. Through the use of deep learning, your company will avoid common errors and save the time normally spent fixing them.\\n\\nWith the improvements in intelligent automation, your employees might become concerned about their jobs, but they actually have the potential for growth.\\n\\n3. Job growth.\\n\\nCompanies implementing machine learning must train or hire employees to use the software. This can mean advancement for the right human worker.\\n\\nOther employees can improve and grow in their current creative positions without the distractions of repetitive and time-consuming tasks that can be delegated to AI. When your employees are free to create, you are free to watch your company grow.\\n\\nRelated: A VC Perspective - How Valid Is the Threat of Artificial Intelligence on the Job Market?\\n\\nUtilizing deep learning for your business will save you money and time in more ways than one. You, your company, and your employees will benefit when you decide to take advantage of what AI has to offer. Will you make a move toward deep learning in your company?\\n\\nMaybe they aren’t like the ones in books or movies, but robots are here, in one form or another. What is the future of AI? In some ways, that is up to you.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Step by Step, AI Is Accelerating the Search for a Cancer Cure', '/\\n\\nAI already is delivering breakthroughs in cancer diagnosis, but even optimists believe a cure is a decade away.\\n\\nMarch 29, 2018 5 min read\\n\\nOpinions expressed by Entrepreneur contributors are their own.\\n\\nFor about a million years, the human race has been on a curve of innovation, an upward arc that has progressed from controlling fire, to inventing the mechanical movable-type printing press, to creating artificial intelligence (AI) systems that can defeat mankind\\'s most accomplished game players. Today, AI systems are bending the human innovation curve ever further skyward, accelerating the pace of progress and putting major breakthroughs within reach -- such as ending terrorism or curing cancer.\\n\\nIndeed, some researchers believe that an AI-assisted cancer cure is less than a decade away. However, even with the accelerant of AI, the journey toward a cancer-free world will be iterative, built on small steps -- just as past innovations led to today\\'s cutting-edge technologies.\\n\\nAI already is delivering breakthroughs in cancer diagnosis, but the technology will undergo multiple iterations, solving a plethora of smaller problems before taking on the ultimate challenge. This process will present copious opportunities for AI technology providers to contribute to the monumental challenge.\\n\\nRelated: Why \\'Fail Fast\\' Is a Disaster When It Comes to Artificial Intelligence\\n\\nBut to participate in this effort, technology providers need to understand the sequence of innovations that have led us to where we are today and to where we will eventually arrive in the future. Let\\'s look at a timeline of select AI innovations that potentially ends with the conquest of cancer:\\n\\n1952 -- Marvin Minsky unveils the Stochastic Neural Analog Reinforcement Calculator (SNARC), the first connectionist neural network learning machine -- and possibly the first self-learning machine.\\n\\n1975 -- Backpropagation algorithm is developed that solves challenges with computational machines, allowing the training of multilayer neural networks and leading to the widespread usage of neural networks in the 1980s.\\n\\nCirca 2000 -- The first use of the expression \"deep learning\" to describe a type of machine learning that creation of networks capable of learning from unstructured data in an unsupervised fashion.\\n\\n2011-2012 -- Convolutional neural network AlexNet achieves unprecedented levels of accuracy in visual recognition, paving the way for deep learning to enter the mainstream.\\n\\nJanuary 2017 -- Researchers at Stanford University develop deep-learning technology that can visually identify cancerous skin moles and lesions with the same level of accuracy as a human dermatologist.\\n\\nFebruary 2017 -- Microsoft establishes Healthcare NExT, an initiative designed to apply AI and machine-learning technologies to health issues, including cancer treatment.\\n\\nMarch 2017 -- Google\\'s GoogleNet deep-learning technology detects cancerous tumors with higher accuracy than human clinicians.\\n\\nOctober 2017 -- Intel announces first silicon for its Nervana Neural Network Processor (NNP) chip, which can accelerate deep learning tasks, including diagnosing cancer.\\n\\nCirca 2021 to circa 2026 -- Microsoft is projected to release an AI-powered computer that operates inside the human body to detect and reprogram cancerous cells, rendering them harmless.\\n\\nAs this timeline shows, the pace of innovation in deep-learning and AI-based cancer research is accelerating. However, progress at this stage still involves relatively small steps leading up to the ultimate goal in the future.\\n\\nRelated: 10 Artificial Intelligence Trends to Watch in 2018\\n\\nThis situation reflects the status quo in AI innovation, which involves using single-task-specific cognitive engines to perform mundane and repetitive tasks that are challenging for people, such as examining large numbers of images of tissue samples to detect signs of cancerous lesions.\\n\\nThese technologies are collectively called artificial narrow intelligence (ANI). Today\\'s most successful AI technologies are leveraging these engines for a wide range of specific purposes, from the object-recognition technology that powers Amazon\\'s DeepLens video camera to the face-recognition algorithms that control the Face ID authentication on Apple\\'s iPhone X.\\n\\nThese solutions are called \"one-time\" (1x) AI transformations. They represent pragmatic tools that satisfy immediate needs while promoting strategic objectives.\\n\\nSuch 1x transformations are playing a critical role the development of AI. Businesses that successfully integrate 1x AI innovations into their operations are expected to expand their workforce by 10 percent and revenues by 38 percent during the next five years, according to a report from Accenture.\\n\\nThese types of innovations will lead to the next generation of AI: two-fold (2x) transformations. Such 2x transformations take things a step further by using ANI to look at the bigger picture. For example, they can combine large amounts of data from a variety of sources, processing it and analyzing it to make it useful for specific tasks.\\n\\nAt the next level are 10x transformations, where AI technologies become powerful enough to solve major challenges. The 10x transformations will be enabled by the future development of two technologies: artificial general intelligence (AGI) and artificial superintelligence (ASI).\\n\\nRelated: Is Artificial Intelligence Replacing Your Intelligence?\\n\\nAGI is defined as a machine that can perform any intellectual task as well as a human does. Artificial superintelligence goes beyond AGI by delivering machines with intellectual capabilities that are superior to humans\\'.\\n\\nThe road to cancer\\'s cure will progress from today\\'s ANI-enabled 1x transformations, through 2x solutions, to the AGI- and ASI-driven technologies of the future. In order to participate in this process, providers and users of medical AI technology will have to participate in the iterative process of AI innovation, taking small steps toward the ultimate goal.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('What Entrepreneurs Need to Know About Facial Recognition Technology', '/\\n\\nThe opportunities are intriguing, as long as ethical concerns are managed appropriately.\\n\\nMarch 30, 2018 5 min read\\n\\nOpinions expressed by Entrepreneur contributors are their own.\\n\\nAs the Fourth Industrial Revolution unfolds with billions of people sharing a wide and deep array of data -- texts, tweets, GPS coordinates, all manner of photos, videos, environmental data, clickstreams, status updates, likes and reposts, pumping trillions of real-time signals into the digital universe -- what does the future hold? This data is like food for the whale of artificial intelligence.\\n\\nRelated: 17 Amazing -- and Surprising -- Uses of Facial Recognition Technology\\n\\nIn terms of a resource, this AI-food-rich data ocean makes the California gold rush, or the Texas oil boom, seem like tiny puddles. Vast amounts of data are flooding the digital space on a global level. AI-based algorithms will be propelling innovation in every sales arena from products to services and the more data you have, the more accurate the algorithm. Collecting and processing \"big data\" has become a focus for companies large and small.\\n\\nAnd how does the AI whale digest this data? Through interconnected devices with embedded \"eyes.\" Termed \"deep learning,\" these artificial neural networks use layered machine learning algorithms that mimic the structure of animal brains. Utilizing gigantic data pools, deep learning can identify and interpret complex patterns much in the same way as the human brain. Some artificial neural networks are now extremely adept at employing these patterns to mimic the way humans recognize faces.\\n\\nSo, which company is in front of the deep learning fleet? Facebook, of course. Facebook holds the single largest collection of facial data, and in 2015 it introduced a greatly enhanced version of its \"tag photos\" feature, DeepFace, which employs a nine-layer neural network that matches features in separate photographs with 97.25 percent accuracy. DeepFace not only connects your face with your name, but it can literally pick your face out of a crowd, and a human brain is only .28 percent better at this than the program. Facebook has invested big-time in DeepFace, spending billions of dollars devouring the competition (including Face.com, Masquerade and Faciometrics).\\n\\nRelated: 3 Competitive Advantages of Deep Learning for Your Company\\n\\nRecently Facebook was granted a new patent, \"Techniques for emotion detection and content delivery,\" which captures users\\' facial expressions via the camera in real time as they scroll through their feed, tracking their emotions when exposed to various content. This emotional data could not only personalize your Facebook feed at a whole new level, but could also link to live in-store cameras, matching and identifying shoppers, calling up information gleaned from Facebook and identifying the shopper\\'s current moods. Your shopper is sad today? Why not play her favorite song as she approaches the shoe rack?\\n\\nThe possibilities for a radically personalized shopping experience are endless. And despite its dominance, Facebook is far from the only company plunging into these waters. Ebookers, a travel site owned by Expedia, has introduced a tool called SenseSational, which uses real-time facial recognition software to track users\\' faces as they choose images and sounds that are most appealing to their senses. The tool then sorts the user into one of four \"tribes\": The Adventurer, Culture Collector, Sun Seeker and Bon Vivant. And it suggests destinations and activities that match their tribe\\'s travel preferences.\\n\\nSingapore Technologies Electronics is now promoting its Advance Fare Gate System to private and public transportation organizations. This product identifies the facial features of commuters as they pass through fare gates, and charges a prepaid account accordingly, eliminating the need to show a fare card, and potentially easing crowding during busy rush hours.\\n\\nRelated: Why \\'Fail Fast\\' Is a Disaster When It Comes to Artificial Intelligence\\n\\nAs the bounty of AI expands before us, many tech giants, scientists and entrepreneurs say that now is the time to discuss the potential challenges and consequences of this new frontier. Facebook has already faced resistance to DeepFace on several fronts -- a lawsuit and bans in Europe. Privacy concerns abound. If Facebook can watch you when you scroll your feed, can it watch you while you look at other sites? Or while you move around your home? Does collecting data straight from a camera require consent? Where is the data stored, and who can access it?\\n\\nWhat about civil liberties? Could facial recognition be used to identify someone participating in a lawful protest? In the wrong hands, could artificial intelligence and deep learning turn our society into a sci-fi dystopia? One thing is certain -- those questions will not vanish and businesses and developers will have to navigate them as they chart these new waters.\\n\\nIn the right hands, however, facial recognition can be put to good use. Be it ethical, such as the Central Railway in Mumbai, which has announced it will implement cameras with facial recognition by the end of the year to trace past movements of criminal offenders and be prepared to arrest them when they travel again; or to improve customer service, as when candy retailer Lolli&Pops uses in store cameras to recognize members and connect them with a profile, so that sales associates can greet them by name and provide them with personalized product suggestions.\\n\\nLeaving the dialogue open for ethical debate, companies can approach deep learning from any angle they can envision. The whale of artificial intelligence is hungry. You can feed it whatever big data you wish, and watch it grow.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"If You Still Call Amazon an Online Retailer, Here's One More Reason to Stop\", '/\\n\\nIt depends heavily on its cloud business, not ecommerce.\\n\\nFebruary 1, 2018 2 min read\\n\\nAmazon is staggeringly good at selling things online. In just North America last year, the company generated $2.8 billion in operating income. That\\'s a 20 percent jump from 2016, according to a release connected to its latest earnings report.\\n\\nBut, that\\'s not the complete picture. It\\'s still-growing international sales segment saw a $3 billion loss in operating income.\\n\\nRelated: The Surprising Ways Amazon, Apple and Microsoft Really Make Their Billions\\n\\nThank goodness for Amazon Web Services. The company\\'s cloud division helped propel Amazon to some of the best numbers in the company\\'s history. The cloud service saw $17.5 billion in net sales last year. After expenses, the division still brought in an operating income of $4.3 billion, a 39 percent increase from 2016, ensuring a total annual net income for the company of $3 billion.\\n\\nAnd this segment of Amazon\\'s business will continue to grow. The company announced several high-profile clients for AWS in its report, including Disney, Turner, Symantec and the NFL. Amazon also expanded AWS infrastructure in France and China, and plans to open new zones in Bahrain, Hong Kong and Sweden between now and early 2019.\\n\\nThe company is also expanding its AI offering to include transcribing and translating services and deep-learning powered video analysis. It also announced AWS Media Services, which will provide cloud tools for video providers.\\n\\nRelated: Here\\'s How You Can Use Alexa at Work\\n\\nAmazon also further entrenched itself in the workplace with Alexa for Business.\\n\\nWhile the company continues to burn money in its bid to get the world to buy everything from the \"everything store,\" it will have AWS propping it up.\\n\\nOf course, Amazon\\'s incredibly strong retail segments are still essential to the company and only part of its many growing offerings. The lesson for entrepreneurs is to watch how AWS\\'s growth and continued success is helping the company invest and make inroads in AI, international ecommerce or media that many other companies can\\'t.\\n\\n\"Our 2017 projections for Alexa were very optimistic, and we far exceeded them,\" said Jeff Bezos, Amazon\\'s CEO and founder, in a statement. \"Expect us to double down.\"')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Think Your Company Needs a Data Scientist? You're Probably Wrong.\", '/\\n\\nAsk yourself these four questions before jumping on the data science bandwagon.\\n\\nMarch 23, 2018 7 min read\\n\\nA Note From The Editor Think your company has what it takes to make our Top Company Cultures list? Apply now. Apply now »\\n\\nOpinions expressed by Entrepreneur contributors are their own.\\n\\nWhen I started my career in data 15 years ago, I could never have envisioned a sexy rebranding of my work with the coining of the term \"data scientist,\" let alone the immense popularity it\\'s achieved in recent years. Widely considered one of the worlds hottest and most sought after positions, data scientists are re-writing what it means to be cool in the modern tech era. There has never been a better time for my fellow nerds. Jobs are overflowing with demand far exceeding supply. The industry has become so hot it\\'s not uncommon for board members of startups to demand hiring of data scientists early in the product life cycle. It is in that capacity that I\\'m frequently brought in to meet with executives and more often than not, inform them that they do not need a data scientist.\\n\\nRelated: Who Owns the Data Your Business Uses? Not Knowing Could Hurt the Sale of Your Company.\\n\\nHow can a data evangelist such as myself argue this sudden interest in all things data science is on the verge of backfiring? Before I begin, let me start by saying that there are indeed many great uses for hiring a data scientist! I\\'m not going to argue that data science is not needed or is not useful, because when used correctly it\\'s an incredibly powerful business weapon (yeah, I went there with \"weapon\"). I\\'m simply going to argue it\\'s an overused term with little formal accreditation that refers to a large swath of data-related activities, not a tidy suite of skills that can be learned in a 12-month course. So, when it comes time to hire, organizations should put real thought and consideration into when and what kind of data scientist your organization needs.\\n\\nWhen new prospective clients come to me, at least 50 percent of the time it\\'s under the guise of \"My CEO/board member/etc. told me I need to hire a data scientist.\" To which I generally ask the following four questions:\\n\\n1. How much data do you have?\\n\\nI say four questions, but many organizations never make it past the first. If you are a startup and you have not launched yet, you do not need a full-time data scientist. Full Stop. In fact, even if you are well-established but with a small customer/product/membership base, again you do not need a data scientist. Why, you ask? Because not surprisingly, data scientists need data. Not just any data will do. Many techniques require a minimum of tens of thousands, if not hundreds of thousands or even millions of data points to build.\\n\\nRelated: 5 Things to Keep in Mind When Using Data for Artificial Intelligence\\n\\nCurrently, there is a huge focus on deep learning. Job descriptions for data scientists are flooded with terms like neural networks, machine vision and natural language processing (NLP). The issue? These types of techniques rely on having massive amounts of training data. Consider the widely popular Google Translate, a type of neural network built on top of a lexicon of over 150 million words. The volume of data needed for successful deployment of these types of models exceeds what many companies own.\\n\\nThere are many techniques that use less data than deep learning, however, they still require reasonably large samples, not to mention a working knowledge of when to use which methodology. There is still valuable work to be done at this stage to create an environment where data science can thrive in the future, it just doesn\\'t require a full-time, expensive resource to achieve.\\n\\n2. Do you have established key performance indicators (KPIs) and regular business intelligence reporting?\\n\\nWithout basic understanding of what drives the organization, it\\'s going to be very difficult to make use of advanced techniques. For example, a data scientist can use machine learning to make predictions such as which users will churn or become highly active, however, if the business does not have a definition for churn or highly active, that becomes a requirement prior to building the predictive models. Furthermore, it\\'s difficult to validate models if you don\\'t have sufficient metrics with which to evaluate them. Other techniques such as A/B testing require advanced selection of an overall evaluation criteria (OEC), which is typically a business-driven KPI.\\n\\nRelated: 6 Critical Questions to Help Businesses Cut Through the AI Hype\\n\\n3. What do you imagine this data scientist will do once hired?\\n\\nPerhaps the most subjective and interesting of the questions I ask, \"What do you want this data scientist to do?\" The most common answer I get is. \"We don\\'t know, that\\'s why we need to hire one.\" In that case, I gently tell the organization that they are setting up their data scientist to fail. There is no need to be an expert in data science to hire one, however you should have a good idea of what is and isn\\'t possible so that you don\\'t set unrealistic expectations.\\n\\nData science isn\\'t magic and it\\'s not even a traditional science. It\\'s just as much an art as it is a science, which means the variability in skills and ability is substantial. You may even have existing team members able to grow into many data science applications. An easy entry into data science for an existing analyst is to begin forecasting the KPIs they already report on. Here they have the opportunity to learn on data they are familiar with, which is not just good for employee morale; investing in your staff now means less need to recruit in a highly competitive market in the future.\\n\\nRelated: What Hiring Managers Don\\'t Understand About Hiring for Data Science\\n\\n4. What support networks are available to your data scientist(s)?\\n\\nIf you don\\'t have the right support network for your data scientists, don\\'t bother investing in hiring them. In recent years, there has been a huge surge in data science programs, however the graduates are for the most part simply not ready to tackle business problems without careful hand holding. The vast majority of programs have students solving pre-established problems on clean data. In the real world, you want your data scientist to help determine what problems are being solved, and clean data never exists.\\n\\nHiring a junior data scientist without a senior resource for guidance can not only lead to frustration on the part of the junior, it can often lead to bad analysis. Junior team members tend to struggle translating business problems into technical problems and the wrong translation could result in months of work on a product that misses the target.\\n\\nThis problem is not completely mitigated by hiring more senior, partly because certifying your senior hires are actually good and competent is extremely difficult. If you luck out and hire a talented and self-motivated data scientist, she will still need a lot of support at the executive level to succeed. Imagine a situation where models are created but never used because there is no buy-in from team leads. Or where A/B tests are conducted but the results ignored. Worse yet, the data tracking needed to analyze a problem isn\\'t being collected at all.\\n\\nRelated: 4 Things a Data Scientist Can Do for Entrepreneurs\\n\\nFrequently, a necessary first step is a robust data collection program, which is likely resourced by an engineer or database administrator, not a data scientist. At many organizations, the senior data scientist(s) spends exorbitant amounts of her time simply fighting for the data requirements and deployment of her team\\'s work. That\\'s a surefire way to lose that talented, self-motivated, senior data scientist.\\n\\nThe landscape for hiring and retaining good data science talent is competitive and expensive, but being smart and conscientious on when, who and how to hire can mitigate the pain and cost. Don\\'t fall into the trap of job postings that are laundry lists of skills. Don\\'t expect magic pixie dust from your data scientist. Do take inventory of your true requirements and if possible, consult with a trusted professional prior to hiring. The success of your data program depends on it.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Why Are Some Bots Racist? Look at the Humans Who Taught Them.', '/\\n\\nHere are a few things we can do about human biases in machine learning.\\n\\nMarch 8, 2018 5 min read\\n\\nOpinions expressed by Entrepreneur contributors are their own.\\n\\nWith its scientific algorithms, we trusted robots to deliver neutral, fair, impartial answers. Since they are supposed to be free from unjust human biases and filters from past experiences, we believe that just as 2+2 = 4, mathematical formulas are black and white. We have found instead that the data scientists who create these algorithms have their own unconscious biases, which are subtly filtered down inside of the algorithms and act on these predispositions. More revealing is that even without the presence of bias during the development phase, machines are learning from the discriminatory undertones they perceive in our society.\\n\\nRelated: Artificial Intelligence Is Likely to Make a Career in Finance, Medicine or Law a Lot Less Lucrative\\n\\nWe can see this bias at play when we use Google to search for images of CEOs, with significantly more white males appearing than women or minorities, and search results for higher-paying jobs appear more to men than women. An algorithm designed to predict the likelihood of a person committing a future crime became unfairly racist. Researchers at Boston University and Microsoft New England found that machines associated the word \"programmer\" with the word \"man,\" rather than \"woman,\" and that the most similar word for \"woman\" is \"homemaker.\"\\n\\nAI\\'s predictive power creates solutions.\\n\\nArtificial Intelligence (AI) is arriving at a time of great necessity. AI\\'s importance is reflected in research conducted by McKinsey, which found total annual external investment in AI was between $8 billion to $12 billion in 2016. With its increasing ability to collect vast amounts of data, AI quickly turns this information into actionable insights, providing critical strategic advantages. Chatbots can find answers to customer queries much faster than humans, diagnose diseases and make accurate predictions that can drive product innovation. The predictive data can be used to detect fraud or match the unemployed with available jobs that utilize their skills, and solve complex traffic problems.\\n\\nRelated: A Humanoid Robot Called Sophia Mocked Elon Musk After Being Asked About the Dangers of AI\\n\\nAre chatbots reflecting a biased world?\\n\\nThe most straightforward solution, it seems, would be to train programmers so that they do not unintentionally write code or use data that is biased. Neutrality can be difficult to discern during the development phase, as Beauty.AI learned when the results of its first machine-judged beauty pageant resulted in winners that all had fair skin. Developers realized after the fact that the machine was not taught to recognize those with dark skin.\\n\\nEnlightened developers are not enough.\\n\\nThe creation of fair and balanced algorithms is critical and necessary yet reflects only part of the solution. Machines have the potential to have hidden biases, whether or not they originated from any predisposition of the designer. Machines continuously learn from outside data to do their task better. We are designing machines to think and learn. The extreme example of continuous improvement is Elon Musk\\'s scenario where bots trained to eliminate spam might eventually learn to wipe out the humans who create spam. While we are far from this extreme, it\\'s important to understand that the algorithms used by AI depend upon deep learning, which uses neural networks. However pure the original code is, the machines will always be vulnerable to replicating the bias it sees as it engages with the world.\\n\\nRelated: 5 Reasons Machine Learning Is the Future of Marketing\\n\\nThis type of learning was evident after Microsoft\\'s Tay chatbot imitated what it read on Twitter, regardless of how vicious the behavior. Tay lived less than 24 hours before it was influenced by racist tweets. As Tay learned and engaged through conversation and dialogue, it very quickly learned to use these statements to spin its own racist tweets.\\n\\nAI can perpetuate and reinforce a bias that already exists. For example, if an organization has traditionally hired male CEOs, a bot trained to find future CEOs would look at the past for likely candidates, based upon real data that indicates that previous CEOs were male. The machine would use male candidates as a predictor of someone who is qualified for a job.\\n\\nAlgorithms that only provide results \"similar to\" previous data create a bubble of their own, as we have experienced in news feeds that are free from conflicting viewpoints. Without opposing points, we lack necessary insights for significant decisions that foster creativity and innovation.\\n\\nRelated: 10 Artificial Intelligence Trends to Watch in 2018\\n\\nValue alignment is a teachable skill.\\n\\nMark O. Riedl, an associate professor at the College of Computing, School of Interactive Computing at Georgia Tech, proposes an attractive solution, Quixote, that involves immersing bots in literature. In his paper, which he presented at the AAAI Conference on Artificial Intelligence, he explains that strong values can be learned, and, \"that an artificial intelligence that can read and understand stories can learn the values tacitly held by the culture from which the stories originate.\" He explains that stories are encoded with implicit and explicit sociocultural knowledge that could build a moral foundation in the bot and could teach the bot how to solve a problem without harming humans.\\n\\nLiterature can be ambiguous and abstruse, and some of the greatest books of all time were once regarded as nefarious by school boards, calling for their ban. The more significant question is how will we, as a society, come to a consensus regarding what is socially responsible literature reading material for our bots?\\n\\nAI is not a replacement for people.\\n\\nThe goal has never been for AI to replace humans, but rather to support, amplify and enlighten. At a minimum, AI developers can take a more active role to ensure biases are not inadvertently created. Human oversight is still needed at all levels of development, and to ensure there is a healthy mix of opposing views to encourage diversity. Additionally, this must be supported by an equal opportunity for everyone to develop these systems.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"The Big 3 Tech Categories That Will Contribute the Most to Your Startup's Success in 2018\", 'Embrace the future of technology by focusing first on Blockchain, artificial intelligence and communication platforms.\\n\\nJanuary 23, 2018 6 min read\\n\\nOpinions expressed by Entrepreneur contributors are their own.\\n\\nFew startups these days can exist -- let alone succeed -- without technology. In fact, Forrester’s mid-year tech outlook, published in September, predicted that tech spending would increase across multiple sectors in 2018. Theoutlook forecast that 4 percent more would be spent across the board for global purchases of software, hardware and technology services by corporations and government agencies alike.\\n\\nThat\\'s the kind of growth that will push the tech industry past the $3 trillion mark for the first time in history.\\n\\nBut as each new day seems to herald a new technology that promises to make marketing, communication and creativity ever more dynamic, entrepreneurs may be left wondering which technologies they should invest in -- or whether they should invest. Here\\'s how they should form those decisions:\\n\\nEmbrace the future of technology -- it\\'s already here.\\n\\nEffectively utilizing a technology is about having a great idea that genuinely deploys that tech platform in an effective way and bolsters a startup\\'s productivity and execution.\\n\\nTrying to shoehorn Blockchain or voice recognition into an app that has no use for either technology in the first place is a worthless endeavor. On the other hand, when it comes to Blockchain, you should think about the coded smart contracts it facilitates and how those contracts can execute themselves when the agreed-upon conditions are met.\\n\\nBlockchain, then, is a useful tool for entrepreneurs working with vendors, because the blockchain network can store these contracts securely and allow both parties to obtain proof of the agreement any time they want.\\n\\nAI developments are useful tools as well: They\\'ve streamlined many fundamental office tasks by making devices self-manageable. Examples include printers that can reorder their own paper and ink, and computers that can debug their own software.\\n\\nAs with any product, of course, entrepreneurs must first understand the problem they need to solve by surveying the landscape of technologies out there to understand each one\\'s benefits, implementation requirements and drawbacks. Next, entrepreneurs must select the tool or tools best suited for their intended application, then consider the existing scenarios in their businesses that each new technology might supplement or improve upon.\\n\\nThink about these big three technologies shaping startup success in 2018.\\n\\nIn the end, onboarding new technologies is about problem-solving. While the problems you face may feel entirely idiosyncratic, the following three primary technologies that I deem most helpful to startups in the coming year are pretty universal.\\n\\n1. Communication platforms. Communication is a key for any startup, especially as employees increasingly work remotely, as the New York Times has pointed out. Communication technologies, in fact, offer a host of benefits, from increased productivity to tighter teams and the ability to foster better company cultures. These technologies, further, can help entrepreneurs keep budgets intact, a good enough reason alone why communications platforms can be fruitful.\\n\\nRelated: The 4 Best Tools For Internal Business Communication\\n\\nMessaging services like Slack or HipChat are great for fostering real-time communications when your teams work outside the office. And tools such as Basecamp and other project management solutions streamline operations regardless of your staffers\\' location, while LinkedIn and Ripple enhance connect individuals on a more personal level.\\n\\n2. Artificial intelligence. For computing power that boosts the abilities of your at-home workforce, look no further than AI. AI-driven technology is a must on any tech list for the coming year because the category is expanding so rapidly. As data becomes increasingly unwieldy, deep-learning techniques are evolving to process that data into insights that even remote employees can use.\\n\\nRelated: 10 Artificial Intelligence Trends to Watch in 2018\\n\\nTo this end, Google Home and Amazon Alexa created the virtual assistant. While the ability to interact with devices through the internet has long existed, users needed a shared vernacular (\"Hey, Google!\") to make it part of their daily habits. That\\'s what Google Home and Amazon\\'s Alexa have provided.\\n\\nIn the same context, research from McKinsey has suggested that 45 percent of work activities can be automated using existing technologies coupled with AI. For example, the same voice-recognition software utilized by smart devices will likely increase productivity and efficiency once users no longer have to learn keyboard controls and command-line prompts, or conduct manual inputs.\\n\\n3. Blockchain. The latest cryptocurrency crash may have entrepreneurs wondering whether Bitcoin will even make it through the year, but Blockchain itself -- which can do much more than financial transactions -- isn’t going anywhere. This technology has been around for a while, but it\\'s having a heyday now. Market Reports Hub forecasts that the market will exceed $2 billion by 2021.\\n\\nRelated: 8 Benefits of Blockchain to Industries Beyond Cryptocurrency\\n\\nWhat will happen in 2018 most likely is an explosion of companies trying to roll Blockchain into their products, or have it be their product. By the end of the year, we\\'ll also likely witness a major fallout because only a few of those companies will survive, as is the case with anything that\\'s investment-friendly.\\n\\nStill, where Blockchain really shines is its ability to foster trust between organizations. The secure nature of the Blockchain ledger means that transactions that formerly required intermediaries no longer do. Instead, \"ownership\" can now be tokenized, and its digital life cycle instantly tracked. And that opens up big security opportunities for intrepreneurial individuals and businesses. The result: Blockchain portends far-reaching implications extending well beyond the monetary transactions so talked about in the news cycle.\\n\\nOverall, all this forward-looking technology may seem like a daunting undertaking for startups and entrepreneurs already trying to navigate marketing, sales, communication, finances and creativity. But whatever your own industry and its journey, understanding the tech landscape and capitalizing on these three most important technologies during 2018 will help you make this the year of new and refined success.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('4 Ways You Can Use AI to Enhance Every Step of the Customer Journey', '/\\n\\nFor artificial intelligence to work effectively, you need to have a strong data foundation.\\n\\nJanuary 25, 2018 5 min read\\n\\nOpinions expressed by Entrepreneur contributors are their own.\\n\\nAI has disrupted the retail industry, and that’s a good thing. Consumer behavior is increasingly managed by AI, leading to an unparalleled online shopping experience. AI-powered tools have made it easier for even non-techie CMO’s to deliver a highly personalized experience by anticipating consumer needs and making real-time predictions. AI-powered marketing is helping marketers to satisfy increasing customer expectations. It’s with the help of artificial intelligence that even small- and mid-scale retailers are able to provide an exceptional one-to-one customer experience like Amazon, Walmart and Nordstrom.\\n\\nTo engage your customers in the right way at any contact point along their journey, consider these four techniques.\\n\\n1. Look for out-of-the-box possibilities.\\n\\nGive them something they haven’t even thought of. Targeted communications that are relevant and useful can create lasting customer loyalty, as well as an upswing in conversions.\\n\\nThe customer lifecycle is non-linear. One of the most effective ways to win them over is by touching them at the right stage in their customer journey. For example, a shopper who has browsed but didn’t purchase anything will appreciate getting an email with personalized recommendations based on his behavior.\\n\\nRelated: 5 Ways Artificial Intelligence Is Shaping the Future of Ecommerce\\n\\nNow, product recommendations are nothing new, but recommendation engines were not as intelligent earlier as they are now. These tailored product recommendations are powered by deep learning and advanced machine learning frameworks that take into account the product-data along with unique user behavior and preferences.\\n\\nCompanies like Amazon and Netflix have become pioneers at using powerful machine learning algorithms to create an out-of-the-box experience to delight their shoppers, thus converting them into customers and loyal shoppers thereafter.\\n\\n2. Map your customer journey across touch points.\\n\\nCustomers now have endless online and off-line touch points. Marketers need a single customer view to acknowledge all touch points rather than a series of snapshots.\\n\\nMarketers who deny the importance of a centralized customer repository are fundamentally wrong in their approach. In the absence of a unified customer view, it is tough to send personalized messaging. To provide a seamless customer experience across channels and deliver real-time personalization, AI has to be built-in at the core of the system.\\n\\nAI helps in providing a wholesome customer experience throughout the customer journey, from the first touch to the final sale. For instance, the traditional approach to predicting customer lifetime value is based solely on customer’s historical data. But the CLTV models powered by machine intelligence take a lot of use cases into consideration to make better predictions, including the purchasing behavior. For example, how recently and frequently they have purchased, monetary value of the purchase and soon, to infer their future actions.\\n\\n3. Identify key steps in the customer journey.\\n\\nBe consistent. It’s all about in-the-moment personalization. Marketers are swamped with tons of data with almost no actionable insights. But, data alone won’t lead you anywhere. With artificial intelligence, marketers can get deep and powerful insights to identify key moments in the customer journey to automatically send the right message to the right person on the right channel, making the customer most likely to convert.\\n\\nRelated: Aggregate Data to Grasp the Whole Customer Journey\\n\\nFor example, AI-driven intelligence can determine which step in a customer journey had the biggest impact, when a customer is going to make his next purchase. All these predictions are made automatically and in real-time, based on a lot of past data, and machine-learning wizardry.\\n\\n4. Transform customer experience with chatbots.\\n\\nMost customer journey communication is flawed at some point. Most marketers target particular segments instead of one-to-one personalization. This leads to sending the same marketing communication to all customers in a segment.\\n\\nLeveraging artificial intelligence to identify your customer needs is the key to \"humanizing\" your customer experience. Chatbots are AI-powered virtual assistants. They are the most significant example of how retailers are transforming their customer experience by making it proactive and interactive. Modern chatbots no longer frustrate customers with canned messages and delayed responses; they are data-driven to assist the customers in real-time.\\n\\nSome of the world’s smartest chatbot technology offers automation with contextual data to power AI systems. The advanced chatbots also enable speech-to-text translation by using natural language processing to engage the shoppers with a human dialogue-like interaction, thus speeding the discovery process in online shopping. Shoppers simply state their needs, and a chatbot presents a selection of items to match it. Companies like 1-800-Flowers and The North Face have leveraged AI-powered chatbots to transform the customer experience throughout the buying journey.\\n\\nRetailers are also using AI-driven chatbots to enable online shoppers to ask questions in natural language to understand their intent and shopping preferences.\\n\\nRelated: Chatbots Are the Next Big Platform. Here\\'s How Entrepreneurs Can Stay on Top of It\\n\\nThe customer journey is more than the total of events a customer has done in his quest to purchase. An AI-driven customer journey facilitates a deeper understanding of the customer that enables meaningful interactions.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Who Owns the Data Your Business Uses? Not Knowing Could Hurt the Sale of Your Company.', \"/\\n\\nData usage comes with limitations.\\n\\nFebruary 6, 2018 4 min read\\n\\nOpinions expressed by Entrepreneur contributors are their own.\\n\\nIn late 2016, San Francisco-based software engineer Anthony Levandowski got a new job when car-sharing service Uber acquired Otto, the autonomous trucking company that he had started less than a year earlier for a reported $680 million. Eventually, his technology found its way into Uber’s self-driving car project and Levandowski himself was named Uber’s vice president of engineering.\\n\\nRelated: 6 Things Big Companies Look for When Buying Your Startup\\n\\nBut there was a problem.\\n\\nSoon after the acquisition, Levandowski’s former employer, Waymo, an Alphabet subsidiary, sued him, accusing the star engineer of stealing thousands of documents related to the company’s self-driving technology, including blueprints, testing documents and design files. The lawsuit alleges that this intellectual property went into his work at Otto, and was later incorporated by Uber.\\n\\nAs the case dragged on, Levandowski refused to cooperate with investigators and Uber eventually fired him, all because of questions surrounding the data he used to build his business. The case is not yet set for trial.\\n\\nThis is an extreme example, but it is reflective of what can happen when a company is accused of misappropriating the data it used to underpin its business. And the situation is going to get a lot worse before it gets better -- using data obtained from various sources, such as consumers using apps or other services, tools that scrape web sites, engineers downloading information from Wikipedia as well as data provided by suppliers, customers and other parties has become commonplace in a wide range of industries.\\n\\nRelated: Time to Sell Your Business? You'll Need Metrics.\\n\\nThis extends beyond data monetization and includes, among other things, running data through advanced machine learning algorithms (such as neural networks, deep learning and natural language processing) to train their own algorithms, determine trends, identify risks and create intelligent applications.\\n\\nBeyond Silicon Valley\\n\\nThis is not just a trend in the technology industry. As traditional industries have begun transforming their products and customer engagement models around IoT, well-managed databases from both legacy and new sources, combined with artificial intelligence, are playing a critical role.\\n\\nBut, there is a downside to this practice that is being overlooked. There is a lot of data out there being leveraged, and not enough oversight going into who owns it, who pays for it and who can use it.\\n\\nRelated: 5 Things to Keep in Mind When Using Data for Artificial Intelligence\\n\\nFor the most part, data is not free for the taking. Although everyone today wants to use data, the fact is they probably don't have the rights to actually use it. This is because a lot of the information available these days is coming from third parties, either without their permission or under contract terms that limit what can be done with that data.\\n\\nSo, we have companies pulling content off the web indiscriminately. They're scraping websites, they're downloading data from Wikipedia, they're using information their customers provide (whether the customer knows it or not) when using their services, and they're not really paying any attention to whether or not they actually have the right to grab the data or use it.\\n\\nOnce it comes time to raise money, find a buyer or close a deal, this can become a major problem if a smart investor or purchaser understands the risks and asks the right questions.\\n\\nWide-reaching risks\\n\\nBuyers, sellers and even firms still don't yet understand the limitations on the data that sellers are using. They may think representations and warranties around intellectual property and data privacy will reveal these issues, but a significant amount of data being used is not an intellectual property right, nor is personal data, which is covered by data privacy legislation.\\n\\nRelated: 3 Fundamental Ways Machine Learning Will Change Business in 2018\\n\\nThe Supreme Court’s 2014 decision in Alice v. CLS Bank was generally viewed in the tech community as weakening the protections afforded to software inventions, leading to a decline in patent filings and IP cases in Silicon Valley. As litigation surrounding artificial intelligence increases, so should the attention to whether companies even own the rights to the underlying data.\\n\\nThe time to prepare for this is now.\\n\\nFor companies and investors, data brings with it great power but also great risks. It may seem trivial, but when it comes time to review a deal or back a new company, data rights and governance are important considerations. For those that don’t, the results could be costly.\\n\\nRelated Video: What to Do When You're Ready to Sell Your Business\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You Don’t Have to Be a Data Scientist to Fill This Must-Have Analytics Role', 'Success with AI and analytics requires not just data scientists but entire cross-functional, agile teams that include data engineers, data architects, data-visualization experts, and — perhaps most important — translators. Translators are neither data architects nor data engineers. They’re not even necessarily dedicated analytics professionals, and they don’t possess deep technical expertise in programming or modeling. Instead, translators play a critical role in bridging the technical expertise of data engineers and data scientists with the operational expertise of marketing, supply chain, manufacturing, risk, and other frontline managers. Given the urgent need for translators, hiring externally might seem like the quickest fix. However, new hires lack the most important quality of a successful translator: deep company knowledge. As a result, training existing employees often proves to be the best option for filling the translator void.\\n\\nArtur Debat/Getty Images\\n\\nIt’s no secret that organizations have been increasingly turning to advanced analytics and artificial intelligence (AI) to improve decision-making across business processes — from research and design to supply chain and risk management.\\n\\nAlong the way, there’s been plenty of literature and executive hand-wringing over hiring and deploying ever-scarce data scientists to make this happen. Certainly, data scientists are required to build the analytics models — including machine learning and, increasingly, deep learning — capable of turning vast amounts of data into insights.\\n\\nMore recently, however, companies have widened their aperture, recognizing that success with AI and analytics requires not just data scientists but entire cross-functional, agile teams that include data engineers, data architects, data-visualization experts, and — perhaps most important — translators.\\n\\nWhy are translators so important? They help ensure that organizations achieve real impact from their analytics initiatives (which has the added benefit of keeping data scientists fulfilled and more likely to stay on, easing executives’ stress over sourcing that talent).\\n\\nWhat exactly is an analytics translator?\\n\\nTo understand more about what translators are, it’s important to first understand what they aren’t. Translators are neither data architects nor data engineers. They’re not even necessarily dedicated analytics professionals, and they don’t possess deep technical expertise in programming or modeling.\\n\\nInstead, translators play a critical role in bridging the technical expertise of data engineers and data scientists with the operational expertise of marketing, supply chain, manufacturing, risk, and other frontline managers. In their role, translators help ensure that the deep insights generated through sophisticated analytics translate into impact at scale in an organization. By 2026, the McKinsey Global Institute estimates that demand for translators in the United States alone may reach two to four million.\\n\\nWhat does a translator do?\\n\\nAt the outset of an analytics initiative, translators draw on their domain knowledge to help business leaders identify and prioritize their business problems, based on which will create the highest value when solved. These may be opportunities within a single line of business (e.g., improving product quality in manufacturing) or cross-organizational initiatives (e.g., reducing product delivery time).\\n\\nTranslators then tap into their working knowledge of AI and analytics to convey these business goals to the data professionals who will create the models and solutions. Finally, translators ensure that the solution produces insights that the business can interpret and execute on, and, ultimately, communicates the benefits of these insights to business users to drive adoption.\\n\\nGiven the diversity of potential use cases, translators may be part of the corporate strategy team, a functional center of excellence, or even a business unit assigned to execute analytics use cases.\\n\\nWhat skills do translators need?\\n\\nThe wide range of responsibilities — leader, communicator, project manager, industry expert — inherent in the translator role makes the following skills essential:\\n\\nDomain knowledge\\n\\nDomain knowledge is by far the most important skill for any translator. Translators must be experts in both their industry and their company to effectively identify the value of AI and analytics in the business context. They must understand the key operational metrics of the business and their impact on profit and loss, revenue, customer retention, and so on. Additionally, knowledge of common use cases (e.g., predictive maintenance, supply chain management, inventory management, personalized marketing, churn prediction, etc.) in their domain is important.\\n\\nThe Analytics Translator Role At each step of the analytics initiative, the translator has an important role to play: Step 1. Identifying and prioritizing business use cases\\n\\nTranslator role: Works with business-unit leaders to identify and prioritize problems that analytics is suited to solve. Step 2: Collecting and preparing data\\n\\nTranslator role: Helps identify the business data needed to produce the most useful insights. Step 3: Building the analytics engine\\n\\nTranslator role: Ensures the solution solves the business problem in the most efficient and interpretable form for business users. Step 4: Validating and deriving business implications\\n\\nTranslator role: Synthesizes complex analytics-derived insights into easy-to-understand, actionable recommendations that business users can easily extract and execute on. Step 5: Implementing the solution and executing on insights\\n\\nTranslator role: Drives adoption among business users.\\n\\nGeneral technical fluency\\n\\nIn addition to their domain knowledge, translators must possess strong acumen in quantitative analytics and structured problem solving. They often have a formal STEM background, or self-taught knowledge in a STEM field. And while they don’t necessarily need to be able to build quantitative models, they do need to know what types of models are available (e.g., deep learning vs. logistic regression) and to what business problems they can be applied. Translators must also be able to interpret model results and identify potential model errors, such as overfitting.\\n\\nProject management skills\\n\\nA mastery of project management skills is a must. Translators should be able to direct an analytics initiative from ideation through production and adoption and have an understanding of the life cycle of an analytics initiative and the common pitfalls.\\n\\nAn entrepreneurial spirit\\n\\nIn addition to these “teachable” skill sets, translators also should have an entrepreneurial mind-set. They need the enthusiasm, commitment, and business savvy to navigate the many technical, political, and organizational roadblocks that can emerge. This is often less teachable – or at least less straightforwardly so – and the availability of entrepreneurial individuals can depend in part on the organization’s culture.\\n\\nWhere can organizations find translators?\\n\\nGiven the urgent need for translators, hiring externally might seem like the quickest fix. However, new hires lack the most important quality of a successful translator: deep company knowledge. As a result, training existing employees often proves to be the best option for filling the translator void.\\n\\nInsight Center The Risks and Rewards of AI Sponsored by SAS Assessing the opportunities and the potential pitfalls.\\n\\nOf course, this route presents its own challenges, considering there are currently no certifications or degrees for translators. In response, many companies have created their own translator academies. One global steel company, for example, is training 300 managers in a one-year learning program. At McKinsey, we’ve even created an academy in our own firm, training 1,000 translators in the past year.\\n\\nAcademy curricula frequently ranges from exploring the art of the possible to studying specific AI techniques and methods. Formats include both courses and immersion.\\n\\nSome organizations train translators through apprenticeships in multifunctional, agile teams on real AI and analytics transformation projects. These companies often combine apprenticeship programs with an academy, designing deliberate learning journeys, typically a year in length, for each individual.\\n\\nWho is currently responsible in your organization for connecting AI and analytics with business goals? In many organizations, data professionals and business leaders often struggle to articulate their needs in a language that the other can execute on.\\n\\nTranslators bring a unique skill set to help businesses increase the return on investment for their analytics initiatives. They’re instrumental in identifying, from the myriad possible opportunities, which are the right opportunities to pursue, and they can help ensure that all participants, from data professionals to business executives, work in harmony to realize the promise these technologies offer.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Big Companies Are Embracing Analytics, But Most Still Don’t Have a Data-Driven Culture', 'For six consecutive years NewVantage Partners has conducted an annual survey on how executives in large corporations view data. This year, results show nearly every firm is investing in some form of analytics, and most are seeing value. But only one-third say they have succeeded in creating a data-driven culture. And many fear disruption by firms better equipped to use AI. Almost four in five respondents said they feared disruption or displacement from firms like those in the fintech sector or firms specializing in big data. The technology judged most disruptive is AI — by far. Seventy-two percent chose it as the disruptive technology with the most impact — far more than cloud computing (13%) or blockchain (7%).\\n\\nPASIEKA/Getty Images\\n\\nFor six consecutive years NewVantage Partners has conducted an annual survey on how executives in large corporations view data. Each year the response rate increases, and the reported urgency of making effective use of data increases as well. This year the results are both more encouraging and more worrisome than in the past.\\n\\nSix years ago, the primary focus of questions and answers in the survey was big data, which was relatively new on the business scene. In the 2018 survey, the primary attention has moved to artificial intelligence. AI is now a well-established focus at these large, sophisticated firms. There is both a stronger feeling that big data and AI projects deliver value and a greater concern that established firms will be disrupted by startups.\\n\\nThe survey includes senior executives from 57 large corporations. The industry group with the most firms represented in the survey is one of the most data-intensive: financial services. Companies from the life sciences, manufacturing, telecom, and online industries also participated. The actual respondents are changing somewhat from the first surveys: It has always involved a high proportion of C-level executives responsible for data, but this year chief data officers are 56% of the respondents, up from 32% last year. Only 12% of firms in the 2012 survey had even appointed a chief data officer.\\n\\nWhile AI gets the headlines here and elsewhere in the world, the survey addresses both big data and AI. Terminology comes and goes, but the constant is a data explosion and the need to make sense of it. Big data and AI projects have become virtually indistinguishable, particularly given that machine learning is one of the most popular techniques for dealing with large volumes of fast-moving data. It’s also the case that statistical approaches to AI — deep learning, for example — are increasingly popular. Therefore, we view traditional data analytics, big data, and AI as being on a continuum. Virtually all of the respondents (97%) say they are investing in these types of projects.\\n\\nPerhaps the best news in this survey is that companies continue to believe they are getting value from their big data and AI projects. 73% of respondents said they have already received measurable value from these initiatives. That number is half again higher than in the 2017 survey, which suggests that more value is being achieved as companies grow familiar with the technologies.\\n\\nThe types of value received are perhaps consistent with other previous types of technology. Consistent with our view that big data and AI are extensions of analytical capabilities, the most common objectives — and those most likely to achieve success — are “advanced analytics/better decisions.” Thirty-six percent had that as their top priority, and 69% of those had already achieved success with the objective. Better customer service and expense reduction are also common objectives. Just over one-quarter of firms (27%) are pursuing some combination of innovation and disruption, speed to market, or data monetization initiatives. Data monetization programs had the lowest priority and the lowest percentage of success (27%).\\n\\nOne of the greatest issues for concern in the survey for large enterprises is the risk of disruption from new entrants. Almost four in five respondents said they feared disruption or displacement from firms like those in the fintech sector or firms specializing in big data. The technology judged most disruptive is AI — by far. Seventy-two percent chose it as the disruptive technology with the most impact — far more than cloud computing (13%) or blockchain (7%).\\n\\nAnother important and continuing issue is the slow speed with which these established firms make the shift to a data-driven culture. Virtually all respondents (99%) say their firms are trying to move in that direction, but only about one-third have succeeded at this objective. This gap appears every year in the surveys, and the level of success hasn’t improved much over time. Clearly firms need more-concerted programs to achieve data-related cultural change. Many startups have created data-driven cultures from their beginning, which is a key reason why large, established firms fear disruption from them.\\n\\nOne of the approaches that firms have established to deal with data-driven disruption and change is to establish new management roles. However, there is still a lack of clarity about how different data-oriented roles (chief information officer, chief data officer, chief digital officer, chief analytics officer, etc.) relate to each other.\\n\\nWith respect to the chief data officer role, there is substantial disagreement about the major responsibilities of the role and what types of backgrounds are appropriate for CDO jobs. Thirty-nine percent say their CDO has primary responsibility for data strategy and results, but 37% assign that responsibility to other C-level executives, and 24% say there is no single point of accountability for it. In terms of backgrounds, 34% of respondents believe the CDO should be a change agent from outside the company, while 32% believe the person should be a company veteran from inside the firm. Role clarity in senior data-related roles is critical for both leading AI/big data projects and accomplishing cultural change. And while all respondents believed it important, the majority of firms still lack an enterprise data strategy.\\n\\nThis continuing rise in the importance and challenges of big data is one of the most important features of contemporary economy and society. The survey results over time provide interesting and useful documentation of this revolution. The rise of AI is only exacerbating this trend. The keys to success are to determine how your firm should respond, assign clear responsibilities for data strategy and results, and then move ahead to execute the needed changes in a systematic and effective fashion.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('How AI Can Improve How We Work', 'Paul Daugherty and James Wilson, senior technology leaders at Accenture, argue that robots and smarter computers aren’t coming for our jobs. They talk about companies that are already giving employees access to artificial intelligence to strengthen their skills. They also give examples of new roles for people in an AI workplace. Daugherty and Wilson are the authors of the new book Human + Machine: Reimagining Work in the Age of AI.\\n\\nDownload this podcast\\n\\nSARAH GREEN CARMICHAEL: Welcome to the HBR IdeaCast, from Harvard Business Review. I’m Sarah Green Carmichael.\\n\\nBy now, I’m used to the idea that machines are going to be an ever more present part of work.\\n\\nBut if I try to imagine what those machines will be doing or what they’re gonna look like, that’s when the picture starts to get a little fuzzy.\\n\\nOur guests today say machines will be doing all sorts of functions. And they stress that if we humans develop and deploy AI responsibly, the technology will take us to new levels of productivity.\\n\\nPaul Daugherty is Accenture’s chief technology & innovation officer. James Wilson is the consulting firm’s managing director of IT and business research.\\n\\nTogether they’re the authors of the book Human + Machine: Reimagining Work in the Age of AI. And they’re here to talk with us about the impact this emerging technology is having on people and organizations, and the roles and skills we will all need in the future.\\n\\nThank you both for taking the time.\\n\\nJAMES WILSON: It’s great to be here. Thanks.\\n\\nPAUL DAUGHERTY: Great to be here, Sarah,\\n\\nSARAH GREEN CARMICHAEL: So, there’s been a lot of, oh my God, robots are coming for your job hype in the media about the future of human and machine collaboration. Do you buy that hype?\\n\\nPAUL DAUGHERTY: No, you know, I think — if you look at it, it’s the 50th anniversary of 2001: A Space Odyssey 50 years ago, last week, and that set in motion, this whole narrative about, you know, machines taking over the human race. And the other narrative that started is, you know, the machines are coming from our jobs, and then we had this whole thing about, you know, the machine, the machine is beating us at chess and checkers and Go and all these games, and we really think that’s all misplaced. And, of course, technology always does some things better than people can do. That’s what technology is. And that’s been the history of technology. But we wrote the book about is the fact that really, you know, AI, artificial intelligence, you know, robotics and the machines we’re talking about, like any other technology, really helps us as people, as humans, do things more effectively. And hence the title, Human + Machine.\\n\\nSARAH GREEN CARMICHAEL: Yeah. So, one of the things I think that worries people is that, you know, in the first stage of companies adopting some of these technologies, they do use them to replace some people. But why, in your view, does a company that does that, that uses machines to replace people, eventually stall out?\\n\\nJAMES WILSON: Well, in our research, we have really seen that there is this early-stage shift from a automation focused with artificial intelligence to an imagination and re-imagination focus. And we’re seeing that companies that focus on imagination and re-imagination are able to do a lot more and to do things differently than the companies that are just focusing on automating the old ways of doing things. So, if you think about typical process design, there might be 12 steps in a process, and if you come in and you say, all right, we’re gonna automate six of these 12 steps, you’re basically all you’re doing is speeding up an old way of doing things. You’re putting a new catalytic converter into a Model T, for example, as opposed to really rethinking your way of getting around.\\n\\nSARAH GREEN CARMICHAEL: So, what have you figured out about which tasks people do best and which kinds of things machines do best?\\n\\nPAUL DAUGHERTY: Yeah, people are good at emotive capability, communication, improvisation, generalization, things like that. And the machines are good at, you know, memorization, transactions, prediction, and one thing we don’t think has been looked at enough that we — was really the core focus of our research was what happens when you combine those two, and what’s, what’s that middle ground of collaboration? So, in the book, you know, in our work, we actually talked about collaborative intelligence, which is what happens when you put the strengths of the human together with the strength of the machine, and that’s where you found these, these new categories of jobs being created that we call the missing middle because, you know, missing because there really hasn’t been as much focus on it. We’ve seen this for binary focus of how can I replace people with the machines rather than thinking about how can I make people more effective and productive? And really, give people superpowers using this new technology to do their jobs much more effectively or as citizens or consumers, you know, live, live our lives more effectively.\\n\\nJAMES WILSON: One thing that we have seen is that leading companies are really setting a precedent for creating unprecedented new types of jobs. So, we’re, we’re starting to see the emergence of new job categories that we haven’t seen before in this missing middle, in between kind of the human side and the machine side of work.\\n\\nSARAH GREEN CARMICHAEL: Yeah. So, give us an example of what some of those jobs might be and how they might be shaped by this human plus machine collaboration.\\n\\nJAMES WILSON: Well, there are two kind of big buckets. One bucket of jobs are where people help build and manage smart machines, and the other bucket are where people are helped by machines. And within the bucket where people are building machines, we see kind of three clusters of jobs that companies can kind of predictably put into the organization and really need to start thinking about today. One of those jobs is called a trainer, another, an explainer, and the third category that we talk about are called sustainers. But within that trainer category, there are interesting new types of jobs that we’re seeing emerging like personality trainers who use natural language processing and work to build intelligent agents and chatbots. And those don’t necessarily require a background in software engineering. It might require, you know, having a background in psychology or background in drama.\\n\\nThat second category that we talk about is called an explainer. And these are roles that where people explain why machines are doing what they do. So, for instance, at one organization, uh, we interviewed a guy who’s basically an AI detective. And so, when his company’s pricing model starts doing things that are unexpected, he has to go and explain to colleagues why it’s behaving in a certain way. We’re going to see a huge need for this explainer role for any company that’s operating in Europe these days with the GDPR that’s coming into effect quite soon. I saw one piece of research that estimated companies, global companies, are going to need about 75,000 new data compliance officers in their organization to explain to customers algorithmic decisions, say, if a customer calls in and say in a bank and that sort of thing.\\n\\nAnd then the third category are what we call sustainers, and sustainers really manage the tradeoffs between what’s good for the business and what’s good for society and really use a kind of an ethical and responsible AI lens when making decisions. So, one type of role that we’ve seen there are called AI safety engineers. One of the things they think about are unintended consequences. You know, so what happens if this robot is hacked, you know, even though it’s a consumer robot, but if it’s a robot, you know, if it’s being used at say an airport or for industrial reasons and that sort of thing, what happens if the robot hacks itself for some reason trying to get a more efficient result.\\n\\nSARAH GREEN CARMICHAEL: So, in this world where we are working with more different kinds of machines and algorithms and bots that can do certain tasks very easily, what are some of the skills you think that people will need to develop either to sort of remain employable or to get more out of these machines to make sure that they are really using them to give them superpowers?\\n\\nJAMES WILSON: Well, I think that the roles that and the people that are most vulnerable to displacement from artificial intelligence are the ones that aren’t using artificial intelligence. So, how do you use AI as quickly as possible on the job? How can you start learning AI tomorrow on the job? So, I think there are two that executives really need to think about. Uh, the first might be a bit more obvious: doubling down on training. And it might be obvious, but our research really shows that companies still aren’t investing in retraining and reskilling at the level that they need to be.\\n\\nThe other thing that executives need to start to think about is really lowering the barrier to using AI. I think a lot of people think AI is rocket science, and in fact in a lot of ways it is. It requires really high-end math or stats skills, so the, you know the question facing a lot of workers today is, well, yeah, I’d like to be able to use it, but how do I use it? There are a lot of companies these days that are beginning to say, well, let’s democratize AI. Let’s, let’s make it as easy for a salesperson to use as their Excel spreadsheet or as PowerPoint. A one company that we’ve looked at, for instance that’s doing this is AT&T, and they’re putting point-and-click types of AI tools onto the desktops of about 50,000 people. So, if I’m in a call center, if I’m a salesperson opening up an account, I can interact with AI, can upload a data set, link it to natural language processing, just point-and-click approaches and kind of give myself superpowers in that customer interaction in ways that weren’t possible before. But the key point there is that happened as a result of executive intervention.\\n\\nPAUL DAUGHERTY: Broadly speaking, when you think about the skills that people need, I think there’s two broad categories.\\n\\nOne is the skills you need for the people who will do AI. Those are, you know, the machine learning experts, data experts in a lot of STEM technology, coding types of experts and clearly we need more of those and there’s a lot of focus on building those skills, and a lot of companies are focusing on acquiring those skills. But relatively speaking that’s a lot smaller number than the other category, which is the people that use AI which will be almost every profession, and I when we look at that category of people who need to use AI to do their job, to do these new missing middle jobs that we talk about, that’s where I think we need to think about skills. And Jim talked about some good examples there. And in that category I think we need to focus on the one hand on building, building the more human-like skills. Because you know, AI technology will continue to do the things that machines are good at. That’s just been the history of how we apply technology to do anything to automate anything that, uh, that we can through throughout civilization. So, what we really need to focus on is what human skills can we really accentuate using the technology.\\n\\nAlso then focused on getting people more facile and more familiar and more comfortable with using technology. So, the last chapter of our book, we talk about eight fusion skills, which are the new fused, you know, human plus AI capabilities. For example, one is called a judgment integration, which is how do you make a decision combining your human judgment with the judgment from an algorithm? And you have to think about things a little differently. You have to apply and use the technology differently. And an example of this being used as some new wealth management approaches that the banks are using using AI to, to give agents better tools to recommend products to their customers and better make judgments about how to advise their customers. And that’s the kind of fusion scale that we think is important going forward. But it’s going to be a combination of that human skill and the technology or the AI skill. You know, one of the predictions that I would make is that when we look several years out right now, we bemoan the shortage of STEM skills and decoding skills and the AI skills.\\n\\nI think, you know, somewhere around five years out in the future, I think we’ll be talking about the lack of more human-oriented skulls, humanities types of skills that can design the experiences and manage the experiences that we’re creating using AI as we reimagine businesses, as we reshape products to use technology to interact with humans in a more human-like way, we’re going to find there’s a dramatic need for many, many more professionals who can bring in that kind of ability to shape our human experiences using technology. And those will be some of the softer skills applied rather than a hardcore, you know, AI or tech or coding skills.\\n\\nSARAH GREEN CARMICHAEL: You know, often in these conversations we’re focused on customer interactions, analyzing customer data, or managing customer relationships. But in the book, you have some examples of how companies are also using some of these tools to manage their own internal processes. And I wanted to ask you about how Unilever for example, is using AI in hiring.\\n\\nJAMES WILSON: I think we usually think about the use of advanced analytics in HR in terms of crunching through resumes, but what we’re actually seeing is that AI is moving into the interviewing process as well, and that’s really transforming recruiter’s ability to interview more candidates. At Unilever, for instance, they’ve incorporated AI into kind of the first two rounds of interviews. The first round they play online games, and that gives the company a sense of the skill set of the person, kind of their behaviors. Maybe there’s a job posting that maps to their skill set that the person wasn’t aware of. The second round of interview actually is done with a video analysis system that can evaluate the person’s comfort level with certain types of questions, their gestures, their facial cues, and that sort of thing. So, it really allows the company to talk to three or four more times candidates than before.\\n\\nAnd then, by the way, that third and final round of interview, the candidates are actually talking to the human recruiters. But by using that approach, the company has been able to expand diversity fourfold. They’ve been able to get candidates into interviews from many more universities, I think about three or four times more universities. And maybe most importantly they’ve been able to drastically reduce that cumbersome recruiting process that can drag on for months. At Unilever, I think before they brought in this solution, it was about four months long from that first interview to the final decision. They’ve been able to reduce that to about four weeks. So that’s a huge improvement both for the company but also for the candidate and their experience interacting with the company.\\n\\nPAUL DAUGHERTY: Yeah. There’s another example that I would give in the HR area is something that we’re doing within our own company, within Accenture. We have over 400,000 employees, and we’re using AI in a creative way, still at an experimental level, but we’re using machine learning and AI to, based on, you know, a person’s profile, to understand their current job, experience, their resume, what they’ve done, their assignments. And based on the changes in technology, it’ll learn and, uh, recommend to how soon that individual might need to change their profession because what they’re doing now may become obsolete. So, in other words, how long will your skills will be relevant. But then it goes further and says, OK, based on what, you know, what should you start learning that based on what you know, that’ll help you be relevant and effective as you know, as technology changes. So, that’s still a little bit of an experiment, but I think it points the way to how we can use AI itself to help with this, you know, jobs and reskilling issue that we’re talking about earlier.\\n\\nSARAH GREEN CARMICHAEL: In your view, how many industries will be affected by this? Because occasionally you’ll hear, everything’s going to be affected. This will affect everyone. And then other times people are like, no, really won’t. What’s your view on that?\\n\\nJAMES WILSON: Well, you know, we don’t have a stock answer for that, and I think you know, uh, it’s going to vary by region, by country. France, for example, is going to focus on healthcare, mobility, and transportation. I think in regulated industries there’s going to be a bit slower adoption rate. Though certain types of algorithms, you know, high-speed trading algorithms and that sort of thing have been around for a long time in Wall Street. But when you start using a more sophisticated deep learning techniques in situations where you wouldn’t be able to explain to the customer or to the institutional investor why a certain transaction was made — those might take quite a while to be adopted if they’re adopted it at all.\\n\\nPAUL DAUGHERTY: On the bigger landscape, we do believe that AI is right now really what I would say is the alpha trend driving other trends that we see in the market, and it will impact every industry. And we just talked about some specific industries that it’s going to impact more quickly, and I think it’s, it’s those industries that are very data intensive, those industries that have a lot of human interaction, those industries that have a lot of compliance and regulatory implications, we’re finding a lot of applicability. Those industries that have a lot of supply chain logistical components to them because those are the kinds of problems that we’re, we’re finding AI can solve very well. And then broadly speaking, health generally, there’s so much opportunity to improve wellness and health outcomes by applying AI that that generally speaking will be a big benefit that we’ll continue to see as AI is applied.\\n\\nSARAH GREEN CARMICHAEL: Well, listen, it’s been really fun talking with you guys today about all of this. Thank you both so much for sharing your time with us.\\n\\nPAUL DAUGHERTY: Thank you, Sarah.\\n\\nJAMES WILSON: Yeah, thanks, Sarah. It’s been a great conversation.\\n\\nSARAH GREEN CARMICHAEL That’s James Wilson and Paul Doherty. They’re the authors of Human + Machine: Reimagining Work in the Age of AI. You can find it at HBR.org.\\n\\nThanks for listening to the HBR IdeaCast. I’m Sarah Green Carmichael.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I Fed A Neural Network 1200 Video Game Names', 'Image: id/Kotaku\\n\\nA lot of the time, video game names sound like they were generated by a machine. So, I decided to see if I could teach a neural network to name a game.\\n\\n\\n\\nSometimes it’s hard not to groan at some video game names. There was a period of time when I kept confusing Absolver for Observer because they sounded similar. When game names aren’t similar, they can be nonsensical. How are you supposed to pronounce JYDGE?\\n\\nIt occured to me that maybe us humans are just out of ideas, so, with the help of Deadspin’s Tim Burke, I fed 1200 game names to a neural network in order to make up a few new game names. Burke told me that usually you train a neural network on a much larger dataset, but when he tried he came up with mostly weird Tom Clancy names like Tom Clancy’s Tomb Roids or Tom Clancy’s Legends: The Hag. I wanted to generate some names that sounded like you could grab them off the shelf right now, so I used a dataset consisting of all of the English language releases for the XBox One, Playstation 4 and Wii U from the consoles’ releases to around June, 2017. Some of the results sounded like they were real games; others not so much.\\n\\nMost interestingly, there were a lot of sequels in the generated results.\\n\\nThe Dead Charter 2 Glomepers: The Fantare 2 Songers of Stare 2 The Ganing 2 Call of the Marder 2 The Game Park 2\\n\\nThe Dead Charter 2 sounds like it could be one of the many pirate-y games that are coming out this year. I am also very curious about what happened in the first Call of the Marder.\\n\\nAdvertisement\\n\\n“War” also showed up a lot in these names, as well as “Heroes” and “Battles.”\\n\\nThe Warling The Tank Wars Elite of Wars The Bommerns of War Shorker Call Heroes The Battle Evil Heroes\\n\\nTank Wars is an actual game, but it was not in the dataset that was fed to the neural network. Deep learning invented Tank Wars on its own.\\n\\nAdvertisement\\n\\nThese were the scant few that really felt like the names of real games. They feel fantasy flavored, probably because they use a lot of made up words. In particular, Toton of the Legenders is definitely a weird, sprawling JRPG that Jason probably loves.\\n\\nThe Tome Core The Gold of Deor Toton of the Legenders: Starner Carsing The Tock of the Seningang The Tale of the Gare 2\\n\\nThese ones were just funny. Why did this neural net think games are so dank?\\n\\n\\n\\nSamer 2: Dank Gane Edition Wraloy Legend: Dank Dank Super Date: Sparo Edition Dong Devin Shartords The Bank Crant Sons The World Cant\\n\\nEven with a limited dataset, I feel like these game names strayed pretty close to reality. Legends, battles, heroes and apparently dankness are recurring themes in the names of games. A lot of games concern themselves with sweeping, epic quests and combat, so it makes sense. Still, I’m disappointed that these were the things that rose to the top. Even if they’re fewer and further between, video games are about more than just legends and war. Overall, I think my favorite name of the group is The World Cant. Girl, same.\\n\\n')\n",
      "('This Week in the Business: The Real Problem With Switch Ports', 'QUOTE | “Switch is not as powerful as a PS4 or Xbox One. Everyone knows that. But the difference isn’t as big as the general perception would have you believe.” - Abstraction Games CEO Ralph Egas says the studio’s biggest challenge in porting Ark: Survival Evolved to the Switch was not processor power, but getting everything they needed to fit on a 16GB cartridge (32GB cartridges can be used, but are pricey).\\n\\n\\n\\nQUOTE | “For Sony and Microsoft to support their customers well they have to be open to all their customer’s friends - their real-world friends - otherwise they’re breaking up real-world social groups.” - Epic CEO Tim Sweeney explains why it’s inevitable Xbox and PlayStation players will eventually be able to play with each other across platforms.\\n\\nQUOTE | “Why would rabbits be shooting inks? Why would rabbits be able to submerge themselves in ink when they shot it? Something about that concept didn’t fit right.” - Splatoon producer Hisashi Nogami describes some of the nonsense ideas the development team tried out before settling on the game’s perfectly reasonable squid-kid hybrid characters.\\n\\nQUOTE | “Our only goal is to protect our ability to tell more stories in the Star Control multiverse.” - Kevin Unangst of Star Control rights holder Stardock explains the company’s spat with original Star Control developers Paul Reiche and Fred Ford, who are working on their own successor to the franchise.\\n\\nSTAT | $225,000 - The amount of money Reiche and Ford would pay Stardock as part of a settlement the company allegedly proposed in the dispute. Other conditions included that Reiche and Ford would issue a public apology, not work on any game similar to Star Control for the next five years, and surrender all their IP rights to the alien races used in the original Star Control games.\\n\\nAdvertisement\\n\\nQUOTE | “The thing with player behaviour, and the thing with systems like Honor, is that we need to continue to invest. Communities change over time; we learn new things, we screw up in new ways, and we need to adapt and keep evolving these systems...” - Riot Games senior technical designer Kimberly Voll says cultivating positive player behavior is an ongoing commitment.\\n\\nQUOTE | “We envision a world where games are free of harassment, discrimination, and abuse, and where players can express themselves through play.” - An excerpt from the Fair Play Alliance mission statement. The FPA was formed this week by a coalition of more than 30 game companies, including Blizzard, Riot, Xbox, Epic, Supercell, and Twitch.\\n\\nQUOTE | “There are two sides to the coin here. If you just let developers do whatever the hell they want, it will cost a lot of money. Something that’s been missing, but is becoming more and more common, is to have a very clear vision from the beginning.” - A Way Out director Josef Fares wants publishers to start taking bigger risks on creators, and for creators to respect the risks being taken in them.\\n\\nAdvertisement\\n\\nQUOTE | “In future titles, as deep learning technology matures, I expect self-learning agents to be part of the games themselves, as truly intelligent NPCs that can master a range of tasks, and that adapt and evolve over time as they accumulate experience from engaging with human players.” - EA SEED’s Magnus Nordin, who has been training an AI to play Battlefield, sees tremendous future potential for AI in games.\\n\\nQUOTE | “They need to understand what can be achieved. They need to understand what the steps to unionisation are. They need to decide amongst themselves who they are going to align with or are they going to start their own organization.” - Steve Kaplan of the International Alliance of Theatrical Stage Employees made the trip to the Game Developers Conference in San Francisco to sit in on what had been shaping up to be an anti-union panel run by the International Game Developers Association.\\n\\nQUOTE | “When you send cold emails, they don’t really read it or get interested unless it’s something they’ve heard of. But what really works is if they watch their fellow streamers streaming the game on Twitch and they don’t have access? They want it.” - PUBG Corportion lead community manager Sammie Kang explains how PlayerUnknown’s Battlegrounds used exclusivity and a core community of low- and mid-tier streamers to build a pre-launch following with no marketing budget.\\n\\n')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Up AlphaGoer Five', 'AlphaGo is the deep learning program that can beat humans at the game Go. You can read Google’s highly technical paper on it, but you’ll have to wade through some very academic language. [Aman Agarwal] has done us a favor. He took the original paper and dissected the important parts of in in plain English. If the title doesn’t make sense to you, you need to read more XKCD.\\n\\n[Aman] says his treatment will be useful for anyone who doesn’t want to become an expert on neural networks but still wants to understand this important breakthrough. He also thinks people who don’t have English as a first language may find his analysis useful. By the way, the actual Go matches where AlphaGo beat [Sedol] were streamed and you can watch all the replays on YouTube (the first match appears below).\\n\\nInterestingly, the explanation doesn’t assume you know how to play Go, but it does presuppose you have an understanding of some kind of two-player board game. As an example of the kind of language you’ll find in the original paper (which is linked in the post), you might see:\\n\\nThe policy network is trained on randomly sampled state-action pairs (s,a) using stochastic gradient ascent to maximize the likelyhood of the human move a selected in state s.\\n\\nThis is followed by some math equations. The post explains stochastic gradient ascent and even contrasts it to another technique for backpropagation, stochastic gradient descent.\\n\\nWe have to say, we’d like to see more academic papers taken apart like this for people who are interested but not experts in that field. We covered the AlphaGo match at the time. Personally, we are always suckers for a good chess computer.')\n",
      "('Self-driving cars face a huge challenge in detecting bicycles', \"Self-driving cars face a huge challenge in detecting bicycles\\n\\nHowever, when it comes to spotting and orienting bikes and bicyclists, performance drops significantly. Deep3DBox is among the best, yet it spots only 74 percent of bikes in the benchmarking test. And though it can orient over 88 percent of the cars in the test images, it scores just 59 percent for the bikes.\\n\\nSelf-driving cars have a hard time predicting bicycle movement , and workarounds that require cyclists to buy transmitters are running into resistance from some.\\n\\nCops use backdoored WhatsApp photo to extract fingerprints and convict user South Wales Police announced they were able to access a WhatsApp user’s photos through a backdoor, then extract fingerprint data from a picture of a weed dealer’s hand to help convict 11 involved people. READ THE REST\\n\\nThe end of Windows closes in Five years ago, Steve Ballmer said “we can make Windows devices once again the devices to own.” Last week, Microsoft announced that Windows will no longer be a standalone unit at Microsoft, ending a division dedicated to personal OS that started in 1980. Via Ben Thompson at Stratechery: READ THE REST\\n\\nScientists design fire alarm wallpaper made from the same mineral as bone and teeth Researchers demonstrated a prototype “fire alarm wallpaper” that’s meant to be flame-resistant while also integrating a nanotechnology-based sensor that triggers a siren and warning lights. Ying-Jie Zhu at the Chinese Academy of Sciences and colleagues published their work in the journal ACS Nano. The new wallpaper is based on hydroxyapatite, which is the primary inorganic […] READ THE REST\\n\\nThe Nix Pro Color Sensor matches colors with extreme precision Our world is a colorful one, and when it comes time to repaint the house or create a new design, many of us look to our surroundings for inspiration. However, matching colors from the outside world to our canvas isn’t the most precise process when we’re just eyeballing it. The Nix Pro Color Sensor removes the […] READ THE REST\\n\\nHere's an XL 4/20 surprise You probably remember the Twisty Glass Blunt since we love to write about it. And you may also remember its little buddy, the Twisty Glass Mini. Well, today we’ve got a fun surprise that isn’t so little. Less isn’t always more, and on those days when you need to decompress with a good smoke, the Twisty XL […] READ THE REST\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('A proposal to stop 3D printers from making guns is a perfect parable of everything wrong with information security', 'A proposal to stop 3D printers from making guns is a perfect parable of everything wrong with information security\\n\\nMany people worry that 3D printers will usher in an epidemic of untraceable \"ghost guns,\" particularly guns that might evade some notional future gun control regime that emerges out of the current movement to put sensible, minimal curbs on guns, particularly anti-personnel guns.\\n\\nCertainly, it\\'s possible to 3D print a gun specifically designed to kill people (as opposed to target shooting or hunting). It\\'s also possible to have such a gun machined in pieces by stores that will overnight the components to you for on-site, simple assembly, but 3D printers make that process simpler and may eventually make it cheaper.\\n\\nEnter \"C3PO,\" a notional solution to this problem proposed by a group of researchers based at U Syracuse and Buffalo U. In a preprint on Arxiv, the C3PO team proposes that 3D printers could come pre-installed with a database of hundreds of thousands of images that they would attempt to match to print-jobs to determine whether they were being asked to make a gun (or any other unlawful object) so they could reject jobs that seemed to match the prohibition list.\\n\\nIt\\'s DRM for 3D printers, in other words, and it has all the problems of DRM and then some.\\n\\nFirst, for this to work, it has to be impossible for the user to alter the configuration of the printer. To make that practical, the printer has to obfuscate its operations from the user, lock down its bootloader, and generally treat the user as an adversary (the paper explicitly describes the printer being an adversary of its owner).\\n\\nThen, the manufacturer will have to invoke Section 1201 of the DMCA, as well as CFAA and other censoring rules to suppress bug-reports, because any defect in a printer could be exploited to overrule the preloaded prohibition on printing objects on the banned list.\\n\\nSo now you\\'ve got a printer that can run on free/open source software (because this is intrinsically user-modifiable). It has a \"Ring -1\" in which code executes without the ability of users to inspect or terminate processes. Any malware that runs in that zone -- anything that leverages a bug like the the ones AMD is contending with right now -- is, by definition, undetectable to the user and can do anything from staging attacks on the rest of the user\\'s network to tampering with the user\\'s printouts to introduce subtle (and since we\\'re talking about high-performance materials, potentially lethal) flaws into them, etc.\\n\\nAnd what\\'s more, it won\\'t stop 3D printed guns. The burgeoning body of research on adversarial examples reveals the inadequacy of this kind of fuzzy matching. The tldr is that if the fact that you can train your printer to recognize models of guns generated by people who weren\\'t trying to fool it tells you nothing about whether it\\'s possible to fool it. Think of how Google was once able to use inbound links as an incredibly reliable signal of page-relevance, and how quickly and easily attackers were able to generate spurious inbound links to fool Google\\'s Pagerank algorithm.\\n\\nWhat\\'s more, the intrinsic secrecy of the DRM model means that legit security researchers who discover defects in the gun-detection system won\\'t be able to publish (since they\\'ll face legal retaliation under DMCA 1201, etc) while people who want to make guns will be able to freely develop and productize systems to bypass the gun-detector, because those people are already doing something illegal and have already demonstrated their indifference to the law.\\n\\nFinally, these secret blacklists are an invitation to mischief and a moral hazard. The \"special purpose,\" \"narrowly constrained\" blacklists of child sexual abuse imagery developed by governments in Europe, Australia and elsewhere were first stuffed with material that powerful people just wanted to block (information about online gambling or assisted suicide, for example) and then used as justification to expand national censorship regimes to block copyright infringement, then trademark infringement, then \"extremist content\" and so on.\\n\\nOnce you equip a 3D printer with a blocklist of things that they notionally can\\'t print, everyone will want to add to that list: Erdogan and the King of Thailand will demand that satirical statuettes depicting them in caricature be banned; Disney and the copyright lobby will demand that models matching their proprietary characters and objects be banned; Ikea will want to ban third-party connectors; patent holders will want to ban third-party dinguses; the Saudi Committee for the Promotion of Virtue and the Prevention of Vice will demand a ban on depictions of Muhammad, and so on, and so on.\\n\\nSo this is an idea that neatly encapsulates virtually every terrible idea from the last 30 years of computing, learning none of its hard-earned lessons.\\n\\nFor the record, I believe in gun control and am mildly alarmed at the implications for gun control from 3D printing. But this won\\'t solve the problem, and will make it worse, and it\\'s precisely because gun control is an important issue that we can\\'t surrender to the security syllogism of \"Something must be done; there, I\\'ve done something.\"\\n\\nThe abuse of 3D printing technology to produce illegal weapons requires an intelligent 3D printer with early stage malicious activity detection. The 3D printer should identify the objects to be printed, so that the manufacturing procedure of an illegal weapon can be terminated at early stage. The lack of large-scale dataset obstructs the development of the intelligent 3D printer equipped with deep learning techniques. The construction of 3D printing image database in such scale with the recognition benchmarks has not been addressed until this work. We attempt to design two working scenarios for an intelligent 3D printer and provides corresponding image datasets (tens of hundreds and tens of thousands images). We also conduct quantitative performance benchmarking on ten 3D object recognition given single images and image sequences using C3PO database. This work brings the new thought of designing an object-aware 3D printing system. The main goal is to initiate the fusion of 3D printing technology and deep learning techniques in the computer vision domain enabling the secure use of 3D printing technology. As the 3D models are highly customized and diverse, building a robust recognition system remains a tough task. For the future work, C3PO will include more common 3D models especially for firearms. Re\\n\\nC3PO: Database and Benchmark for Early-stage Malicious Activity Detection in 3D Printing [Zhe Li, Xiaolong Ma, Hongjia Li, Qiyuan An, Aditya Singh Rathore, Qinru Qiu, Wenyao Xu and Yanzhi Wang/Arxiv]\\n\\n(via 4 Short Links)\\n\\n(Image: Cryteria, CC-BY)')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('4 deep learning breakthroughs business leaders should understand', 'It’s a given that artificial intelligence will change many things in our world in 2018. But with new developments arising at a rapid pace, how can business leaders keep up with the latest AI to improve their performance?\\n\\nPerhaps the best place for executives to start is gaining an understanding of deep learning. As one of the most exciting and powerful branches of AI, deep learning has led to important breakthroughs that expand the possibilities of applying AI to business problems.\\n\\nFirst, let me provide a quick intro to the technology. Deep learning is a type of machine learning. It’s a subfield of AI that deals with how computers learn as opposed to focusing on how we explicitly program them. In deep learning, researchers place concepts into a hierarchy. At each layer, a machine learns a concept and passes it to the next layer, which in turn uses it to build a more sophisticated concept. The more layers these models have (or the “deeper” they are), the more concepts they can learn, putting them at the vanguard of AI.\\n\\nIf that all sounds a bit complicated, don’t worry — we’ll dive into concrete examples below. Here are the top four deep learning breakthroughs business leaders should be aware of, arranged from the most immediately applicable to the most cutting edge.\\n\\n1. Image understanding\\n\\nWe can train deep learning algorithms to identify objects in an image. As of 2015, these algorithms (called convolutional neural networks) can achieve better image classification results than human beings.\\n\\nImage Credit: ImageNet dataset\\n\\nSo how have business leaders applied these powerful algorithms to date? One application we’re all familiar with is Google Image Search. By understanding what’s contained in images, Google serves up appropriate responses to search queries.\\n\\nAnother example is self-driving cars, which identify and respond to what they “see,” enabling an entirely new industry. Deep learning models have used detailed image analysis in health care to greatly improve disease diagnoses, including diabetic retinopathy and some cancers.\\n\\nAs you can see, companies and researchers have applied image understanding in drastically different ways to overcome various challenges. Thinking about the kind of image data your business possesses or the ways image understanding can aid your operations could help you come up with the next great product or service based on this type of deep learning.\\n\\n2. Sequence prediction\\n\\nAnother breakthrough of deep learning is the ability to understand sequential data, like text (a sequence of characters) or a set of observations over time. Neural network architectures built for these purposes are called recurrent neural nets.\\n\\nIn this scenario, a researcher would train the neural networks to look at huge amounts of past sequences, learn their patterns, and generate future sequences that follow those patterns.\\n\\nWe’ve applied sequence prediction in several domains. One early experiment showed that, by representing handwriting as a series of points with X and Y coordinates, the neural network could learn to produce new handwriting that looked real.\\n\\nImage Credit: University of Toronto Computer Science Department\\n\\nIn the field of time series prediction, here’s one example that may have already improved your commute. Uber found ways to predict user demand by modeling the number of rides its customers take over time as a sequence. Now you know what algorithms to thank (or curse) when you look up how many drivers there are in your area.\\n\\nSequence prediction has proven itself with a number of different applications in business. It’s well worth investigating how you can apply it to yours.\\n\\n3. Language translation\\n\\nMachine translation has long been a dream of AI researchers. Deep learning brought that dream much closer to reality with sequence-to-sequence architecture, which uses recurrent neural networks under the hood.\\n\\nAs you can see from the chart below, this architecture blows other translation techniques out of the water, with the exception (so far) of human translation:\\n\\nImage Credit: Google Research Blog\\n\\nThe goal of sequence-to-sequence is to optimize for language translation. Researchers discovered the technology in 2014 and have continued to improve upon it each year. The technology now powers Google Translate and Apple’s Siri. Startups are also working on using sequence-to-sequence for chatbots. This area has significant promise, but so far seems to work best when we train it on narrowly defined domains, such as customer service for an app.\\n\\nAs these models improve, you’ll no doubt want to keep a close eye on how they could drive innovation in your own field.\\n\\n4. Generative models\\n\\nOur last huge breakthrough achieved with deep learning is the creation of models that generate complex data, like images that look like faces but are not actual faces. This is possible due to architectures called generative adversarial networks, which use convolutional neural nets under the hood.\\n\\nImage Credit: Nvidia\\n\\nGenerative models are perhaps the most intriguing of all four deep learning breakthroughs, though as of now, their applications in business are limited.\\n\\nOne early use of this deep learning breakthrough has been to aid image classification models. These models can learn to understand objects in images much more efficiently if researchers train them to distinguish real images from fake ones that a generative adversarial network generates.\\n\\nAs data scientists refine the uses of this breakthrough, you’ll want to take note of how companies use generative models in new and exciting ways so you can begin applying their power to your own business challenges.\\n\\nA final note\\n\\nEach of the breakthroughs above has many open source implementations. That means you can almost always download a pre-trained model and apply it to your data. For example, you can purchase pre-trained image classifiers that allow you to feed your data through to classify new images. In this case, because the company that sold you the product has done much of the work for you, you don’t need to develop the deep learning to take advantage of these cutting-edge techniques. Rather, you just need to do the development work to get models others have created to work for your problem.\\n\\nNow that you have a better understanding of the capabilities of deep learning models, you’re a bit closer to joining companies like Uber and Google in actually using them. Remember that the next generation of business applications of deep learning is still to come. For example, when Apple introduced the iPhone, nobody was thinking about using it for ride-sharing. Now is the time to discover new ways to apply these techniques to your own data.\\n\\nSeth Weidman is a senior data scientist at Metis, a company that provides full-time immersive bootcamps, evening part-time professional development courses, online learning, and corporate programs to accelerate the careers of data scientists.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('7 examples of how AI and machine learning are changing the arts', 'Data, artificial intelligence, machine learning (ML), and deep learning (DL): These make up the zeitgeist of our current times. We read these words constantly. We also hear they will leave no area or industry untouched. But what about the arts? How have they influenced or aided highly creative areas? Let’s take a look at some ways people have started to pair these technologies with art.\\n\\nArt installations and performances\\n\\nAstrocyte, Philip Beesley Architect Inc.\\n\\nAt Toronto’s 2017 Expo for Design, Innovation, and Technology (EDIT), architects suspended in the air a large, intricate glasswork, inspired by the shape of a nerve cell, in an old, abandoned soap factory. The exhibit was interactive. Participants could walk around and under the art piece, and it changed its light pattern and surround sound in response to participants’ movements, which were usually reverential. The exhibit used artificial intelligence to identify participants’ movements and respond. It used electronic actuators and sensors to respond to viewers’ movement.\\n\\n+/- Human, Wayne McGregor and rAndom International\\n\\nLondon also saw artificial intelligence play a part in a recent 2017 Roundhouse art exhibit. Several white plastic spheres floated freely and elegantly in the air — sometimes moving in a group, sometimes moving individually, but always mesmerizing their grounded visitors below. As people in the space moved or danced, the autonomous orbs dipped, hovered, and lifted intelligently in response, all the while instructed by complex algorithms and motion sensors. The art exhibit extended drone technology to awe and inspire all who visited.\\n\\nMai Hi Ten Yu, Kaiji Moriyama, and Yamaha Corporation\\n\\nIn Tokyo on November 22, 2017, Kaiji Moriyama, a world-famous dancer, played the piano without placing a single finger on its ivories. He mesmerized the audience as he danced around the stage with sensors attached to his back, and it was through these sensors that he was able to manipulate the instrument’s sounds with his body movements. As he danced, the Berlin Philharmonic Orchestra Scharoun Ensemble accompanied him. It’s interesting to consider how this could lead to future fusions of playing music with body movement. Yamaha sees this fusion of body movement and music helping move new art forms forward.\\n\\nPaintings and drawings\\n\\nLive performances and one-time art installations are fascinating, but what about artificial intelligence and machine learning with longer-term art? Here are a couple examples for you in the painting and drawing realm.\\n\\nGoogle Arts & Culture App\\n\\nPerhaps two of the most popular recent applications of artificial intelligence and machine learning in arts in the United States are (1) an application that takes your carefully selected selfie and sifts through hundreds of fine-art pieces to match it to your museum-owned lookalike, and (2) an application that shows you all the fine-art pieces based on a specific color palette, which can be amazing for giving ideas for decorating your house. For the fine-art selfie, not only is it fun to see your selfie side-by-side with a recognized art piece, it’s a helpful way for museums to encourage appreciation for the arts in its visitors. By personalizing an art piece for you, this app works to foster in you the insatiable desire to know more about the subject of the painting, who painted the piece and when, and other contextual details.\\n\\nMicrosoft’s AI bot\\n\\nFor people who appreciate art better than they can create it, there’s a bot that may be able to take the stress off the creation process. Microsoft created an artificial intelligence that draws from a written description of a picture. Do you need a picture of a lemur jumping through the forest for your project? Capture that idea in written text and feed the words to this bot. It’ll draw what you’ve described with impressive artistry. It could be especially helpful for people who need pictures for a low-budget initiative and can’t afford to purchase art for it.\\n\\nPhotography\\n\\nThere have also been several examples of AI in photography. Adobe even recently started using it more, including in its Sensei platform, intended to better cut subjects in photographs out from their backgrounds. But let’s look at more specific examples.\\n\\nPrisma.AI\\n\\nFor people who can’t get enough of Van Gogh or Picasso or certain popular patterns, there’s a way to transform their world with these signature looks. The Prisma app uses artificial intelligence to take a photograph and translate it into one that looks just like a certain artist painted it. It creates some beautiful and interesting new visual displays that are worthy of hanging on a wall. It’s an interesting new way to see and interact with the world, one that can turn the everyday and mundane into interesting vistas of creation.\\n\\nArsenal\\n\\nOne photography tool that will be released soon is Arsenal, a deep-learning photography assistant that will likely revolutionize advanced picture taking, at least for newbies. It easily chooses control settings, like shutter speed, aperture, and ISO, for a professional camera. The preliminary results look impressive, and this assistant will save photographers lots of time.\\n\\nApplying these technological advances to art has and will continue to hatch interesting bursts of artistic creativity. Hopefully, this short list has given you a taste of some diverse and niche use cases in the space.\\n\\nGinette Methot runs operations at Vault Analytics, a company reporting on the latest applications of artificial intelligence. She also cohosts the Data Crunch Podcast.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Here’s what top investors are asking about AI', 'Last week I attended the KeyBanc Capital Markets Emerging Technology Summit. As a participant in their Mosaic industry leaders program, my role at the conference was to participate in one-on-one and group meetings with their institutional investor clients as a subject matter expert on machine learning and artificial intelligence.\\n\\nHaving spent a couple of packed days answering questions, from various angles, about the state of the AI market and those serving it, I thought I’d reflect on some of the key themes that arose from their questions, along with my take on each.\\n\\nThe rise of enterprise machine learning\\n\\nOver the course of the past five years or so, enterprises have worked hard to deploy machine learning. Much of this work began in those parts of an organization with experience applying statistical analyses to core businesses challenges. These groups began experimenting with applying machine learning techniques in a few high-value areas, like upselling/cross-selling, site selection, lead scoring, or churn reduction.\\n\\nIn the past couple of years, building on their early successes, these organizations focused on making ML more easily and broadly used in the enterprise. This involved building or expanding their data science teams and supporting them with end-to-end data pipelines and shared machine learning tools, platforms, and practices.\\n\\nEnter conversational AI\\n\\nMore recently, early adopter enterprises have begun to explore “real AI.” These efforts have evolved very differently and from different parts of the organization. They often begin as C-suite initiatives and find a home under the stewardship of a chief digital officer or a vice president responsible for customer experience. Most commonly, the starting point for these efforts involves applying conversational AI to the challenge of customer sales and service via chatbots, personal assistant devices, or plain old email and telephone.\\n\\nUnlike tech companies, which are also very active in the conversational space but shun pre-built platforms, enterprises tend to use commercially available platforms to support their chatbot and voice assistant efforts. This seems to be a rather fragmented space, with all the large IT players in the mix but also a good many specialty vendors. With most of these projects still in their infancy, the host organizations are learning as they go along and building the tools required to overcome the gaps inherent in today’s commercial offerings.\\n\\nAI and the cloud\\n\\nThis came up in nearly every discussion. In a nutshell, applications and data are quickly moving to the cloud, and thus ML and AI will move to the cloud as well. The big cloud vendors, AWS, Microsoft, Google, and IBM, all have similar offerings at each layer of the data stack. Thus, in these early days, most enterprises are choosing a cloud first based on broad criteria and making do with that cloud’s data stack. This scenario tends to advantage AWS and, to a lesser extent, Microsoft. Highly technical buyers and enterprises making a decision based primarily on the data stack often go Google. IBM shops and those looking for complete solutions often turn to that vendor.\\n\\nGoogle is betting big on Tensorflow, by seeding the market with it and trying to build the best cloud upon which to run it and the models it produces. The company is making a similar bet with Kubernetes. In both cases, the market recognizes the threat and is responding in ways that impede Google’s ability to translate open source success into cloud dominance. Examples include AWS throwing its weight behind the Open Neural Network Exchange project (ONNX), which promises interoperability among deep learning frameworks, and EKS, Amazon’s managed Kubernetes service.\\n\\nAI chips and hardware\\n\\nAnother topic that came up often is the state of the AI acceleration market. Nvidia, with its GPUs, is the clear leader here, and it’s naturally got everyone and their brother gunning for them. The company has created a pretty big moat with CUDA and its ecosystem, and all the deep learning frameworks are built to take advantage of it. My thinking, though, is that the significance of this moat is reduced in a hyperscale (i.e., cloud-first) world. None of the cloud giants want to be beholden to a single source, and they’re all building their own chips (e.g., Google’s TPU) to reduce the dependency on Nvidia and to lower costs and increase performance. Nvidia knows this and has been running like hell to drive up performance/price.\\n\\nBased on the hyperscale assumption, I’m somewhat bearish on the prospects of independent chip vendors like Graphcore, Cerebras, and Wave Computing as long-term, sustainable, independent companies. That said, if they can get their products to market quickly enough and carve out a niche, plenty of interesting exit opportunities remain.\\n\\nIntel remains a wildcard in this space. It owns the broader server and datacenter CPU market, has great enterprise and hyperscale relationships, and generally brings tremendous scale and resources to the fight. It was slow off the starting blocks, though, and faces the classic innovator’s dilemma at every turn. Yet they recognize the threat to their business and are running hard (and acquiring) to catch up. It’s looking to edge out Nvidia with projects like nGraph, which aspires to compile deep neural networks written in any framework to run efficiently on any hardware backend, including current and future CPUs, GPU, and accelerators.\\n\\nWinners and losers\\n\\nEven when it wasn’t directly articulated, underlying all the investor questions was an ultimate desire to suss out the winners and losers in the shift to AI. While I wasn’t dispensing any investment advice, if you buy into an AI-everywhere and cloud-first vision of how this all plays out, there are some clear winners and losers.\\n\\nCloud vendors that can differentiate themselves and execute well enough to retain scale will win big in ML and AI. For startups in the ML/AI tools space, the path to a long-term, sustainable, independent company is a difficult one, as they’re squeezed by both open source and cloud (increasingly funded by the same adversary). But the market is still young, so, again, there’s an opportunity to differentiate, create outsized customer value, and still win.\\n\\nEven bigger potential winners are application providers — for example, SaaS vendors who are early to AI and build it deeply into their products. This will allow them to create outsized value by better serving their customers and, in the process, building superior proprietary data sources, a virtuous cycle indeed.\\n\\nSam Charrington is host of the podcast This Week in Machine Learning & AI (TWiML & AI) and founder of CloudPulse Strategies.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('4 ways AI could help shape the future of medicine', 'At times, progress occurs so quickly that it’s difficult to separate science fiction from real life. Just five decades ago, computers were massive, unwieldy machines running on punch cards and primitive circuits. Today, a single smartphone has more processing power than the computer used on the Apollo missions.\\n\\nAI has benefited greatly from this explosion in computing power and capability. Today, highly complex deep learning algorithms, patterned on the structure of the human brain, can master Go, trade stocks, and even write Harry Potter novels (though admittedly not very good ones). Given this versatility, some fear that deep learning AIs will reshape our economy by force, rendering hundreds, if not thousands, of occupations obsolete. It seems the world will no longer need humans. But is this really the case? No two occupations are alike, and the Great AI Reckoning will not affect all industries (and certainly not all employees) equally.\\n\\nBefore we go any further, it’s important to figure out the automatability of the medical profession. How likely is it that an algorithm could replace a doctor? At some point in the future, will hospitals force us to enter our symptoms into touchscreens and wait for a disembodied electronic voice to give us a diagnosis?\\n\\nThankfully, that doesn’t seem likely. Based on research carried out by Oxford University and NPR, physicians and surgeons only have a 0.4 percent chance of falling victim to automation. Overall, most of the medical professions seem to have a far lower chance of automation than others. Physician assistants, for instance, have an automatability rating of 14.5 percent, while tax preparers have an automatability rating of 98.7 percent.\\n\\nAny automatability study will take a close look at the duties associated with a job. At its core, what sort of tasks will a worker perform? Will they spend their time on routine tasks that a machine can easily break down into steps and replicate, or are they required to negotiate, exercise empathy, and use creativity and lateral thinking?\\n\\nDoctors, needless to say, are the latter. Diagnosing diseases, performing surgery, and prescribing medicine aren’t simply complex, life-threatening tasks — they also require a good deal of empathy. Thus far, computers fail at this crucial requirement (hence Silicon Valley’s fear of AI). Though efforts are underway to teach computers empathy, it’s unclear whether it will ever be possible to build a computer that can understand the depths of human emotion.\\n\\nWhen it comes to AI’s impact on medicine, I think we’ll find the result to be both significant and subtle.\\n\\n1. AIs will catch mistakes\\n\\nEven though computers will never replace flesh-and-blood physicians, artificial intelligence still has a place in medicine as a partner.\\n\\nThink about what is required of a doctor: an understanding of biochemistry, such as new drugs, existing ones, and how such substances interact with individual patients; general information about each patient’s medical history, including whether they have any pre-existing conditions or risk factors that could be exacerbated by surgery or treatment; and a deep knowledge of diseases and conditions, which often evolve incredibly quickly.\\n\\nThe fact of the matter is that these requirements aren’t just conflicting ones, they’re humanly impossible. After all, a human brain consists of about one billion neurons, and each neuron has around 1,000 connections (for a total of one trillion connections). As impressive as this may sound, it’s not much. One brain has several gigabytes of working memory. Anything else is not so easily recalled.\\n\\nFortunately, AIs have no such problem. IBM’s Watson, for instance, can comb through millions of pages of data, read countless medical articles, and far surpass any human doctor in its breadth and scale of knowledge. Even if a doctor may forget that a patient’s unique biology makes them susceptible to a certain drug’s side effects, an AI won’t. And even if an overworked medical resident may miss a clue, an AI won’t.\\n\\n2. AIs can help with rare conditions\\n\\nOn a related note, the powerful network of an AI will revolutionize the treatment of rare diseases. Yes, individual AIs are powerful, intelligent programs. However, when they’re networked together, they are unstoppable. In this configuration, they can draw from each others’ insights, see where one person went wrong, and devise innovative fixes.\\n\\nSuch tools already exist. A Wired feature discusses Modernizing Medicine, an AI-powered database that helps practitioners tap into knowledge from a database of 3,700 providers and over 14 million patient visits. Based on a technology similar to Amazon’s notoriously powerful recommendation engine, Modernizing Medicine mines data, recommends treatments, and, through the power of the network, helps busy doctors tackle an unfamiliar, threatening disease.\\n\\n3. AIs will assist with surgery\\n\\nIt’s no wonder the drama depicted on shows like Grey’s Anatomy is so compelling. Surgery is incredibly complex, requires intense (and intensive) specialist training, and is, quite literally, a matter of life and death. AI could help reduce some of this drama.\\n\\nWhen paired with augmented reality programs, which overlay digital cues and images atop real ones (think Pokemon Go), AI can provide surgeons with real-time information. This ranges from dividing the brain into various regions for neurosurgery to laying MRI scans and other imagery on top of a patient’s body (giving doctors X-ray vision). Complex software will power advanced hardware, giving doctors an extra safety net and providing some peace of mind for patients.\\n\\n4. AIs will predict disease\\n\\nA critical benefit of AI comes from its strength in gathering and analyzing reams of data and drawing conclusions from its analysis. Who is more likely to get cancer? What are the risk factors that make a patient more susceptible to, say, heart attacks as opposed to strokes?\\n\\nGoogle, the king of analytics, has already jumped on the bandwagon. Several years ago, Google created its Baseline study, a comprehensive, ambitious undertaking that involved thousands of volunteers and 100 specialists in different medical fields. As the name suggests, the goal of the study was to establish a sort of baseline for human health from which algorithms and researchers could isolate biological clues that could predispose a person to specific illnesses.\\n\\nToday, the Baseline study has continued under the banner of Verily, a division of Alphabet (Google’s parent company) and is set to expand in both scope and resources. In the near future, it’ll be easy to imagine a time when non-communicable diseases (strokes, cancers, heart attacks) or hereditary conditions are identified from a single visit to the doctor’s office. Not only can patients see their chances of contracting a specific disease, but doctors can also help their patients preempt these conditions with a clear-sighted, long-term plan of action.\\n\\nIt’s important to note that while AI will certainly revolutionize our relationships with medicine, it is far more likely to do so in a subtle, understated way. After all, AI’s most promising changes are related to systems and procedures in the form of back-end interfaces — not in, say, talking screens. Don’t be fooled, however: Even if most of the change occurs off-screen, medical practice will change for the better. Health care will become more accurate, more comprehensive, and cheaper over time, which is welcome news for everyone.\\n\\nEd Sappin is CEO of Sappin Global Strategies (SGS), a strategy and investment firm dedicated to the innovation economy.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('What we actually have to fear from killer robots', 'You’ve probably seen the latest Boston Dynamics video, which shows one of its recent quadruped creations, the SpotMini, opening a door despite being repeatedly accosted by a company employee. Boston Dynamics’ videos are notorious for eliciting both excitement and fear across social media and the internet. Nicholas King’s new parody of the Planet Earth documentary shows herds of SpotMinis taking over the planet. And the most recent season of Netflix’s Black Mirror features a murderous, highly autonomous SpotMini look-alike. So should we be concerned about killer robots taking over?\\n\\nMy take: I don’t expect the robot uprising anytime soon, but there’s plenty for us to worry about here — as a society, a culture, and a species.\\n\\nThe autonomous killer robots we imagine are so fearsome because (a) they’re autonomous, (b) they’re driven to kill, for some reason, and (c) they’re armed. Looking at these, in turn, allows us to isolate the true areas of concern and understand how we can work to head off our fears.\\n\\nAutonomy\\n\\nAutonomy is a layered, even nuanced concept. At the base level, autonomy can refer simply to the ability to get from A to B without human intervention.\\n\\nWhen I see videos like the SpotMini demonstration, I immediately think back to my interview with Aaron Ames, professor of mechanical and civil engineering at Caltech, which focused on intelligent robots. Our discussion inevitably turned to the then-latest Boston Dynamics video which, at the time, featured the Atlas robot performing a backflip. Ames’ take on the level of autonomy involved essentially boiled down to: not much.\\n\\n“It’s a preplanned behavior, so this robot has no knowledge of its environment, in the sense that it’s not observing where those blocks are and in real time adjusting its behavior and learning how to do this behavior,” said Ames. Rather, “they put those obstacles in the memory of the computer, they preplan those behaviors, [and] they do a bunch of experiments until they get the right behavior.”\\n\\nIn other words, there’s still lots of work to do before we see autonomously walking robots able to deftly navigate the real world. According to Ames, not only are we not there yet, but researchers don’t even agree on the right basic approach to get us there. Some, like Pieter Abbeel, advocate an approach based on end-to-end deep learning, while Ames suggests a more integrative approach.\\n\\nSo, for the time being, you can probably evade and outrun the robot, especially on varying terrain, but they’re getting there.\\n\\nAgency\\n\\nStill, autonomy in the sense of locomotion doesn’t quite get at what’s scary about the “autonomous killer robots” scenario. This is more about agency; the idea that the robot can have a beef with a human in the first place.\\n\\nI can think of a few scenarios in which a robot would have it in for a human:\\n\\nThe robot is acting under its own volition, and it has decided that a particular offending individual needs to go. This implies some degree of general intelligence, goal-directedness, and intrinsic motivation. We’re very far from achieving this type of artificial intelligence, and don’t even really know how to define it. My interview with Greg Brockman explores this topic in detail. It seems quite premature to worry about this.\\n\\nThe robot kills a person as an unintended consequence of some human instruction that it’s following. This strikes me as more worthy of concern, especially given the general difficulty we have as humans with anticipating unintended consequences. And it’s what makes AI safety research so important. Check out my interview with Greg’s colleague Dario Amodei to hear about OpenAI’s work in this space.\\n\\nThe robot kills a person as an intended consequence of some human tasking. This is really the most likely near-term scenario, and thus one we should be most concerned about. Essentially the robot is a weapon, and its autonomy acts both as a potential multiplier on the amount of damage it can do and also, critically, so as to decouple any human from the ultimate taking of life.\\n\\nI think it stands to reason that for the foreseeable future, the humans, as opposed to the robots, are the greater concern.\\n\\nArmed\\n\\nAccidents aside, what makes a killer robot a killer robot comes down to the fact that they were armed in the first place.\\n\\nThis seems like an inevitability that we’re quickly racing toward. Military robots already exist, and many governments are researching and developing more of them. According to Statista, global spending on military robotics was $6.9 billion dollars in 2015, and is expected to grow to $15 billion by 2025.\\n\\nIf you agree that the real risk of autonomous killer robots is when humans use them as weapons, and not because they’ve become self-aware, it seems natural that our best defense is to stop arming them.\\n\\nIt turns out that roboticists, ethicists, and AI researchers are already calling for the banning of weaponized autonomous robots. A number of organizations have formed around or taken up this cause, including the Human Rights Watch, the International Committee for Robot Arms Control, Article 36, and the Future of Life Institute, backed by Stephen Hawking, Elon Musk, and more.\\n\\nSo, back to our original question: Should we fear autonomous killer robots? To be honest, probably not. The chances that anyone reading this will perish at the hands of an autonomous killer robot is really, really, really small. But that doesn’t mean we shouldn’t be thinking about them and the many moral issues that they raise.\\n\\nThis story originally appeared in the This Week in Machine Learning & AI newsletter. Copyright 2018.\\n\\nSam Charrington is host of the podcast This Week in Machine Learning & AI (TWiML & AI) and founder of CloudPulse Strategies.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('How AI could help improve the education enrollment process', 'Education is one of the domains where artificial intelligence is showing great promise. But while most efforts have focused on augmenting and enhancing the learning experience, challenges remain in the registration and enrollment domains. Students and learners often struggle to find their way to the right courses and classes, whether because of the confusing variety of courses available on online platforms or the complicated processes for enrolling at higher education institutions.\\n\\nA handful of startups are poised to solve these challenges through AI tools that provide personalized assistance to students during the registration process. So far, their efforts show positive results.\\n\\nHelping students find the right opportunities\\n\\nOnline learning hub Udacity made its debut in 2012 when Stanford professors Sebastian Thrun and Peter Norvig decided to make their course “Introduction to Artificial Intelligence” available online.\\n\\nThe platform has since grown to offer dozens of courses and nanodegree programs in various computer science fields. The expansion has helped Udacity serve an increasing number of people. More than 160,000 students worldwide enroll for online courses on Udacity every year. However, the expansion of courses has also introduced new challenges.\\n\\nNew and returning students have to navigate the platform’s vast catalog of courses to find the right fit for their skills and experience. Without consultation and assistance, the process can be challenging and result in course abandonment and lower enrollment rates. Meanwhile, providing personalized counseling to thousands of users requires human resources that would go beyond the company’s capacity and budget.\\n\\nTo meet this challenge, Udacity recently teamed up with Passage AI, a startup that specializes in conversational interfaces. The result of the partnership was an AI-powered chatbot that helps students visiting Udacity’s website to find appropriate courses and enroll with ease.\\n\\n“Our students are looking for guidance and quick responses to questions while browsing our course list,” says Sam Mazaheri, director of digital marketing at Udacity. “It is important for them to find classes that fit their needs and experience level. A chatbot can provide immediate responses to common questions, giving prospective students confidence that they’re enrolling in the right program at Udacity.”\\n\\nPassage AI’s chatbot uses natural language processing (NLP), a branch of artificial intelligence that enables software to analyze and respond to the meaning behind human-generated text. NLP helps chatbots move away from rigid command forms and enable their users to interact with them in a comfortable, conversational fashion. This provides students with an experience similar to what they would get from talking to a professor or counselor instead of manually finding their way around the courses.\\n\\nThe chatbot was initially tested with 5 percent of Udacity’s visitors and eventually rolled out to the entire audience. The results, according to Passage AI CEO Ravi Raj, was a 40 percent increase in click-through rates, indicating a decrease in confusion and more students finding the course they’re looking for.\\n\\n“We see AI-powered chatbots guiding students to find the right course or nanodegree on a site like Udacity or answering frequently asked questions like how much a course costs or how long it would take to complete it,” Raj says. He foresees chatbots eventually helping with career planning and counseling by having an ongoing dialog with students and recommending the right courses of action to accomplish their goals.\\n\\nProviding individual assistance\\n\\nThe process of applying to college involves several challenging tasks, such as completing financial aid applications, submitting a final high school transcript, obtaining immunizations, accepting student loans, and paying tuition.\\n\\nThe process is especially difficult for students who come from low-income backgrounds or who don’t have a family member who can guide them through the process. Not providing support to students results in many of them failing to matriculate. Meanwhile, every student is faced with their own unique set of challenges, which means generic outreach tools may prove inefficient and cause further confusion.\\n\\nAn alternative would be to provide individual counselor outreach, but it would not scale as the number of students increases. An interesting case study in this regard is Georgia State University, which partnered with AI-powered messaging platform AdmitHub to provide assistance to students in their transition to higher education.\\n\\nGSU used AdmitHub’s conversational AI system Pounce to automatically reach out to would-be college freshmen based on their individual pain points. Pounce analyzes the university’s data on each student’s progress in accomplishing pre-matriculation tasks to understand where they might be struggling and where they need help. Using this information, it generates personalized text messages to only reach out to students who are in need of assistance instead of sending out generic messages to everyone.\\n\\nPounce also uses NLP/NLG technology to provide smooth conversational assistance on general inquiries around the clock, providing assistance at scale. As with all technology that involves deep learning algorithms, Pounce grows smarter as it interacts more and more with students.\\n\\nWhat makes Pounce especially interesting is its proactive working model. Instead of requiring students to reach out for guidance, Pounce makes the outreach based on the data it has.\\n\\nAn experimental study found that students who received Pounce outreach completed their required pre-matriculation tasks and enrolled on time at significantly higher rates than those who received the standard assistance. As a result, GSU observed a 21 percent decline in summer melt, the phenomenon of prospective college students’ motivation to attend higher education “melting” away during the summer between the end of high school and the beginning of college.\\n\\nAI has already proven that it can be a handy tool to provide quality, personalized education. Now, with the help of these and other similar projects, it will assume an active role in putting more students in classes.\\n\\nBen Dickson is a software engineer and the founder of TechTalks, a blog that explores the ways technology is solving and creating problems.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Milwaukee’s startup community is growing with help from local and national organizations', 'Historically a manufacturing hub, Wisconsin is known for producing iconic products such as tractors, motorcycles, and, of course, beer, but its economy has significantly changed in recent years. In Milwaukee, Wisconsin’s largest city, manufacturing has ceased to dominate the local economy. Instead, businesses in the service sector, as well as startups, now employ most workers.\\n\\nWhile there has been a lot of recent growth in the Milwaukee startup scene, the startup community has historically faced challenges. Many have reported a lack of support resources available to local startups, including a lack of capital. This has made it difficult for startups to stay and grow their businesses in the greater Milwaukee area. Recent reports have also cited that millennial population growth in Milwaukee is lower than in other major cities around the U.S., posing a challenge for attracting and retaining young talent in the area.\\n\\nNow, Milwaukee organizations are working to address these challenges by finding ways to support a united source of capital, resources for entrepreneurs and startups, and other efforts to make Milwaukee a vibrant tech hub for 21st century businesses and talent.\\n\\nFostering a statewide tech workforce\\n\\nSeveral universities in the state, including the University of Wisconsin system — a network of 13 universities — have faced some challenges. Enrollment at some of the campuses has decreased over the past two years. But universities are responding by pioneering new ways of learning, developing interdisciplinary programs in fields such as entrepreneurship, and collaborating closely with industry.\\n\\nThe Milwaukee School of Engineering recently announced a $34 million grant to build a dedicated computer science facility focused on artificial intelligence, deep learning, cybersecurity, robotics, cloud computing, and other next-generation technologies. The grant will prepare undergraduates for jobs in these emerging technologies. Engineering, business, and nursing students will also have the opportunity to take these courses, putting them on the cutting edge of advancing technologies in their fields.\\n\\nPromise also lies in Wisconsin’s Technical College System. According to an Outcomes Report for 2016, 93 percent of graduates choose to stay in Wisconsin, indicating that they see opportunity in the state thanks to a lower cost of living and more job opportunities.\\n\\nMaking Milwaukee a tech hub\\n\\nEconomic factors, such as low business costs compared to places like Silicon Valley, and the growing availability of tech talent make Milwaukee an attractive place for early-stage companies and entrepreneurs seeking investments. In 2017, we saw many cases of both Milwaukee and Wisconsin-based companies investing in startups in their own backyard, as well as outside investors looking to the Heartland for new investments.\\n\\nExamples of this include household names like the Green Bay Packers. In October 2017, the Packers and Microsoft announced that they are each investing $5 million in Wisconsin startups. The partnership, dubbed TitletownTech, will create a new building in Titletown District in Ashwaubenon, Wisconsin, located two hours north of Milwaukee, that will house a VC fund, accelerator, and labs for technology development. Elsewhere, Aurora Health Care established a $5 million InvestMKE fund, and Northwestern Mutual started Cream City Venture Capital, a $5 million early stage fund for Milwaukee startups.\\n\\nThese established companies have helped aid recent organic growth in the Milwaukee startup community, giving small, emerging companies the means to grow their businesses.\\n\\nMilwaukee and the surrounding areas have also drawn interest from Silicon Valley investors. This past fall, three Milwaukee startups pitched at the Green Bay Rise of the Rest competition, a VC bus tour led by cofounder of America Online Steve Case and JD Vance, author of Hillbilly Elegy. Rise of the Rest is now a $150 million fund with additional backing from other tech giants like Jeff Bezos. The fund will continue to invest in companies throughout the Midwest, which means more potential opportunities for Milwaukee-based startups. Heavy hitters at a national level investing locally in Wisconsin provides an opportunity for startups across the state — and the Midwest region as a whole — that need seed money.\\n\\nGrowing together\\n\\nWhile Milwaukee’s startup scene is still young, some of its most notable organizations now have several years’ worth of experience. Ward4, a Milwaukee coworking space, is home to tech startups like Scanalytics, a fast-growing Internet of Things company, and Gener8tor, a well-known Wisconsin startup accelerator that now has a program in Minnesota and soon-to-be-opened programs in Cincinnati and Indianapolis. BREW Accelerator, a Milwaukee-based accelerator program dedicated to water-focused technology startups, has trained dozens of startups since it was founded in 2013.\\n\\nAdditionally, organizations have become increasingly aware of the need to not just provide startups with the resources to launch, but to scale as well. Scale Up Milwaukee, an initiative of the Greater Milwaukee Committee, has recently launched Sparc, a new training program for Milwaukee-based startups. Gener8tor has hosted a conference series since 2013 called OnRamp (which Northwestern Mutual has participated in) that brings together investors and startups to discuss potential partnerships.\\n\\nMilwaukee has a strong state talent pool to draw from and the entrepreneurial spirit necessary to become a premier ecosystem for startups. With increased support from more established companies, as well as local and national investors, we can help the local startup community grow and thrive at a quicker pace.\\n\\nTim Schaefer is an executive vice president at Northwestern Mutual.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Inside China’s growing use of facial recognition technology', 'As China’s Lunar New Year approaches, citizens and tourists will spot the country’s police force with facial recognition glasses. These glasses will help officials use real-time ID verification to fight crimes during the celebrations around this year’s event. But interestingly, such facial recognition technologies are not new to the people of China.\\n\\nIn the past, the country has relied heavily on cardless identification. The “Smile to Pay” feature, launched by Alibaba’s Ant Financial affiliate, lets users take a grinning selfie to authenticate a digital payment. Facial recognition in China also allows students to enter university halls, travelers to board planes, and employees to enter office premises with ease and no ID cards.\\n\\nIn 2015, the Chinese Ministry of Public Security announced it was looking to implement an “omnipresent, completely connected, always on and fully controllable” network using facial recognition systems and CCTV hardware. So it is not surprising that private firms and facial recognition startups in China are actively partnering with the government to monitor fraudulent and criminal behavior.\\n\\nFacial recognition powerhouses\\n\\nHere are some of the firms in China that have already introduced facial recognition technology to keep a watchful eye on the country’s citizens.\\n\\nDahua Technology\\n\\nIn 2016, Dahua Technology set a new record for labeled faces in the wild (LFW) facial recognition, beating previous records from Baidu, Tencent, and Google. Consisting of a network with over one hundred layers, Dahua’s facial recognition system is currently the deepest network among facial recognition systems. This allows a new type of metric learning method, which can detect suspects in the crowds of Beijing and even monitor pedestrians at traffic lights in the Fujian Province. Dahua says its technology helped police arrest a number of fugitives during the G20 summit.\\n\\nHikvision\\n\\nChina’s Hangzhou-based Hikvision Digital Technology claims to be the world’s largest maker of surveillance technology. The company weaves facial image modeling and a similarity calculation into the system to enable facial recognition capabilities. Users apply this solution to blacklist criminals and prevent admission of known offenders to venues like sports stadiums, parks, and casinos.\\n\\nReportedly, Hikvision facial tech has helped capture five criminal suspects picked out from a database of 973,661 facial images.\\n\\nFace++\\n\\nRanked 11 out of 50 on MIT’s list of the world’s smartest companies, Face++, which may be the first facial-recognition AI startup, sits on a whopping valuation of $1 billion.\\n\\nFace++’s software tracks faces using 83 different data points. The technology already powers popular applications like Alipay and Didi Chuxing. For example, the software enables Didi passengers to confirm that the person behind the wheel is a valid driver.\\n\\nSenseTime\\n\\nSenseTime has used facial recognition to help the police department nab 69 suspects in one month in southwest China. The company has a growing business relationship with the government and large data sets of a vast population, which has raised concerns about privacy. Among many other things, the company’s technology can trace the route of targets in a billion-scaled database of face image capture records and has a flexible surveillance for different times, places, and targets.\\n\\nStrengths of facial recognition tech\\n\\nAccurate facial recognition algorithms are based on deep learning and require a large dataset to train the system. In 2017, China built a giant facial recognition database that identifies citizens in its population of 1.3 billion within seconds, with the goal of achieving 90 percent accuracy.\\n\\nThe huge database and the number of partnerships the country has have helped grow China’s domination of the physical security equipment market. According to IHS Markit’s Physical Security Equipment & Services Report, the market in China will account for 38 percent of the global market by 2021. That is larger than the North American and Western European markets combined.\\n\\nLimitations of facial recognition tech\\n\\nEven though facial recognition allows China to fight crime and thus contributes to public safety, it comes with a long list of limitations. For one, it raises the issue of human rights. Human Rights Watch raised objections when iris scans and fingerprints were made compulsory for the residents of Xin Jiang. And some fear the government will use the information to “clamp down” on petitioners and human rights activists.\\n\\nOn top of this, the heavy reliance on facial recognition could lead to data hacks that leak loads of sensitive information, resulting in a security and privacy breach. Moreover, a slight inaccuracy in face detection could lead to a wrongful arrest.\\n\\nAnother concern about China’s reliance on facial recognition is that the technology could discriminate against people. The South China Morning Post recently reported that some restaurants employing facial recognition offered discounts based on a machine ranking of the customer’s looks. In other words, customers with “beautiful” features would get better scores and cheaper meals than those with noses the machine deemed too big or too small.\\n\\nWithout a doubt, facial recognition is transforming the way China does business and fights crime. However, researchers must continue to strive for 100 percent accuracy to minimize the negative effects and maximize the positive impacts.\\n\\nDeena Zaidi is a Seattle-based contributor for financial websites like TheStreet, Seeking Alpha, Truthout, Economy Watch, and icrunchdata.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Why AI has yet to solidify its role in health care', 'If you’re searching for an industry that’s defined by innovation and technological progression, look no further than health care. Despite heavy regulations and high barriers to entry, this sector continues to see major breakthroughs on a regular basis.\\n\\nSo is the health care industry ready for a breakthrough when it comes to artificial intelligence? The building blocks are in place and opportunities certainly exist, but there are still significant challenges to overcome.\\n\\nRise of AI\\n\\n“I have no doubt that sophisticated learning and AI algorithms will find a place in health care over the coming years,” says Andy Schuetz, a senior data scientist at Sutter Health. “I don’t know if it’s two years or 10 — but it’s coming.”\\n\\nIndeed, the innovation happening behind closed doors is astounding. However, technological, psychological, and regulatory limitations are holding the industry back.\\n\\nAs we consider the state of AI in health care (and when the next breakthrough will occur), it’s helpful to identify some of the key opportunities and challenges in the present environment.\\n\\nSignificant technological limitations\\n\\nThe term “artificial intelligence” is often used to refer to a variety of machine learning methods. In order for AI to really make a significant difference, it’s important that we reach artificial narrow intelligence (ANI).\\n\\nANI does more than complete basic tasks, it can actually defeat humans in fairly complex scenarios. IBM Watson’s Jeopardy victory is a great example of this. When researchers can realistically introduce ANI into health care, the pieces will start to fall into place.\\n\\nConvenience and savings will drive innovation\\n\\nFor AI to truly transform health care, innovation needs to be centered on two key factors: cost and convenience. Both doctors and patients want to see costs go down and convenience go up. Technologists and companies that can provide both will thrive.\\n\\niCliniq is a good example of a company that’s using AI to lower costs and increase convenience in the health care space. Labeled a “virtual hospital,” iCliniq gives users access to doctors, medical advice, and second opinions from licensed health care professionals all over the world. It uses AI to help doctors answer patients’ queries faster, which brings down the cost of consultations and makes health care more affordable and readily available.\\n\\nAs more entrepreneurs and tech companies focus on cost and convenience, we’ll see greater acceptance in the marketplace. In turn, this will open new doors and increase pressure on those in leadership positions to make room for AI.\\n\\nMoney isn’t the real problem\\n\\nWhile there are certainly smaller companies and less-connected innovators who are having trouble funding their ideas, a lack of money isn’t AI’s biggest problem. Tech giants like IBM, Alphabet, Philips, and a variety of pharmaceutical companies are pouring billions of dollars into startups and products. According to estimates from Frost & Sullivan, the market for AI in health care and life sciences is expected to grow by 40 percent per year to reach $6.6 billion by 2021.\\n\\nEarly applications emphasize diagnoses\\n\\nAI can go in dozens of different directions, but when you look at the present landscape, a few trends stand out.\\n\\n“To date, the sweet spot in health care AI has been pairing algorithms with structured exercises in reading patient data and medical images to train machines to detect abnormalities. This training is called ‘deep learning’,” health care consultant Brian Scogland explains. “In the same way, algorithms are being used to sift through vast amounts of medical literature to inform treatment decisions where it would be too onerous a task to have a human read through the same journals.”\\n\\nMedyMatch is a great example of a company that’s finding success in this space. The company’s goal is to “[Bring] accuracy to physicians and capacity to health care to prevent chronic conditions and improve patient outcomes with the right treatment at the right time.”\\n\\nIt does this by creating AI-driven diagnostic tools that leverage 3D imaging, patient-specific data, and machine learning to deliver precise advice that medical teams can use to improve care.\\n\\nA future in which AI and health care work together to provide exceptional, reliable, and cost-effective care isn’t far off. Competition in this space is heated as companies race to see who can deliver the most accurate and consistent results first. As soon as the industry removes some of the major challenges and roadblocks, growth will quickly follow.\\n\\nLarry Alton is a contributing writer at VentureBeat covering artificial intelligence.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('How you can train AI to convert design mockups into HTML and CSS', 'The field took off last year when Tony Beltramelli introduced the pix2code paper and Airbnb launched sketch2code .\\n\\nWithin three years, deep learning will change front-end development. It will increase prototyping speed and lower the barrier for building software.\\n\\nCurrently, the largest barrier to automating front-end development is computing power. However, we can use current deep learning algorithms, along with synthesized training data, to start exploring artificial front-end automation right now.\\n\\nIn this post, we’ll teach a neural network how to code a basic HTML and CSS website based on a picture of a design mockup using deep-learning platform FloydHub. Here’s a quick overview of the process:\\n\\n1) Give a design image to the trained neural network\\n\\n2) The neural network converts the image into HTML markup\\n\\n3) Rendered output\\n\\nWe’ll build the neural network in three iterations.\\n\\nFirst, we’ll make a bare minimum version to get a hang of the moving parts. The second version, HTML, will focus on automating all the steps and explaining the neural network layers. In the final version, Bootstrap, we’ll create a model that can generalize and explore the LSTM layer.\\n\\nAll the code is prepared on Github and FloydHub in Jupyter notebooks. All the FloydHub notebooks are inside the floydhub directory and the local equivalents are under local .\\n\\nThe models are based on Beltramelli‘s pix2code paper and Jason Brownlee’s image caption tutorials. The code is written in Python and Keras, a framework on top of TensorFlow.\\n\\nIf you’re new to deep learning, I’d recommend getting a feel for Python, backpropagation, and convolutional neural networks. My three earlier posts on FloydHub’s blog will get you started:\\n\\nCore Logic\\n\\nLet’s recap our goal. We want to build a neural network that will generate HTML/CSS markup that corresponds to a screenshot.\\n\\nWhen you train the neural network, you give it several screenshots with matching HTML.\\n\\nIt learns by predicting all the matching HTML markup tags one by one. When it predicts the next markup tag, it receives the screenshot as well as all the correct markup tags until that point.\\n\\nHere is a simple training data example in a Google Sheet.\\n\\nCreating a model that predicts word by word is the most common approach today. There are other approaches, but that’s the method we’ll use throughout this tutorial.\\n\\nNotice that for each prediction it gets the same screenshot. So if it has to predict 20 words, it will get the same design mockup twenty times. For now, don’t worry about how the neural network works. Focus on grasping the input and output of the neural network.\\n\\nLet’s focus on the previous markup. Say we train the network to predict the sentence “I can code.” When it receives “I,” then it predicts “can.” Next time it will receive “I can” and predict “code.” It receives all the previous words and only has to predict the next word.\\n\\nThe neural network creates features from the data. The network builds features to link the input data with the output data. It has to create representations to understand what is in each screenshot, the HTML syntax, that it has predicted. This builds the knowledge to predict the next tag.\\n\\nWhen you want to use the trained model for real-world usage, it’s similar to when you train the model. The text is generated one by one with the same screenshot each time. Instead of feeding it with the correct HTML tags, it receives the markup it has generated so far. Then, it predicts the next markup tag. The prediction is initiated with a “start tag” and stops when it predicts an “end tag” or reaches a max limit. Here’s another example in a Google Sheet.\\n\\n“Hello World” Version\\n\\nLet’s build a “hello world” version. We’ll feed a neural network a screenshot with a website displaying “Hello World!” and teach it to generate the markup.\\n\\nFirst, the neural network maps the design mockup into a list of pixel values. From 0–255 in three channels — red, blue, and green.\\n\\nTo represent the markup in a way that the neural network understands, I use one hot encoding. Thus, the sentence “I can code” could be mapped like the below.\\n\\nIn the above graphic, we include the start and end tag. These tags are cues for when the network starts its predictions and when to stop.\\n\\nFor the input data, we will use sentences, starting with the first word and then adding each word one by one. The output data is always one word.\\n\\nSentences follow the same logic as words. They also need the same input length. Instead of being capped by the vocabulary, they are bound by maximum sentence length. If it’s shorter than the maximum length, you fill it up with empty words, a word with just zeros.\\n\\nAs you see, words are printed from right to left. This forces each word to change position for each training round. This allows the model to learn the sequence instead of memorizing the position of each word.\\n\\nIn the below graphic there are four predictions. Each row is one prediction. To the left are the images represented in their three color channels: red, green and blue and the previous words. Outside of the brackets are the predictions one by one, ending with a red square to mark the end.\\n\\ngreen blocks = start tokens | red block = end token\\n\\nHere is a link to the Jupyter notebook with the Hello World code snippet.\\n\\nIn the hello world version, we use three tokens: start , <HTML><center><H1>Hello World!</H1></center></HTML> and end . A token can be anything. It can be a character, word, or sentence. Character versions require a smaller vocabulary but constrain the neural network. Word level tokens tend to perform best.\\n\\nOutput\\n\\n10 epochs: start start start\\n\\n100 epochs: start <HTML><center><H1>Hello World!</H1></center></HTML> <HTML><center><H1>Hello World!</H1></center></HTML>\\n\\n300 epochs: start <HTML><center><H1>Hello World!</H1></center></HTML> end\\n\\nMistakes I made:\\n\\nBuild the first working version before gathering the data. Early on in this project, I managed to get a copy of an old archive of the Geocities hosting website. It had 38 million websites. Blinded by the potential, I ignored the huge workload that would be required to reduce the 100K-sized vocabulary.\\n\\nEarly on in this project, I managed to get a copy of an old archive of the Geocities hosting website. It had 38 million websites. Blinded by the potential, I ignored the huge workload that would be required to reduce the 100K-sized vocabulary. Dealing with a terabyte worth of data requires good hardware or a lot of patience. After having my mac run into several problems I ended up using a powerful remote server. Expect to rent a rig with 8 modern CPU cores and a 1GPS internet connection to have a decent workflow.\\n\\nAfter having my mac run into several problems I ended up using a powerful remote server. Expect to rent a rig with 8 modern CPU cores and a 1GPS internet connection to have a decent workflow. Nothing made sense until I understood the input and output data. The input, X, is one screenshot and the previous markup tags. The output, Y, is the next markup tag. When I got this, it became easier to understand everything between them. It also became easier to experiment with different architectures.\\n\\nThe input, X, is one screenshot and the previous markup tags. The output, Y, is the next markup tag. When I got this, it became easier to understand everything between them. It also became easier to experiment with different architectures. Be aware of the rabbit holes. Because this project intersects with a lot of fields in deep learning, I got stuck in plenty of rabbit holes along the way. I spent a week programming RNNs from scratch, got too fascinated by embedding vector spaces, and was seduced by exotic implementations.\\n\\nBecause this project intersects with a lot of fields in deep learning, I got stuck in plenty of rabbit holes along the way. I spent a week programming RNNs from scratch, got too fascinated by embedding vector spaces, and was seduced by exotic implementations. Picture-to-code networks are image caption models in disguise. Even when I learned this, I still ignored many of the image caption papers, simply because they were less cool. Once I got some perspective, I accelerated my learning of the problem space.\\n\\nRunning the code on FloydHub\\n\\nFloydHub is a training platform for deep learning. I came across them when I first started learning deep learning and I’ve used them since for training and managing my deep learning experiments. You can install it and run your first model within 10 minutes. It’s hands down the best option to run models on cloud GPUs.\\n\\nIf you are new to FloydHub, do their 2-min installation or my 5-minute walkthrough.\\n\\nClone the repository\\n\\ngit clone https://github.com/emilwallner/Screenshot-to-code-in-Keras.git\\n\\nLogin and initiate FloydHub command-line-tool\\n\\ncd Screenshot-to-code-in-Keras floyd login floyd init s2c\\n\\nRun a Jupyter notebook on a FloydHub cloud GPU machine:\\n\\nfloyd run --gpu --env tensorflow-1.4 --data emilwallner/datasets/imagetocode/2:data --mode jupyter\\n\\nAll the notebooks are prepared inside the FloydHub directory. The local equivalents are under local. Once it’s running, you can find the first notebook here: floydhub/Helloworld/helloworld.ipynb .\\n\\nIf you want more detailed instructions and an explanation for the flags, check my earlier post.\\n\\nHTML Version\\n\\nIn this version, we’ll automate many of the steps from the Hello World model. This section will focus on creating a scalable implementation and the moving pieces in the neural network.\\n\\nThis version will not be able to predict HTML from random websites, but it’s still a great setup to explore the dynamics of the problem.\\n\\nOverview\\n\\nIf we expand the components of the previous graphic it looks like this.\\n\\nThere are two major sections. First, the encoder. This is where we create image features and previous markup features. Features are the building blocks that the network creates to connect the design mockups with the markup. At the end of the encoder, we glue the image features to each word in the previous markup.\\n\\nThe decoder then takes the combined design and markup feature and creates a next tag feature. This feature is run through a fully connected neural network to predict the next tag.\\n\\nDesign mockup features\\n\\nSince we need to insert one screenshot for each word, this becomes a bottleneck when training the network (example). Instead of using the images, we extract the information we need to generate the markup.\\n\\nThe information is encoded into image features. This is done by using an already pre-trained convolutional neural network (CNN). The model is pre-trained on Imagenet.\\n\\nWe extract the features from the layer before the final classification.\\n\\nWe end up with 1536 eight by eight-pixel images known as features. Although they are hard to understand for us, a neural network can extract the objects and position of the elements from these features.\\n\\nMarkup features\\n\\nIn the hello world version, we used a one-hot encoding to represent the markup. In this version, we’ll use a word embedding for the input and keep the one-hot encoding for the output.\\n\\nThe way we structure each sentence stays the same, but how we map each token is changed. One-hot encoding treats each word as an isolated unit. Instead, we convert each word in the input data to lists of digits. These represent the relationship between the markup tags.\\n\\nThe dimension of this word embedding is eight but often varies between 50–500 depending on the size of the vocabulary.\\n\\nThe eight digits for each word are weights similar to a vanilla neural network. They are tuned to map how the words relate to each other (Mikolov et al., 2013).\\n\\nThis is how we start developing markup features. Features are what the neural network develops to link the input data with the output data. For now, don’t worry about what they are, we’ll dig deeper into this in the next section.\\n\\nThe Encoder\\n\\nWe’ll take the word embeddings and run them through an LSTM and return a sequence of markup features. These are run through a Time distributed dense layer — think of it as a dense layer with multiple inputs and outputs.\\n\\nIn parallel, the image features are first flattened. Regardless of how the digits were structured, they are transformed into one large list of numbers. Then we apply a dense layer on this layer to form a high-level feature. These image features are then concatenated to the markup features.\\n\\nThis can be hard to wrap your mind around — so let’s break it down.\\n\\nMarkup features\\n\\nHere we run the word embeddings through the LSTM layer. In this graphic, all the sentences are padded to reach the maximum size of three tokens.\\n\\nTo mix signals and find higher-level patterns, we apply a TimeDistributed dense layer to the markup features. TimeDistributed dense is the same as a dense layer, but with multiple inputs and outputs.\\n\\nImage features\\n\\nIn parallel, we prepare the images. We take all the mini image features and transform them into one long list. The information is not changed, just reorganized.\\n\\nAgain, to mix signals and extract higher-level notions, we apply a dense layer. Since we are only dealing with one input value, we can use a normal dense layer. To connect the image features to the markup features, we copy the image features.\\n\\nIn this case, we have three markup features. Thus, we end up with an equal amount of image features and markup features.\\n\\nConcatenating the image and markup features\\n\\nAll the sentences are padded to create three markup features. Since we have prepared the image features, we can now add one image feature for each markup feature.\\n\\nAfter sticking one image feature to each markup feature, we end up with three image-markup features. This is the input we feed into the decoder.\\n\\nThe Decoder\\n\\nHere we use the combined image-markup features to predict the next tag.\\n\\nIn the below example, we use three image-markup feature pairs and output one next tag feature.\\n\\nNote that the LSTM layer has the sequence set to false. Instead of returning the length of the input sequence, it only predicts one feature. In our case, it’s a feature for the next tag. It contains the information for the final prediction.\\n\\nThe final prediction\\n\\nThe dense layer works like a traditional feedforward neural network. It connects the 512 digits in the next tag feature with the 4 final predictions. Say we have 4 words in our vocabulary: start, hello, world, and end.\\n\\nThe vocabulary prediction could be [0.1, 0.1, 0.1, 0.7]. The softmax activation in the dense layer distributes a probability from 0–1, with the sum of all predictions equal to 1. In this case, it predicts that the 4th word is the next tag. Then you translate the one-hot encoding [0, 0, 0, 1] into the mapped value, say “end”.\\n\\nHere is a link to the Jupyter notebook with the HTML code snippet.\\n\\nOutput')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('IBM’s new AI toolbox puts your deep learning network to the test', 'IBM today announced the launch of its Adversarial Robustness Toolbox for AI developers. The open-source kit contains everything a machine learning programmer needs to attack their own deep learning neural networks (DNN) to ensure they’re able to withstand real-world conditions.\\n\\nThe toolbox comes in the form of a code library which includes attack agents, defense utilities, and benchmarking tools that allow developers to integrate baked-in resilience to adversarial attacks. The company says it’s the first of its kind.\\n\\nAccording to IBM Security Systems CTO Sridhar Muppidi:\\n\\nOne of the biggest challenges with some of the existing models to defend against adversarial AI is they are very platform specific. The IBM team designed their Adversarial Robustness Toolbox to be platform agnostic. Whether you’re coding/developing in Keras or TensorFlow, you can apply the same library to build defenses in.\\n\\nIt’s like a mixed martial arts trainer for AI that assesses a DNN’s resilience, teaches it customized defense techniques, and provides a sort of internal anti-virus layer. That last one might not be standard practice in boxing gyms, but it’s absolutely crucial to DNNs.\\n\\nAdversarial attacks are perpetrated against DNNs by bad actors hoping to disrupt, re-purpose, or deceive an AI. They’re carried out in a number of ways ranging from physical obfuscation to counter-AI in the form of machine learning attacks against a DNN.\\n\\nIf the idea that AI has to defend itself against an opponent capable of learning isn’t scary enough, the potential for danger to humans is absolutely terrifying.\\n\\nCredit: IBM\\n\\nIn China, facial recognition software is an integral part of the country’s law enforcement tech, including AI-equipped CCTV cameras capable of picking out a single face in a crowd of more than 60,000 people. The western world is likely to follow suit as AI becomes more capable.\\n\\nTNW reported earlier this year on the speech system vulnerability pictured in the above image, explaining that fooling speech-to-text systems meant bad news for voice assistants. Hackers don’t necessarily have to rely on you selecting a song from your favorite playlist, they could simply sit across from you on public transportation, or in an office, and pretend to listen to a track themselves, or just play silence with the offending signals embedded.\\n\\nCredit: IBM\\n\\nThese threats also include spoofing GPS to misdirect vessels, hacking shipboard systems, and disguising vessel IDs in order to fool AI-powered satellites. As more global AI systems come online, the potential for state-sponsored actions against military vessels is also becoming an increasing reality. Last year US Navy leaders found themselves answering questions about a series of mysterious collisions at sea, and the idea of adversarial systems attacks came up more than once.\\n\\nOther areas where AI systems are particularly vulnerable include driverless cars and military drones, both of which could be weaponized by hackers if their security were compromised. Realistically, all DNNs need to be resilient to attack or they’re about as useful as a computer without antivirus protection.\\n\\nFor more information on IBM’s new Adversarial Robustness Toolbox you can check out the company’s blog post.\\n\\nThe Next Web’s 2018 conference is just a few weeks away, and it’ll be 💥💥. Find out all about our tracks here.\\n\\nRead next: Report: Social media use is increasing despite privacy fears')\n",
      "('IBM rolls out deep learning as a service for AI developers', 'IBM today announced the launch of its new Deep Learning as a Service (DLaaS) program for AI developers. With DlaaS, users can train neural networks using popular frameworks such as TensorFlow, PyTorch, and Caffe without buying and maintaining costly hardware.\\n\\nThe service lets data scientists train models using only the resources they need, paying only for GPU time. Each cloud processing unit is set up for ease-of-use and prepared for programming deep learning networks without the need for infrastructure management from users. According to a white paper published by IBM researchers working on the project:\\n\\nUsers can choose from a set of supported deep learning frameworks, a neural network model, training data, and cost constraints and then the service takes care of the rest, providing them an interactive, iterative training experience.\\n\\nIn order to use the service users just have to prepare their data, upload it, begin training, then download the training results. It seems fairly straightforward and could potentially shave days or weeks off of training times.\\n\\nIBM seems to be working towards taking the difficulty out of training neural networks, or at least lowering the bar for entry. According to a company blog post:\\n\\nThis Deep Learning as a Service is an experiment-centric model training environment, meaning users don’t have to worry about getting bogged down with planning and managing training runs themselves. Instead, the entire training life-cycle is managed automatically and the results can be viewed in real-time and revisited later. Each training run is automatically started, monitored, and stopped upon completion, saving users time and money as they only pay for the resources they use.\\n\\nA single GPU setup can, for example, take nearly a week to train a visual image processing neural network on a couple million pictures. With IBMs cloud solution that could potentially be cut down to mere hours.\\n\\nThis is further evidence that AI is preparing to infiltrate every industry at just about every level of business. Not only does IBM’s DLaaS give developers access to hardware that’s relatively cost-prohibitive for smaller startups or companies with limited budgets, but it also saves on salary.\\n\\nMaintaining deep learning systems requires manpower, and this investment of time is exacerbated as projects scale: clustering just a few GPUs for deep learning models is an entirely different skillset than training neural networks.\\n\\nRather than require all data scientists to become Jacks or Jills of all trades, a cloud based solution could save money and time. And there’s few companies more equipped to provide DLaaS than IBM.\\n\\nThe company’s DLaaS runs on its excellent Watson platform. This means its been tested with one of the most advanced AI systems on the planet.\\n\\nFor more information you can check out IBM’s blog.\\n\\nThe Next Web’s 2018 conference is just a few months away, and it’ll be 💥💥. Find out all about our tracks here.\\n\\nRead next: This 100TB drive is the largest capacity SSD ever made')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Stopping racist AI is as difficult as stopping racist people', 'In early 2016, Microsoft launched Tay, an AI chatbot that was supposed to mimic the behavior of a curious teenage girl and engage in smart discussions with Twitter users. The project would display the promises and potential of AI-powered conversational interfaces.\\n\\nHowever, in less than 24 hours, the innocent Tay became a racist, misogynist and a holocaust denying AI, debunking—once again—the myth of algorithmic neutrality. For years, we’ve thought that artificial intelligence doesn’t suffer from the prejudices and biases of its human creators because it’s driven by pure, hard, mathematical logic.\\n\\nHowever, as Tay and several other stories have shown, AI might manifest the same biases as humans, and in some cases, it might even be worse. The phenomenon, known as “algorithmic bias,” is rooted in the way AI algorithms work and is becoming more problematic as software becomes more and more prominent in every decision we make.\\n\\nThe roots of algorithmic bias\\n\\nMachine learning and deep learning, the most popular branches of AI, are the reason our software become biased. Deep learning algorithms are dependent on data, lots of it. Give an image classification algorithm millions of labeled cat pictures and it will be able to tell you whether a photo it hasn’t seen before contains a cat. Give a speech recognition algorithm millions of voice samples along with their corresponding written words, and it will be able to transcribe spoken language faster than most humans.\\n\\nThe more labeled data an algorithm sees, the better it becomes at the task it performs. However, the tradeoff to this approach is that deep learning algorithms will develop blind spots based on what is missing or is too abundant in the data they’re trained on.\\n\\nFor instance, in 2015, Google’s photos app mistakenly tagged a photo of two black people as gorillas because its algorithm hadn’t been trained with enough images of dark-skinned persons. In another case, the AI judge of a beauty contest mostly chose white participants as winners because its training was done on images of white people.\\n\\nThese are trivial cases that can be easily remedied by providing the AI with more samples in areas where it doesn’t have enough data. In other cases where AI is working with vast amounts existing data in the endless sea of online information, finding and countering bias becomes much more difficult.\\n\\nAn example is a joint project by researchers at Microsoft and Boston University, in which they found sexist biases in word embedding algorithms, which are used in search engines, translation and other software that depend on natural language processing.\\n\\nAmong their findings of the behavior of word embedding algorithms was a tendency to associate words such as “programming” and “engineering” to men and “homemaker” to women. In this case, the bias was ingrained in the thousands of articles the algorithms had automatically scavenged and analyzed form online sources such as Google News and Wikipedia.\\n\\nFor instance, the tech industry is mostly dominated by men. This means that you’re more likely to see male names and pronouns appear next to engineering and executive tech jobs. As humans, we acknowledge this as a social problem that we need to address. But a mindless algorithm analyzing the data would conclude that tech jobs should belong to men and wouldn’t see it as a lack of diversity in the industry.\\n\\nIn the case of Tay, the Twitter users who interacted with the chatbot were more interested in teaching it hateful speech than engaging in meaningful conversations. Again, the AI was not to blame. The culprit was the general culture that Twitter as a social media breeds.\\n\\nWhy is algorithmic bias a serious problem?\\n\\nAlgorithmic bias is not new. Academics and experts have been warning about it for years. However, what makes it especially critical at this time is the prominence algorithms are finding in everyday decisions we make.\\n\\nTake the word embedding algorithm problem we visited in the previous section. This can be the kind of technology that powers the next generation of recruitment software. It’s not hard to imagine that software discriminating against women when searching for and selecting candidates for a programming job.\\n\\nFor instance, separate reports recently showed that both Google’s and LinkedIn’s platforms were showing high-paying job ads more frequently to men than women.\\n\\nAlgorithmic bias can have even more damaging effect in other areas such as law enforcement. In 2016, a ProPublica investigation found that an AI-powered tool used by law enforcement was more likely to declare black people as under the high risk of recidivism than white people. In some states, judges rely on such tools to decide who stays in jail and who walks free, sometimes without doing further investigation themselves.\\n\\nSimilar cases can happen in other areas such as loan approval, where people who are underrepresented will be further marginalized and deprived of service. In healthcare, where AI is making great inroads in diagnosing and curing diseases, algorithms can harm populations whose data has not been included in the training sets.\\n\\nIn fact, if not addressed, algorithmic bias can lead to the amplification of human biases.\\n\\nUnder the illusion that software isn’t biased, humans tend to trust the judgment of AI algorithms, oblivious that those judgments are already reflecting their own prejudices. As a result, we will accept AI-driven decisions without doubting them and create more biased data for those algorithms to further “enhance” themselves on.\\n\\nHow to fight algorithmic bias?\\n\\nThe first step to avoiding algorithmic bias is to acknowledge the limits of artificial intelligence. Deep learning algorithms are not racist, but we are, and they will pick up whatever biases we intentionally or absentmindedly have.\\n\\nKnowing this, we need to take measures to make sure the data we feed to our algorithms is diversified, especially when developing applications that make decisions that can have severe repercussions on the lives and health of the people who directly or indirectly use them. There are a handful of efforts that use statistical methods to spot hidden biases in algorithms.\\n\\nAnother necessary step is for companies that develop AI applications to be more transparent about their products. Presently, most companies tend to hide their algorithms’ inner workings as trade secrets. This makes it difficult to scrutinize those algorithms and find potential pain points.\\n\\nWe also need to address AI’s black box problem. When deep learning algorithms become too complicated, finding the reasons behind their decisions becomes very difficult. Not knowing how an algorithm reached a conclusion can make it hard to find and counter biased functionality. This too is an area where several organizations, including the U.S. Defense Department’s Advanced Research Projects Agency (DARPA), are leading efforts to make deep learning algorithms open to scrutiny or self-explainable.\\n\\nAt the end of the day, algorithmic bias is a human problem, not a technical one, and the real solution is to start removing bias in every aspect of our personal and social lives. This means endorsing diversity in employment, education, politics and more. If we want to fix our algorithms, we should start by fixing ourselves.\\n\\nThis story is republished from TechTalks, the blog that explores how technology is solving problems… and creating new ones. Like them on Facebook here and follow them down here:\\n\\n\\n\\nRead next: Want to avoid a copyright lawsuit? This stockpile of royalty-free images for under $20 will do the trick')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('AI is giving the entire medical field super powers', 'The field of medicine has, arguably, been more positively affected by modern deep learning techniques than any other industry. And, despite the unending deluge of panic-ridden articles declaring AI the path to apocalypse, we’re now living in a world where algorithms save lives every day. Welcome to Utopia.\\n\\nThanks to AI, an iPhone can detect cancer and a smart watch can detect a stroke. Machine learning is infiltrating and optimizing nearly every aspect of medicine from the way 911 emergency services are dispatched to assisting doctors during surgery. You can even quit smoking or kick your opiate addiction with the help of AI.\\n\\nEmergency service dispatchers in Copenhagen are using a new voice-assistant technology called Corti.\\n\\nCorti works by listening in on emergency phone calls. It analyzes the conversation for cues that relate to its entire database of medical knowledge and provides on-screen information for dispatchers. The combination of human and machine has proven to be a successful one.\\n\\nAccording to Freddy Lippert, CEO at EMS Copenhagen:\\n\\nOur dispatchers do an incredible job under high pressure and in circumstances where the person on the line is often scared or experiencing panic-inducing symptoms. During the time that we have used Corti to augment our existing service, we have seen how the platform can support dispatchers by expediting accurate diagnoses. There are people in Copenhagen today for whom Corti has proved a real life-saver.\\n\\nLars Maaløe, Head of Machine Learning for Corti, understands the need for AI to augment — not replace — humans in the medical field. He told TNW:\\n\\nThe big difference between a driverless car and Corti is that our system proposes collaboration between machine learning and the human being. The more skilled a medical professional gets, the more accurate the machine learning framework gets. Who wouldn’t like to have a helping hand when taking life-threatening decisions? From our studies we can safely say that the collaboration between a medical professional and Corti can increase your chance of survival.\\n\\nAI is also one of the world’s most powerful allies in the fight against cancer.\\n\\nFor example, the IT Department of Moscow Government (DIT) last week released an open-source cancer detection code on github that uses deep learning to look for signs of lung cancer in radiology scans.\\n\\nCredit: RadIO\\n\\nThis technology, called RadIO, is freely available for anyone to use and can be found (with tutorials) on Github. And while it doesn’t represent a magical cancer detection-button, it will almost certainly save lives. According to the DIT:\\n\\nRadIO allows for creation of deep learning algorithms of cancer detection in a short piece of clean, easy-to- read, reproducible and fast Python code. RadIO implements a lot of preprocessing operations, including load data from disk, resize scans to a different shape, and dump preprocessed data to disk. Importantly, actions of RadIO are quite fast.\\n\\nHow fast? The researchers who created RadIO believe it could process radiology scans from the entire population of Moscow, 13 million people, in 30 minutes. That’s pretty fast.\\n\\nAnother company, Art Medical, is using AI to solve a big problem with medicine: namely, that people get sicker in hospitals. Its smart feeding tubes and monitors, currently undergoing clinical trials, are designed to prevent life-threatening complications in ICU patients.\\n\\nCredit: Art Medical\\n\\nArt Medical founder and CEO, Liron Elia says:\\n\\nWith these new algorithms, we hope to save more lives in hospitals and prevent complications in ICUs that are unrelated to the original reason of hospitalization. Aspiration pneumonia is a major risk for intubated patients, and we’re committed to leveraging our technology to solve this problem.\\n\\nIt may not seem like it, but we’re living in the most prosperous era in human history. Just 10 or 20 years ago these advances would have been considered out of reach.\\n\\nThe future of machine learning isn’t sentient killer robots. It’s longer human lives.\\n\\nRead next: Tech group wants device makers to join fight against technology addiction')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You can’t play EA’s newest game because you’re not a robot', \"Electronic Arts (EA) yesterday revealed its latest title: a custom game environment for deep learning networks to learn how to play video games. In the future the “computer” player in games won’t rely on basic scripts; it’ll react to you, and play against you, using the same information and controls a human player does.\\n\\nIf you’re a gamer, you’ve probably played one of EA’s games. The list of hits in the company’s catalog contains some of the greatest selling franchises of all time. Many of us have grown up with games like Battlefield, Madden, and FIFA, each titles with robust computer (CPU) opponents built-in.\\n\\nAnd, over time, those CPU players have evolved. They’ve been programmed to call in for help or take cover, often even serving as a “squad mate” that replaces human team members. But they’ve still mainly relied on pre-programmed scripts and random number generators. Once you’ve figured out the “AI” tendencies, you’re likely to find success against it. That means beating a computer player doesn’t compare to the bliss of crushing an actual person.\\n\\nA CPU opponent that ‘thinks,’ however, could change that.\\n\\nResearchers from EA’s Search for Extraordinary Experiences Division (SEED) set out to expand on previous work using games to train neural networks. In order to test their novel training system they built a custom 3D environment for a machine to navigate.\\n\\nThe problem EA is working on, mastering imperfect information games, can’t be solved in the same way that, for example, DeepMind approaches training AI to master Chess or Go. Playing a game like Titanfall 2 taps into a skill set that’s closer to playing poker than a board game where all players are given the same information.\\n\\nEarlier efforts by teams working on the same problem were hindered by simplicity, but EA’s approach draws inspiration from humans. If you’ve ever played a multiplayer first-person shooter like Battlefield 1, you know there’s no end to human ingenuity when it comes to exploiting gaming environments and controls.\\n\\nAI is pretty good at breaking the system too, which makes it beneficial for any gaming company to study. EA’s researchers essentially set out to level up their neural network, according to a company blog post:\\n\\nReinforcement learning agents are generally limited to carrying out one action at a time, due to the nature of the neural network architecture and the way they are trained. As such, agents often encounter situations where they are forced to choose one out of many actions, when in reality the optimum solution is to perform multiple actions at once; such as moving forward whilst turning. Indeed, certain behaviors will never emerge when an agent is limited to performing only one action at a time … these techniques can be used to teach an agent to play a challenging fully 3D first person shooter (FPS) style video game, an important milestone on the way to training neural networks to play modern AAA FPS games.\\n\\nAnd indeed, the project’s white paper indicates that EA’s novel method is superior to previous efforts. It “provides a 4x improvement in training speed by having the agent learn useful behaviors by imitating the play style of an expert human player” by beginning with a 50/50 blend of imitation learning and reinforcement training.\\n\\nOver time the AI’s experience is guided more by reinforcement learning as the amount of imitation learning is gradually decreased.\\n\\nEA’s goal to train neural networks on AAA titles doesn’t seem very far off. And using AI to test the limits of a gaming environment could help eliminate (or at least lessen) launch-day bugs: anything that can be used as a tool to create better games is good for gamers.\\n\\nIn the meantime, whenever EA’s AI is ready for a real challenge: TNW has gamers on staff.\\n\\nWant to hear more about AI from the world’s leading experts? Join our Machine:Learners track at TNW Conference 2018. Check out info and get your tickets here.\\n\\nRead next: Logitech's first gaming speakers pack RGB lighting and ample bass for $199\")\n",
      "('Google taught AI to multitask with an intense gaming sesh', 'Google’s DeepMind team last week revealed a speedy new approach to training deep learning networks that combines advanced algorithms and old school video games.\\n\\nDeepMind, the team responsible for AlphaGo, appears to believe machines can learn like humans do. Using its own DMLab-30 training set, which is built on ID Software’s Quake III game and an arcade learning environment running 57 Atari games, the team developed a novel training system called Importance Weighted Actor-Learner Architectures (IMPALA).\\n\\nWith IMPALA, an AI system plays a whole bunch of video games really fast and sends the training information from a series of “actors” to a series of “learners.”\\n\\nCredit: DeepMind\\n\\nNormally, deep learning networks figure things out like a single gamer traversing a gaming engine. Developers tell the computer what the controller inputs are and it plays the game just like a person with an actual gamepad would.\\n\\nWith IMPALA, however, not only does the system play the game 10 times more efficiently than other methods, but it plays a whole bunch of games at once. It’s like having 30 or more gamers learning how to play Quake with one “borg” brain gaining all the experience.\\n\\nHere’s a human testing the DMLab-30 environment:\\n\\nNow let’s check out a machine using IMPALA:\\n\\nOne of the biggest challenges faced by AI developers is the amount time and processing power it takes to train a neural network. Unlike traditional programming — where a smart person bangs out a bunch of code that eventually turns into a program — autonomous machines require rules they can experiment with in order to try and discover a way to deal with real world problems.\\n\\nSince we can’t just build robots and let them loose to figure things out, simulations are used for the brunt of development. For this reason, deep reinforcement learning is crucial for tasks requiring contextual autonomy.\\n\\nAn autonomous car, for example, should be capable of determining on its own if it should speed up or slow down. But it should not be given a choice of whether to drive through the front of a convenience store. It learns what kind of decisions it should be making and how to make them in a simulation environment.\\n\\nAnother problem IMPALA solves is scalability. It’s one thing to tweak algorithms and tune things to shave a few minutes off of training time, but at the end of the day the requirements for successfully training an AI aren’t based on hours logged.\\n\\nIn order for current neural networks to achieve success rates high enough to justify their implementation in any autonomous machinery that could potential harm humans or damage inventories, they have to proccess billions of ‘frames’ (images) from the training environment.\\n\\nAccording to the researchers “given enough CPU-based actors — the IMPALA agent achieves a throughput rate of 250,000 frames/sec or 21 billion frames/day.” This makes DeepMind’s AI the absolute fastest we’re aware of when it comes to these types of tasks.\\n\\nAnd, perhaps more stunning, according to the IMPALA white paper, the AI performs better than both previous AI systems and humans.\\n\\nWe already knew DeepMind’s AI was better than us at games, but now it’s just showing off.\\n\\nRead next: Twitter’s profitable. Now it’s time to address the bot issue.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Of course robots will do our cryptocurrency investing', 'There’s nothing that we as a society get more excited about and panicked by than technological progress.\\n\\nOn one hand, we understand that innovations like artificial intelligence and Bitcoin are autonomous, powerful entities that many of us will never truly know — and what we don’t know, we don’t always trust. On the other hand, we recognize that these technological advancements were created to make our lives easier.\\n\\nYou might ask yourself how it’s possible to be grateful for something when you are also so skeptical of it. But think about this on a smaller scale: how many times have you used your smartphone to shop online when you were away from home? Or scheduled an Uber to drive you from the airport to a hotel?\\n\\nTo be wary of these innovations is to dismiss their profound impact on our personal lives and beyond. I am the most interested in how technology is working to improve our financial prosperity. The coupling of artificial intelligence and digital currency in the world of investing has already become such a profound new trend in fintech. As AI and cryptocurrency become more intertwined, they are actively working to improve the way we make money.\\n\\nRemoving all emotions makes investing more efficient\\n\\nFirst, rid yourself of the imagery of human-detesting robots that futuristic movies have built up in your mind. In reality, artificial intelligence is just self-writing software that “learns” from data patterns and teaches itself human-like tasks, including decision-making and speech. This is beneficial to you because financial institutions are using these capabilities of AI to yield higher profits.\\n\\nAI uses deep learning to track patterns of the existing financial markets and uses that data in order to predict their outcomes. Over time, as the software continues to collect more data on the financial markets, it only grows more intelligent.\\n\\nFintech skeptics may believe that humans have already mastered this, but even the most experienced traders often fail to achieve the same level of success as their robotic counterparts. So, what is the barrier that hinders this success? The very thing that makes them human: their emotions. Whether it’s greed or doubt that influences their decisions, there is no way for a person to completely rid themselves of the connection between their emotions and corresponding behavior.\\n\\nPeople have benefited from investing before AI, but there has never been a more efficient way to invest. Certain hedge funds are beginning to take notice of these discrepancies between humans and technology. Statistics show that hedge funds are more profitable when they replace human traders with AI.\\n\\nTake, for example, one hedge fund based out of New York that is already completely reliant on artificial intelligence. One of their funds has been profiting by 20-98 percent consistently between 2002 and 2016.\\n\\nIf we want to prosper, our hopes should rest on efficiency. Since that is what more people desire, NS Parthasarathy of Mindtree says that “we have to move towards systems and technologies that make work happen in the most efficient way in terms of service and price.” That is exactly what artificial intelligence works to do.\\n\\nDigital currency is making cash obsolete\\n\\nIf you’ve been paying attention to investment trends, you know that Bitcoin has been gaining traction over the past few years, which comes as no surprise. As cash grows obsolete, cryptocurrency becomes a more and more attractive form of currency.\\n\\nThis is why Bitcoin is replacing our society’s traditional views of currency as a great alternative to paper money. It’s a decentralized form of currency, which means that no government or bank has control of it; it’s exclusively an anonymous, peer-to-peer transactional platform.\\n\\nAnonymity, however, does not mean that these transactions aren’t being recorded — quite the opposite. Every transaction is tracked in a huge network referred to as the blockchain. Blockchain is a secure network that manages Bitcoin, ensuring that no data can be tampered with and solving the issue of double spending, something no network could do previously without the involvement of a third-party (although double-spending is still a possibility).\\n\\nBecause of the availability of Bitcoin and the fact that the government cannot intercede in it (although it’s definitely trying), it tends to react quickly to market bias. While this can seem risky to some investors, others use it to their advantage and profit from these influxes.\\n\\nThe merging of artificial intelligence and digital currency\\n\\nThese digital currency markets are still relatively young, but that could work in the favor of artificial intelligence. Traders are using the deep learning capabilities of AI to measure data now that will help to predict the fluctuations of this new market in the near future.\\n\\n\\n\\nBefore technology, it would have taken a human multiple years to collect and organize the data they gleaned from watching patterns in the financial market in order to better influence an investor’s decision. Artificial intelligence significantly cuts down on this time. There are already some cryptocurrency trading bots out there, but they still have some considerable faults.\\n\\n\\n\\nThis financial market may be more volatile than more traditional markets, but the data it provides AI software can work to change the face of investing for both expert and amateur investors sooner than ever before — and trading bots will keep getting better. This gives individuals better control over their investing success and makes for a more rewarding long-term commitment.\\n\\nAs the cryptocurrency market matures, it will become easier to predict positive fluctuation and foresee where higher profit margins exist. With a bot serving as your eyes and ears behind the veil of the stock market, you can expect these innovations to bring you a better understanding of where and how to invest your money.\\n\\nRead next: Advertising in the digital age: why online-first is the future')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('AI rips objects from video and reimagines them in 3D AR', \"There’s an AI capable of teleporting John Travolta and Uma Thurman into your living room and forcing them to dance for you. The machine takes a 2D image, like the dance scene from Pulp Fiction, and reimagines it in augmented reality as a 3D object.\\n\\nThe tool is called Volume and it’s being developed by artists Or Fleisher and Shirin Anlen. It’s currently in the experimental stage, but the concept is simply incredible. Fleisher told TNW:\\n\\nOur experiment with Pulp Fiction allows users to step inside one the film’s scenes in Augmented Reality, using Apple’s ARKit framework on an iPad. This experiment, is one of a few we are conducting at the moment, which illustrate the power of being able to reconstruct 3D scenes from 2D images. The possibilities of being able to reconstruct archival and static footage into 3D environments are one of the main motivations behind the development of the tool used to create these experiments called Volume.\\n\\nThe developers use a deep learning system called a convolutional neural network to train the AI. In essence, they give it a flat image to break down and analyze. Once the network processes the pixels and information, it tries to reproduce the 2D images as realistically as possible in AR.\\n\\nThis isn’t the first machine learning application that takes a 2D image and converts it into 3D, but this project doesn’t aim to provide another suite of back-end development magic for industry insiders to create content with – at least not specifically. It’s being designed as an end-to-end web solution, which means anyone using almost any smartphone, laptop, or tablet, will be able to input images or video for automated conversion.\\n\\nAs you can see, it’s not perfect. But there were will be innumerable applications for the technology once it matures. If Volume’s AI can get past the nausea-inducing blurs and wobbles, VR and AR enthusiasts are going to love this. And there’s potential beyond just watching 90s pop icons dance in front of your couch.\\n\\nWith this tool, for example, Dr. King’s “I Have A Dream” speech from 1963, or The Beatles 1969 rooftop performance could come alive in any classroom. It’s easy to imagine a high-school sports coach using clips from pro games to demonstrate fundamentals using AR, or a custom VR experience you create by feeding it your favorite horror or fantasy clips.\\n\\nLike many cutting-edge AI applications there just isn’t enough research yet to determine the full implications of this technology. But one thing’s for certain: this is definitely going to change the way I watch UFC fights.\\n\\nRead next: Missed Tinder connection leads to campus-wide search for the girl of this guy's dreams\")\n",
      "('Google teaches AI to fool humans so it can learn from our mistakes', 'Fooling robots into seeing things that aren’t there, or completely mis-categorizing an image, is all fun and games until someone gets decapitated because a car’s autopilot feature thought a white truck was a cloud.\\n\\nIn order avoid such tragedies, it’s incredibly important that researchers in the field of artificial intelligence understand the very nature of these simple attacks and accidents. This means computers are going to have to get smarter. That’s why Google is studying the human brain and neural networks simultaneously.\\n\\nSo far, neuroscience has informed the field of artificial intelligence through endeavors such as the creation of neural networks. The idea is that what doesn’t fool a person shouldn’t be able to trick an AI.\\n\\nA Google research team, which included Ian Goodfellow, the guy who literally wrote the book on deep learning, recently published its white paper: “Adversarial Examples that Fool both Human and Computer Vision.” The work points out that the methods used to fool AI into classifying an image incorrectly don’t work on the human brain. It posits that this information can be used to make more resilient neural networks.\\n\\nLast year when a group of MIT researchers used an adversarial attack against a Google AI all they had to do was embed some simple code into an image. In doing so, that team convinced an advanced neural network it was looking at a rifle, when in fact it was seeing a turtle. Most children over the age of three would’ve known the difference.\\n\\nCredit: MIT\\n\\nThe problem isn’t with Google’s AI, but with a simple flaw that all computers have: a lack of eyeballs. Machines don’t “see” the world, they simply process images – and that makes it easy to manipulate the parts of an image that people can’t see in order to fool them.\\n\\nTo fix the problem, Google is trying to figure out why humans are resistant to certain forms of image manipulation. And perhaps more importantly, it’s trying to discern exactly what it takes to fool a person with an image.\\n\\nAccording to the white paper published by the team:\\n\\nIf we knew conclusively that the human brain could resist a certain class of adversarial examples, this would provide an existence proof for a similar mechanism in machine learning security.\\n\\nCredit: Google Left: original image of a cat. Right: The same image successfully manipulated to fool humans into thinking they’re seeing a dog.\\n\\nIn order to make people see the cat as a dog the researchers zoomed in and fudged some of the details. Chances are, it passes at-a-glance, but if you look at it for more than a few seconds it’s obviously a doctored-up image. The point the researchers are making is that it’s easy to fool humans, but only in some ways.\\n\\nCredit: Google\\n\\nRight now people are the undisputed champions when it comes to image recognition. But completely driverless cars will be unleashed on roadways around the world in 2018. AI being able to “see” the world, and all the objects in it, is a matter of life and death.\\n\\nWant to hear more about AI from the world’s leading experts? Join our Machine:Learners track at TNW Conference 2018. Check out info and get your tickets here.\\n\\nRead next: Apple patent details dual display laptop with no physical keyboard')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Exchange OKEx just listed CUBE — the coin based on autonomous car network security', 'OKEx is a global digital asset exchange platform that offers fiat-to-token trading, token trading and futures trading. We will demystify all this jargon in the rest of this article. For now, imagine how a foreign currency exchange platform works, but with cryptocurrencies being traded e.g. Bitcoin instead of real-world currency e.g. Euro. The platform serves more than 20 million customers in over 100 countries, which is more than the entire population of the Netherlands.\\n\\nFiat-to-token trading\\n\\nTraditional fiat-to-token trading is the exchange between digital assets e.g. Bitcoin (BTC) and fiat currency e.g. US Dollars (USD). However, OKEx provides a different “Fiat to Token” service – a C2C platform.\\n\\nAt their C2C trading, people can exchange their fiat currency to tokens. The feature allows users to post advertisements where they state exchange rate for buying or selling tokens. It is a decentralized platform, on where you make a trade directly with another person.\\n\\nToken trading\\n\\nToken-to-token trading refers to exchanging a digital asset with another digital asset. The same concept of currency pairing applies for pricing LTC with BTC where a LTC/BTC pair emerges. It represents how many BTC is required to buy one LTC, or how many BTC will be received for selling one LTC. For example, according to OKEx’s index on 18 Aug, 1LTC=0.0099BTC.\\n\\nThere are currently 4 markets available in OKEx’ Token Trading: Bitcoin (BTC), Ether (ETH), U.S Dollar held in the reserves of the Tether Limited (USDT) & Bitcoin Cash (BCH) markets. The platform supports over a hundred tokens.\\n\\nToken trading has several benefits including reduced costs between token exchange, the creation of arbitrage opportunities, staying anonymous while trading and promoting the exchange and circulation of tokens.\\n\\nTraditional digital asset exchange platforms only offer fiat-to-token trading. If one wants to buy ETH with BTC, one would have to sell BTC first which incurs a fee and subsequently buy ETH which incurs another fee. This results in the service fee being applied twice. Taking this scenario in a token trading setting, one can buy ETH with BTC directly which saves the user time and fees.\\n\\nPricing in token trading\\n\\nSimilar to traditional fiat-to-token trading, the market prices of token trading are determined by supply and demand. When there is a buying pressure aka a lot of people want to buy, the price rises. When there is a selling pressure aka a lot of people want to sell, the price drops. For example, if there are more people willing to exchange BTC to LTC in the market, there will be more demand than supply of LTC. Therefore, LTC/BTC price will rise, meaning that buying a LTC would require more BTC.\\n\\nLaunch of the OKB token\\n\\nOKEx launches a global utility token “OKB” that issued by OK Blockchain foundation. The available supply of OKB will be one billion tokens, but there will be no ICO or private fundraising. 60% of these tokens will be given out to customers for community building and through marketing promotions. Their vision is to build a sharing community on the blockchain infrastructure, allowing every user to participate and contribute to the development of the OKEx platform.\\n\\nListing CUBE (AUTO) token\\n\\nOKEx also listed CUBE (AUTO) in February to provide another option for their customers. CUBE is a platform for securely protecting the network of existing autonomous vehicles with blockchain technology. Imagine you’re in an autonomous vehicle and the network gets hacked, leaving the hacker to control where the car goes. It’s a pretty scary situation to be in. CUBE wants to secure the network by using blockchain, deep learning and quantum hash technology so that hacking the car won’t be possible.\\n\\nWith exciting new tokens being issued, there is a possibility of making good returns from your investment. However, it is important to note that trading digital assets involves significant risk and can result in the loss of your invested capital. Before making any token purchases, you should ensure that you fully understand the risks involved and consider your level of experience, investment objectives and seek independent advice if necessary. We know this sounds boring, but it’s much better than the drama of losing big money because you went for it with eyes half closed.\\n\\nToken Trading Loyalty Program\\n\\nOKEx launches a Token Trading Loyalty Program on March 19 this year with 250 million OKB as incentives. The program received a great response and the packages for phase one and two were fully subscribed within two minutes. Phase three of the Loyalty Program will be open for subscriptions on March 23, 2018.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Here’s how to remove the bias from your UX design', 'From the way we ask our users an ethnicity question in an online form to the promise of designing an entire world in Virtual Reality, we, as designers, could be missing the opportunity of breaking stereotypes and misconceptions, instead of perpetuating them.\\n\\nDesign can’t just be incidental. Our work has an impact on people’s lives.\\n\\nInclusion should be a key design principle, just like (if not above) any other design principle your company decides to follow. When inclusion is not part of a brand’s or designer’s core values, there is a chance it will be forgotten, de-prioritized, and become a distant, utopian concept.\\n\\nDesigning for inclusion can be a long walk, but hopefully throughout this article we will be able to break it down in a few steps.\\n\\nHere are a few ways we can design more inclusive experiences.\\n\\nBy not making “diversity” just a buzz word on a slide\\n\\nWell, actually, let’s start by removing the cap from the pen.\\n\\nThe first step is, of course, bringing diversity to our design teams. We need to acknowledge the problem and understand the benefits of a diverse team.\\n\\nJust keep in mind that learning and talking about diversity is a big step, but far from being the solution.\\n\\nAny progress is dubious if your organization treats diversity just as a nice word in a slide deck and does not act accordingly in their hiring policies and through more tangible inclusion initiatives.\\n\\nAs Rachel Thomas, a Deep learning Researcher who writes about diversity and tech on Medium, stated on her article about the issues with companies using diversity as a branding strategy:\\n\\nNo donation is ever a substitute for the hard work of self-reflection and company-wide change. The “easy” changes are necessary, but they are not sufficient.\\n\\nWhat are your company’s inclusion and diversity goals for this year? When was the last time your team talked about diversity?\\n\\nBy removing bias from the design process\\n\\nYou can start with a post-it on your desk reminding you to question your bias in every step of your process. But there is also an opportunity to be more inclusive on the methodologies and tools that you use on your day-to-day as a designer. Here are a few examples:\\n\\nWhen creating your personas, move away from assumptions and stereotypical information about your audience.\\n\\nDoes it really need to be a man? Are you assuming that your persona is not familiar with technology just because of their age? How does the location of your persona impacts your design decisions? Isn’t the funny/catchy name carrying too much bias?\\n\\nThis is also valid for user journeys, task analysis and any other methodology trying to document who the user is and how the user behaves.\\n\\nWhen asking for feedback on a project, go beyond your inner circle: sometimes just getting feedback and thoughts from someone in a different team is a good start to have your bias questioned.\\n\\nIdeas need to be prototyped, tested and iterated upon with many different inputs.\\n\\nWhen setting up a usability test and recruiting participants, the insights will be more helpful if you have a diverse pool of users, representing the same diversity than your audience likely has. Remove specific demographics criteria from your recruitment process and ask the recruiter for a good mix of gender, age, location.\\n\\nIf you are doing just some guerrilla testing, try to go to different places and neighborhoods to get your participants.\\n\\nWhat can you improve in your process and tools to bring more diversity to your designs?\\n\\nBy making interfaces accessible\\n\\nThe power of the Web is in its universality. Access by everyone regardless of disability is an essential aspect.\\n\\n— Tim Berners Lee, inventor of the World Wide Web\\n\\nTo achieve a truly inclusive design solution, accessibility should be a no-brainer. After all, in a diverse world like ours, we cannot simply assume that all users access and experience digital products in the same way.\\n\\n“Accessibility” might sound like a complicated thing, but it’s actually pretty far from it. The renowned World Wide Web Consortium (W3) has created an accessibility Web standard detailing the steps to make websites that work for everyone. There is no need to re-invent the wheel.\\n\\nTo make our lives even easier, Adhithya, Product Designer at OpenDNS, created a useful guide so you can apply the basic concepts of accessibility in your next project: checking the contrast, labeling the elements correctly and fixing the focus behavior are a few of small actions that can make a significant impact for the users.\\n\\nAccessible design aids in creating a better experience not just for people with disability, but also for people without it\\n\\nAs Conversational UIs, Virtual Reality, and Augmented Reality are looming, there are new challenges and cases to be considered — hearing, speech, mental and physical attributes also comes to focus. These will require a new set of guidelines that still need to be better standardized. And designers willing to create them.\\n\\nWhen was the last time that you checked the accessibility of your product? Are you bringing visually impaired users to your user testing sessions?\\n\\nBy making icons and labels inclusive\\n\\nBeyond the accessibility of the graphic and the interactive elements of your product, how the users understand the words and the icons has also a relevant impact on inclusion.\\n\\nIcons and text labels form the language used by your product to communicate with users. As such, they need to be carefully crafted, since icons and words can carry more meaning beneath the surface. John Saito wrote a great article about how words can carry different meaning or baggage in various countries, cultures and generations:\\n\\nProduct design is all about metaphors. Every icon, every button, and every interaction is a metaphor for something in the physical world. But some metaphors mean different things in different cultures. In the United States, an owl represents wisdom. In Finland and India, an owl can represent foolishness.\\n\\nAnother prominent example is the emojis with different skin tones and family compositions to make all users feel included and recognized. A milestone for bringing more diversity to our interfaces, but still a work in progress.\\n\\nDo all your icons mean the same thing across different cultures?\\n\\nBy making our products inclusive\\n\\nWhenever a user has to input a personal data, from a name to a profile picture, we can’t simply assume that all the names are like “John Doe” or that every user will upload a hip B&W profile picture. How many last names can someone have, if at all? How long can it be? Will the photo work with the color element of the page? If we are doing something clever with the photo, like a filter, will it work with every skin color?\\n\\nThere are even more complex questions like gender and ethnicity. Because someone’s identity can be anywhere in a broad spectrum of variables, the input provided needs to be mindful about it. Facebook, for example, lets the user customize the gender and nicely asks which pronoun they prefer.\\n\\nFacebook offers a custom option for the gender and explains how the pronoun will be used. Every day we design digital products and interfaces that prompt users to identify themselves using pre-defined categories provided by the system. It is part of our responsibility as designers to create mechanisms and design solutions that enable less binary, more flexible, and more inclusive denominations.\\n\\nTo solve this issue, we need to step back first. How is that information going to be used throughout the product experience?\\n\\nIdentity questions add a cognitive load and unnecessary discomfort for the user. For example, I don’t expect Mint, my finance management app, to know my gender or ethnicity. Removing unnecessary question fields is still the best way to respect someone’s time, effort and privacy.\\n\\nHowever, let’s say that your website will use that information in a meaningful way. Then, it needs to be clear to the users why and how this information is going to be used and, of course, provide a meaningful array options for them to choose.\\n\\nWhat personal information are you requesting from users? Are all of them really necessary? How can you make sure it covers a diverse range of answers?\\n\\nBy removing bias from the user’s behavior\\n\\nWe have talked about our biases as designers, but we also have to take into consideration that our users have their own bias.\\n\\nBecause the concepts of inclusion and diversity are still not fully understood and followed in our society, it is likely that a user, many times unconsciously, might be using your product in a harmful way to others. This is especially true if your product has any social interaction, like online games, social networks or even an e-commerce marketplace.\\n\\nIn the past years, companies started to realize that they have a big responsibility on this journey, especially as they learn that it can harm their businesses profitability as well. The neighborhood social network Nextdoor, is already an iconic example of it:\\n\\nAs Nextdoor has become one of the places where neighbors talk about how to make their local communities better, it is natural for the issue of race to be discussed and debated. But it’s not acceptable when mentions of race take the form of racial profiling. Racial profiling runs counter to everything that Nextdoor represents. Over the last year, we’ve made a number of significant changes to our product to address it, including a new racial profiling flag on posts, updates to our member guidelines, and a mandatory warning screen before posting in Crime and Safety.\\n\\nAfter a research paper from Harvard pointed out the racial discrimination is an issue in the sharing economy business, Airbnb, a leading company in this market, made inclusion and fighting bias one of their goals:\\n\\nWe have clear goals: we want to eliminate unconscious bias in the Airbnb community and fight discrimination. Airbnb has demonstrated the ability to bring people together and make it easier for more people to explore the world and we’ve seen how the simple act of sharing a home can unite people from all walks of life.\\n\\nIt’s a tough, ongoing, battle, but it shouldn’t hold us from using technology to build communities.\\n\\nWhen designing for social interactions, we need to question and consider how a user generated-content or action can be negatively perceived by others. And keep an eye on user’s reports, keywords and other signs that can help you identify any issue in our platform.\\n\\nDoes your product have a clear mission and guideline for making its community respectful and inclusive? Are you looking beyond just quantitative metrics to understand how people are using your platform? How else can you check and measure the health of the user’s interactions?\\n\\nThis article was originally posted on uxdesign.cc.\\n\\nRead next: Ethereum scammers disguised as Vitalik Buterin are out to steal your coins')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I used neural networks to see what a self-driving car sees', 'Here are a few examples to make it clearer:\\n\\nThe goal of the challenge was to recognize the traffic light state in images taken by drivers using the Nexar app. In any given image, the classifier needed to output whether there was a traffic light in the scene and whether it was red or green. More specifically, it should only identify traffic lights in the driving direction.\\n\\nDon’t worry — you don’t need to be an AI expert to understand this post. I’ll focus on the ideas and methods I used as opposed to the technical implementation.\\n\\nIn this post, I’ll describe the solution I used. I’ll also explore approaches that did and did not work in my effort to improve my model.\\n\\nI recently won first place in the Nexar Traffic Light Recognition Challenge , computer vision competition organized by a company that’s building an AI dash cam app.\\n\\nThe images above are examples of the three possible classes I needed to predict: no traffic light (left), red traffic light (center) and green traffic light (right).\\n\\nThe challenge required the solution to be based on Convolutional Neural Networks, a very popular method used in image recognition with deep neural networks. The submissions were scored based on the model’s accuracy along with the model’s size (in megabytes). Smaller models got higher scores. In addition, the minimum accuracy required to win was 95%.\\n\\nNexar provided 18,659 labeled images as training data. Each image was labeled with one of the three classes mentioned above (no traffic light/red/green).\\n\\nSoftware and hardware\\n\\nI used Caffe to train the models. The main reason I chose Caffe was because of the large variety of pre-trained models.\\n\\nPython, NumPy & Jupyter Notebook were used for analyzing results, data exploration, and ad-hoc scripts.\\n\\nAmazon’s GPU instances (g2.2xlarge) were used to train the models. My AWS bill ended up being $263 (!). Not cheap. 😑\\n\\nThe code and files I used to train and run the model are on GitHub.\\n\\nThe final classifier\\n\\nThe final classifier achieved an accuracy of 94.955% on Nexar’s test set, with a model size of ~7.84 MB. To compare, GoogLeNet uses a model size of 41 MB, and VGG-16 uses a model size of 528 MB.\\n\\nNexar was kind enough to accept 94.955% as 95% to pass the minimum requirement 😁.\\n\\nThe process of getting higher accuracy involved a LOT of trial and error. Some of it had some logic behind it, and some were just “maybe this will work”. I’ll describe some of the things I tried to improve the model that did and didn’t help. The final classifier details are described right after.\\n\\nWhat worked?\\n\\nI started off with trying to fine-tune a model which was pre-trained on ImageNet with the GoogLeNet architecture. Pretty quickly this got me to >90% accuracy! 😯\\n\\nNexar mentioned in the challenge page that it should be possible to reach 93% by fine-tuning GoogLeNet. Not exactly sure what I did wrong there, I might look into it.\\n\\nSqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size.\\n\\nSince the competition rewards solutions that use small models, early on I decided to look for a compact network with as few parameters as possible that can still produce good results. Most of the recently published networks are very deep and have a lot of parameters. SqueezeNet seemed to be a very good fit, and it also had a pre-trained model trained on ImageNet available in Caffe’s Model Zoo which came in handy.\\n\\nSqueezeNet network architecture. Slides\\n\\nThe network manages to stay compact by:\\n\\nUsing mostly 1×1 convolution filters and some 3×3\\n\\nReducing number of input channels into the 3×3 filters\\n\\nFor more details, I recommend reading this blog post by Lab41 or the original paper.\\n\\nAfter some back and forth with adjusting the learning rate I was able to fine-tune the pre-trained model as well as training from scratch with good accuracy results: 92%! Very cool! 🙌\\n\\nRotating images\\n\\nSource: Nexar\\n\\nMost of the images were horizontal like the one above, but about 2.4% were vertical, and with all kinds of directions for “up”. See below.\\n\\nDifferent orientations of vertical images. Source: Nexar challenge\\n\\nAlthough it’s not a big part of the data-set, I wanted the model to classify them correctly too.\\n\\nUnfortunately, there was no EXIF data in the jpeg images specifying the orientation. At first, I considered doing some heuristic to identify the sky and flip the image accordingly, but that did not seem straightforward.\\n\\nInstead, I tried to make the model invariant to rotations. My first attempt was to train the network with random rotations of 0°, 90°, 180°, 270°. That didn’t help 🤔. But when averaging the predictions of 4 rotations for each image, there was improvement!\\n\\n92% → 92.6% 👍\\n\\nTo clarify: by “averaging the predictions” I mean averaging the probabilities the model produced for each class across the 4 image variations.\\n\\nOversampling crops\\n\\nDuring training the SqueezeNet network first performed random cropping on the input images by default, and I didn’t change it. This type of data augmentation makes the network generalize better.\\n\\nSimilarly, when generating predictions, I took several crops of the input image and averaged the results. I used 5 crops: 4 corners and a center crop. The implementation was free by using existing caffe code for this.\\n\\n92% → 92.46% 👌\\n\\nRotating images together with oversampling crops showed very slight improvement.\\n\\nAdditional training with lower learning rate\\n\\nAll models were starting to overfit after a certain point. I noticed this by watching the validation-set loss start to rise at some point.\\n\\nValidation loss rising from around iteration 40,000\\n\\nI stopped the training at that point because the model was probably not generalizing anymore. This meant that the learning rate didn’t have time to decay all the way to zero. I tried resuming the training process at the point where the model started overfitting with a learning rate 10 times lower than the original one. This usually improved the accuracy by 0-0.5%.\\n\\nMore training data\\n\\nAt first, I split my data into 3 sets: training (64%), validation (16%) & test (20%). After a few days, I thought that giving up 36% of the data might be too much. I merged the training & validations sets and used the test-set to check my results.\\n\\nI retrained a model with “image rotations” and “additional training at lower rate” and saw improvement:\\n\\n92.6% → 93.5% 🤘\\n\\nRelabeling mistakes in the training data\\n\\nWhen analyzing the mistakes the classifier had on the validation set, I noticed that some of the mistakes have very high confidence. In other words, the model is certain it’s one thing (e.g. green light) while the training data says another (e.g. red light).\\n\\nNotice that in the plot above, the right-most bar is pretty high. That means there’s a high number of mistakes with >95% confidence. When examining these cases up close I saw these were usually mistakes in the ground-truth of the training set rather than in the trained model.\\n\\nI decided to fix these errors in the training set. The reasoning was that these mistakes confuse the model, making it harder for it to generalize. Even if the final testing-set has mistakes in the ground-truth, a more generalized model has a better chance of high accuracy across all the images.\\n\\nI manually labeled 709 images that one of my models got wrong. This changed the ground-truth for 337 out of the 709 images. It took about an hour of manual work with a python script to help me be efficient.\\n\\nAbove is the same plot after re-labeling and retraining the model. Looks better!\\n\\nThis improved the previous model by:\\n\\n93.5% → 94.1% ✌️\\n\\nEnsemble of models\\n\\nUsing several models together and averaging their results improved the accuracy as well. I experimented with different kinds of modifications in the training process of the models involved in the ensemble. A noticeable improvement was achieved by using a model trained from scratch even though it had lower accuracy on its own together with the models that were fine-tuned on pre-trained models. Perhaps this is because this model learned different features than the ones that were fine-tuned on pre-trained models.\\n\\nThe ensemble used 3 models with accuracies of 94.1%, 94.2%, and 92.9% and together got an accuracy of 94.8%. 👾\\n\\nWhat didn’t work?\\n\\nLots of things! 🤕 Hopefully, some of these ideas can be useful in other settings.\\n\\nCombatting overfitting\\n\\nWhile trying to deal with overfitting I tried several things, none of which produced significant improvements:\\n\\nincreasing the dropout ratio in the network\\n\\nmore data augmentation (random shifts, zooms, skews)\\n\\ntraining on more data: using 90/10 split instead of 80/20\\n\\nBalancing the dataset\\n\\nThe dataset wasn’t very balanced:\\n\\n19% of images were labeled with no traffic light\\n\\n53% red light\\n\\n28% green light.\\n\\nI tried balancing the dataset by oversampling the less common classes but didn’t notice any improvement.\\n\\nSeparating day & night\\n\\nMy intuition was that recognizing traffic lights in daylight and nighttime is very different. I thought maybe I could help the model by separating it into two simpler problems.\\n\\nIt was fairly easy to separate the images to day and night by looking at their average pixel intensity:\\n\\nYou can see a very natural separation of images with low average values, i.e. dark images, taken at nighttime, and bright images, taken at daytime.\\n\\nI tried two approaches, both didn’t improve the results:\\n\\nTraining two separate models for day images and night images\\n\\nTraining the network to predict 6 classes instead of 3 by also predicting whether it’s day or night\\n\\nUsing better variants of SqueezeNet\\n\\nI experimented a little bit with two improved variants of SqueezeNet. The first used residual connections and the second was trained with dense→sparse→dense training (more details in the paper). No luck. 😕\\n\\nLocalization of traffic lights\\n\\nAfter reading a great post by deepsense.io on how they won the whale recognition challenge, I tried to train a localizer, i.e. identify the location of the traffic light in the image first, and then identify the traffic light state on a small region of the image.\\n\\nI used sloth to annotate about 2,000 images which took a few hours. When trying to train a model, it was overfitting very quickly, probably because there was not enough labeled data. Perhaps this could work if I had annotated a lot more images.\\n\\nTraining a classifier on the hard cases\\n\\nI chose 30% of the “harder” images by selecting images which my classifier was less than 97% confident about. I then tried to train classifier just on these images. No improvement. 😑\\n\\nDifferent optimization algorithm\\n\\nI experimented very shortly with using Caffe’s Adam solver instead of SGD with linearly decreasing learning rate but didn’t see any improvement. 🤔\\n\\nAdding more models to ensemble\\n\\nSince the ensemble method proved helpful, I tried to double-down on it. I tried changing different parameters to produce different models and add them to the ensemble: initial seed, dropout rate, different training data (different split), a different checkpoint in the training. None of these made any significant improvement. 😞\\n\\nFinal classifier details\\n\\nThe classifier uses an ensemble of 3 separately trained networks. A weighted average of the probabilities they give to each class is used as the output. All three networks were using the SqueezeNet network but each one was trained differently.\\n\\nModel #1 — Pre-trained network with oversampling\\n\\nTrained on the re-labeled training set (after fixing the ground-truth mistakes). The model was fine-tuned based on a pre-trained model of SqueezeNet trained on ImageNet.\\n\\nData augmentation during training:\\n\\nRandom horizontal mirroring\\n\\nRandomly cropping patches of size 227 x 227 before feeding into the network\\n\\nAt test time, the predictions of 10 variations of each image were averaged to calculate the final prediction. The 10 variations were made of:\\n\\n5 crops of size 227 x 227: 1 for each corner and 1 in the center of the image\\n\\nfor each crop, a horizontally mirrored version was also used\\n\\nModel accuracy on validation set: 94.21%\\n\\nModel size: ~2.6 MB\\n\\nModel #2 — Adding rotation invariance\\n\\nVery similar to Model #1, with the addition of image rotations. During training time, images were randomly rotated by 90°, 180°, 270° or not at all. At test-time, each one of the 10 variations described in Model #1 created three more variations by rotating it by 90°, 180°, and 270°. A total of 40 variations were classified by our model and averaged together.\\n\\nModel accuracy on validation set: 94.1%\\n\\nModel size: ~2.6 MB\\n\\nModel #3 — Trained from scratch\\n\\nThis model was not fine-tuned but instead trained from scratch. The rationale behind it was that even though it achieves lower accuracy, it learns different features on the training set than the previous two models, which could be useful when used in an ensemble.\\n\\nData augmentation during training and testing are the same as Model #1: mirroring and cropping.\\n\\nModel accuracy on validation set: 92.92%\\n\\nModel size: ~2.6 MB\\n\\nCombining the models together\\n\\nEach model output three values, representing the probability that the image belongs to each one of the three classes. We averaged their outputs with the following weights:\\n\\nModel #1: 0.28\\n\\nModel #2: 0.49\\n\\nModel #3: 0.23\\n\\nThe values for the weights were found by doing a grid-search over possible values and testing it on the validation set. They are probably a little overfitted to the validation set, but perhaps not too much since this is a very simple operation.\\n\\nModel accuracy on validation set: 94.83%\\n\\nModel size: ~7.84 MB\\n\\nModel accuracy on Nexar’s test set: 94.955% 🎉\\n\\nExamples of the model mistakes\\n\\nSource: Nexar\\n\\nThe green dot in the palm tree produced by the glare probably made the model predict there’s a green light by mistake.\\n\\nSource: Nexar\\n\\nThe model predicted red instead of green. A tricky case when there is more than one traffic light in the scene.\\n\\nThe model said there’s no traffic light while there’s a green traffic light ahead.\\n\\nConclusion\\n\\nThis was the first time I applied deep learning on a real problem! I was happy to see it worked so well. I learned a LOT during the process and will probably write another post that will hopefully help newcomers waste less time on some of the mistakes and technical challenges I had. Here are the visualizations of the results.\\n\\nI want to thank Nexar for providing this great challenge and hope they organize more of these in the future! 🙌')"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "('Pot Meet Kettle Alert: Google’s AI Guru Attacks Facebook’s Algorithms', 'Last night Google’s deep-learning expert François Chollet tweeted a long and slightly illuminating thread , decrying the state of artificial intelligence programming. In light of Facebook’s Cambridge Analytica debacle –where a data firm was able to obtain millions of user profiles–Chollet sees the increasingly frightening prospect of algorithms being trained to control information supply. While it’s scary that Facebook essentially allowed a Trump-affiliated program to execute a series of targeted ad campaigns, the hidden story is Facebook’s innate power and how it can be used for psychological control.\\n\\nIndeed, this is a very interesting and horrifying problem. Platforms adopting algorithms to fine-tune the content it feeds to users presents an easy pathway for psychological control. “If Facebook gets to decide, over the span of many years, which news you will see (real or fake), whose political status updates you’ll see, and who will see yours, then Facebook is in effect in control of your political beliefs and your worldview,” writes Chollet.\\n\\nBut this criticism is somewhat rich coming from the deep-learning guy at Google. While perhaps we should pause, nod, and agree that this is a problem, Chollet may not be the most appropriate messenger. Google, after all, is a company that itself has created some very powerful algorithms, ones that people rely on day in and day out. I would go as far to say that Google’s artificial intelligence programs are far more insidious than Facebook’s.\\n\\nEvery time you search for something on Google, a piece of software is analyzing what you wrote and populating what it considers to be the most relevant result. This is exactly the feedback loop Chollet describes, whereby an algorithm helps dictate its end user’s worldview. While Google may seem a bit more cognizant of the weight of this challenge, that does not absolve it of its role as one of the main contributors to it.\\n\\nBad Actors Love Facebook And Google\\n\\nTime and time again, we’ve seen Google’s algorithm manipulated by bad actors looking to promulgate misinformation. Like Facebook, Google has pledged to crack down on the problem, but many have wondered if either of the top platforms have done enough. Google introduced a “fact check” feature some time ago to highlight whether or not a result is true. Outside experts told Fast Company late last year that it seemed the company wasn’t doing enough to keep up with the problem.\\n\\nEarlier this week, the company described changes to its search algorithm as part of its new Google News Initiative. “We’ve developed a more systematic approach,” said Google’s VP of News, Richard Gingras. The company has adjusted its software to “show results from more authoritative sources.” The two pillars Google considers most important when weighing its search algorithm, said Gingras, are “relevance and authoritativeness.”\\n\\nGoogle-owned YouTube, however, has proven to be a wasteland of search results. Nearly every time a tragedy has occurred, a spate of videos touting fake news and conspiracy theories flood the top of YouTube search results. Only after numerous articles were written about this problem did the company seem to crack down on it. The issue, it seems, is that YouTube has a trending element to its results. So if there’s something that is catching people’s eyes–whether or not it is real or fake–it has a higher likelihood of being caught in the algorithm because it creates more engagement.')\n",
      "('Smartphone Battery Life Would Suck If It Weren’t For These Guys', 'Your phone’s battery life could always be better, but it could have been much worse. Decades after John Hennessy and David Patterson invented the technology that made high-performance, low-power gadgets possible, they have received the ultimate honor—the $1 million ACM A.M. Turing Award, billed as the “Nobel Prize of Computing.” You can thank them quietly whenever you pull your phone out.\\n\\nadvertisement\\n\\nIn the 1980s, the two professors (Hennessy from Stanford, Patterson from Berkeley) developed technology called RISC—the reduced instruction set computer. The gist: A CPU runs more efficiently if software feeds it a lot of simple instructions instead of fewer, but more complicated ones. Early programmers and chip designers took the latter route, since they could write shorter code, and leave the CPU to unpack it. But as programming languages got more sophisticated in the 1970s and 1980s, they took over the grunt work, translating high-level code written by humans into the litany of instructions the CPU needs. That made Hennessy and Patterson’s RISC concept make sense without inconveniencing software engineers. In the 1990s, RISC chips like IBM’s PowerPC and Sun Microsystems’ SPARC (which Patterson helped develop) foundered against rivals that used Intel’s venerable complex-instruction set, x86, which has powered most PCs since the beginning. Intel-based chips’ brute force (and power from a wall outlet) compensated for their more-demanding code. But in this century, RISC took off as mobile phones needed chips that were both fast and power-efficient. Nearly every smartphone, tablet, and other mobile gadget now uses a RISC architecture called ARM, the basis of processors developed by companies like Apple, Samsung, and Qualcomm. I chatted with Hennessy and Patterson about their role ushering in the mobile age and also about the big shakeups on computing’s horizon. Fast Company: When you were developing RISC, what was the problem, and how did you try to address it? David Patterson: When software talks to hardware, it has a special vocabulary. The fashion before we did RISC was a very elaborate vocabulary with lots of $5 words . . . We went instead with a very small vocabulary that had very simple words. So the reduced instruction set means reducing the number and complexity of the vocabulary that the software speaks to the hardware. FC: Why was that more efficient for the processor?\\n\\nadvertisement\\n\\nJohn Hennessy: Imagine with those really big words, you need fewer of them, but the reader is four times slower because he has to spend all the time looking them up. FC: Is there something that’s analogous to pulling out the dictionary? Patterson: Yeah, it’s called microcode. They had to run a little tiny program that would interpret that instruction. FC: How do full-size computer CPUs like Intel’s work differently? Hennessy: They have this older polysyllabic instruction set . . . They would translate the $5 words into several simple words, and then the rest of the processor would use the simpler words. So you need extra hardware, and that takes extra time. Patterson: When you talk about the processors in a mobile phone, and you care about battery life, then that extra overhead is a big issue, which is why [x86 chips] are in your desktop, but they’re not in your mobile phone.\\n\\nadvertisement\\n\\nFC: SPARC and PowerPC were based on RISC, and they lost out. Was that disappointing? Patterson: When x86 did hardware translation to RISC, Intel was able to build very cost-efficient servers, which is where SPARC was trying to sell . . . We never got everybody [unified] around one architecture. And the result is the RISC world was fractured, and Intel wasn’t fractured. FC: ARM [avoided fracturing] on the mobile side, right? Patterson: They came in the back door into the mobile phone, and they didn’t try to compete with Intel . . . And once they were in there, it was very easy to go into smartphones and tablets, because you already owned the mobile phone business. Related: The Everything Chip: Qualcomm’s Plan To Power Drones, VR, Cars, PCs, And More FC: Looking to the future, what are you most excited about?\\n\\nadvertisement')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Uber’s self-driving car crash might have been prevented by older tech, says Intel', 'The death of Elaine Herzberg, who was struck by a Volvo XC90 Uber car in self-driving mode, has launched frenzied theorizing throughout the tech and automotive industries. Intel took its turn this week, through the car-sensor subsidiary Mobileye, which it acquired in 2017. In a blog post , Mobileye’s combo CEO/CTO Amnon Shashua says that the self-driving industry relies too much on emerging technologies:\\n\\n“Recent developments in artificial intelligence, like deep neural networks, have led many to believe that it is now easy to develop a highly accurate object detection system and that the decade-plus experience of incumbent computer vision experts should be discounted.”\\n\\nComputer vision, an older form of image recognition, requires painstaking annotation of video streams. It’s powered advanced driver assistance systems (ADAS), such as automatic emergency braking and lane-keeping support. Shashua argues that computer vision should serve as a backstop for the newer technologies, in which the AI figures out how to identify objects after seeing many, many examples.\\n\\nAs a test, Shashua ran Mobileye’s ADAS software on the dash cam video from the Uber crash. It was able to identify both Herzberg and the bicycle she was pushing, says Shashua, who released images showing the detection. However, the ADAS registered only a “low confidence” that it was correct, and it kicked in during the last second before the crash–too late to stop in time. (While that would seem to undercut the argument that old-school tech could have prevented the crash, Shashua points out that the second-hand dash cam video was much lower-quality than what a dedicated safety system would capture.)\\n\\nMoreover, it’s far from clear if the crash happened because the deep-learning AI wasn’t up to the task or simply because engineers screwed up in how they implemented it. The car’s Lidar (laser scanner) and radar should have easily spotted Herzberg as some kind of obstacle long before she was visible on video.\\n\\nAvoiding this accident wasn’t a “hard problem”–the term engineers use to describe big technical challenges. The incident could have been a celebrated example of self-driving technology exceeding human capability to avoid an accident. Instead, it’s a dumbfounding example of failure.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('NASA’s Silicon Valley Summer Camp Is Just Trying To Save Us From Space', 'In a nondescript meeting room in a fluorescent-lit office park in Silicon Valley, a group of computer and planetary scientists were given the problem of deciding what to do with an asteroid on a collision course with Earth.\\n\\nadvertisement\\n\\nThe group–split into teams representing different world leaders, scientists, and citizens with different ideas of how to deal with impending doom–proposed solutions to the theoretical threat. Role-playing wasn’t necessary, but a Brazilian astronomer did his best Russian leader impression and tried to cajole the others. A computer scientist designated as a planetary scientist gave his “expert” opinion on the threat of breaking up the asteroid into multiple asteroids. Some wanted to use a rocket to nudge the asteroid, others wanted to nuke it and others wanted to collect more information. Issues like the path of the asteroid and what exactly it was made of further complicated the scenario. As in a Dungeons and Dragons game, a 20-sided dice was rolled to decide if the action was successful or not. Eventually, the asteroid approached and Japan looked like it was going to take a direct hit. The group decided to nuke the asteroid, saving Tokyo. The room erupted into cheers. “This was like model UN but probably a little more fun,” one team member says. Silicon Valley hosts so many interns, research groups, and accelerators that many hotels are fully booked through the summer months. Only one accelerator in the Bay Area last summer, however, was comprised of a dozen planetary scientists and a dozen machine learning experts, strategizing ways to save us all from threats in space like asteroids, solar flares, and comets—all within the space of eight weeks. Frontier Development Lab, or FDL, is the brainchild of James Parr, a New Zealand space enthusiast and serial entrepreneur who saw an opportunity to bring more artificial intelligence and machine learning to space. He was inspired in 2013, when the Obama administration launched the Asteroid Grand Challenge to get more people working at new ways of finding asteroids, including those that could be mined for valuable resources and those that could destroy life on Earth. “AI is the one thing that astronomers haven’t had the benefits of,” Parr says. “They just haven’t had time to work with it.”\\n\\nadvertisement\\n\\nWhile the chance of a near-Earth object hitting the planet is distant, the stakes are existential. Sixty-five million years ago, a 10 trillion-ton asteroid hit the Yucatan Peninsula and killed 70% of life on Earth, including the dinosaurs. More recently, the meteor that exploded over Russia in 2013 caused a shock wave that smashed windows, damaged buildings, and injured 1,200 people. No one died, but if that same asteroid came in a little differently, at a more vertical angle, or in a more populated area, it could have been disastrous. While scientists have known about these threats for decades, the extent of the risk wasn’t truly clear until recent years, as computers, better telescopes, and digital cameras drew it all into stark relief. “It was an ‘oh shit’ moment,” Parr says. Those same technologies have also given scientists some tools to fight back. Bruce Willis’s mission in the movie Armageddon to blow up an asteroid with a nuclear weapon was not entirely made up (although, if you ask them, many planetary scientists think the plot of Deep Impact was more realistic). It’s an option. After NASA launched its asteroid initiative, Parr approached the space agency to explore a new public-private initiative that would bring more of Silicon Valley’s AI talent to bear on the task of protecting Earth from asteroids and other threats. The world’s leading space agency is risk averse compared with today’s technology companies, and that prudent judgment means it rarely even lends its name out. But there is also an argument to be made that the more risky option would be to neglect what new technologies like AI could do to help humans survive a threat of extinction–and possibly even help us become a space-faring species. After some cajoling, iterating, and fundraising, Parr finally got NASA on board and launched FDL in the summer of 2016 at the Mountain View, California-based headquarters of the SETI Institute, the nonprofit organization dedicated to the search for alien intelligence, with a class of 12 and a handful of mentors. Campers apply online–they’re often post-doc researchers in computer, data, and planetary sciences from across academia and the private sector–and those accepted receive a stipend and accommodations. Last summer’s researchers came from seven countries, companies like Apple and Hyperloop, and universities from Oxford to Cornell. With a modest financial commitment from NASA, which provided only stipends for FDL’s U.S.-based participants in 2017, the camp relies heavily on private funding. So far, Parr says, finding companies that want to contribute funds and resources to the accelerator hasn’t been difficult. Its recent partners, who provide funding, cloud resources, and expertise, have included SpaceResources.lu, an initiative launched by the government of Luxembourg, as well as Lockheed Martin, IBM, Intel, Kx, and Nvidia.\\n\\nadvertisement\\n\\nPrivate research into artificial intelligence has skyrocketed over the last few years with tech companies scrambling to hire AI researchers, or, at the very least, add terms like “deep learning” and “neural nets” to their business plans. An AI accelerator with the added benefit of a NASA partnership working on interesting problems attracted some attention. And this, Parr points out, is a pretty interesting problem for just about everyone on Earth. “For the first time in human history,” he says, “we have the ability to do something about an act of god. We don’t have to be victims.” Extinction Level Events The first near-Earth asteroid, or NEA, was identified in 1898, and another 500 or so were found in the succeeding century. (To be considered “near-Earth,” an asteroid or a comet must be on an orbit that brings it no further from Earth than the sun is to the Earth.) With new instruments, astronomers have since discovered a total of more than 17,000 NEAs. About half of the known NEAs have been found since 2010, and another 2,000 or so are discovered every year. One the size of the asteroid that hit Russia comes around about every three decades. In 2005, lawmakers in the U.S. passed a congressional mandate: the country would find 90% of near-Earth objects bigger than 140 meters, or 459 feet, by 2020. At the moment, astronomers know about almost all the near-Earth asteroids that are bigger than one kilometer in size, but it’s commonly thought that anything over 100 meters could be devastating if it were to hit a city. Current estimates put the number of those NEAs at hundreds of thousands to 5 million. Even if the actual number is at the lower end of that estimate, astronomers would have to increase their asteroid discovery rates by around a factor of 50 to get anywhere close to finding them all by 2020. In 2013, NASA launched the Asteroid Grand Challenge to encourage scientists to “find all asteroid threats to human populations and know what to do about them”; in 2016, the U.S. opened a new office dedicated to planetary defense. In addition to a live tally of near-Earth objects, NASA’s Jet Propulsion Laboratory also keeps a running list of asteroids that could possibly hit Earth in the next 100 years. The chances are often small, but some can get alarmingly large–a few have had a 1 in 1,000 chance of hitting Earth. Just this month, astronomers spotted an asteroid thought to be a bit larger than the one that hit Russia, and only a few days before it flew past us at a mere distance of 39,000 miles. By comparison, the moon is 238,900 miles from Earth.\\n\\nadvertisement\\n\\nA comet colliding with Earth is a similar but scarier threat for us. (There is some research that points to a comet and not an asteroid as the real dinosaur killer.) So-called short-period comets are on orbits that take their icy bodies on a short enough trip around the sun that the human race has had the technology to see them at some point on their orbit. Long-period comets–like Hale-Bopp in 1997–are more of a danger: They have orbital periods that take them past the sun every 200 to thousands of years. According to Kepler’s Third Law, celestial bodies on ellipsis move faster as they get closer to the sun. That means those far-away comets that we don’t know much if anything about speed up as they pass Jupiter on their way toward the sun, and the Earth. “They sneak up on us,” says J.L. Galache, an FDL mentor and former consultant to NASA, who is the cofounder and CTO of Aten Engineering, an asteroid prospecting company. Currently scientists can see a long-period comet approaching about six months to a year out. Comets are on average about 60 kilometers wide and, unlike asteroids, can come from any direction. It doesn’t help that these fast comets only reflect 3% to 4% of light, making them darker than coal and harder to track. Lucky for us, they are also pretty unlikely to hit Earth anytime soon. (In November, an 800-foot-long cigar-shaped rock that flew past the sun was initially assumed to be a comet, but was reclassified as an asteroid a week later; finally, the object, nicknamed ‘Oumuamua, was designated as the first of a new class of interstellar object.) A less severe but more probable event is a coronal mass ejection, or CME, a large explosion of plasma and magnetic field from the sun that fires a cloud of magnetized particles across space. When that cloud hits Earth’s magnetic field, it enhances auroras and causes electromagnetic fluctuations that could disrupt the electrical grid, blow transformers, and take out satellites. Pilots would need to land planes without GPS. Astronauts in the International Space Station would be on their own. Depending on the severity, anything plugged in could be fried. So little is known about the effects since the largest one on record, the Carrington Event of 1859, was before the modern technological era. What we do know is that it sent currents through telegraph cables, causing fires and shocking operators. A Carrington-like event today would cause damage costing between $600 billion and $2.6 trillion in America alone, according to a 2013 estimate by British insurance giant Lloyd’s. It could take weeks, if not months to get systems back online. “They call that the trillion-dollar storm,” Parr says. Currently there is no surefire way to predict one.\\n\\nadvertisement\\n\\nDefending Earth With Mountains Of Data If we could predict that kind of storm, we could shut down the electric grid in advance. A better understanding of solar weather could also help reduce astronauts’ radiation exposure. As for dealing with a dangerous near-Earth object, blowing it up or nudging it out of its path is one possibility: Three near-Earth asteroids have already been visited by spacecraft, and NASA is currently planning testing of its first asteroid deflection mission. Another option is paint. Asteroids heat up from the sun and speed up, in a process called the Yarkovsky effect. By landing on an asteroid and painting it, the amount of sun the asteroid absorbs would be affected, thus altering its path. Comets could be dealt with in a similar way. To begin to design these solutions, researchers are turning to mountains of data. But space data is massive, multidimensional, and difficult or impossible to fully parse. Deep learning, on the other hand–part of a larger set of machine learning methods–promises to find patterns in large data sets and make connections that are unseen by humans. Steve Chien, who leads the AI group at NASA’s Jet Propulsion Lab, argues that the data-digesting talents of deep learning aren’t just useful for designing autonomous robot explorers for the oceans believed to be on Jupiter’s moon Europa, for instance, but for finding signs of extraterrestrial life. In December, a Google researcher and University of Texas astronomer announced that, after feeding NASA data into a machine learning algorithm, they had discovered two new planets that astronomers had missed in previous searches. At the outset of FDL’s summer session, participants were split into teams to try to apply deep learning to five challenges: how to model the shapes of NEAs, find water on the moon, learn more about mysterious long-period comets, get better at predicting space weather, and anticipate solar storms. The first week of the program was spent getting the computer scientists acquainted with space and catching planetary scientists up on the tools currently available in the AI field. Speakers at the top of the fields gave presentations, including Francois Chollet, who developed a deep neural network API called Keras that many of the teams were using. By the third week, all of the groups had a plan of attack for their projects and had received the go-ahead from their mentors. The team focused on notoriously hard-to-track long-period comets began by looking at their signatures. Comets have gas and dust tails that form the meteor showers we see on Earth. If you can record images or movies of those “shooting stars,” you can model their orbits and, in theory, obtain the orbit of the comet and where to look for it. Already, projects around the globe are capturing thousands of images of the night sky each night to try to learn more about comets from these meteor showers. Currently, astronomers rely on data collected every night from projects like the EXOSS Citizen Science Project and the Cameras for Allsky Meteor Surveillance (CAMS), but because everything from fireflies to airplanes can look like meteors, all that noise has to be pulled out manually. The long-period comet team wanted to see if they could automate that process to reclaim time for more research. The shape-modeling team, meanwhile, sought ways to use AI to map the surface of asteroids: To blow one up with a nuke or even nudge it away from Earth, scientists need good information on the dimensions and makeup of the thing. But photos of the NEAs are two-dimensional, low resolution, and don’t do a great job at capturing details of an already pretty dull object in the darkness of space.\\n\\nadvertisement\\n\\nThe campers weren’t only thinking about the risks of space. Asteroids are also valuable for future space exploration. The ability to extract water, and potentially minerals, for deep space flight is the only way to travel to Mars and beyond. Water, which is useful for drinking and can be broken down into the components needed for rocket fuel, is pretty common in asteroids. In the best case, asteroids are 20% water. Not having to bring that water from Earth could save a lot of money: NASA estimates carrying water means $25,000 per gallon in fuel costs. If you priced water at $100 a liter, or about $378 a gallon, a 50-meter asteroid has about a billion dollars worth of water. “They can kill us,” says Galache, the former NASA consultant. “But they are a source of resources that we are hopefully going to tap into.” Read more: Forget Coal: Asteroid Mining Is Coming Sooner Than You Think The lunar team was also examining ways to gain access to natural resources, like rare earth metals, silicates, and–most importantly for any future settlements–water. It’s likely that water is at the moon’s poles, where, out of the sun’s rays, it stays cold enough to not melt and float away with the moon’s lower gravity. First, though, you need to get to the poles, and that’s not easy: Communication devices need a direct line of sight to Earth, solar cells need a direct line of sight to the sun, and the terrain needs to be mapped so rovers don’t go careening into a crater. On the last day, everyone gathered at Intel’s headquarters to present their results in front of peers from Google, NASA, Intel, and other research institutions. Equipped with lessons from a media training session earlier that week, the lunar water team opened up the presentations with a joke. “We were wondering the best way to start a talk about lunar volatiles,” said Eleni Bohacek, a planetary scientist in the group. Her teammate, an AI researcher named Timothy Seabrook, had the answer. “First you have to break the ice.”\\n\\nadvertisement\\n\\nTheir team showed that it could identify lunar craters with the deep learning algorithm they had used with 98% accuracy. Since 2011, 77 crater detection algorithms have been published, mostly done by hand and eye; none have come close to 98% accuracy. “Twenty months ago that was kind of impossible,” says Parr. These type of results are tracking closely with the rest of the AI community. “Achieving human level accuracy isn’t surprising anymore,” says Naveen Rao, the vice president and general manager of the Artificial Intelligence Products Group at Intel. “There is a forward progression.” That forward progression means completing tasks with human-level accuracy in a fraction of the time it would take a human. The long-period comet team presented an automated and improved meteor classification with a system that takes a few minutes to get through 200,000 images and boasts a 90% accuracy rate. Their work even found a new meteor shower previously undetected. Someone in the audience actually gasped “wow.” The team working on how to determine the shapes of asteroids showed they were able to speed up pre-processing of their data, figure out an asteroid’s spin state faster, and create synthetic models for their neural net to train on. With the help of a GPU-based cloud platform provided by IBM, the team reduced a modeling process that normally could take a week down to one to two hours. The two teams looking at the sun also reported advances. The solar flare team used computer vision on photos of the sun to build a forecasting model that, though theoretical, marked an improvement over NOAA’s current prediction model: In theory it could warn stakeholders at least an hour out. The space weather team ingested multiple streams of data into their algorithm to determine important predictors of solar events. Scientists have thought that data from stations at the equator were crucial for figuring out solar events, and their algorithm backed that up.\\n\\nadvertisement\\n\\nStill, for many of the planetary scientists, AI and deep learning, with their mysterious, sometimes black-boxed methods for coming up with results, are not ready to replace humans just yet. Burcu Kosar, a member of the space weather team and a physicist at NASA, says that some of the ways the algorithm was able to automate previously tedious data cleaning and labeling was a thrill. Computer scientists might be comfortable with an algorithm with a 98% accuracy rate even if they can’t explain how it got those results. Not for physicists. That discomfort has helped fuel an emerging research area known as explainable artificial intelligence, or XAI. A growing number of AI researchers are warning against relying on systems that come up with outputs that are not easy to interpret. Human errors, biases, or assumptions could get baked in to the cryptic black boxes of AI systems without any way to double-check. NASA–again, with its safety-critical projects that send humans to space with billions of dollars of equipment–is unlikely to trust or rely on artificial intelligence until it’s easy to investigate why it’s coming to decisions. Those concerns are also an argument for FDL’s interdisciplinary, public-private approach, educating a range of researchers on the promises and limits of AI alongside the challenges of space. With FDL getting results within just eight weeks, its proponents insist that more resources and more time could result in some major advancements. And the participation of NASA researchers at FDL makes it a self-reinforcing mission. Parr and his team are now accepting applications for next summer’s program, which will add space debris and astrobiology to FDL’s basket of machine learning problems. Kosar, the NASA physicist who expressed reservations about AI, sees enough potential that she’s now on a committee that is beginning to explore ways to bring more machine learning into the space agency. “It’s a baby step in the giant leap that NASA will have to take,” she says. “This is going to be increasingly accepted in the scientific community.” @jackiesnow')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Love And Compassion Will Save Us When AI Takes Our Jobs', 'As artificial intelligence swiftly creeps into the mainstream, it will take over jobs. A lot of them, mostly those that are repetitive and require not a lot of creativity. The first to go will be things like data analysis and retail operations, followed by more complex endeavors like driving.\\n\\nThe advent of AI is often discussed as something of a selective guillotine falling on our employment ecosystem: Either your job is knocked out, or its spared. But Kai-Fu Lee, a former Google, Apple, and Microsoft employee turned venture capitalist with a speciality in artificial intelligence, sees a more humanistic way to integrate AI into our economies.\\n\\nLee, during the height of his career, put work above all else, he tells the audience at TED 2018 in Vancouver. He nearly left his wife’s side as she was giving birth to their daughter in order to give a presentation on early AI in 1991, and it wasn’t uncommon for him to work what in China is called “9/9/6”–from 9 am to 9 pm, six days a week. As he worked on artificial intelligence systems, he thought of them as ways to streamline routine work to create more room for innovation, and his job, as one of the early developers of these technologies, was to innovate faster and better to support this future.\\n\\nThat changed when Lee was diagnosed with stage-four lymphoma several years ago. He was given just several months to live, and as he underwent chemotherapy, he read books on how people approached the end of life. “Facing death, nobody regretted that they didn’t work hard enough,” Lee says.\\n\\nHe’s now in remission, but his experience with life-threatening illness caused him to think about work in new ways. “What’s more serious than the loss of jobs is the loss of meaning,” Lee says. “Love is what differentiates us from AI.” And when AI begins to replace routine jobs, Lee says we need to focus our energies on creating more jobs that require love and compassion–jobs like caretakers, teachers, social workers, and tour guides, in addition to those jobs like research analyst, artist, scientist, and CEO that we’ve already largely determined will survive the AI takeover due to the necessity of strategizing and creativity, which deep learning can’t mimic.\\n\\nIt’s a nice idea, and certainly true–we’re facing a shortage of teachers, whose jobs will be crucial in bolstering young peoples’ abilities to participate in modern economies, and caretakers for the elderly will become ever more important as lifespans extend. But the jobs that Lee cited during his talk as replacements for those made redundant by AI are notoriously underpaid, and often lack critical benefits like health care (at least in the United States). If we’re going to follow Lee’s recommendation, we should be sure to do so alongside implementing stronger financial securities for people working these jobs–ideally, in the form of a universal basic income and universal benefits.')\n",
      "('5 Ways Artificial Intelligence Can Help Save The Planet', 'If the world’s natural resources are increasingly stressed and depleted, the silver lining may be that we’re becoming better equipped at tracking that destruction and potentially doing something about it. Cheap, widespread sensor networks, the internet of things, magnitude-improvements in computing power, open source algorithms–these all allow us to manage oceans and forests more effectively, if we want the opportunity. Artificial intelligence systems that can sense, think, learn, and act on their own could allow a major upgrade in conservation efforts, in dealing with climate change, and living in a more energy-efficient manner.\\n\\nA report released during the recent Davos World Economic Forum meeting laid more than 80 potential environmental applications for AI, ranging from the mundane to the futuristic. We spoke with Celine Herweijer, a partner at consultants PwC and one of the authors of the report. She argues that AI is now going mainstream: Algorithms and supercomputers that once were limited to specialist researchers at universities and government labs are now open to startups and everyday corporations. New ways of managing ecologically relevant systems are opening up as never before.\\n\\nAutonomous energy and water networks\\n\\nSolar, wind, and other renewables have the advantage of being carbon-free and ubiquitous. They can be situated in villages and towns and out-of-the-way places, bringing energy closer to everyone who needs it. The challenge is stitching these disparate sources together into a coherent, functional whole. That’s where autonomous systems come in. They can deal with the intermittency of renewables and react to the ebb and flow: when one source of power is coming online or going down, or when one user is ramping up demand and another is clocking off for the night. AI systems are flexible and they can do more work, and be in more places, than human grid managers.\\n\\n“When you have a complex system with so many sources of renewables, you need them to talk to one another, so you can do storage and optimize the load,” Herweijer tells me. “That can’t happen without artificial intelligence enabling all these new sources to come together. They will enable these future systems where we have peer-to-peer energy trading and community exchange. They are what we need for a decentralized, autonomous grid.”\\n\\nSimilarly, AI will allow for a more decentralized water system, driven by sensors and new technologies like blockchain, Herweijer says. Smart contracts–legal arrangements automated with code–can enable swift trading of assets, including water rights. “Blockchain is vital for recording provenance, then you can have smart contracts and have people trading between parts of the decentralized network,” Herweijer says. “Utilities of the future, whether water or energy, will be more decentralized because that improves productivity.” The Department of Energy has some early-stage AI-based grid systems in development.\\n\\nOpening up climate modeling\\n\\nModeling future weather events and climate patterns means processing complicated physical equations, like the fluid dynamics of the atmosphere and oceans. Climate scientists have relied on supercomputers, like the one at the Argonne National Laboratory, outside Chicago, to do their calculations. But there are only a few dozen true supercomputers around the world, meaning that access is limited: Many other scientific fields also require big computational capacity.\\n\\nDeep-learning techniques, inspired by the way the human brain processes information, incorporate some of the complexity of the real world in climate modeling, allowing computers to run faster and do more calculations within a given period. “We’ll do simulations and modeling on home computers than we do now on supercomputers,” Herweijer says. “We can model small-scale features like wind storms that we struggled with in the past. Once you put AI in the system, you’ve got more people doing simulations and they’re doing it quickly. Forecasting of weather and climate impacts is going to get better rapidly over the next 10 years.”')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Bring Deep Learning Algorithms To Your Security Cameras', 'AI is quickly revolutionizing the security camera industry. Several manufacturers sell cameras which use deep learning to detect cars, people, and other events. These smart cameras are generally expensive though, compared to their “dumb” counterparts. [Martin] was able to bring these detection features to a standard camera with a Raspberry Pi, and a bit of ingenuity.\\n\\n[Martin’s] goal was to capture events of interest, such as a person on screen, or a car in the driveway. The data for the events would then be published to an MQTT topic, along with some metadata such as confidence level. OpenCV is generally how these pipelines start, but [Martin’s] camera wouldn’t send RTSP images over TCP the way OpenCV requires, only RTSP over UDP. To solve this, Martin captures the video stream with FFmpeg. The deep learning AI magic is handled by the darkflow library, which is itself based upon Google’s Tensorflow.\\n\\nMartin tested out his recognition system with some cheap Chinese PTZ cameras, and the processing running on a remote Raspberry Pi. So far the results have been good. The system is able to recognize people, animals, and cars pulling in the driveway. His code is available on GitHub if you want to give it a spin yourself!')\n",
      "('How Companies Like Amazon and Google Turn Data into a Competitive Advantage - and How You Can Too', 'What\\'s the key to Amazon and Google\\'s revenue success? Everyone knows the answer: Data.\\n\\nThe reason for Facebook\\'s social media empire and Spotify\\'s upending of the music streaming business? Data.\\n\\nAll of these companies have managed to leverage the vast amounts of information they get from their multitude of users - whether it be their search habits, the posts they share, the products they buy, or the music they listen to - into major revenue streams. It\\'s not just the fact that these companies have been able to gather data on millions (or billions, in the case of some of these companies); it\\'s that those companies have managed to effectively utilize that data to better understand and market to their users. All of these companies are using artificial intelligence (or, more accurately, deep learning) to do this.\\n\\nOf course, it\\'s important to note that you don\\'t have to be a dominating enterprise like Amazon or Google to turn data into a competitive advantage. As artificial intelligence becomes increasingly advanced and more widely adopted, we\\'ll start to see a lot of companies - big and small - turning to AI in order to come up with better data strategies and win customer adoption, and to better compete against their competition.\\n\\nThe key to beating your competition, according to Jeremy Fain, of pioneering neural network technology Cognitiv, is having better data - not necessarily more of it, but the data that your competitors do not have. In theory, every brand is capable of developing their own unique data assets, because every brand has to be slightly different to compete. This means that a brand\\'s customers are, at the very least, slightly different from those of their competition, which means that they have a unique angle that they can utilize. Every piece of data you get on your customer or potential customer is therefore another piece of information you can use to craft an effective marketing or advertising strategy.\\n\\nIn order to use this information effectively, you must first decide what your goal is. Are you looking for more sales? Are you trying to achieve higher foot traffic in stores? Is your goal to have higher market awareness of your product? Once you\\'ve done that, you can look at the data to see if it is in the right format for use with deep learning. This is something that is hard to explain simply, but fundamentally, data has to be in a disaggregated state - that is to say, it has to come from multiple sources so that more in-depth conclusions can be drawn from it. That means you don\\'t really need to know only how many people visited a store, but instead when exactly each person visited. You no longer need to look at just how many sales you made, but also what each sale was and to whom. To get one step further, you must identify what touchpoints you had with a customer before they transacted with you, what ads they were shown, and when and where all the interactions occurred. Don\\'t gather this type of data yet? Well, that\\'s your first homework assignment.\\n\\nThis means you will have a lot more data to store than you have been used to, but the good news is that storage is cheap. Plus, without that information, you will not be able to take advantage of the power of deep learning and compete in this new world.\\n\\nA 2016 study of Fortune 1000 executives uncovered that only 48.4% of those surveyed reported measurable results as a result of their data initiatives - but 80.7% felt the efforts were a success and essential. This means everyone knows they have to do better and do not see an alternative, but something more is needed before measurable benefits are achieved across the board.\\n\\nMost data initiatives miss one simple ingredient: deep learning. It\\'s an oft-misunderstood topic, defined by Cognitiv\\'s Fain as \"a more advanced type of machine learning that\\'s capable of generating human-like insight.\" Deep learning\\'s ability to get results from big data is now essential not only for competitive reasons, but also to make previous investments in big data pay off. Sadly, 39.3% of those surveyed still said that their organizations were lacking an enterprise Big Data strategy, or were otherwise unaware if one existed - these companies have a long hill to climb. In fact, most data-driven professionals has a steep climb ahead of us. \"Part of the challenge is that the industry itself is immature around data. We\\'ll look back 15 years from now at what we\\'re doing and say, \\'Wasn\\'t that cute?\\',\" said one Director of Programmatic Media for a global media agency interviewed for a recent Winterberry Group IAB study.\\n\\nBig data, data analytics, and artificial intelligence go very much hand-in-hand. Artificial intelligence - and, by extension, deep learning - requires data, reams and reams of it. The only way that deep learning can be effective for your organization is if you have a steady stream of information to feed it.\" Armed with this information, deep learning and neural networks can create algorithms and strategies that are unique to your brand - thus ensuring that the brand remains competitive and innovative. As Fain points out, \"The ability to more fully describe and understand a consumer\\'s behavior is more complete than ever before, and that kind of data will make AI marketing tools even more effective over the next few years.\"\\n\\nAt this point, all brands need a strong data strategy. Just look at brands like Macy\\'s and J.C. Penney\\'s today, who are struggling as a result of the data-centric approaches of e-commerce giants like Amazon and eBay. Having the right strategy and, just as importantly, the right tools to get the most out of your data, is what will help keep your company competitive, and successful.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('5 Artificial Intelligence Companies to Watch in 2018', 'Artificial intelligence hit some key milestones in 2017. At Facebook, chatbots were able to negotiate as well as their human counterparts. A poker-playing system designed by Carnegie Mellon professors mopped the floor with live opponents. There were even some potentially life-saving breakthroughs, like the machine vision system that can determine whether a mole is cancerous with more than 90 percent accuracy--beating out a group of dermatologists.\\n\\nFrom agriculture to medicine and beyond, plenty of startups are using AI in innovative ways. Here are five companies you should expect big things from in 2018.\\n\\n1. SoundHound\\n\\nSoundHound has been around for 13 years, and has spent that time trying to build the most powerful voice assistant ever. The startup began by creating a Shazam-like song recognition app called Midomi; now, the newly released Hound app is capable of answering complex voice prompts like, \"Show me all below-average-priced restaurants within a five-mile radius that are open past 10 p.m. but don\\'t include Chinese or pizza places,\" or \"What\\'s the weather like in the capital of the biggest state in the U.S.?\"\\n\\nInstead of converting language into text, like most virtual assistants do, the app\\'s AI combines voice recognition and language understanding into one step, which helps speed up results and cut down on errors. Investors are excited: After raising $40 million over its first decade-plus, the company closed a $75 million round in 2017 that included Samsung and Kleiner Perkins. Still, SoundHound faces stiff competition from AI behemoths like Google Assistant and Microsoft\\'s Cortana.\\n\\n2. Freenome\\n\\nLiquid biopsies have been all the rage in the medical world, with a growing number of well-funded startups working on finding a way to detect cancer using blood samples. Freenome is one such company, and its goals are loftier than most. While other companies focus on detecting the existence of cancer, Freenome claims its AI can use blood to identify where the bad tissue is--or is likely to be--located, as well as whether it may be malignant or benign. It studies DNA in the blood, detecting biological signs of cancer and then making predictions about where the cancer is and what kind of treatments would work best.\\n\\nThe company has performed thousands of tests and is working on detecting prostate, breast, colon, and lung cancers. Thanks to machine learning, its software becomes better at screening over time. Before it can do much else, Freenome must complete the clinical trials necessary to secure regulatory approval and roll out to the public. The startup closed a $72 million Series A round last year to accomplish those goals.\\n\\n3. Bowery Farming\\n\\nAgriculture might look a lot different in the future--just ask Bowery Farming, a vertical farming startup that grows its crops indoors. Sensors at each facility collect data points regarding factors like humidity, temperature, and light. Cameras observe minor changes as the plants grow, and thanks to machine vision, the company\\'s AI can quickly determine and adjust to the ideal conditions for a particular batch--thus optimizing for yield as well as taste, texture, and color. By output, the company claims a Bowery farm is more than 100 times more efficient per square foot than a traditional farm. The startup is entering a space currently occupied by companies like AeroFarms, which takes less of an AI-focused approach but already has several large vertical farms throughout the northeastern U.S. Bowery emerged from stealth and announced a $20 million funding round in 2017. Its products, which don\\'t require any pesticides or chemicals, are already in stores like Whole Foods, with more likely to follow.\\n\\n4. Arterys\\n\\nExamining MRI scans is tedious work and prone to human error. Arterys is leading the charge on handing that task over to artificial intelligence. Thanks to this startup\\'s tech, diagnoses that usually take humans 45 minutes can now be performed in 15 seconds. Its system uses deep learning to compare new MRI scans with those it\\'s already examined. All the information is stored in a central cloud, which gives the system an ever-growing data set and allows it to deliver more accurate readings over time. In 2017, Arterys\\'s\\n\\ncardiac MRI technology, which examines the heart and blood flow through its ventricles, became the first cloud-based AI platform to be FDA-approved for use in clinical settings. The company, which also closed a $30 million Series B last year, is currently working on getting clearance for similar technology that focuses on the lungs and liver.\\n\\n5. Spoke\\n\\nThis startup\\'s AI aims to know more about your company than you do. Not sure what HR\\'s policy is when it comes to sick day rollovers? Ask its chatbot. There\\'s a light out in the conference room? Let it know, and it will direct a request to the right person. Spoke can answer employee inquiries via an app, in Slack, or by way of email or text, with the idea being that it will save your employees the time they\\'d normally spend asking questions or fielding them and keep your office running smoothly. The system\\'s machine learning component allows it to process and learn information as it converses with people, storing away new answers for the future.\\n\\nFounded in 2016 by three ex-Googlers, the startup closed a $20 million round in November. It\\'s begun collecting clients, and its product is set to launch in early 2018, potentially making the average workday that much smoother for businesses everywhere. It will be up against other workplace productivity tools, including Slack itself, which is building its own fleet of inquiry-answering chatbots.\\n\\nCorrection: An earlier version of this article incorrectly stated the capabilities of Freenome\\'s liquid biopsy tests. The tests are designed to detect biological signs of cancer, not mutations. Freenome has performed thousands of tests, and is still working on additional testing methods for different types of cancers. The company raised a $72 million A round last year.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Science Reveals Where The Best Entrepreneurs Come From (And It's Not Silicon Valley)\", 'Entrepreneurship isn\\'t for anyone and everyone. There\\'s a only a select few who actually possess the drive to become a founder. Let alone succeed as one.\\n\\nThis isn\\'t mere hyperbole. There\\'s mounting scientific evidence to show that there is measurable set of personality traits that lead a person to enter into entrepreneurship--a defined personality profile that predicts the likelihood that a person will: a) become an entrepreneur and b) succeed as an entrepreneur (in the long-run).\\n\\nIt\\'s called the entrepreneurship personality profile (EPP).\\n\\nResearchers have boiled down these core personality traits by calling on the five facets of human personality: openness, conscientiousness, extraversion, agreeableness, and neuroticism, or OCEAN as it\\'s often called.\\n\\nTogether, these five traits form the genetically inherited foundation of our personality. Deeply rooted within us, the traits predict many of our thoughts, emotions, and behaviors, including, in this case, a particular penchant for creating a business.\\n\\nEntrepreneurship is, in fact, hardwired in the brain.\\n\\nThe research supports it\\n\\nIt\\'s a bold claim, but one that continues to garner empirical support. And now researchers are uncovering interesting patterns on a large scale. For instance, the science is showing that there are specific areas in the U.S. where there\\'s a greater proportion of these hardwired entrepreneurs.\\n\\nIn one study, researchers predicted there would be broad patterns and clustering of the EPP at state level. In other words, by looking at regional distributions of personalities, the scientists set out to rank each state, and place it on a scale ranging from \"most hardwired\" to \"least hardwired\" for entrepreneurship.\\n\\nEach state was given a unique score for EPP fit. The higher the score, the better the fit, which meant that the highest scores were indicative of a greater proportion of hardwired founders living in that state.\\n\\nWhere are all the founders?\\n\\nThe findings show that entrepreneurial personality is indeed geographically clustered. The EPP is highest in the West, including Colorado (no. 1), Utah, South Dakota, and Nevada. It was lowest in the South East, including Mississippi (no. 51), Kentucky, Mississippi, and West Virginia. A quick look at a map would show dark shaded states to the West (highest EPP scores) and light shaded states to the East (lowest EPP scores).\\n\\n(Quick aside: Perhaps surprisingly, California failed to make it into top 10, coming in at 15. And the San Francisco area ranked 8th across the 15 largest metropolitan areas. Eat your heart out, Silicon Valley).\\n\\nThe natural follow-up question is, what is it about this specific clustering and most notably, the observed split between East and West?\\n\\nWell, the emergence of entrepreneurial personality in the West is believed to be a result of historical migration patterns in early America.\\n\\nThe \"entrepreneurial\" early settlers were the ones who risked the move and ventured from East to West. Our West-bound ancestors would have possessed strong levels of the EPP, and ultimately would have genetically passed down the entrepreneurial-prone personality to today\\'s Western living Americans.\\n\\nNext, the researchers corroborated their findings by taking the EPP rankings and mapping them onto existing nationwide data showing actual entrepreneurial activity. The Kauffman index, as it\\'s called, provides a reliable set of indicators that measure entrepreneurship activity at state and metropolitan levels.\\n\\nThe state-specific Kauffman rankings exhibit a startling overlap between the map data -- clustering of light and dark regions that, once again, highlight an East versus West split. As for the specific data, state-level correlations were robust and strong: In a given state, the more \"hardwired entrepreneurs\" there are, the greater the levels of startup activity. These results hold even after controlling for various socioeconomic variables including GDP, growth, and unemployment rate.\\n\\nFinally, the same general patterns were found in Germany and the U.K. Like the U.S., there was meaningful geographic clustering in people\\'s EPP across the two national regions. Berlin took the top prize with the overall highest ranking score. No surprise as the German capital is being regarded as Europe\\'s leading tech hub and the next international Silicon Valley.\\n\\nWhat does this all mean?\\n\\nIn showing that specific geographic regions have their own \"founder personalities\", this research will prove useful not just for individuals, but for large scale organizations including VC firms, academic institutions, legislative and funding bodies, etc.\\n\\nConsider, for instance, that with future advances in our data analytics techniques (including deep learning and AI predictive classifier algorithms), it will become increasingly easier for us to locate the hardwired entrepreneurs and to capitalize on their clustered living.\\n\\nAnd for all you young Mississippian entrepreneurs, safe to say that your next move should be to head West. May I suggest the No.1-ranked Colorado? Its ecosystem is uniquely entrepreneurial. Oh, and the mountains are gorgeous, too.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('AI: The Great Accelerator of the Human Innovation Curve', 'Throughout the ages, human innovation has been accelerating at a mind-boggling rate. Consider that 1 million years elapsed between the control of fire and the invention of the wheel, but just 5,400 more years until the creation of the Gutenberg press--and a mere 455 additional years before the development of the light bulb.\\n\\nWhen plotted on a chart to illustrate the human innovation curve, it is clear that there is only one word that can describe the increase in the pace of progress: exponential. However, the innovation curve now is shifting into even higher gear with the proliferation of artificial intelligence (AI).\\n\\nAI grew exponentially in 2017, with no signs of stopping in 2018. AI fundamentally changes the equation of innovation, adding a new variable that dramatically accelerates the rate of advancement. For the first time in history, the invention process is not entirely dependent on human intellect. Machines are now augmenting and will eventually supplant human brainpower.\\n\\nAlthough AI is still in its early stages, the arrival of a new approach --called \"conducted learning\"--will expedite the rate of overcoming current limitations and is set to affect the speed of both AI and the human innovation curve.\\n\\nNarrow-minded AI\\n\\nAI algorithms now are reaching, and even exceeding, human capabilities in areas such as strategic game playing and image classification. However, these algorithms fall under the category of artificial narrow intelligence (ANI) since they are limited to excelling in narrowly defined tasks.\\n\\nWe can train an AI algorithm to recognize the shape of a gun, for example, and it will be able to detect the image faster and better than humans. However, due to this narrowness limitation, in a real-world application, such as in a TSA scanning, this effective scanning method will be restricted only to the specific gun models on which the algorithm was trained.\\n\\nConsequently, we still have a way to go until we reach artificial general intelligence (AGI), which will be more akin to humans and present capabilities similar to what we see in sci-fi movies.\\n\\nThe maestro of AI\\n\\nConducted Learning presents a promising solution towards achieving AGI by leveraging the combined power of separate ANI engines. Conducted learning enables running several cognitive engines in concert, picking the best engine or engines to perform the task, similarly to an orchestral performance. This results in a more accurate outcome than what can be obtained from any single network, while cutting down on computational costs and speed.\\n\\nLike other deep learning models, conducted learning initially formats data, preprocesses it, and generates input. The magic happens during the next stage, when the technology acts like the orchestra\\'s conductor, instructing each cognitive engine when to play its part in the complete composition.\\n\\nThe conducted learning model extracts the accurate parts of the output and recycles the remnant through a process of transformation and rerouting to the relevant engine or engines. By using multiple cognitive engines simultaneously, the algorithm continually learns and improves its capabilities, building a more effective topography to complete the task. This dramatically improves the accuracy and performance.\\n\\nClass consciousness\\n\\nConducted learning facilitates overcoming the limitations of narrowness through two methods: intra-class learning and interclass learning. Intra-class learning uses multiple cognitive engines in the same class (e.g. translation). Interclass learning employs several cognitive engines across different classes (e.g. translation and facial recognition).\\n\\nTaking transcription as an example, with intra-class learning, conducted learning enables the transcription of a soccer match of the English League by first transcribing the words in the English language. It then fills in the gaps of running a transcription engine that is trained on sports terms. The next engine will cover words pronounced in a heavy British accent to catch unclear words that weren\\'t detected in high confidence, etc. All this is done in milliseconds.\\n\\nWith inter-class learning, if the auditory speech recognition engines cannot catch the accurate transcription of the names of the players, then visually trained engines, which use face recognition to match the player\\'s face or \"read\" their names off their t-shirts, will be activated to accurately complete the task.\\n\\nBending the curve\\n\\nThe implications of conducted learning go beyond achieving greater accuracy. It is a big leap for machines, but even more importantly, for AI\\'s ability to teach itself by one engine or class of engines informing the other about the data and completing the task.\\n\\nThe human innovation curve has now changed with the addition of AI to the calculation. AI will become the dominant factor, dramatically outpacing humans\\' capability to invent on their own.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Why Big Tech Firms Are Ignoring Blockchain (For Now)', 'If blockchain was truly revolutionary, why wouldn\\'t top tech firms like Facebook, Amazon, Google, and Apple be doing more with it? originally appeared on Quora - the place to gain and share knowledge, empowering people to learn from others and better understand the world.\\n\\nAnswer by Gaurav Mokhasi, Tech Product Management at Visa, on Quora:\\n\\nThis is a quasi-acid test that blockchain fails, at least so far.\\n\\nAt the NASSCOM Product Conclave in Bangalore recently, Future Group CEO Kishore Biyani was asked what the one thing is that keeps him up at night. He responded that it was \"the fear of missing a trend.\" I suspect the leadership at great technology companies are similarly vigilant; the last thing they want is some new kid on the block disrupting their business models.\\n\\nIn the last 20 years, it\\'s hard to think of a single revolutionary technology that Amazon, Google, Apple, or Facebook did not experiment with. Cloud technology, artificial intelligence, big data, voice assistants, augmented reality, self driving cars, machine/deep learning ... all of these have been embraced (even pioneered) by these companies. But when it comes to blockchain, these firms don\\'t seem fazed by (or bothered with) it.\\n\\nI don\\'t buy the argument that blockchain is just not relevant to these firms, because it\\'s not difficult to imagine scenarios where it could affect these companies. This article cites a few examples.\\n\\n\"Yes, I could absolutely imagine a decentralized Amazon,\" Lubin replied. \"We\\'ve seen the pieces. They\\'re not all connected to one another. They\\'re not all but out or remotely mature, but I could imagine an open platform of many different actors with different roles.\" The same could be done with Facebook, said Lubin, who is also founder of ConsenSys, a Brooklyn-based studio that develops Ethereum-based projects. \"We could stand up a decentralized platform that offers same services.\"\\n\\nThe silence of these companies with respect to blockchain is therefore conspicuous for sure.\\n\\nThe esoteric nature of cryptocurrencies and blockchain technology makes it difficult for regular people to separate the wheat from the chaff. However, Google, Apple, Amazon and Facebook have consistently attracted the best engineering talent and researchers from the top universities in the world. These folks understand Computer Science better than most, and if blockchain did in fact have the technical merits that people claim it does, it\\'s unlikely that the technologists at these companies would seemingly care so little about it.\\n\\nIf you look back at the last five years, you can classify most people promulgating the values of blockchain technology into two buckets:\\n\\nPeople with vested interests -- those who are running or are invested in related startups, offering ICOs, etc... This group is typically experimenting with public blockchains. Big financial institutions -- theirs was an understandable reactionary measure to check whether their business was under threat, and to ensure that they don\\'t look like luddites. This group championed something called private or federated blockchains.\\n\\nIf you look closely at what private blockchains are, it\\'s not apparent what\\'s technically novel about them. Princeton University\\'s Professor Arvind Narayanan, who offers what is perhaps the only reliable MOOC in this space, published a blog post that goes as far as saying that \"Private blockchain\" is just a confusing name for a shared database.\\n\\nEven the decentralization promised by public blockchains, while utopian in theory, is not without its fair share of problems. Firstly, there\\'s the issue of performance. Bitcoin, which uses blockchain in its pure form, has an abysmal throughput of 3-7 transactions per second. Compare this to a traditional system like Visa which can easily process over 25000 transactions per second [1]. Secondly, blockchain is still a solution in search of a problem. It doesn\\'t have a single application so far that\\'s either gone past the proof-of-concept phase or where it\\'s been definitively proven that the proposed blockchain-based solution performs better than the incumbent technology.\\n\\nTherefore, given that companies like Facebook, Amazon, Google and Apple are not doing much with blockchain, even in the face of ever-increasing frenzy surrounding this technology, one could not be blamed for doubting blockchain\\'s potential as a game-changing paradigm.\\n\\nDisclaimer: I make the statements above in a personal capacity. They should not be seen as a reflection of my employer\\'s view on the topic.\\n\\nFootnotes\\n\\n[1] Bitcoin and Ethereum vs Visa and PayPal - Transactions per second - Altcoin Today\\n\\nThis question originally appeared on Quora - the place to gain and share knowledge, empowering people to learn from others and better understand the world. You can follow Quora on Twitter, Facebook, and Google+. More questions:')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"The Scariest Thing About Facebook Isn't Even How It Treats Your Data\", 'Mark Zuckerberg\\'s marathon date with Congress is over. It might as well never have happened.\\n\\nFor two days, Facebook\\'s CEO stuck to his bland talking points, claimed a degree of ignorance about his company\\'s operations that strains credulity, and made sure to preface every answer with \"Senator\" or \"Congresswoman.\" And for two days, no one landed a punch on him, for all the predictable reasons.\\n\\nPoliticians eager to appear tough cut off Zuckerberg\\'s answers, gifting him the opportunity to keep his mouth shut. They asked questions that felt like gotchas but weren\\'t, like Senator Dick Durbin\\'s grandstanding demand for the name of the hotel Zuckerberg was staying in. They betrayed a deep ignorance of how Facebook, and the internet in general, works, inviting Zuckerberg to deploy set-piece responses from deep within his comfort zone. (\"Senator, actually, Facebook doesn\\'t sell anyone\\'s data ... \") And that\\'s when they weren\\'t breaking character to thank Facebook for making so much money off its users and deplore their own jobs as regulators.\\n\\nBut there\\'s a deeper reason no one on Capitol Hill was able to call out Facebook in a way that stuck. No one there truly understands what makes Facebook, among all the big tech companies, uniquely dangerous. Almost no one anywhere does.\\n\\nUnderstandably, in the wake of revelations that a shady firm working for Donald Trump\\'s presidential campaign got backdoor access to as many as 87 million Facebook profiles, much of the questioning focused on Facebook\\'s privacy policies. But Facebook isn\\'t alone in vacuuming up all the data on consumers it can legally get its hands on.\\n\\nGoogle amasses intel about its users at least as sensitive as anything Facebook knows, and considerably more of it. (When someone wants to know if that weird sore might be from an STD, that\\'s a query for a search window, not a status update.) Apple can guess where you\\'re going before you even get in your car by tracking your iPhone\\'s location. Amazon can literally hear what\\'s going on in your living room--or your bedroom, if you have an Echo smart speaker in there.\\n\\nWhat sets Facebook apart isn\\'t the data it collects. It\\'s that it collects all this data on its users while simultaneously seeking to influence their behavior. No other company is doing both of these things on a scale remotely close to Facebook, and mixing the two of them is like combining nitric acid and glycerol: The result is something hard to handle safely.\\n\\nEvery product changes its users\\' behavior in some way; that\\'s practically what it means to be a product. But while Facebook is very good at engineering behaviors that are good for its business--adding more friends, sharing more information with them, spending more time interacting with their content--it doesn\\'t stop there. The company has also induced users to vote who otherwise wouldn\\'t have. It got people to become organ donors. Now it\\'s trying to get people to become more active in their local organizations and support their local newspapers.\\n\\nNone of these things sounds terribly sinister. Rather, they\\'re expressions of a vaguely utopian worldview that infects much of Zuckerberg\\'s thinking. Because he thinks \"human nature is basically positive,\" if more people express their ideas or vote or volunteer, the results will ipso facto be basically positive. But Zuckerberg himself has said being too \"focused on the positive\" for the first 10 years of Facebook\\'s existence blinded the company to much of the abuse it was enabling as well as to emergent effects of social media like hyper-polarization.\\n\\nAnd Facebook doesn\\'t even stop at modifying behavior. It tinkers with users\\' thoughts and emotions as well. A notorious \"emotional manipulation\" study showed that the company could make users feel better or worse by altering the contents of their News Feeds. Recently, Facebook announced it will tweak its algorithms to encourage more \"meaningful interactions\" between friends because those cause its users to experience positive emotions, whereas passive content consumption leaves them feeling worse afterward.\\n\\nFrancois Chollet, a computer scientist who works on deep learning at Google, believes Facebook\\'s ability to both measure and alter its users\\' behavior is dangerous, raising the specter of \"mass population control.\" That\\'s because machine learning algorithms--an area where both Google and Facebook have invested heavily, and which Facebook uses to fine-tune the content of each user\\'s News Feed--are highly effective at connecting inputs and outputs in a recursive optimization loop. \"The human mind is a static, vulnerable system that will come increasingly under attack from ever-smarter A.I. algorithms that will simultaneously have a complete view of everything we do and believe, and complete control of the information we consume,\" Chollet tweeted.\\n\\nGiven Zuckerberg\\'s fuzzy good intentions, there\\'s no reason to think he\\'d ever dream of using power like this for anything other than good. That\\'s scary enough. We\\'ve already seen how a technology intended to make the world \"more open and connected\" instead divided it up into hostile tribes. Whatever laws Congress does or doesn\\'t pass in the wake of Zuckerberg\\'s testimony, he\\'ll remain subject to the law of unintended consequences. The only way to diminish the danger is to diminish the power.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"The Best Way to Master Any Subject: Don't Look for Shortcuts\", 'What can I learn in one minute that will be useful for the rest of my life? originally appeared on Quora - the place to gain and share knowledge, empowering people to learn from others and better understand the world.\\n\\nAnswer by Praveen Tipirneni, CEO of Morphic Therapeutic Inc., on Quora:\\n\\nSo many articles and ads claim to teach people to do something as quickly and painlessly as possible. Hacks, shortcuts, quick solutions.\\n\\nIt reminds me of a counterfeit study done by Dan Ariely, Francesca Gino, and Michael Norton.\\n\\nIn the study, two groups of women were given pairs of designer sunglasses. They took the sunglasses from boxes labeled \"authentic\" or \"counterfeit.\" But in reality, all the sunglasses were authentic.\\n\\nThe two groups of women were then asked to wear the glasses while answering simple math questions. They would be paid for each correct answer. But they were also given the opportunity to cheat when checking their own answers.\\n\\nThe results: 30% of women wearing the \"authentic\" sunglasses cheated. And amazingly, 71% of women wearing what they believed were \"counterfeit\" pairs cheated.\\n\\nFrancesca Gino summed it up perfectly: \"When one feels like a fake, he or she is likely to behave like a fake.\"\\n\\nThis concept is similar to a Richard Feynman quote: \"The first principle is that you must not fool yourself and you are the easiest person to fool.\"\\n\\nThere are always consequences to shortcuts even if they are not visibly apparent. The path to expertise takes patience. Unfortunately, our society encourages us to take the hack road.\\n\\nOur Shortcut Society\\n\\nHow to get a complete workout in 1 minute.\\n\\nConquer your fear and become an entrepreneur. (As if bravery is all that\\'s required!)\\n\\nInstantly become more productive with this one hack.\\n\\nArticles like this are everywhere because our society values speed and convenience. It revels over the 25-year-old millionaire. It worships the altar of convenience and encourages us to hack our way to success in 48-hour increments rather than taking a longer, hard-earned route.\\n\\nJust as the fake sunglasses take a hidden psychological toll on the wearer, there\\'s a cost to taking shortcuts--one you don\\'t fully understand until much later. Always be wary of people who claim to have an exact prescription for doing something hard.\\n\\nI\\'m not saying you should never use hacks, or that they have no benefit. They\\'re only beneficial in a very narrow type of situation.\\n\\nFor example, a hack that claims you\\'ll learn how to play the piano in a month is offering a very weak definition of proficiency.\\n\\nIf you\\'re trying to learn one specific song for your Christmas party, you might be able to play it adequately at the end of the month. But if someone starts singing a new song after you\\'re done playing, you won\\'t be able to improvise along with them. (For those older in age, replace piano with guitar, the middle-age crisis instrument of choice).\\n\\nHacks are not meant to replace a deep learning experience. You\\'ll likely abandon them as you develop a deeper understanding of what you\\'re doing. Hacks are formulaic and may solve simple problems, but complexity requires a holistic and multidimensional understanding.\\n\\nYou have to look at your subject matter from many different dimensions. You have to look at modular components and see how they interact with each other and the system as a whole. Step by diligent step you increasingly understand reality in all its complexity.\\n\\nThe Benefits Of Learning\\n\\nWhat\\'s the rush to learn something new?\\n\\nYou have a long life. You\\'re going to work for a long time, which is perfect because it takes time to really become an expert at something. There will be ups and downs, surprises, scars. There\\'s nothing wrong with making mistakes and learning from them. Those experiences make you who you are.\\n\\nI had the chance to reflect on my own experiences recently. I was with a handful of my MIT undergraduate friends, and we were telling story after story of nights and weekends trying to conquer impossible problem sets. I remember taking 6.111 (Digital Design) with my good friend Chris Mayer. Once, we were so tired that our fraternity brothers put pots on our head and hit them to try to wake us up for the Final Project Review. Neither of us woke up or remember the pots at all.\\n\\nBut we do remember the class and what we learned. No weekend working on Arduino could replace my in-depth 6.111 experience.\\n\\nBy taking shortcuts, you lose out on formative moments.\\n\\nDon\\'t underestimate the discipline you acquire when you put yourself through challenging situations. The failures, the suffering, the winding road--there\\'s a long-term benefit in those things that you don\\'t get when you take a shortcut.\\n\\nTaking the time to learn something and master it provides you with a whole repertoire of skills and approaches. These allow you to recognize and adapt to new situations, and gain confidence in your abilities.\\n\\nIf all you know are hacks, uncertain or unexpected situations overwhelm you. You have no foundation to rely on--and this limited toolkit can only get you so far.\\n\\nFrom Fake To Expert\\n\\nNo one can pretend to be an expert for too long. Your experience will always show through. I know, because I was afraid of being considered a fake at one point.\\n\\nI was already a fully trained and licensed medical doctor when I started business school. After getting my MBA, I could have been making a substantial salary given my credentials and experience. But I did the opposite. I took a large step backwards, almost all the way to an entry-level position.\\n\\nIt seemed crazy to some people, but it was a conscious decision on my part. I had taken an unorthodox route to business school, and I didn\\'t want to skip any steps or miss the fundamentals others had learned before school.\\n\\nI moved up quickly. But by going through that process, I had the foundation and experience I needed to feel confident in any situation.\\n\\nThere\\'s plenty of societal pressure to race your career forward as fast as possible, but take a step back and give yourself time to understand your field in-depth. You will have far less competition and much more confidence.\\n\\nTake the hard road. It was worth it.\\n\\nThis question originally appeared on Quora - the place to gain and share knowledge, empowering people to learn from others and better understand the world. You can follow Quora on Twitter, Facebook, and Google+. More questions:')\n",
      "('Learn Python for a price that works for you', \"Learn Python for a price that works for you\\n\\nHailed for its versatility and user-friendly design, Python is one of the best first languages for aspiring programmers to learn. However, not all of us have a natural affinity for programming, but you can get the training you need without breaking the bank thanks to the Pay What You Want: Absolute Python Bundle.\\n\\nHere's how the deal works: Simply pay what you want, and you'll unlock one of the collection's five courses. Beat the average price paid, and you'll get the remaining four at no extra charge. Plus, if you beat the top price, you'll be entered into a major giveaway and earn a spot on the leaderboard.\\n\\nFeaturing nearly 60 hours of training, this collection will take you from beginner to expert in Python programming. You'll follow along with the collection's example-based curriculum, taking on core concepts, like input, functions, lists, and loops. Then, you'll dive into more advanced ideas, like automation framework design and creating deep learning models.\\n\\nYou can catalyze your Python education for a price you choose with the Pay What You Want: Absolute Python Bundle.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The Most Valuable Artificial Intelligence Startup in the World Just Raised $600 Million', 'The artificial intelligence industry just recorded its largest funding round in history.\\n\\nSenseTime, a Chinese startup that specializes in facial recognition, announced Monday that it has raised $600 million in Series C funding--making it the most highly valued artificial intelligence startup in the world. The company reportedly has a valuation higher than $4 billion, according to Bloomberg. Chinese e-commerce giant Alibaba Group led the round.\\n\\nSenseTime provides facial-recognition technology used in various fields from security to fintech. The three-year-old startup\\'s biggest contribution, though, may be to the Chinese government\\'s surveillance: the software helps process data captured by China\\'s 170 million CCTV cameras and is built into more than 100 million mobile devices. The tech is also used in checkout-less lanes and customer analysis systems. According to Bloomberg, SenseTime is developing a service that will parse data from thousands of live camera feeds.\\n\\n\"SenseTime is doing pioneering work in artificial intelligence. We are especially impressed by their R&D capabilities in deep learning and visual computing... Our strategic partnership with SenseTime will spark more innovation and create value for society,\" said Joe Tsai, executive vice chairman of Alibaba Group, in a press release.\\n\\nThe startup said it will use the funding to build out its A.I. platform and to expand on its business ventures. SenseTime also said it has established partnerships with more than 400 companies including Honda on autonomous driving and Qualcomm on smart technology.\\n\\nThe announcement of a Chinese startup snagging the title of the highest valued A.I. startup in the world may put pressure on the U.S.\\'s A.I. startups. Last July, China said that it wanted to be a global leader in A.I. by 2030. China\\'s A.I. startups received more funding that its U.S. counterparts last year, according to a CB Insights report. Of the $15.2 billion invested in A.I. startups globally, 48 percent went to China and only 36 percent went to the U.S.')\n",
      "('4 Proven Methods for Learning Faster', 'What can we do to learn better and faster? originally appeared on Quora: the place to gain and share knowledge, empowering people to learn from others and better understand the world.\\n\\nAnswer by Brett Wingeier, CTO and co-founder of Halo Neuroscience, on Quora:\\n\\nLearning can be hard work. That\\'s what it comes down to, and it doesn\\'t lead to a simple answer or an easy answer. We have one thing going for us, though. Our brains love to learn. Our brains are constantly optimizing themselves, and when we feel this happening successfully it\\'s deeply satisfying. The way to learn better and faster is to create conditions that maximize this optimization.\\n\\nFirst, learn by doing. Optimal learning involves not just taking in information but synthesizing it, rearranging it, and learning from mistakes. Think about how you\\'d learn a new athletic skill, like rowing. First, you\\'d have a coach talk you through the fundamentals. But you wouldn\\'t get very far by just watching and listening. Soon, you\\'d need to get in the boat and start actually rowing. It might be rough at first, but you would have constant feedback - how your body and the boat feel, more advice from your coach, and quantitative data on your speed. Based on this feedback, on every stroke, your brain strengthens the patterns that lead to success. This is learning.\\n\\nLearning academic or professional material is no different, but in the classroom it\\'s easy to forget that learning isn\\'t just about taking in information. To activate your brain\\'s internal feedback loop, you need to use what you learn, solve problems, answer questions. And you need to mess up! Getting something wrong, and eventually understanding why, is just as useful as getting it right.\\n\\nPace yourself. Like every other part of your body, your brain needs enough rest to function at peak performance. This is especially true if you\\'re learning by doing. An essential part of learning - the consolidation of short-term memory into long-term memory - actually happens \"offline,\" after the learning task itself, and sleep is crucial for this process. Remember that learning anything is a subtle but pervasive physical change in the brain. Millions of synapses are changing slightly with every fact you learn. So, again there\\'s a parallel between learning and athletic training. You wouldn\\'t try to train for a 5K race by running a marathon the night before, because the physical changes you\\'re looking for need to happen over weeks or months, not hours. Similarly, the physical changes that represent deep learning - and learning at a rapid but sustainable pace - need to happen over at least days of effort, not hours.\\n\\nPractice self-awareness. Everyone learns differently. Personally, if you try to give me verbal directions, I\\'ll smile and nod (sorry about that!) and eventually I\\'ll ask you to show me on a map. You may be the exact opposite. It\\'s all about how your brain likes to take in and process information. To optimize learning, it\\'s important to figure this out, and nobody is in a better position than you yourself are to figure out what works best for you. If you\\'re frustrated, I guarantee it\\'s not because you\\'re fundamentally unable to learn something. Remember, your brain loves to learn. Maybe you\\'re not starting from the beginning and you need to back up to more basic principles that you never mastered. Or, maybe you need to change how you\\'re working - find a different way of taking in the information, or find a study partner for some friendly competition. It takes self-awareness to figure this out and activate your ability to learn.\\n\\nCognitive control. All these tools - hard work, pacing and organization, self-awareness - are related to what\\'s called \"cognitive control.\" This is the brain\\'s ability to focus on what needs to be focused on, getting the job done while avoiding distractions. As you might expect, it\\'s a powerful trait of highly effective people. The good news is that even this brain super-power can be trained. The more you practice it, the better you get!\\n\\nOf course, there are also promising new technologies for faster and better learning. Research groups across the world - such as the Neuroscape group at UCSF - are developing neuroscience-based games to maximize cognitive control. Great apps like Duolingo are helping people learn languages faster than ever before. Biohackers are exploring nootropics and medications to increase focus and accelerate learning. EEG products like Muse and Emotiv offer people tools to practice focus and gain insight into how their brains work. My own company, Halo Neuroscience, offers a neuromodulation headset to accelerate movement learning.\\n\\nBut while neurotech will surely advance, even the most advanced brain technology will continue to leverage the fundamentals of learning. And these fundamentals are just as relevant today for everyone who owns a brain and wants to learn: the amazing power of neuroplasticity, the principle of cognitive control, and good old hard work.\\n\\nThis question originally appeared on Quora - the place to gain and share knowledge, empowering people to learn from others and better understand the world. You can follow Quora on Twitter, Facebook, and Google+. More questions:')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Neural Networks Are The New Apps', 'Shazam was the must-have app of the late aughts. It seemed to fulfill every promise of the post iPhone world: With the mere tap of the screen, you could beam information to the cloud to identify a random song playing in a commercial, at the bar, or on the radio. But it required Shazam to build a huge server farm–its own entire data center to handle the loads.\\n\\nFor a tangible example of how things have changed in the decade since Shazam’s smartphone app debuted, think about this: On the Pixel 2, with a feature called Now Playing, Google has shrunk the equivalent of Shazam’s countless servers of yore to run entirely on the phone. It can match 70,000 songs, no internet required. And instead of you asking it what song is on, Now Playing listens all the time and tells you before you even ask.\\n\\nWhat made this possible? “There’s been a deep learning revolution,” says Matt Sharifi, a software engineer at Google, who first helped bring music identification to Google’s own search bar back in 2010. “When we started working on this problem, the approaches to music recognition were different than in 2017. We did everything with deep learning and machine learning.”\\n\\nThe benefits of running Now Playing on the Pixel were clear: It would be faster for the user, and it would ensure more privacy, too, since the audio snippets didn’t need to be sent to the cloud. But perhaps the biggest reason Google brought Now Playing onto the phone was the simplest: That, suddenly, it could be done at all.\\n\\nTo the uninitiated, the first moment you see Now Playing work is semi-creepy omniscience, even if the feature is technically opt-in. The Pixel 2’s lock screen has the feel of an old clock radio. It shows the time along with a few minimal notifications. And then, below all that, you see the title of the song playing in your room–a moment of technological prowess that’s presented as an afterthought.\\n\\nBy design, Google didn’t want Now Playing to ever look like it was breaking a sweat on your behalf. “The Pixel is all about being helpful and useful, but also being kind of playful, too,” says Brandon Barbello, product manager of Now Playing. “Now Playing sits well at that juncture . . . the moment you have the question, ‘What is this song?’ you can look at your phone and the answer is already there.”\\n\\nBut this effortless “ambient awareness,” as Google calls it, was actually years in development. Even though it was built by the same group that created Google’s own music search technology in 2010, the team had to start from scratch to get audio matching working on a phone. That’s because in the cloud, Google more or less has unlimited computational power. Audio matching, in particular, is resource intensive, even for industrial servers. On a phone’s low-power chips, processing is limited, and every request posed to the silicon sucks away precious battery life. Plus there’s the problem of the music samples themselves. To ID a song, it must be matched up against audio fingerprints stored in a database. In the case of Now Playing, that’s 70,000 song samples that need to be squeezed onto the phone’s chips.')\n",
      "('How Do Neural Networks Dream? Here’s Our First Glimpse', 'Google Brain, the company’s deep learning and AI arm, has published research in this area for years. Back in 2015, a blockbuster paper titled “Inceptionism” introduced the media–and much of the public–to the concept of neural networks by showing how they “dream,” in wild, hallucinogenic images of clouds shaped like fish and fantastical birds. The images gripped our imaginations and changed how we talk about AI. But the point for the researchers was to show off “simple techniques for peeking inside these networks.”\\n\\nThis week, three years later, two of the same Google Brain researchers and their colleagues published a new paper on their progress called “The Building Blocks of Interpretability.” It shows a new way of seeing inside the brains of machines. The researchers’ word for this is “interpretability,” or the ability to understand. In short, they’ve built a series of interfaces for looking inside neural networks to see how they make decisions.\\n\\nSo how does a neural network see our world? Take a look at one example: a photo of a golden lab and a kitten frolicking that a neural network has labeled as a “Labrador retriever” and “tiger cat.” Using one of the researchers’ interfaces, you can see exactly how the neural net came to that decision, isolating certain areas of the photo with each neuron’s best guess about what it “sees” and how sure it is about that. (For instance, it was pretty sure about the dog’s “floppy ears,” and the cat’s “pointy ears.”) As layers of neurons work together, the machine’s understanding evolves, beginning with detecting edges to actual shapes and objects.\\n\\nYou’d be far better served by clicking around their demos yourself rather than listening to me try to explain them.\\n\\nIt’s the “why” of the research that’s so compelling. Right now, researchers can only assess neural networks based on their final decisions–we don’t really understand how they get to those decisions. That’s a big problem. If neural networks are participating in criminal justice, financial markets, national security, healthcare, and government, we need to understand why they make decisions, not just if they got them right or wrong. Transparency and accountability–basic tenets of democracy–are lost when we don’t. The paper proposes an interface that lets researchers peer into their neural networks, like looking through a literal window. Eventually, they posit, these interfaces could help researchers shape the actual thought process going on inside these digital brains.\\n\\n“Human feedback on the model’s decision-making process, facilitated by interpretability interfaces, could be a powerful solution to these problems,” the Google team writes. “It might allow us to train models not just to make the right decisions, but to make them for the right reasons.”')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Helvetica Is Now An Encryption Device', 'Let’s say you’re an operative with the CIA. Your agents are filing intelligence briefs to you in text documents. But how do you know that the document’s text hasn’t been tampered with somehow by someone malicious who got their hands on it?\\n\\nYou don’t. But researchers have figured out a way to embed encryption into typed blocks of text on paper. Their secret weapon? Helvetica and Times New Roman.\\n\\nYou read that right. Fonts have been transformed into encryption tools. A new paper by a group of researchers at Columbia University details a method for making tiny changes to fonts that the human eye can’t detect but that look entirely different to a computer vision algorithm. A demo of the technology, dubbed “Fontcode,” shows how they were able to embed the secret message “Hello World!” into a paragraph taken from The Lord of the Rings.\\n\\nHow does it work? The researchers use an algorithm from previous research that can slowly shift letterforms from one typeface to another to make tiny changes in the shape of every letter that the human eye can’t detect. That could make an “h” slightly thicker in the stem,” or the curve of a “j” slightly sharper.\\n\\nOnce they had these “perturbed” letters, the researchers could make 52 variations of each letter. Each of the 52 variations corresponds to every other lowercase and capital letter in the alphabet (and theoretically every numeral and punctuation mark as well). These 52 variations for each letter go into what the researchers call a “code book” that helps the computer match the perturbed letter it sees with the secret letter it’s encoding.\\n\\nTo make the system work, the researchers used a deep learning neural network to train a computer vision algorithm to identify the slight variations in each letter correctly.\\n\\nIt’s a clever use of machine learning, one that has immediate applications in security. The researchers’ code book and machine learning algorithm could be used to ensure that documents haven’t been tampered with by embedding an authentication code–or some kind of secret message–into the font itself. Then, when someone receives the document, they can use the decoding algorithm to ensure the message is still there. If it’s not, then someone has somehow altered the document.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Jeff Bezos Releases Non-Bezos Number: 100 Million Prime Subscribers', 'EX-99.1\\n\\nExhibit 99.1\\n\\nTo our shareowners:\\n\\nThe American Customer Satisfaction Index recently announced the results of its annual survey, and for the 8th year in a row customers ranked Amazon #1. The United Kingdom has a similar index, The U.K. Customer Satisfaction Index, put out by the Institute of Customer Service. For the 5th time in a row Amazon U.K. ranked #1 in that survey. Amazon was also just named the #1 business on LinkedIn\\x92s 2018 Top Companies list, which ranks the most sought after places to work for professionals in the United States. And just a few weeks ago, Harris Poll released its annual Reputation Quotient, which surveys over 25,000 consumers on a broad range of topics from workplace environment to social responsibility to products and services, and for the 3rd year in a row Amazon ranked #1.\\n\\nCongratulations and thank you to the now over 560,000 Amazonians who come to work every day with unrelenting customer obsession, ingenuity, and commitment to operational excellence. And on behalf of Amazonians everywhere, I want to extend a huge thank you to customers. It\\x92s incredibly energizing for us to see your responses to these surveys.\\n\\nOne thing I love about customers is that they are divinely discontent. Their expectations are never static \\x96 they go up. It\\x92s human nature. We didn\\x92t ascend from our hunter-gatherer days by being satisfied. People have a voracious appetite for a better way, and yesterday\\x92s \\x91wow\\x92 quickly becomes today\\x92s \\x91ordinary\\x92. I see that cycle of improvement happening at a faster rate than ever before. It may be because customers have such easy access to more information than ever before \\x96 in only a few seconds and with a couple taps on their phones, customers can read reviews, compare prices from multiple retailers, see whether something\\x92s in stock, find out how fast it will ship or be available for pick-up, and more. These examples are from retail, but I sense that the same customer empowerment phenomenon is happening broadly across everything we do at Amazon and most other industries as well. You cannot rest on your laurels in this world. Customers won\\x92t have it.\\n\\nHow do you stay ahead of ever-rising customer expectations? There\\x92s no single way to do it \\x96 it\\x92s a combination of many things. But high standards (widely deployed and at all levels of detail) are certainly a big part of it. We\\x92ve had some successes over the years in our quest to meet the high expectations of customers. We\\x92ve also had billions of dollars\\x92 worth of failures along the way. With those experiences as backdrop, I\\x92d like to share with you the essentials of what we\\x92ve learned (so far) about high standards inside an organization.\\n\\nIntrinsic or Teachable?\\n\\nFirst, there\\x92s a foundational question: are high standards intrinsic or teachable? If you take me on your basketball team, you can teach me many things, but you can\\x92t teach me to be taller. Do we first and foremost need to select for \\x93high standards\\x94 people? If so, this letter would need to be mostly about hiring practices, but I don\\x92t think so. I believe high standards are teachable. In fact, people are pretty good at learning high standards simply through exposure. High standards are contagious. Bring a new person onto a high standards team, and they\\x92ll quickly adapt. The opposite is also true. If low standards prevail, those too will quickly spread. And though exposure works well to teach high standards, I believe you can accelerate that rate of learning by articulating a few core principles of high standards, which I hope to share in this letter.\\n\\nUniversal or Domain Specific?\\n\\nAnother important question is whether high standards are universal or domain specific. In other words, if you have high standards in one area, do you automatically have high standards elsewhere? I believe high standards are domain specific, and that you have to learn high standards separately in every arena of interest. When I started Amazon, I had high standards on inventing, on customer care, and (thankfully) on hiring. But I didn\\x92t have high standards on operational process: how to keep fixed problems fixed, how to eliminate defects at the root, how to inspect processes, and much more. I had to learn and develop high standards on all of that (my colleagues were my tutors).\\n\\nUnderstanding this point is important because it keeps you humble. You can consider yourself a person of high standards in general and still have debilitating blind spots. There can be whole arenas of endeavor where you may not even know that your standards are low or non-existent, and certainly not world class. It\\x92s critical to be open to that likelihood.\\n\\nRecognition and Scope\\n\\nWhat do you need to achieve high standards in a particular domain area? First, you have to be able to recognize what good looks like in that domain. Second, you must have realistic expectations for how hard it should be (how much work it will take) to achieve that result \\x96 the scope.\\n\\nLet me give you two examples. One is a sort of toy illustration but it makes the point clearly, and another is a real one that comes up at Amazon all the time.\\n\\nPerfect Handstands\\n\\nA close friend recently decided to learn to do a perfect free-standing handstand. No leaning against a wall. Not for just a few seconds. Instagram good. She decided to start her journey by taking a handstand workshop at her yoga studio. She then practiced for a while but wasn\\x92t getting the results she wanted. So, she hired a handstand coach. Yes, I know what you\\x92re thinking, but evidently this is an actual thing that exists. In the very first lesson, the coach gave her some wonderful advice. \\x93Most people,\\x94 he said, \\x93think that if they work hard, they should be able to master a handstand in about two weeks. The reality is that it takes about six months of daily practice. If you think you should be able to do it in two weeks, you\\x92re just going to end up quitting.\\x94 Unrealistic beliefs on scope \\x96 often hidden and undiscussed \\x96 kill high standards. To achieve high standards yourself or as part of a team, you need to form and proactively communicate realistic beliefs about how hard something is going to be \\x96 something this coach understood well.\\n\\nSix-Page Narratives\\n\\nWe don\\x92t do PowerPoint (or any other slide-oriented) presentations at Amazon. Instead, we write narratively structured six-page memos. We silently read one at the beginning of each meeting in a kind of \\x93study hall.\\x94 Not surprisingly, the quality of these memos varies widely. Some have the clarity of angels singing. They are brilliant and thoughtful and set up the meeting for high-quality discussion. Sometimes they come in at the other end of the spectrum.\\n\\nIn the handstand example, it\\x92s pretty straightforward to recognize high standards. It wouldn\\x92t be difficult to lay out in detail the requirements of a well-executed handstand, and then you\\x92re either doing it or you\\x92re not. The writing example is very different. The difference between a great memo and an average one is much squishier. It would be extremely hard to write down the detailed requirements that make up a great memo. Nevertheless, I find that much of the time, readers react to great memos very similarly. They know it when they see it. The standard is there, and it is real, even if it\\x92s not easily describable.\\n\\nHere\\x92s what we\\x92ve figured out. Often, when a memo isn\\x92t great, it\\x92s not the writer\\x92s inability to recognize the high standard, but instead a wrong expectation on scope: they mistakenly believe a high-standards, six-page memo can be written in one or two days or even a few hours, when really it might take a week or more! They\\x92re trying to perfect a handstand in just two weeks, and we\\x92re not coaching them right. The great memos are written and re-written, shared with colleagues who are asked to improve the work, set aside for a couple of days, and then edited again with a fresh mind. They simply can\\x92t be done in a day or two. The key point here is that you can improve results through the simple act of teaching scope \\x96 that a great memo probably should take a week or more.\\n\\nSkill\\n\\nBeyond recognizing the standard and having realistic expectations on scope, how about skill? Surely to write a world class memo, you have to be an extremely skilled writer? Is it another required element? In my view, not so much, at least not for the individual in the context of teams. The football coach doesn\\x92t need to be able to throw, and a film director doesn\\x92t need to be able to act. But they both do need to recognize high standards for those things and teach realistic expectations on scope. Even in the example of writing a six-page memo, that\\x92s\\n\\nteamwork. Someone on the team needs to have the skill, but it doesn\\x92t have to be you. (As a side note, by tradition at Amazon, authors\\x92 names never appear on the memos \\x96 the memo is from the whole team.)\\n\\nBenefits of High Standards\\n\\nBuilding a culture of high standards is well worth the effort, and there are many benefits. Naturally and most obviously, you\\x92re going to build better products and services for customers \\x96 this would be reason enough! Perhaps a little less obvious: people are drawn to high standards \\x96 they help with recruiting and retention. More subtle: a culture of high standards is protective of all the \\x93invisible\\x94 but crucial work that goes on in every company. I\\x92m talking about the work that no one sees. The work that gets done when no one is watching. In a high standards culture, doing that work well is its own reward \\x96 it\\x92s part of what it means to be a professional.\\n\\nAnd finally, high standards are fun! Once you\\x92ve tasted high standards, there\\x92s no going back.\\n\\nSo, the four elements of high standards as we see it: they are teachable, they are domain specific, you must recognize them, and you must explicitly coach realistic scope. For us, these work at all levels of detail. Everything from writing memos to whole new, clean-sheet business initiatives. We hope they help you too.\\n\\nInsist on the Highest Standards\\n\\nLeaders have relentlessly high standards \\x96 many people may think these standards are unreasonably high.\\n\\n-- from the Amazon Leadership Principles\\n\\nRecent Milestones\\n\\nThe high standards our leaders strive for have served us well. And while I certainly can\\x92t do a handstand myself, I\\x92m extremely proud to share some of the milestones we hit last year, each of which represents the fruition of many years of collective effort. We take none of them for granted.\\n\\n\\x95 Prime \\x96 13 years post-launch, we have exceeded 100 million paid Prime members globally. In 2017 Amazon shipped more than five billion items with Prime worldwide, and more new members joined Prime than in any previous year \\x96 both worldwide and in the U.S. Members in the U.S. now receive unlimited free two-day shipping on over 100 million different items. We expanded Prime to Mexico, Singapore, the Netherlands, and Luxembourg, and introduced Business Prime Shipping in the U.S. and Germany. We keep making Prime shipping faster as well, with Prime Free Same-Day and Prime Free One-Day delivery now in more than 8,000 cities and towns. Prime Now is available in more than 50 cities worldwide across nine countries. Prime Day 2017 was our biggest global shopping event ever (until surpassed by Cyber Monday), with more new Prime members joining Prime than any other day in our history.\\n\\n\\x95 AWS \\x96 It\\x92s exciting to see Amazon Web Services, a $20 billion revenue run rate business, accelerate its already healthy growth. AWS has also accelerated its pace of innovation \\x96 especially in new areas such as machine learning and artificial intelligence, Internet of Things, and serverless computing. In 2017, AWS announced more than 1,400 significant services and features, including Amazon SageMaker, which radically changes the accessibility and ease of use for everyday developers to build sophisticated machine learning models. Tens of thousands of customers are also using a broad range of AWS machine learning services, with active users increasing more than 250 percent in the last year, spurred by the broad adoption of Amazon SageMaker. And in November, we held our sixth re:Invent conference with more than 40,000 attendees and over 60,000 streaming participants.\\n\\n\\x95 Marketplace \\x96 In 2017, for the first time in our history, more than half of the units sold on Amazon worldwide were from our third-party sellers, including small and medium-sized businesses (SMBs). Over 300,000 U.S.-based SMBs started selling on Amazon in 2017, and Fulfillment by Amazon shipped billions of items for SMBs worldwide. Customers ordered more than 40 million items from SMBs worldwide during Prime Day 2017, growing their sales by more than 60 percent over Prime Day 2016. Our Global Selling program (enabling SMBs to sell products across national borders) grew by over 50% in 2017 and cross-border ecommerce by SMBs now represents more than 25% of total third-party sales.\\n\\n\\x95 Alexa \\x96 Customer embrace of Alexa continues, with Alexa-enabled devices among the best-selling items across all of Amazon. We\\x92re seeing extremely strong adoption by other companies and developers that want to create their own experiences with Alexa. There are now more than 30,000 skills for Alexa from outside developers, and customers can control more than 4,000 smart home devices from 1,200 unique brands with Alexa. The foundations of Alexa continue to get smarter every day too. We\\x92ve developed and implemented an on-device fingerprinting technique, which keeps your device from waking up when it hears an Alexa commercial on TV. (This technology ensured that our Alexa Super Bowl commercial didn\\x92t wake up millions of devices.) Far-field speech recognition (already very good) has improved by 15% over the last year; and in the U.S., U.K., and Germany, we\\x92ve improved Alexa\\x92s spoken language understanding by more than 25% over the last 12 months through enhancements in Alexa\\x92s machine learning components and the use of semi-supervised learning techniques. (These semi-supervised learning techniques reduced the amount of labeled data needed to achieve the same accuracy improvement by 40 times!) Finally, we\\x92ve dramatically reduced the amount of time required to teach Alexa new languages by using machine translation and transfer learning techniques, which allows us to serve customers in more countries (like India and Japan).\\n\\n\\x95 Amazon devices \\x96 2017 was our best year yet for hardware sales. Customers bought tens of millions of Echo devices, and Echo Dot and Fire TV Stick with Alexa were the best-selling products across all of Amazon \\x96 across all categories and all manufacturers. Customers bought twice as many Fire TV Sticks and Kids Edition Fire Tablets this holiday season versus last year. 2017 marked the release of our all-new Echo with an improved design, better sound, and a lower price; Echo Plus with a built-in smart home hub; and Echo Spot, which is compact and beautiful with a circular screen. We released our next generation Fire TV, featuring 4K Ultra HD and HDR; and the Fire HD 10 Tablet, with 1080p Full HD display. And we celebrated the 10th anniversary of Kindle by releasing the all-new Kindle Oasis, our most advanced reader ever. It\\x92s waterproof \\x96 take it in the bathtub \\x96 with a bigger 7\\x94 high-resolution 300 ppi display and has built-in audio so you can also listen to your books with Audible.\\n\\n\\x95 Prime Video \\x96 Prime Video continues to drive Prime member adoption and retention. In the last year we made Prime Video even better for customers by adding new, award-winning Prime Originals to the service, like The Marvelous Mrs. Maisel, winner of two Critics\\x92 Choice Awards and two Golden Globes, and the Oscar-nominated movie The Big Sick. We\\x92ve expanded our slate of programming across the globe, launching new seasons of Bosch and Sneaky Pete from the U.S., The Grand Tour from the U.K., and You Are Wanted from Germany, while adding new Sentosha shows from Japan, along with Breathe and the award-winning Inside Edge from India. Also this year, we expanded our Prime Channels offerings, adding CBS All Access in the U.S. and launching Channels in the U.K. and Germany. We debuted NFL Thursday Night Football on Prime Video, with more than 18 million total viewers over 11 games. In 2017, Prime Video Direct secured subscription video rights for more than 3,000 feature films and committed over $18 million in royalties to independent filmmakers and other rights holders. Looking forward, we\\x92re also excited about our upcoming Prime Original series pipeline, which includes Tom Clancy\\x92s Jack Ryan starring John Krasinski; King Lear, starring Anthony Hopkins and Emma Thompson; The Romanoffs, executive produced by Matt Weiner; Carnival Row starring Orlando Bloom and Cara Delevingne; Good Omens starring Jon Hamm; and Homecoming, executive produced by Sam Esmail and starring Julia Roberts in her first television series. We acquired the global television rights for a multi-season production of The Lord of the Rings, as well as Cortés, a miniseries based on the epic saga of Hernán Cortés from executive producer Steven Spielberg, starring Javier Bardem, and we look forward to beginning work on those shows this year.\\n\\n\\x95 Amazon Music \\x96 Amazon Music continues to grow fast and now has tens of millions of paid customers. Amazon Music Unlimited, our on-demand, ad-free offering, expanded to more than 30 new countries in 2017, and membership has more than doubled over the past six months.\\n\\n\\x95 Fashion \\x96 Amazon has become the destination for tens of millions of customers to shop for fashion. In 2017, we introduced our first fashion-oriented Prime benefit, Prime Wardrobe \\x96 a new service that brings the fitting room directly to the homes of Prime members so they can try on the latest styles before they buy. We introduced Nike and UGG on Amazon along with new celebrity collections by Drew Barrymore and Dwyane Wade, as well as dozens of new private brands, like Goodthreads and\\n\\nCore10. We\\x92re also continuing to enable thousands of designers and artists to offer their exclusive designs and prints on demand through Merch by Amazon. We finished 2017 with the launch of our interactive shopping experience with Calvin Klein, including pop-up shops, on-site product customization, and fitting rooms with Alexa-controlled lighting, music, and more.\\n\\n\\x95 Whole Foods \\x96 When we closed our acquisition of Whole Foods Market last year, we announced our commitment to making high-quality, natural and organic food available for everyone, then immediately lowered prices on a selection of best-selling grocery staples, including avocados, organic brown eggs, and responsibly-farmed salmon. We followed this with a second round of price reductions in November, and our Prime member exclusive promotion broke Whole Foods\\x92 all-time record for turkeys sold during the Thanksgiving season. In February, we introduced free two-hour delivery on orders over $35 for Prime members in select cities, followed by additional cities in March and April, and plan continued expansion across the U.S. throughout this year. We also expanded the benefits of the Amazon Prime Rewards Visa Card, enabling Prime members to get 5% back when shopping at Whole Foods Market. Beyond that, customers can purchase Whole Foods\\x92 private label products like 365 Everyday Value on Amazon, purchase Echo and other Amazon devices in over a hundred Whole Foods stores, and pick-up or return Amazon packages at Amazon Lockers in hundreds of Whole Foods stores. We\\x92ve also begun the technical work needed to recognize Prime members at the point of sale and look forward to offering more Prime benefits to Whole Foods shoppers once that work is completed.\\n\\n\\x95 Amazon Go \\x96 Amazon Go, a new kind of store with no checkout required, opened to the public in January in Seattle. Since opening, we\\x92ve been thrilled to hear many customers refer to their shopping experience as \\x93magical.\\x94 What makes the magic possible is a custom-built combination of computer vision, sensor fusion, and deep learning, which come together to create Just Walk Out shopping. With JWO, customers are able to grab their favorite breakfast, lunch, dinner, snack, and grocery essentials more conveniently than ever before. Some of our top-selling items are not surprising \\x96 caffeinated beverages and water are popular \\x96 but our customers also love the Chicken Banh Mi sandwich, chocolate chip cookies, cut fruit, gummy bears, and our Amazon Meal Kits.\\n\\n\\x95 Treasure Truck \\x96 Treasure Truck expanded from a single truck in Seattle to a fleet of 35 trucks across 25 U.S. cities and 12 U.K. cities. Our bubble-blowing, music-pumping trucks fulfilled hundreds of thousands of orders, from porterhouse steaks to the latest Nintendo releases. Throughout the year, Treasure Truck also partnered with local communities to lift spirits and help those in need, including donating and delivering hundreds of car seats, thousands of toys, tens of thousands of socks, and many other essentials to community members needing relief, from those displaced by Hurricane Harvey, to the homeless, to kids needing holiday cheer.\\n\\n\\x95 India \\x96 Amazon.in is the fastest growing marketplace in India, and the most visited site on both desktop and mobile, according to comScore and SimilarWeb. The Amazon.in mobile shopping app was also the most downloaded shopping app in India in 2017, according to App Annie. Prime added more members in India in its first year than any previous geography in Amazon\\x92s history. Prime selection in India now includes more than 40 million local products from third-party sellers, and Prime Video is investing in India original video content in a big way, including two recent premiers and over a dozen new shows in production.\\n\\n\\x95 Sustainability \\x96 We are committed to minimizing carbon emissions by optimizing our transportation network, improving product packaging, and enhancing energy efficiency in our operations, and we have a long-term goal to power our global infrastructure using 100% renewable energy. We recently launched Amazon Wind Farm Texas, our largest wind farm yet, which generates more than 1,000,000 megawatt hours of clean energy annually from over 100 turbines. We have plans to host solar energy systems at 50 fulfillment centers by 2020, and have launched 24 wind and solar projects across the U.S. with more than 29 additional projects to come. Together, Amazon\\x92s renewable energy projects now produce enough clean energy to power over 330,000 homes annually. In 2017 we celebrated the 10-year anniversary of Frustration-Free Packaging, the first of a suite of sustainable packaging initiatives that have eliminated more than 244,000 tons of packaging materials over the past 10 years. In addition, in 2017 alone our programs significantly reduced packaging waste, eliminating the\\n\\nequivalent of 305 million shipping boxes. And across the world, Amazon is contracting with our service providers to launch our first low-pollution last-mile fleet. Already today, a portion of our European delivery fleet is comprised of low-pollution electric and natural gas vans and cars, and we have over 40 electric scooters and e-cargo bikes that complete local urban deliveries.\\n\\n\\x95 Empowering Small Business \\x96 Millions of small and medium-sized businesses worldwide now sell their products through Amazon to reach new customers around the globe. SMBs selling on Amazon come from every state in the U.S., and from more than 130 different countries around the world. More than 140,000 SMBs surpassed $100,000 in sales on Amazon in 2017, and over a thousand independent authors surpassed $100,000 in royalties in 2017 through Kindle Direct Publishing.\\n\\n\\x95 Investment & Job Creation \\x96 Since 2011, we have invested over $150 billion worldwide in our fulfillment networks, transportation capabilities, and technology infrastructure, including AWS data centers. Amazon has created over 1.7 million direct and indirect jobs around the world. In 2017 alone, we directly created more than 130,000 new Amazon jobs, not including acquisitions, bringing our global employee base to over 560,000. Our new jobs cover a wide range of professions, from artificial intelligence scientists to packaging specialists to fulfillment center associates. In addition to these direct hires, we estimate that Amazon Marketplace has created 900,000 more jobs worldwide, and that Amazon\\x92s investments have created an additional 260,000 jobs in areas like construction, logistics, and other professional services.\\n\\n\\x95 Career Choice \\x96 One employee program we\\x92re particularly proud of is Amazon Career Choice. For hourly associates with more than one year of tenure, we pre-pay 95% of tuition, fees, and textbooks (up to $12,000) for certificates and associate degrees in high-demand occupations such as aircraft mechanics, computer-aided design, machine tool technologies, medical lab technologies, and nursing. We fund education in areas that are in high demand and do so regardless of whether those skills are relevant to a career at Amazon. Globally more than 16,000 associates (including more than 12,000 in the U.S.) have joined Career Choice since the program launched in 2012. Career Choice is live in ten countries and expanding to South Africa, Costa Rica, and Slovakia later this year. Commercial truck driving, healthcare, and information technology are the program\\x92s most popular fields of study. We\\x92ve built 39 Career Choice classrooms so far, and we locate them behind glass walls in high traffic areas inside our fulfillment centers so associates can be inspired by seeing their peers pursue new skills.\\n\\nThe credit for these milestones is deserved by many. Amazon is 560,000 employees. It\\x92s also 2 million sellers, hundreds of thousands of authors, millions of AWS developers, and hundreds of millions of divinely discontent customers around the world who push to make us better each and every day.\\n\\nPath Ahead\\n\\nThis year marks the 20th anniversary of our first shareholder letter, and our core values and approach remain unchanged. We continue to aspire to be Earth\\x92s most customer-centric company, and we recognize this to be no small or easy challenge. We know there is much we can do better, and we find tremendous energy in the many challenges and opportunities that lie ahead.\\n\\nA huge thank you to each and every customer for allowing us to serve you, to our shareowners for your support, and to Amazonians everywhere for your ingenuity, your passion, and your high standards.\\n\\nAs always, I attach a copy of our original 1997 letter. It remains Day 1.\\n\\nSincerely,\\n\\nJeffrey P. Bezos\\n\\nFounder and Chief Executive Officer\\n\\nAmazon.com, Inc.\\n\\n1997 LETTER TO SHAREHOLDERS\\n\\n(Reprinted from the 1997 Annual Report)\\n\\nTo our shareholders:\\n\\nAmazon.com passed many milestones in 1997: by year-end, we had served more than 1.5 million customers, yielding 838% revenue growth to $147.8 million, and extended our market leadership despite aggressive competitive entry.\\n\\nBut this is Day 1 for the Internet and, if we execute well, for Amazon.com. Today, online commerce saves customers money and precious time. Tomorrow, through personalization, online commerce will accelerate the very process of discovery. Amazon.com uses the Internet to create real value for its customers and, by doing so, hopes to create an enduring franchise, even in established and large markets.\\n\\nWe have a window of opportunity as larger players marshal the resources to pursue the online opportunity and as customers, new to purchasing online, are receptive to forming new relationships. The competitive landscape has continued to evolve at a fast pace. Many large players have moved online with credible offerings and have devoted substantial energy and resources to building awareness, traffic, and sales. Our goal is to move quickly to solidify and extend our current position while we begin to pursue the online commerce opportunities in other areas. We see substantial opportunity in the large markets we are targeting. This strategy is not without risk: it requires serious investment and crisp execution against established franchise leaders.\\n\\nIt\\x92s All About the Long Term\\n\\nWe believe that a fundamental measure of our success will be the shareholder value we create over the long term. This value will be a direct result of our ability to extend and solidify our current market leadership position. The stronger our market leadership, the more powerful our economic model. Market leadership can translate directly to higher revenue, higher profitability, greater capital velocity, and correspondingly stronger returns on invested capital.\\n\\nOur decisions have consistently reflected this focus. We first measure ourselves in terms of the metrics most indicative of our market leadership: customer and revenue growth, the degree to which our customers continue to purchase from us on a repeat basis, and the strength of our brand. We have invested and will continue to invest aggressively to expand and leverage our customer base, brand, and infrastructure as we move to establish an enduring franchise.\\n\\nBecause of our emphasis on the long term, we may make decisions and weigh tradeoffs differently than some companies. Accordingly, we want to share with you our fundamental management and decision-making approach so that you, our shareholders, may confirm that it is consistent with your investment philosophy:\\n\\n\\x95 We will continue to focus relentlessly on our customers.\\n\\n\\x95 We will continue to make investment decisions in light of long-term market leadership considerations rather than short-term profitability considerations or short-term Wall Street reactions.\\n\\n\\x95 We will continue to measure our programs and the effectiveness of our investments analytically, to jettison those that do not provide acceptable returns, and to step up our investment in those that work best. We will continue to learn from both our successes and our failures.\\n\\n\\x95 We will make bold rather than timid investment decisions where we see a sufficient probability of gaining market leadership advantages. Some of these investments will pay off, others will not, and we will have learned another valuable lesson in either case.\\n\\n\\x95 When forced to choose between optimizing the appearance of our GAAP accounting and maximizing the present value of future cash flows, we\\x92ll take the cash flows.\\n\\n\\x95 We will share our strategic thought processes with you when we make bold choices (to the extent competitive pressures allow), so that you may evaluate for yourselves whether we are making rational long-term leadership investments.\\n\\n\\x95 We will work hard to spend wisely and maintain our lean culture. We understand the importance of continually reinforcing a cost-conscious culture, particularly in a business incurring net losses.\\n\\n\\x95 We will balance our focus on growth with emphasis on long-term profitability and capital management. At this stage, we choose to prioritize growth because we believe that scale is central to achieving the potential of our business model.\\n\\n\\x95 We will continue to focus on hiring and retaining versatile and talented employees, and continue to weight their compensation to stock options rather than cash. We know our success will be largely affected by our ability to attract and retain a motivated employee base, each of whom must think like, and therefore must actually be, an owner.\\n\\nWe aren\\x92t so bold as to claim that the above is the \\x93right\\x94 investment philosophy, but it\\x92s ours, and we would be remiss if we weren\\x92t clear in the approach we have taken and will continue to take.\\n\\nWith this foundation, we would like to turn to a review of our business focus, our progress in 1997, and our outlook for the future.\\n\\nObsess Over Customers\\n\\nFrom the beginning, our focus has been on offering our customers compelling value. We realized that the Web was, and still is, the World Wide Wait. Therefore, we set out to offer customers something they simply could not get any other way, and began serving them with books. We brought them much more selection than was possible in a physical store (our store would now occupy 6 football fields), and presented it in a useful, easy-to-search, and easy-to-browse format in a store open 365 days a year, 24 hours a day. We maintained a dogged focus on improving the shopping experience, and in 1997 substantially enhanced our store. We now offer customers gift certificates, 1-Click SM shopping, and vastly more reviews, content, browsing options, and recommendation features. We dramatically lowered prices, further increasing customer value. Word of mouth remains the most powerful customer acquisition tool we have, and we are grateful for the trust our customers have placed in us. Repeat purchases and word of mouth have combined to make Amazon.com the market leader in online bookselling.\\n\\nBy many measures, Amazon.com came a long way in 1997:\\n\\n\\x95 Sales grew from $15.7 million in 1996 to $147.8 million \\x96 an 838% increase.\\n\\n\\x95 Cumulative customer accounts grew from 180,000 to 1,510,000 \\x96 a 738% increase.\\n\\n\\x95 The percentage of orders from repeat customers grew from over 46% in the fourth quarter of 1996 to over 58% in the same period in 1997.\\n\\n\\x95 In terms of audience reach, per Media Metrix, our Web site went from a rank of 90th to within the top 20.\\n\\n\\x95 We established long-term relationships with many important strategic partners, including America Online, Yahoo!, Excite, Netscape, GeoCities, AltaVista, @Home, and Prodigy.\\n\\nInfrastructure\\n\\nDuring 1997, we worked hard to expand our business infrastructure to support these greatly increased traffic, sales, and service levels:\\n\\n\\x95 Amazon.com\\x92s employee base grew from 158 to 614, and we significantly strengthened our management team.\\n\\n\\x95 Distribution center capacity grew from 50,000 to 285,000 square feet, including a 70% expansion of our Seattle facilities and the launch of our second distribution center in Delaware in November.\\n\\n\\x95 Inventories rose to over 200,000 titles at year-end, enabling us to improve availability for our customers.\\n\\n\\x95 Our cash and investment balances at year-end were $125 million, thanks to our initial public offering in May 1997 and our $75 million loan, affording us substantial strategic flexibility.\\n\\nOur Employees\\n\\nThe past year\\x92s success is the product of a talented, smart, hard-working group, and I take great pride in being a part of this team. Setting the bar high in our approach to hiring has been, and will continue to be, the single most important element of Amazon.com\\x92s success.\\n\\nIt\\x92s not easy to work here (when I interview people I tell them, \\x93You can work long, hard, or smart, but at Amazon.com you can\\x92t choose two out of three\\x94), but we are working to build something important, something that matters to our customers, something that we can all tell our grandchildren about. Such things aren\\x92t meant to be easy. We are incredibly fortunate to have this group of dedicated employees whose sacrifices and passion build Amazon.com.\\n\\nGoals for 1998\\n\\nWe are still in the early stages of learning how to bring new value to our customers through Internet commerce and merchandising. Our goal remains to continue to solidify and extend our brand and customer base. This requires sustained investment in systems and infrastructure to support outstanding customer convenience, selection, and service while we grow. We are planning to add music to our product offering, and over time we believe that other products may be prudent investments. We also believe there are significant opportunities to better serve our customers overseas, such as reducing delivery times and better tailoring the customer experience. To be certain, a big part of the challenge for us will lie not in finding new ways to expand our business, but in prioritizing our investments.\\n\\nWe now know vastly more about online commerce than when Amazon.com was founded, but we still have so much to learn. Though we are optimistic, we must remain vigilant and maintain a sense of urgency. The challenges and hurdles we will face to make our long-term vision for Amazon.com a reality are several: aggressive, capable, well-funded competition; considerable growth challenges and execution risk; the risks of product and geographic expansion; and the need for large continuing investments to meet an expanding market opportunity. However, as we\\x92ve long said, online bookselling, and online commerce in general, should prove to be a very large market, and it\\x92s likely that a number of companies will see significant benefit. We feel good about what we\\x92ve done, and even more excited about what we want to do.\\n\\n1997 was indeed an incredible year. We at Amazon.com are grateful to our customers for their business and trust, to each other for our hard work, and to our shareholders for their support and encouragement.\\n\\nJeffrey P. Bezos\\n\\nFounder and Chief Executive Officer')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"China's AI imperative\", 'Thank you for your interest in spreading the word about Science.\\n\\nNOTE: We only request your email address so that the person you are recommending the page to knows that you wanted them to see it, and that it is not junk mail. We do not capture any email address.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Instagram Launches New API With Scheduling', 'Welcome to this week’s edition of the Social Media Marketing Talk Show, a news show for marketers who want to stay on the leading edge of social media.\\n\\nOn this week’s Social Media Marketing Talk Show, we explore Instagram’s new API with scheduling with Alisa Meredith, more Facebook news feed updates, new Instagram Stories post types, and other breaking social media marketing news of the week!\\n\\nWatch the Social Media Marketing Talk Show\\n\\nIf you’re new to the show, click on the green “Watch replay” button below and sign in or register to watch our latest episode from Friday, February 2, 2018. You can also listen to the show as an audio podcast, found on iTunes/Apple Podcast, Android, Google Play, Stitcher, and RSS.\\n\\n\\ufeff\\n\\nFor this week’s top stories, you’ll find timestamps below that allow you to fast-forward in the replay above.\\n\\nInstagram Launches Content Publishing Beta for Businesses and Apps: Instagram has unveiled a change to the API that will allow business accounts to directly schedule and publish their Instagram posts. Previously, social media managers would have to use third-party tools to push reminders to post on Instagram. The latest update to the Instagram API now gives applications like Hootsuite, Tailwind, and more access to post scheduling. This new functionality is also open to Facebook Marketing Partners and Instagram Partners, but isn’t currently available within the product itself nor has it been extended to Instagram ads. (4:28)\\n\\nInstagram notes that this API update is part of a larger revamp happening over the next 2 years beginning in July 2018. The old API will be replaced by a newer Instagram Graph API and will include the ability to analyze performance metrics and insights, moderate comments, view other businesses’ profile info and media, and more.\\n\\nInstagram Brings Carousel Ads to Instagram Stories: Instagram is introducing carousel ads for Instagram Stories, which will allow advertisers to include up to three pieces of media per Stories ad. Previously, advertisers were limited to one ad per story on Instagram. The company notes that carousel ads for Instagram Stories behave the same way as other stories content, “meaning you can tap through, swipe back and forth or pause the content whenever you’d like” and “with two additional pieces of media, advertisers can add depth to their storytelling in the same way the community does today.” Instagram is currently testing this new ad format with a handful of advertisers including the Gap, Coca-Cola, and Paramount before rolling it out more broadly. (19:09)\\n\\nInstagram Officially Rolls Out Type Mode: Instagram officially introduced Type mode, which allows users to incorporate unique text styles, different background colors, and highlight words and phrases to create text-only stories. Type mode can also be layered over any photo or video in stories. This update is available as part of Instagram version 30 for both iOS and Android. (22:59)\\n\\n.\\n\\nFacebook Boosts Posts From Local News Sources in the News Feed: This past week, Facebook CEO and founder Mark Zuckerberg shared more upcoming changes to the news feed. He announced that in addition to elevating content from friends and family and sources deemed to be “high quality,” Facebook will now prioritize news items from users’ local towns or cities in the news feed. This update is first rolling out in the U.S. and then in more countries over the year.\\n\\nEarlier this month, it was announced that Facebook is testing a dedicated section that connects people to local community news and information, called Today In. This new feature is initially being tested in six U.S. cities and there are plans to expand further in the coming months.\\n\\nFacebook Earnings Report Shows User Growth Slowed in 4Q 2017: In the company’s 4Q 2017 Earnings Report, Facebook reiterated its focus in 2018 is “making sure Facebook isn’t just fun, but also good for people’s well-being and for society… by encouraging meaningful connections between people rather than passive consumption of content.” As part of this initiative, Facebook showed fewer viral videos in the news feed and reduced time spent on Facebook by roughly 50 million hours every day. TechCrunch reports that not only did this update subsequently reduce the total time spent on Facebook by 5%, it also led to a loss of 700,000 daily active users in the U.S. and Canada.\\n\\nFacebook reported that it now has 1.4 billion daily users, which is up 2.18% compared to growing 3.8% to 1.37 billion users in Q3. This represents the lowest quarter-over-quarter percentage daily user growth ever reported by the company. The company also reported that WhatsApp now has 1.5 billion monthly users, up from 1.3 billion in July.\\n\\nFacebook survives Q4 with slowing 1.4B daily users but record $12.97B revenue https://t.co/PW9TZr6yL6 pic.twitter.com/UrcSNnV8cd — TechCrunch (@TechCrunch) January 31, 2018\\n\\nSnapchat Rolls Out New iOS Features: Snapchat rolled out an update to the iOS app that includes new features like the option to save texts in one-on-one conversations and the ability to see a friend’s location on the Snap Map within a group chat. With this update, Snapchat has also removed the white frame that appeared when snaps from a camera roll are added to a story.\\n\\nFacebook Developing Chatbots With Improved Conversation Skills: The Verge reports that Facebook is developing chatbots with a “consistent personality” and the ability to carry on better conversations. The article notes that the key constraints with chatbots are that they don’t provide context based on what they or their chat partners have discussed in the past and fall back on programmed responses like “I don’t know” when faced with questions they don’t understand. Researchers at Facebook are using data and deep learning to train chatbots to look for patterns in large datasets, follow conversations patterned off of natural human dialogue, and develop “personas.”\\n\\nFacebook is trying to teach chatbots how to chit-chat https://t.co/I7xIVEWSWm pic.twitter.com/YSxuNR9mEp — The Verge (@verge) January 28, 2018\\n\\nFacebook Watch Alerts Mobile Viewers of New Episodes: It appears that Facebook is now promoting Facebook Watch shows with a pop-up alert that tells you when a new show is available to view, suggests you click a Follow button to get episodes added to your watchlist, and highlights which of your friends are already following the series. This feature was spotted on a mobile device by Michael Stelzner.\\n\\nTwitter Launches Sponsored Moments: Twitter rolled out a new in-stream sponsorship feature called Sponsored Moments. With Sponsored Moments, advertisers can run interstitial tweets packaged around a specific event or theme and a branded cover against content from premium partners like Bloomberg Media. Similar to other in-stream sponsorships, advertisers can promote the moment to their specific target audience and expand their reach beyond the content partner’s existing followers. Twitter announced that Sponsored Moments is now broadly available to all publishers who participate in in-stream sponsorships worldwide.\\n\\nTwitter Rolls Out Full Archive Search With Premium API: Twitter launched a new premium product for developers that provides access to an account’s full Twitter archive. Twitter notes that this level of access was previously only available to enterprise API customers. Non-enterprise customers could only choose from the standard (public) API that provides access to the last 7 days of tweets or the more recently launched premium API (the Search Tweets API) that goes back 30 days. This new premium API endpoint, called Full-Archive Search, will offer all tweets from all time to the wider developer community.\\n\\nNew year, new access for our developer community! 🎉 Today, we’re launching our premium Search Tweets: Full-archive endpoint — providing access all the way back to @jack’s first Tweet in 2006. https://t.co/emgyLhnUob — Twitter API (@TwitterAPI) February 1, 2018\\n\\nGoogle Introduces New Search Console Beta: Google is starting to release a new beta version to the search console to all users this week. This new version of search console was rebuilt and expanded “by surfacing the most actionable insights and creating an interaction model which guides you through the process of fixing any pending issues.” Google added the ability to share reports within your organization, have access to 16 months of data, and an updated index coverage report.\\n\\nFacebook Rolls Out New Education Campaign Around User Privacy: Facebook introduced a new program that helps users understand how data is used on Facebook and provides tips on how to best manage one’s privacy on the site. In addition to highlighting useful tools like a Privacy Checkup and its core Privacy Principles, the company is also introducing educational videos on important privacy topics like how to control what information Facebook uses to show you ads, how to review and delete old posts, and what it means to delete your account. These videos will be featured in the news feed. Later this year, Facebook will also launch a new Privacy Center, which will group all of the platform’s core privacy settings into a single hub to make them easier to access and manage.\\n\\nPrivacy Controls Education Campaign Posted by Facebook on Sunday, January 28, 2018\\n\\nWant to catch our next show live? Click here to subscribe or add our show to your calendar.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"A $500 PC and an AI Did a Way Better Job Erasing Henry Cavill's Justice League Mustache Than Expensive VFX\", 'Your browser does not support HTML5 video tag.Click here to view original GIF\\n\\nThere’s a long list of things wrong with the Justice League movie, not the least of which being the hasty and poorly-executed digital removal of Henry Cavill’s mustache that he couldn’t shave during the film’s reshoots. But not to worry, some random dude on the internet with a $500 used PC and a world-changing AI just fixed at least one part of that film.\\n\\n\\n\\nIf you haven’t been checking the news, over the past few weeks it’s come to light that artificial intelligence researchers have made incredible leaps and bounds when it comes to applying deep learning technology to manipulating images and video. It’s reached the point where easily accessible programs running on an average desktop PC can swap the face of one performer with another, with staggeringly lifelike results.\\n\\nAdvertisement\\n\\nYour browser does not support HTML5 video tag.Click here to view original GIF\\n\\nIn order to pull off these face-swaps you need to collect and train an AI using thousands of images of the person you want to swap in, but online scraping tools are able to easily access a near unlimited supply of celebrity headshots, like those of Henry Cavill, thanks to search engines like Google. The results aren’t instantaneous, but a few days of intense CPU processing later, some guy working on a run-of-the-mill Windows PC is now able to pull off Hollywood-caliber visual effects tricks, like removing Cavill’s mustache without making him look like a monster in the process.\\n\\nThe less-than-scrupulous parts of the internet have already embraced these AI tools to create fake porn videos of their favorite celebrities, which is understandably causing a lot of concern and scandal. It will soon be impossible to believe anything you see online. But at the same these tools are going to completely revolutionize the visual effects industry, allowing filmmakers without a Hollywood-sized budget to produce FX-laden movies that can easily compete with those coming out of the big studios.\\n\\nAdvertisement\\n\\nIf you run a visual effects studio and aren’t at least paying attention to these recent developments in deep learning and AI image manipulation tools, let alone working to adopt them into your pipeline, you’re missing out on a revolution that could soon put you out of work.\\n\\n[YouTube]')\n",
      "('The latest weapon in the fight against potholes? Your smartphone', 'Share\\n\\nKeeping tabs on the quality of roadways isn’t an easy job. With tens of thousands of miles for public officials to monitor, and a limited budget to do so with, it’s no surprise that some public roads can fall into a state of disrepair that makes them unpleasant to drive on.\\n\\nThat’s something a tech company with the brilliant (if you love puns as much as we do!) name RoadBotics is hoping to help solve. It has developed some smart AI algorithms that work with the cameras found in smartphones to continuously monitor road conditions as drivers travel around the United States. Its deep learning technology is designed to spot the kind of anomalies that experienced roadway inspectors are trained to identify. It then uses this data to create a dynamic map so that public officials can understand the status of their roads, streets, bike paths, walkways and bridges in almost real-time.\\n\\n“We use a standard smartphone and any vehicle, in combination with our cloud-based deep learning platform, to assess the quality of roadways including road surfaces, signage and other common features of urban, rural roads and highways,” RoadBotics CEO Mark DeSantis told Digital Trends. “A standard cell phone is mounted anywhere on a dash or windshield with the phone’s camera pointed forward. The app is turned on and begins collecting video data. That video data is stored on the phone until the the phone sees a friendly Wi-Fi, at which point all of the image data is automatically uploaded to our platform, which then produces a multicolored road network assessment map.”\\n\\nThe technology is currently being used in 22 municipalities, towns, cities, and counties across eight states. DeSantis said that the first deployment outside the U.S. is set to be announced soon.\\n\\n“Currently, we collect the data on behalf of our customers to add to our customer’s convenience as well as learn in detail some of the challenges with collecting data,” he said. “However, we’ve been testing several fleet, customer and even crowd-sourced data collection tools.”\\n\\nWhether private citizens would be willing to collect data for their public officials in exchange for better maintained roadways remains to be seen. Hey, maybe local governments could throw in a small tax credit as a thank you to users who were happy to help!')\n",
      "('Poachers don’t stand a chance against these A.I.-powered camera drones', 'Share\\n\\nSimon Eeman/123RF\\n\\nThe use of artificial intelligence to protect wildlife is something researchers have been working on for a while. However, computer scientists at the University of Southern California are taking it to the next level with the creation of a deep learning-based A.I. system that is able to spot poachers in near real-time, based on video shot from drones.\\n\\nBecause poachers are normally active at night, the challenge the researchers faced was finding a way to distinguish between poachers and animals — both of which give off heat — when viewed on infrared cameras. To solve this, a team led by USC Viterbi School of Engineering Ph.D. student Elizabeth Bondi trained a special type of deep learning neural network, called a Fast R-CNN, on 180,000 labeled images of humans and animals in infrared videos. This taught the computer to distinguish between poachers and the animals they hope to hunt.\\n\\n“Air Shepherd is using unmanned aerial vehicles to locate poachers at night when poaching typically occurs by using thermal infrared cameras aboard the UAVs,” Bondi told Digital Trends. “They then have a monitoring team watch the videos all night to locate potential poachers in the videos and alert law enforcement. Because this is a difficult, arduous process, we have created SPOT (Systematic POacher deTector) to help. SPOT is a deep-learning system that automatically detects humans and animals in thermal infrared videos captured aboard UAVs, and it has been designed for near real-time use in the field, where computing or internet resources may be limited. In practice, SPOT will help the monitoring team by alerting them of humans or animals in the video, and the monitoring team will then be able to contact law enforcement if necessary.”\\n\\nOne of the impressive advances of the USC technology, which can be run on a laptop, is that they were able to reduce the amount of time it takes to process each image from 10 seconds to just 0.3 seconds. When dealing with a moving target, this reduction in time can make all the difference.\\n\\n“During [the technology’s] field test, our partners at Air Shepherd flew their UAV with the thermal infrared camera while law enforcement officials conducted a training exercise,” Bondi said. “Air Shepherd ran SPOT to help their monitoring team identify the humans in the video. We plan to expand trials in Botswana in the next few months, again to be used to help the monitoring team or park rangers identify humans in thermal infrared videos.”')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Like an A.I. acid trip, this neural net rebuilds reality with flowers and fire', 'Share\\n\\nWhen computers get creative, the results are frequently fascinating — as a new project created by artist and machine learning Ph.D. student Memo Akten admirably demonstrates. A bit like projects such as Google’s Deep Dream image generator, Akten has been applying artificial neural networks to create some unusual visual effects. His “Learning To See” project uses image recognition neural nets to interpret the images it sees on a live video feed. The twist? He trained his different neural networks exclusively on a diet of only water, sky, flowers or fire still images so that regardless of what image they actually see, they interpret it as waves crashing, fires roaring, or flowers growing.\\n\\n“In some ways, this was a response to the binary polarization that we see politically in the U.K., in the United States, and in Turkey, which is where I’m from,” Akten told Digital Trends. “The idea is that all of us are only capable of seeing the world through the lens of what we’ve seen before. We incapable of seeing it through other people’s eyes because we’re so colored by what we know. In the case of this piece of work, the neural network has been trained only on certain images — such as waves or fire or flowers. As a result, everything it sees it can only make sense of based on its own experience.”\\n\\nIt’s an intriguing concept, both conceptually and technologically. Particularly impressive from a tech point of view is how fluid the movements look, despite the fact that Akten says the neural networks were trained exclusively on still images. Nonetheless, through analyzing only still images the A.I. has approximated a fairly accurate idea of how fires burn or water moves.\\n\\n“With any emerging technology, artists will always think about how they can apply it to their own domain, whether that’s painting, dance, performance, or whatever else,” Akten continued. “Right now these machine learning technologies are still a bit complex and inaccessible for a lot of people. But there’s a lot of work being done to make these tools into things which can be used by everyone.”')\n",
      "('This tech startup wants to use deep learning to reduce food waste', 'Share\\n\\nAgShift, a technology startup building the first ever autonomous food inspection system, has raised $2 million seed funding from India’s Exfinity Ventures and other companies. The purpose of the fundraising was to bolster product development and reach more customers, to help the startup pursue its mission of reducing global food waste.\\n\\n“Current food inspection processes are paper-based and tedious, needing continuous personal training. Inconsistent & subjective inspections result in a loss of $15.6 billion a year for the organizations responsible — not counting the millions of dollars in recovery costs, claim management and loss of brand reputation incurred by the companies involved,” said AgShift founder and CEO Miku Jha. “At AgShift, we are re-imagining food inspection at various layers — starting from digitizing product specifications, using a mobile-first approach for operational efficiencies to leveraging Deep Learning to make inspections autonomous. Our goal is to standardize food inspection across the entire supply chain and reduce food wastage resulting from inconsistencies in food quality interpretation.”\\n\\nUsing Deep Learning with Computer Vision, AgShift inspects produce for defects, assessing its quality and deciding whether food meets USDA specifications. According to the company, the deep learning models learn to analyze defects with consistency and accuracy, having examined extensive real-world image data to determine and predict the overall quality of a product. By reducing the need for manual inspection of produce quality, the technology can reduce labor costs and ensure standardized quality levels across the board. The food inspection system can also help encourage higher quality of food around the world, thus helping to reduce food waste domestically and internationally.\\n\\n“We have just begun to see the capabilities that this technology can bring to the food industry. AgShift can make a huge impact on reducing 1.3 Billion tons of annual food loss and waste — a complex, real world challenge for everyone,” said Miku Jha.\\n\\nExfinity Ventures was among the companies that financially backed AgShift. “The AgShift team is truly leading the transition of food inspection and quality assessment into the data-driven era, AgShift’s unique application of deep learning with a challenging proposition truly excites us,” said Shailesh Ghorpade, managing partner and CIO of Exfinity Ventures.')\n",
      "('Thanks to A.I., there is finally a way to spot ‘deepfake’ face swaps online', 'Share\\n\\nThe ability to use deep learning artificial intelligence to realistically superimpose one person’s face onto another person’s body sounds like good, wholesome fun. Unfortunately, it’s got a sinister side, too, as evidenced by phenomenon like the popularity of “deepfake” pornography starring assorted celebrities. It’s part of a wider concern about fake news and the ease with which cutting- edge tech can be used to fraudulent effect.\\n\\nResearchers from Germany’s Technical University of Munich want to help, however — and they are turning to some of the same A.I. tools to help them in their fight. What they have developed is an algorithm called XceptionNet that quickly spots faked videos posted online. It could be used to identify misleading videos on the internet so that they could be removed when necessary. Or, at the very least, reveal to users when they have been manipulated in some way.\\n\\n“Ideally, the goal would be to integrate our A.I. algorithms into a browser or social media plugin,” Matthias Niessner, a professor in the university’s Visual Computing Group, told Digital Trends. “Essentially, the algorithm [will run] in the background, and if it identifies an image or video as manipulated it would give the user a warning.”\\n\\nThe team started by training a deep-learning neural network with a dataset of more than 1,000 videos and 500,000 images. By showing the computer both the doctored and undoctored images, the machine learning tool was able to figure out the differences between the two — even in cases where this would be difficult to spot for a human.\\n\\n“For compressed videos, our user study participants could not tell fakes apart from real data,” Niessner continued. On the other hand, the A.I. is able to easily distinguish between the two. Where humans were right 50 percent of the time, making it the equivalent of random guesses, the convolution neural network could get compressed videos right anywhere from 87 percent to 98 percent of the time. This is particularly impressive since compressed images and video are harder to distinguish than uncompressed pictures.\\n\\nCompared to other fraudulent image-spotting algorithms, XceptionNet is way ahead of the curve. It’s another amazing illustration of the power of artificial intelligence and, in this case, of how it can be used for good.\\n\\nA paper describing the work titled, “FaceForensics: A Large-scale Video Data Set for Forgery Detection in Human Faces,” is available to read online.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('When diagnosis time means life or death, NVIDIA’s advanced AI can save lives', 'Share\\n\\nWe may buy new smartphones and laptops every year or two, but when it comes expensive medical computers, that’s not an option. There are more than three million medical equipment installed in hospitals today, and more than 100,000 new instruments added each year — that’s according Nvidia CEO Jensen Huang said at the company’s GPU Technology Conference (GTC). At this rate, it would take more than 30 years to replace all the old hospital equipment. So how do we advance medical care without adding more cost?\\n\\nNvidia’s technique is to leverage the cloud to provide a “virtual upgrade” to existing medical equipment. Dubbed Project Clara, the medical cloud is described by Huang as a “medical imaging supercomputer that is a data center virtualized, remoted, and is a multi-modality, multi-user supercomputer.”\\n\\nIt’s an end-to-end solution leveraging the power of Nvidia’s GPU and its cloud infrastructure, allowing medical practitioners the capability to upload data, analyze and interpret data. Herer at GTC this year, Nvidia is showing off how it uses deep learning to make inferences to detect diseases and pathologies at an earlier state, which could save lives.\\n\\nEarly Detection to Save Lives\\n\\nEarly detection is extremely important in the case of sepsis, a disease that claims more lives each year than the combined mortality rates of breast and prostate cancers. It’s also an area of interest to Johns Hopkins University researchers, and Associate Professor of Computer Science Suchi Saria has worked on models to train AI to make early detection a reality. And similar to Huang, Saria’s AI-trained model examines existing patient sensor data without the need for hospitals to purchase more costly equipment to improve early detection and make medical diagnoses.\\n\\nDeep learning has completely turbocharged modern AI.\\n\\nThis is particularly important because early signs of sepsis are hard to detect, and the condition is often misdiagnosed or ignored until it is too late to treat, Saria said. In fact, for each hour that treatment is delayed — likely as a result of incorrect diagnosis — the mortality rate for sepsis jumps seven to eight percent. Fortunately, though, sepsis is treatable, but only if it’s detected early, Saria noted, highlighting that the condition is the eleventh leading cause of death.\\n\\nIn a case study on sepsis, a female patient was admitted to Johns Hopkins Medical Center for what was believed to be medical pneumonia, Saria said. Doctors administered the usual course of antibiotics, and they weren’t too concerned. Her condition worsens, and on the seventh day, the patient demonstrated visible symptoms of septic shock and was subsequently transferred to the intensive care unit. Once she was in the ICU, her kidneys and lungs began to fail, and the patient passed away on day 22.\\n\\n“The challenge is that sepsis is extremely hard to diagnose,” Saria said. A study conducted by Harvard University revealed that medical experts weren’t able to agree on early diagnosis of sepsis when presented with symptoms of the condition. However, late stages of sepsis, including septic shock, are easier to identify, but at that point, the mortality rate dramatically jumps.\\n\\nTo make early sepsis detection possible, Saria and her team created a framework called TREWS (pronounced “trues”), which stands for Targeted, Real-Time Early Warning System. TREWS is a class of machine learning that utilizes deep learning to identify symptoms and makes medical diagnosis.\\n\\n“Deep learning has completely turbocharged modern AI,” Huang exclaimed. “This incredible algorithm that can automatically detect important features out of data, and from this algorithm it can construct hierarchically knowledge representations. And if you feed it more data, it will become more and more robust.”\\n\\nThe earlier the detection, the better.\\n\\nMaking comparisons to a modern smartphone, the Nvidia medical cloud essentially allows hospitals and medical providers to upload existing data and information collected from patients, create models and leverage the power of artificial intelligence. The result is that diseases can be detected earlier, pathologies can be modeled and easier to understand, and scans become richer with more details and information.\\n\\nThe system “leverages high dimensional, noisy health system data to build things that are very patient specific,” explained Saria. “This brings precision into precision healthcare.” Essentially, TREWS takes a look at all the data and “moves from a reactive to a proactive prevention system.”\\n\\nThe challenge with deep learning, Huang explained, is that “it needs a ton of data and a ton of computers.”\\n\\nIn the case of identifying sepsis, Saria relies on historical data from past patients, utilizing a sequential learning framework, and the end result of TREWS is to have the AI system detect sepsis as early as possible and alert doctors, or ask doctors to perform more tests to determine if the patient does in fact have sepsis.\\n\\nIn the example of the patient succumbing to sepsis, AI would have been able to detect sepsis 12 hours before doctors did. But even beyond sepsis, AI could be used to detect all sorts of other medical conditions such as hypertension and heart disease, Saria said.\\n\\nInferring More Information\\n\\nTo demonstrate some of the advancements that Nvidia’s hardware and software provides researchers with, AI was used to make inferences to show how a patient’s left ventricle would look in 3D and display data such as the heart’s ejection fraction.\\n\\nIn another example, Philips, makers of ultrasound machines, was able to take a black-and-white ultrasound of a fetus in 2D and reconstruct it into a three-dimensional image complete to give it a more life-like appearance. Additionally, using the GPU’s ray tracing capabilities, the ultrasound scan was visualized as if there was virtual light inside the uterus, complete with some subsurface scattering for tones of flesh.\\n\\nOutside of Project Clara, Nvidia is also building the hardware needed to make some of these complex processes happen quickly. Medical imaging requires even more powerful hardware, and Huang claims that “the world wants a gigantic GPU.”\\n\\nTo facilitate early disease detection, more comprehensive scans and deep machine learning and artificial intelligence uses, Nvidia introduced the DGX-2 supercomputer at the GPU Technology Conference.\\n\\nThe company states that the DGX-2 is up to ten times faster than the DGX-1, which was introduced a mere six months prior, and the system is capable of replacing 300 dual-core CPU servers valued at $3 million. Essentially, the DGX-2 is an eighth the cost and occupies a sixtieth the space of existing equipment, while only consuming only an eighteenth the power.\\n\\n“Our strategy at Nvidia is to advance GPU computing for deep learning in AI at the speed of light,” Huang said.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Baidu’s pocket translator is a ‘Star Trek’ dream come to life', 'Share\\n\\nChinese search giant Baidu has long been investing in artificial intelligence research, whether it’s hiring away the creator of the Google Brain project, building autonomous cars or creating a recent A.I. which can mimic your voice after listening to it for only a short period of time. At this week’s MIT Technology Review’s EmTech Digital conference in San Francisco, Baidu gave a demo of one of its most impressive consumer-facing examples of A.I. to date: A Star Trek-style pocket universal translator that is able to translate spoken sentences from English into Mandarin and back again.\\n\\nShown off on stage, the device was used to facilitate a conversation between English speaking senior editor Will Knight and Baidu chief scientist Hua Wu. The device appears to be capable of easily translating questions like “Where can I buy this device?” and “When will machines replace humans?” into Mandarin, and the Mandarin language responses back into English. To do this, it draws on Baidu’s deep-learning neural networks: The same technology which drives Google’s ever-improving machine translation and voice-recognition technology.\\n\\nThe gadget has reportedly been available since December, although right now it can only be leased at travel agencies and airports in China. Its immediate goal is to use its English, Chinese and Japanese translations (more languages will follow in the future) to help people navigate around cities. However, MIT Technology Review claims that other markets are planned for expansion in the future — hopefully meaning that it won’t be too long before we can get our hands on one of these devices.\\n\\nWhile a number of mobile devices can carry out impressive machine translation at present, that doesn’t diminish how impressive Baidu’s achievement is. Not only does it underline how successful the company has been at taking on the Silicon Valley tech giants, but its apparently straightforward ease of use absolutely makes this a gadget we would be interested in picking up. Especially if, at some point, Baidu can combine this with its aforementioned voice soundalike technology.\\n\\nAlong with Google’s in-ear translation earbuds, it seems that we’re living in an incredibly exciting time for machine translation.')\n",
      "('Baidu’s new A.I. can mimic your voice after listening to it for just one minute', 'Share\\n\\nWe’re not in the business of writing regularly about “fake” news, but it’s hard not to be concerned about the kind of mimicry technology is making possible. First, researchers developed deep learning-based artificial intelligence (A.I.) that can superimpose one person’s face onto another person’s body. Now, researchers at Chinese search giant Baidu have created an A.I. they claim can learn to accurately mimic your voice — based on less than a minute’s worth of listening to it.\\n\\n“From a technical perspective, this is an important breakthrough showing that a complicated generative modeling problem, namely speech synthesis, can be adapted to new cases by efficiently learning only from a few examples,” Leo Zou, a member of Baidu’s communications team, told Digital Trends. “Previously, it would take numerous examples for a model to learn. Now, it takes a fraction of what it used to.”\\n\\nBaidu Research isn’t the first to try and create voice-replicating A.I. Last year, we covered a project called Lyrebird, which used neural networks to replicate voices including President Donald Trump and former President Barack Obama with a relatively small number of samples. Like Lyrebird’s work, Baidu’s speech synthesis technology doesn’t sound completely convincing, but it’s an impressive step forward — and way ahead of a lot of the robotic A.I. voice assistants that existed just a few years ago.\\n\\nThe work is based around Baidu’s text-to-speech synthesis system Deep Voice, which was trained on upwards of 800 hours of audio from a total of 2,400 speakers. It needs just 100 5-second sections of vocal training data to sound its best, but a version trained on only 10 5-second samples was able to trick a voice-recognition system more than 95 percent of the time.\\n\\n“We see many great use cases or applications for this technology,” Zou said. “For example, voice cloning could help patients who lost their voices. This is also an important breakthrough in the direction of personalized human-machine interfaces. For example, a mom can easily configure an audiobook reader with her own voice. The method [additionally] allows creation of original digital content. Hundreds of characters in a video game would be able to have unique voices because of this technology. Another interesting application is speech-to-speech language translation, as the synthesizer can learn to mimic the speaker identity in another language.”\\n\\nFor a deeper dive into this subject, you can listen to a sample of the voices or read a paper describing the work.')\n",
      "('Sunglasses, masks won’t fool this facial recognition — and it’s cheaper to run', 'Share\\n\\nFacial recognition systems can enhance security — but many of them can’t adjust for a pair of sunglasses or a surgical mask. Panasonic’s new facial recognition software algorithms, however, can soon account for both in a program that’s up to five times more accurate than earlier programs. The new face recognition server software, WV-ASF950, can still read faces with sunglasses, or faces turned up to 45 degrees from the camera — and before the end of the year, will also be able to recognize faces obscured by a surgical mask.\\n\\nAccording to comparison tests by the National Institute of Standards and Technology (NIST), the software improves facial recognition by up to 500 percent. The deep learning software uses a similarity calculation method that allows the program to recognize faces in scenarios most systems struggle with. Along with faces turned from the camera and sunglasses, the program can still recognize faces from the database that have aged from the image in the server.\\n\\nWhile designed for security applications, the software also integrates some of the same technology Panasonic uses in their consumer cameras. An A.I. auto-optimizes the camera’s settings in order to achieve a better exposure for higher-quality video. Along with detecting the amount of light in the scene, the system also works to detect how fast subjects are moving to find those optimal settings.\\n\\nArtificial intelligence also helps the software actually keep costs down, too, Panasonic says. Instead of sending every image to the server, on-camera processing chooses the best shots and sends only those images to the server. By sending images selectively to the server, the system lowers bandwidth requirements, which Panasonic says lowers the cost of operating and building the system. That same concept also allows 20 cameras to run from one server.\\n\\nThe program can recognize up to 10,000 faces in a database, while larger facilities can use an expansion pack to bring that number up to 30,000.\\n\\nPanasonic worked with the National University of Singapore on the development of the algorithms. The facial recognition system will launch in July (excluding availability in Japan, which will come one month later) and the ability to recognize faces obscured by a surgical mask will be added to the system before the end of the year.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Horizon Robotics’ smart security camera uses A.I. for serious facial recognition', 'Share\\n\\nThe smart cameras of 2018 are getting smarter still. Startup Horizon Robotics has debuted a new HD smart camera that boasts serious artificial intelligence capabilities and can identify faces with an accuracy of up to 99.7 percent, the company claims. That’s probably better than most humans. The company claims the new HR-IPC2143 smart camera is the “first of its kind with applications in public security for the real-time detection of potential suspects. In less ominous applications, it can identify important customers.\\n\\n“The smart camera is a reflection of Horizon Robotics’ mission — to provide high-performance, low-power, and low-cost products utilizing cutting-edge artificial intelligence technology,” said Dr. Kai Yu, CEO and founder of Horizon Robotics. “Other companies can only deliver a portion of the equation, but we are able to integrate all these form factors to create a complete business solution rather than just a stand-alone camera.”\\n\\nThe HR-IPC2143 leverages recognition capabilities alongside its local capture to give users facial capture, feature extraction, and facial feature comparison based on deep-learning algorithms. By using an “algorithm + chip + cloud” system architecture, this camera can quickly turn raw video data into actionable insights, with multi-target location detection, multi-target tracking, multi-target recognition, and pedestrian attribute classification. Thanks to its 50,000 feature library, the camera is able to support high-performance facial recognition, as well as process videos at 1920x1080p at 30 frames per second in real time. It also allows the camera to detect, track, and identify up to 200 objects simultaneously, all in a single frame.\\n\\nThe HR-IPC2143 has some pretty impressive surveillance chops, as it can recognize faces even in crowded environments, like packed hallways, entries and exits, and large events. Or, if a store owner wants to identify VIP shoppers as they enter the premises to ensure that they receive special treatment, the smart camera can be leveraged in this way as well.\\n\\nKey to the camera’s success is the “Sunrise” embedded A.I. computer vision processor, which boasts the Brain Processing Unit (BPU) architecture. This proprietary A.I. processor architecture claims to combine A.I. algorithms with hardware design, and allows the relatively small camera to still perform at its peak. “Compared with previous smart cameras, the ability to capture and identify images simultaneously is a major breakthrough in the field of embedded A.I. in security and commercial applications,” Horizon Robotics noted in a release.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ModiFace replaces makeup brushes with neural networks, and it’s coming to the S9', 'Share\\n\\nBrenda Stolyar/Digital Trends Samsung’s Galaxy S9 and S9 Plus come with a variety of new camera features including AR Emoji, Super Slow Motion, and a variable aperture. But there’s one more new trick that we’ve never seen built into a smartphone before – the augmented reality makeup tool in Bixby Vision. Baked into the S9’s camera app, Makeup lets you apply different products to your face with augmented reality. Point the selfie camera at your face, and the makeup styles you pick are layered over your face, exactly like Snapchat filters. The benefit is you’re trying on makeup in the comfort of your own home, while also avoiding the need to clean up afterwards. It’s the first time we’ve seen this kind of feature built into a smartphone, but the technology is far from new. Samsung tapped ModiFace, a company with more than a decade of research in this field, to integrate its beauty AR technology into the camera. From skincare to augmented reality makeup\\n\\nIf you’ve ever used the Sephora app to virtually try-on makeup, or Benefit’s Brow Try-On app to test out a new eyebrow shape, then you’ve used ModiFace’s AR technology. The company now powers more than 200 custom augmented reality apps for high-end beauty brands, but it all initially started with skincare. Smartphone popularity inspired ModiFace to test its technology with augmented reality. Before the smartphone revolution, ModiFace worked with dermatologists and certain brands to help people find solutions for skincare concerns. People could upload a photo of themselves to a web app, and the company would process it to pinpoint problem areas and suggest improvements to skincare routines. ModiFace originally used 2D images with the web app, but as technology progressed, the company was soon able to move to allowing users to upload video. When smartphones got popular, ModiFace began testing its technology with augmented reality, and added makeup and hair to its roster of supported features. “With the smartphone boom, it was the perfect opportunity for us to expand because it allowed us to bring AR right to our users in the palm of [their] hand,” Jeff Houghton, ModiFace vice president of Technology, told Digital Trends. Its current software development kits (SDKs) are the culmination of 10 years of engineering. By working closely with beauty brands, ModiFace is able to provide a lightning fast and easy-to-follow user experience, while also encouraging product discoverability. ModiFace’s technology powers more than 200 augmented reality apps for high-end beauty brands, including numerous smart mirrors, that allow customers virtually try on makeup in real-time. Photo Credit: ModiFace. Its technology doesn’t stop at smartphone apps, extending to retail stores as well. Back in November, MAC Cosmetics debuted its MAC Virtual Try-On Mirror at certain locations. The mirror lets customers virtually try on makeup in real-time by swiping throughout the interface. It helps eliminate the need to test a lot of products at the store, narrowing it down to the styles you like the most.\\n\\nPutting its tech on the Galaxy S9\\n\\nThe same technology and concept is what’s available on the new Galaxy S9. By collaborating with Samsung, ModiFace was able to optimize the experience.\\n\\nThe partnership\\n\\ncould also kick start a\\n\\nnew trend for phone manufacturers.\\n\\n“Starting in house, we train our Neural Networks on thousands of images to create the base tracking and face analysis for our apps,” Houghton said. “This Neural Network is then embedded inside Samsung’s app. We worked with Samsung to tweak several parameters to make sure we were achieving the effects that brands and end users want to see.”\\n\\nThe partnership between the two companies could kick start a new trend for phone manufacturers. While beauty technology is still in its infancy, makeup has found its way on to our screens for years through social media. Whether it’s YouTube or Instagram tutorials, people are constantly looking to their smartphones to find new inspiration when it comes to makeup. With the Galaxy S9, all you need to do is swipe open the camera and you have a catalog of makeup at your disposal.\\n\\nOn the Galaxy S9, you can search through a variety of different cosmetic products available from both Sephora or Cover Girl, and you can purchase them on the spot. As you scroll through each product, it will apply itself to your face like Snapchat filters.\\n\\nUsing facial tracking and 3D video rendering, the makeup filters are mapped to the face at 30 frames per second. There’s no lag, and nothing is misaligned. It works as instantly as you to tap the next product you want to try on. You can try on complete looks – which includes lipstick, foundation, eyeshadow, blush, mascara and eyeliner – or you can try them all separately.\\n\\nTrying on foundation can be extremely tricky, almost always requires being at the store and having an employee help you find your exact shade. There’s also different foundation types to take into account – matte, sheer, water-based, or ones that provide full coverage. To make sure the shades are as accurate as possible, the ModiFace team tediously went through each product and compared how it looked in real life versus how it looked on the smartphone. They worked with both Samsung and the brands directly to create a matching render.\\n\\n“I myself was actually involved with that process and it’s kind of a lot of fun,” Houghton said. “You get to try on a lot of different things, and then you get to wipe them off and see how they look in the app. It involves a lot of screenshot comparison and manual work to make sure that the product we’ve included is just perfect.”\\n\\nThe team went through each product and compared how it looked in real life\\n\\nversus how it looked\\n\\non the smartphone.\\n\\nBut there’s still always room for improvement. Using deep learning and image research, the team studies internal data — gathered from those who are testing the final product — to learn more about a user’s face in order to train its algorithms. This helps to produce better results when it comes to factors like how colors mix on to your lip, how light gets added into those colors, and how your face should look under different lighting if you have a specific foundation on. The data also helps to improve texture and overall coverage of the makeup.\\n\\nWith this kind of deep learning, the company is able to create new and more realistic effects. Recently, ModiFace partnered with L’Oreal to create 3D hair tracking that can recolor your hair in real time. The company has plans to work with Samsung to bring similar effects to its phones.\\n\\nBut for those with privacy concerns, Modiface doesn’t collect or monitor any data from its applications when in the hands of users. Any images you take with the apps will never leave your device.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Crime-predicting A.I. isn’t science fiction. It’s about to roll out in India', 'Share\\n\\nArtificial intelligence programs promise to do everything, from predicting the weather to piloting autonomous cars. Now AI is being applied to video surveillance systems, promising to thwart criminal activity not by detecting crimes in progress but by identifying a crime–before it happens. The goal is to prevent violence such as sexual assaults, but could such admirable intentions turn into Minority Report-style pre-crime nightmares?\\n\\nSuch a possibility may seem like a plot line from an episode of Black Mirror, but it’s no longer the stuff of science fiction. Cortica, an Israeli company with deep roots in security and AI research, recently formed a partnership in India with Best Group to analyze the terabytes of data streaming from CCTV cameras in public areas. One of the goals is to improve safety in public places, such as city streets, bus stops, and train stations.\\n\\nIt’s already common for law enforcement in cities like London and New York to employ facial recognition and license plate matching as part of their video camera surveillance. But Cortica’s AI promises to take it much further by looking for “behavioral anomalies” that signal someone is about to commit a violent crime.\\n\\nThe software is based on the type of military and government security screening systems that try to identify terrorists by monitoring people in real-time, looking for so-called micro-expressions — minuscule twitches or mannerisms that can belie a person’s nefarious intentions. Such telltale signs are so small they can elude an experienced detective but not the unblinking eye of AI.\\n\\nAt a meeting in Tel Aviv before the deal was announced, co-founder and COO Karina Odinaev explained that Cortica’s software is intended to address challenges in identifying objects that aren’t easily classified according to traditional stereotypes. One example Odinaev described involved corner cases (such as a bed falling off a truck on the highway) that are encountered in driving situations, precisely the sort of unique events that programs controlling autonomous cars will have to be able to handle in the future.\\n\\n“For that, you need unsupervised learning,” Odinaev said. In other words, the software has to learn in the same way that humans learn.\\n\\nGoing directly to the brain\\n\\nCortica’s AI software monitors people in real-time, looking for micro-expressions — minuscule twitches or mannerisms that can belie a person’s nefarious intentions.\\n\\nTo create such a program, Cortica did not go the neural network route (which despite its name is based on probabilities and computing models rather than how actual brains work). Instead, Cortica went to the source, in this case a cortical segment of a rat’s brain. By keeping a piece of brain alive ex vivo (outside the body) and connecting it to a microelectrode array, Cortica was able to study how the cortex reacted to particular stimuli. By monitoring the electrical signals, the researchers were able to identify specific groups of neurons called cliques that processed specific concepts. From there, the company built signature files and mathematical models to simulate the original processes in the brain.\\n\\nThe result, according to Cortica, is an approach to AI that allows for advanced learning while remaining transparent. In other words, if the system makes a mistake — say, it falsely anticipates that a riot is about to break out or that a car ahead is about to pull out of a driveway — programmers can easily trace the problem back to the process or signature file responsible for the erroneous judgment. (Contrast this with so-called deep learning neural networks, which are essentially black boxes and may have to be completely re-trained if they make a mistake.)\\n\\nInitially, Cortica’s Autonomous AI will be used by Best Group in India to analyze the massive amounts of data generated by cameras in public places to improve safety and efficiency. Best Group is a diversified company involved in infrastructure development and a major supplier to government and construction clients. So it wants to learn how to tell when things are running smoothly — and when they’re not.\\n\\nBut it is hoped that Cortica’s software will do considerably more in the future. It could be used in future robotaxis to monitor passenger behavior and prevent sexual assaults, for example. Cortica’s software can also combine data not just from video cameras, but also from drones and satellites. And it can learn to judge behavioral differences, not just between law abiding citizens and erstwhile criminals, but also between a peaceful crowded market and a political demonstration that’s about to turn violent.\\n\\nSuch predictive information would allow a city to deploy law enforcement to a potentially dangerous situation before lives are lost. However, in the wrong hands, it could also be abused. A despotic regime, for example, might use such information to suppress dissent and arrest people before they even had a chance to organize a protest.\\n\\nPredictive crime software would allow a city to deploy law enforcement to a potentially dangerous situation before lives are lost. However, in the wrong hands, it could also be abused.\\n\\nIn New York City, during a demonstration of how Cortica’s Autonomous AI is being applied to autonomous cars, Cortica’s vice resident, Patrick Flynn, explained that the company is focused on making the software efficient and reliable to deliver the most accurate classification data possible. What clients do with that information — stop a car or make it speed up to avoid an accident, for example — is up to them. The same would apply to how a city or government might allocate police resources.\\n\\n“The policy decisions are strictly outside of Cortica’s area,” Flynn said.\\n\\nWould we give up privacy for improved security?\\n\\nNevertheless, the marriage of AI to networks that are ubiquitous of webcams is starting to generate more anxiety about privacy and personal liberty. And it’s not just foreign despotic governments that people are worried about.\\n\\nIn New Orleans, Mayor Mitch Landrieu has proposed a $40 million crime-fighting surveillance plan, which includes networking together municipal cameras with the live feeds from private webcams operated by businesses and individuals. The proposal has already drawn public protests from immigrant workers concerned that federal immigration officials will use the cameras to hunt down undocumented workers and deport them.\\n\\nMeanwhile, like subjects trapped in a Black Mirror world, consumers may already be unwittingly submitting themselves to such AI-powered surveillance. Google’s $249 Clips camera, for example, uses a rudimentary form of AI to automatically take pictures when it sees something it deems significant. Amazon, whose Alexa is already the subject of eavesdropping paranoia, has purchased popular video doorbell company Ring. GE Appliances is also planning to debut a video camera equipped hub for kitchens later this year. In Europe, Electrolux will debut a steam oven this year with a built-in webcam.\\n\\nWhile these technologies raise the specter of Big Brother monitoring our every move, there’s still the laudable hope that using sophisticated AI like Cortica’s program could improve safety, efficiency, and save lives. One can’t help wondering, for example, what would have happened if such technology were available and used in the Uber that 19-year-old Nikolas Cruz took on his way to murder 17 people at Marjory Stoneman Douglas High School. The Uber driver didn’t notice anything amiss with Cruz, but could an AI equipped camera have detected microexpressions revealing his intentions and alerted the police? In the future, we may find out.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('LG’s 2018 premium TV lineup offers stunning picture with brains to back it up', 'Share\\n\\nLG’s models were among the most impressive TVs we saw at CES 2018, but the company didn’t go into many details about the overall lineup that we should expect this year. That has changed now as on March 6, the company unveiled its entire 2018 premium TV lineup at an event in Seoul, South Korea. We’ve got all the information below, but if you’re just interested in prices and screen sizes, see the table at the end of the article.\\n\\nWhile all the TVs announced today are premium, the cream of the 2018 LG TV crop are the OLED models, which include the W8, G8, E8, C8, and B8. The top-of-the-line W8 is the newest model of LG’s wallpaper TV and will be available in 77-inch and 65-inch screen sizes. While last year’s W7 model was fantastic on its own, the W8 features improved processing thanks to LG’s Alpha 9 processor, which supports high dynamic range (HDR), native 4K/120 frames-per-second content and features improved algorithms for clearer picture quality. Like last year’s model, Dolby Atmos is included for fantastic sound from the integrated sound bar with nothing else to add. The 77-inch model of the W8 will sell for $15,000 and will be available beginning the week of April 16, while the 65-inch model will retail for $7,000 starting the week of July 16.\\n\\nAll the other OLED models also feature Dolby Atmos and use the Alpha 9 processor as well, aside from the B8 which uses the Alpha 7 found in last year’s models. As with the W8, high dynamic range (HDR) is supported, including Dolby Vision and Hybrid Log Gamma (HLG).\\n\\nThe 77-inch C8 model will sell for $9,000 and will be available starting the week of April 12, while the 65-inch and 55-inch models will sell for $3,500 and $2,500 respectively, and will be available later this month beginning March 19. The E8 series will be offered in 65-inch and 55-inch options, which sell for $4,500 and $3,500 respectively, and will be available starting the week of April 2. Other models are available in the G8 and the B8 series, which will be available in 65-inch and 55-inch varieties. So far, no pricing or availability information is available for either of those two options.\\n\\nOLED might be the brightest star of the show, but LG’s Super UHD models show that you can still get an amazing picture out of an LCD screen. These models use LG’s Nano Cell technology and Full-Array Local Dimming to ensure black levels that approach the ranks of LG’s OLED TVs, with improved shadow details and wider viewing angles than last year’s models. Like the OLED models, the Super UHD TVs support HDR including Dolby Vision and HLG, as well as Dolby Atmos.\\n\\nModel Screen Size Price W8 77-inch $15,000 W8 65-inch $7,000 C8 77-inch $9,000 C8 65-inch $3,500 C8 55-inch $2,500 E8 65-inch $4,500 E8 55-inch $3,500 SK9500 65-inch $2,700 SK9000 65-inch $2,200 SK9000 55-inch $1,600 SK8000 75-inch $2,700 SK8000 65-inch $2,000 SK8000 55-inch $1,200 SK8000 49-inch $1,000\\n\\nThese TVs will be available in nine different sizes ranging from 49 to 75 inches across three different series. The SK9500 series consists of a single 65-inch model that will sell for $2,700 and will go on sale beginning later this month on March 19. The SK9000 series will be available even sooner, with 65-inch and 55-inch models selling for $2,200 and $1,600 respectively and going on sale next week. There are four models in the SK8000 series, with the largest being a 75-inch model that sells for $2,700 and will hit stores the week of April 23. Both the 65-inch and 55-inch models will go on sale the week of April 2, selling for $2,000 and $1,200, respectively. Finally, the 49-inch model will sell for $1,000 beginning the week of May 7.\\n\\nUnsurprisingly given LG’s recent focus on artificial intelligence, as reflected in its ThinQ platform, these new TVs are packed with A.I.-driven features. Natural Language Processing (NLP) lets viewers control their TVs with phrases like “search for this movie’s soundtrack” or “turn off the TV when this program is over,” without having to specify the name of the movie they’re watching or the exact time they want the TV to turn off. This is powered by LG’s Deep ThinQ deep learning platform and will be available in 14 countries including the United States, Canada, Mexico, and the United Kingdom.\\n\\nUpdated on March 6: Added pricing and availability information.')\n",
      "('The Google Lens app is now available on Google Photos for iOS', 'Share\\n\\nAs announced via the official Google Photos Twitter account on March 15, users of Google Photos for iOS can now use Google Lens to analyze and extract text, hyperlinks, and other information from photos.\\n\\nOriginally announced at Google’s I/O 2017 event, Google Lens uses machine learning to extract text and hyperlinks from images, along with its ability to identify various landmarks from around the world and a host of other promised abilities. It first launched on Google’s Pixel phones at the tail end of 2017, before being launched for all Android phones in March 2018. As of today, March 16, iOS users can also access the deep learning of the Google Lens by accessing it through the iOS Google Photos app.\\n\\nStarting today and rolling out over the next week, those of you on iOS can try the preview of Google Lens to quickly take action from a photo or discover more about the world around you. Make sure you have the latest version (3.15) of the app.https://t.co/Ni6MwEh1bu pic.twitter.com/UyIkwAP3i9 — Google Photos (@googlephotos) March 15, 2018\\n\\nAnyone looking to play with Google Lens should make sure that their Google Photos app is updated to the latest version (3.15). Then, open your Google Photos app, open a photo, then tap the Google Lens logo. If you’re struggling to find it, Google has posted a small guide on its support website. Some Twitter users have been complaining that they have not yet been able to access the functionality, and it seems that the update is in the process of rolling out worldwide. It’s also worth noting that Google Lens can only be used if your iOS device’s language is set to English, for the time being.\\n\\nBut what can you do with Google Lens? It’s capable of extracting text from your Google Photos, and while that may not sound impressive, it’s then able to use that text to find businesses, extract hyperlinks, find addresses, or identify books, movies, and games. If you take a picture of a business card, Google Lens will offer to save the information as a new contact, taking some of the fuss out of business networking. Landmarks can also be identified, and information on ratings, tours, and history will be offered as a result.\\n\\nUse Google Lens to copy and take action on text you see. Visit a website, get directions, add an event to your calendar, call a number, copy and paste a recipe, and more. pic.twitter.com/E4ww2cxVUd — Google Photos (@googlephotos) March 15, 2018\\n\\nThe Google Photos account has been sharing more than a few ways to make your Google Lens work for you, and while that fact that it’s currently restricted to the Google Photos app on iOS means it’s a bit harder to use in everyday circumstances, it’s a really cool addition, and a great indication of what the future has in store for us.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Nvidia CEO maintains that cryptocurrency mining “is not their business”', 'Share\\n\\nWith interest in cryptocurrency and crypto mining at an all time high, some were surprised that NVIDIA didn’t focus on that segment at its annual GPU Technology Conference (GTC). The company has had a rocky relationship with crypto-miners, who have in recent years intensified demand for NVIDIA’s GPUs. While this is good for the bottom line, it also makes it hard to keep graphics card in stock for gamers and other users.\\n\\nAfter noting that cryptocurrency mining represents only a small fraction of NVIDIA’s GPU business, CEO Jensen Huang distanced himself from the Bitcoin market, stating, “We’re not involved in Bitcoin at all. Bitcoin [mining] is done largely by ASICs today.” Unlike the GPU, which can handle a range of high-level computing power from deep learning to artificial intelligence, ASICs are efficient application-specific integrated circuit designed for a single task.\\n\\nIt is estimated that cryptocurrency mining accounted for at least six percent of GPU sales last year, and that number is poised to grow this year. Where NVIDIA has a prominent role in the cryptocurrency market is Ethereum, a competing type of cryptocurrency.\\n\\n“What uses our GPU is Ethereum,” Huang said. “Ethereum ‘ether’ was designed as an algorithm to ensure that no singular entity or a few entities has the power to control the ether. It was designed so that the algorithm requires the type of computing capabilities — the type of processing capabilities — that are made possible by the use of GPUs in a distributed system.”\\n\\nHuang didn’t bother with a mention of cryptocurrency at the company’s keynote earlier this week, despite that many gamers assume mining is to blame for the rising prices of GPUs.\\n\\n“The reason why the GPU is so popular with Ethereum is because the GPU is the single largest distributed supercomputer in the world,” he said. “It is the only supercomputer that is literally in everyone’s hands. And as a result, no single entity can control currency.”\\n\\nAhead of the start of the 2018 GTC conference earlier this week, a Susquehanna market analyst raised some concerns that NVIDIA and rival AMD may encounter increased competition in the cryptocurrency mining market as Bitmain may soon release an ASIC that is capable of mining Ethereum.\\n\\nHuang appeared unfazed, however. While promoting the benefits of GPUs for the cryptocurrency market, he maintains that cryptocurrency mining is not NVIDIA’s business. Instead, GTC’s major focus is to leverage the power of the GPU in systems like the newly launched DGX-2, Tesla, Drive, and GV100s to make advancements in gaming, workstation, artificial intelligence, healthcare and cloud computing.\\n\\n“Quite frankly, I prefer that our GPUs are available to be used in those areas,” Huang stated, referring to AI and gaming. “And the reason is that there is so much shortage of GPUs at the moment. It’s hard for us to keep it in stock.”')\n",
      "('Pentax K-1 Mark II uses AI to help you get the perfect shot', 'Share\\n\\nPentax’s full-frame DSLR is back for an encore — on Wednesday, Feb. 21, Ricoh announced the Pentax K-1 Mark II, the successor to the company’s first full-frame DSLR. The Mark II is still built around the same 36.4-megapixel full-frame sensor, but uses a new accelerator unit that works in concert with the Prime IV processor to improve image quality. The camera also sees the signature Pixel Shift Resolution system expanded and gets a new automatic mode that uses artificial intelligence for better results.\\n\\nAs with the K1, the sensor foregoes an antialiasing filter for improved sharpness, while the new accelerator unit “optimizes the image data” from the sensor before sending it to the processor. This, Pentax claims, keeps noise levels lower for improved resolution at all ISOs — all the way to 819,200. Image processing has also been updated for better color, with a focus on “deep blues and lively greens,” according to the press release.\\n\\nThe Pixel Shift Resolution mode was one of the highlights of the original K1, and now it’s even better. It still takes four images, shifting the sensor by a pixel’s width each time to record full RGB color information at every pixel, but now it can be used handheld in the new Dynamic mode. It works by using the sensor-shift system to analyze camera shake, relying on those small movements themselves to produce the four frames of the composite image. It’s not guaranteed to work with all subjects in all conditions, but it’s a clever way of getting a multi-shot high-resolution mode to work without a tripod, something other manufacturers have not been able to do.\\n\\nThe same sensor-shift system behind the Pixel Shift modes also offers five-axis stabilization, which promises up to 5 stops of shake reduction. Like in the first K1, this system can also be used to mimic the effect of an antialiasing filter by intentionally adding microscopic amounts of blur at the pixel level, thus avoiding moiré which is otherwise more common on sensors lacking AA filters.\\n\\nAutoexposure has also gotten smarter, with a new Real-Time Scene-Analysis System based on a deep-learning algorithm. It analyzes brightness, color, and movement to help the camera better interpret the content of a scene, leading to more accurate exposures.\\n\\nBut for as high-tech as it is, the K1 Mark II still falls short of its full-frame DSLR peers in some areas. The 33-point autofocus system isn’t exactly earth-shattering, although Ricoh says it does feature improved subject tracking over the original K1. Even so, with a relatively slow continuous shooting rate of just 4.4 frames per second, and a rather paltry 17-exposure buffer (for RAW files), this isn’t likely a camera many people will be using for shooting sports and action. It also lacks 4K video, offering just 1080p at 30 frames per second — although, it can produce internal time-lapse videos at 4K resolution.\\n\\nThe Pentax K-1 Mark II is expected to start shipping in April, with a suggested retail price of $2,000, putting it in line with entry-level full-frame cameras and well below competing models with similar pixel counts. It could be a good budget option for anyone who needs a ultra-high-resolution camera, particularly landscape and studio shooters who don’t have high speed shooting requirements.\\n\\nCurrent K1 owners can also take advantage of an upgrade program, through which Ricoh will replace the main circuit board on their cameras to add new Mark II features. Cost for the upgrade will be $550, and Ricoh says more information on the program will be coming soon.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Background removal with deep learning', \"Background removal with deep learning\\n\\nThis post describes our work and research on the greenScreen.AI. We’ll be happy to hear thoughts and comments -On Twitter, Linkedin\\n\\nIntro\\n\\nThroughout the last few years in machine learning, I’ve always wanted to build real machine learning products.\\n\\nA few months ago, after taking the great Fast.AI deep learning course, it seemed like the stars aligned, and I have the opportunity: The advances in deep learning technology permitted doing many things that weren’t possible before, and new tools were developed and made the deployment process more accessible than ever.\\n\\nIn the aforementioned course, I’ve met Alon Burg, who is an experienced web developer, an we’ve partnered up to pursue this goal. Together, we’ve set ourselves the following goals:\\n\\nImproving our deep learning skills Improving our AI product deployment skills Making a useful product, with a market need Having fun (for us and for our users) Sharing our experience\\n\\nConsidering the above, we were exploring ideas which:\\n\\nHaven't been done yet (or haven't been done properly) Will be not too hard to plan and implement — our plan was 2–3 months of work, with a load of 1 weekly work day. Will have an easy and appealing user interface — we wanted to do a product that people will use, not only for demonstration purposes. Will have training data readily available — as any machine learning practitioner knows, sometimes the data is more expensive than the algorithm. Will use cutting edge deep learning techniques (which were still not commoditized by Google, Amazon and friends in their cloud platforms) but not too cutting edge (so we will be able to find some examples online) Will have the potential to achieve “production ready” result.\\n\\nOur early thoughts were to take on some medical project, since this field is very close to our hearts, and we felt (and still feel) that there is an enormous number of low hanging fruits for deep learning in the medical field. However, we realized that we are going to stumble upon issues with data collection and perhaps legality and regulation, which was a contradiction with our will to keep it simple. Our second choice was a background removal product.\\n\\nBackground removal is a task that is quite easy to do manually, or semi manually (Photoshop, and even Power Point has such tools) if you use some kind of a “marker” and edge detection, see here an example. However, fully automated background removal is quite a challenging task, and as far as we know, there is still no product that has satisfactory results with it, although some do try.\\n\\nWhat background will we remove? This turned out to be an important question, since the more specific a model is in terms of objects, angle, etc. the higher quality the separation will be. When starting our work, we thought big: a general background remover that will automatically identify the foreground and background in every type of image. But after training our first model, we understood that it will be better to focus our efforts in a specific set of images. Therefore, we decided to focus on selfies and human portraits.\\n\\nBackground removal of (almost) human portrait\\n\\nA selfie is an image with a salient and focused foreground (one or more “persons”) guarantees us a good separation between the object (face+upper body) and the background, along with quite an constant angle, and always the same object (person).\\n\\nWith these assumptions in mind, we embarked on a journey of research, implementation and hours of training to create a one click easy to use background removal service.\\n\\nThe main part of our work was training the model, but we couldn't underestimate the importance of proper deployment. Good segmentation models are still not compact as the classification model (e.g SqueezeNet) and we actively examined both server and browser deployment options.\\n\\nIf you want to read more details about the deployment process(es) of our product, your are welcomed to check out our posts on server side and client side.\\n\\nIf you want to read about the model and it’s training process, keep going.\\n\\nSemantic Segmentation\\n\\nWhen examining deep learning and computer vision tasks which resemble ours, it is easy to see that our best option is the semantic segmentation task.\\n\\nOther strategies, like separation by depth detection also exist, but didn’t seem ripe enough for our purposes.\\n\\nSemantic segmentation is a well known computer vision task, one of the top three, along with classification and object detection. The segmentation is actually a classification task, in the sense of classifying every pixel to a class. Unlike image classification or detection, segmentation model really shows some “understanding” of the images, in not only saying “there is a cat in this image”, but pointing where and what is the cat, on a pixel level.\\n\\nSo how does the segmentation work? To better understand, we will have to examine some of the early works in this field.\\n\\nThe earliest idea was to adopt some of the early classification networks such as VGG and Alexnet. VGG was the state of the art model back in 2014 for image classification, and is very useful nowadays because of it’s simple and straightforward architecture. When examining VGG early layers, it may be noticed that there are high activation around the item to classify. Deeper layers have even stronger activation, however they are coarse in their nature since the repetitive pooling action. With these understandings in mind, it was hypothesized that classification training can also be used with some tweaks to finding/segmenting the object.\\n\\nEarly results for semantic segmentation emerged along with the classification algorithms. In this post, you can see some rough segmentation results that come from using the VGG:\\n\\nlate layer results:\\n\\nSegmentation of the buss image, light purple (29) is school bus class\\n\\nafter bilinear upsampling:\\n\\nThese results comes from merely converting (or maintaining) the fully connected layer into it’s original shape, maintaining its spatial features, getting a fully convolutional network. In the example above, we feed a 768*1024 image into the VGG, and get a layer of 24*32*1000. the 24*32 is the pooled version of the image (by 32) and the 1000 is the image-net class count, from which we can derive the segmentation above.\\n\\nTo smooth the prediction, the researchers used a naive bilienar upsampling layer.\\n\\nIn the FCN paper, the researchers improved the idea above. They connected some layers along the way, to allow a richer interpretations, which were named FCN-32, FCN-16 and FCN-8, according the up-sampling rate:\\n\\nAdding some skip connections between the layers allowed the prediction to encode finer details from the original image. Further training improved the results even more.\\n\\nthis technique showed itself as not so bad as might have been thought, and proved there is indeed potential in semantic segmentation with deep learning.\\n\\nFCN results from the papaer\\n\\nThe FCN unlocked the concept of segmentation, and researchers tried different architectures for this task. The main idea stays similar: using known architectures, up-sampling, and using skip connections are still prominent at the newer models.\\n\\nYou can read about advances in this field in a few good posts: here, here and here. You can alsosee that most architectures keep the encoder- decoder architecture.\\n\\nBack to our project\\n\\nAfter doing some research, we settled on three models, which were available to us: the FCN, Unet and Tiramisu — very deep encoder-decoder atchitecture. We also had some thoughts about the mask-RCNN, but implementing it seemed outside of our projects scope.\\n\\nFCN didn’t seem relevant since its results weren’t as good as we would have like (even as a starting point), but the 2 other models we’ve mentioned showed results that were not bad: the tiramisu on the CamVid dataset, and the Unet main advantage was it’s compactness and speed. In terms of implementations, the Unet is quite straightforward to implement (we used keras) and the Tiramisu was also implementable. To get us started, we’ve used a good implementation of Tiramisu at the last lesson of Jeremy Howard’s great deep learning course.\\n\\nWith these two models, we went ahead and started training on some data-sets. I must say that after we first tried the Tiramisu, we saw that its results had much more potential for us, since it had the ability to capture sharp edges in an image. from the other hand, the unet seemed not fine enough, and the results seemed a bit blobbish.\\n\\nUnet blobbishness\\n\\nThe data\\n\\nAfter having our general direction set with the model, we started looking for proper datasets. Segmentation data is not as common as classification or even detection. Additionally, manual tagging is not really a possibility. The most common datasets for segmentation were the COCO dataset, which includes around 80K images with 90 categories, the VOC pascal dataset with 11K images and 20 classes, and the newer ADE20K datasets.\\n\\nWe chose to work with the COCO dataset, since it includes much more images with the class “person” which was our class of interest.\\n\\nConsidering our task, we pondered if we’ll use only the images that are super relevant for us, or use more general dataset. On one hand, using a more general dataset with more images and classes will allow the model to deal with more sceneries and challenges. On the other hand, on overnight training session allowed us going over ~150K images. If we’ll introduce the model with the entire COCO dataset, we will end with the model seeing each image twice (on average) therefore trimming it a little bit will be beneficial. additionally, it will results in a more focused model for our purposes.\\n\\nOne more thing that is worth mentioning — the Tiramisu model was originally trained on the CamVid dataset, which has some flaws, but most importantly it’s images are very monotonous: all images are road pics from a car. As you can easily understand, learning from such dataset (even though it contains people) had no benefit for our task, so after a short trial, we moved ahead.\\n\\nImages from CamVid dataset\\n\\nThe COCO dataset ships with prety straight-forward API which allowed us to know exactly what objects are at each image (according th 90 predifiened classes)\\n\\nAfter some experimenting, we’ve decided to dilute the dataset: first we filtered only the images with a person in them, leaving us with 40K images. Then, we dropped all the images with many people in them, and left with only 1 or 2, since this is what our product should find. Finally, we left only the images where 20%-70% of the image are tagged as the person, removing the images with a very small person in the background, or some kind of weird monstrosity (unfortunately not all of them). Our final dataset consisted of 11K images, which we felt was enough at this stage.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Understanding Deep Learning Through Neuron Deletion', 'Deep neural networks are composed of many individual neurons, which combine in complex and counterintuitive ways to solve a wide range of challenging tasks. This complexity grants neural networks their power but also earns them their reputation as confusing and opaque black boxes.\\n\\nUnderstanding how deep neural networks function is critical for explaining their decisions and enabling us to build more powerful systems. For instance, imagine the difficulty of trying to build a clock without understanding how individual gears fit together. One approach to understanding neural networks, both in neuroscience and deep learning, is to investigate the role of individual neurons, especially those which are easily interpretable.\\n\\nOur investigation into the importance of single directions for generalisation, soon to appear at the Sixth International Conference on Learning Representations (ICLR), uses an approach inspired by decades of experimental neuroscience — exploring the impact of damage — to determine: how important are small groups of neurons in deep neural networks? Are more easily interpretable neurons also more important to the network’s computation?')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Evolution Is the New Deep Learning', 'By Risto Miikkulainen\\n\\nVice President Research; Professor of Computer Science at the University of Texas at Austin\\n\\n[If you are visiting from Hacker News, please be sure to check out the papers responsible for the development of these apps].\\n\\nAt Sentient, we have an entire team dedicated to research and experimentation in AI. Over the past few years, the team has focused on developing new methods in Evolutionary Computation (EC), i.e. designing artificial neural network architectures, building commercial applications, and solving challenging computational problems using methods inspired by natural evolution. This research builds upon more than 25 years of research at UT Austin and other academic institutions, and coincides with related efforts recently at OpenAI, DeepMind, Google Brain, and Uber. There is significant momentum building in this area; indeed, we believe evolutionary computation may well be the next big thing in AI technology.\\n\\nLike Deep Learning (DL), EC was introduced decades ago, and it is currently experiencing a similar boost from the available big compute and big data. However, it addresses a distinctly different need: Whereas DL focuses on modeling what we already know, EC focuses on creating new knowledge. In that sense, it is the next step up from DL: Whereas DL makes it possible to recognize new instances of objects and speech within familiar categories, EC makes it possible to discover entirely new objects and behaviors—those that maximize a given objective. Thus, EC makes a host of new applications possible: designing more effective behaviors for robots and virtual agents; creating more effective and cheaper health interventions, growth recipes for agriculture, and mechanical and biological processes.\\n\\nToday, Sentient released five papers and a web portal reporting significant progress in taking this step, focusing on three areas: (1) DL architectures are evolved to exceed state of the art in three standard machine learning benchmarks; (2) techniques are developed for increasing performance and reliability of evolution in real-world applications; and (3) evolutionary problem solving is demonstrated on very hard computational problems.\\n\\nThis post focuses on the first of these areas, i.e. optimization of DL architectures with EC.\\n\\nSentient Reveals Breakthrough Research in Neuroevolution\\n\\nMuch of the power of deep learning comes from the size and complexity of the networks. With neuroevolution, the DL architecture (i.e. network topology, modules, and hyperparameters) can be optimized beyond human ability. The three demos that we will cover in this article are Omni Draw, Celeb Match, and the Music Maker (Language Modeling). In all three examples, Sentient successfully surpassed the state-of-the-art DL benchmark using neuroevolution.\\n\\nIn the Language Modeling domain, the system is trained to predict the next word in a “language corpus”, i.e. a large collection of text such as several years of the Wall Street Journal. After the network has made its prediction, this input can be looped back into its input, and the network can generate an entire sequence of words. Interestingly, the same technique applies equally well to musical sequences, where it makes for a fun demo. The user inputs a few initial notes, and the system improvises an entire melody based on that starting point. By means of neuroevolution, Sentient optimized the design of the gated recurrent (Long Short-Term Memory or LSTM) nodes (i.e. the network’s “memory” structure) to make the model more accurate in predicting the next note.\\n\\nIn the language modeling domain (i.e. predicting the next word in a language corpus called Penn Tree Bank), the benchmark is defined by Perplexity Points, a measurement of how well a probabilistic model can predict real samples. The lower the number the better, as we want the model to be less “perplexed” when predicting the next word in a sequence. In this case, Sentient beat the standard LSTM structure by 10.8 Perplexity Points. Remarkably, although several human-designed LSTM variations have been proposed, they have not improved performance much—LSTM structure was essentially unchanged for 25 years. Our neuroevolution experiments showed that it can, as a matter of fact, be improved significantly by adding more complexity, i.e. memory cells and more nonlinear, parallel pathways.\\n\\nWhy does this breakthrough matter? Language is a powerful and complex construct of human intelligence. Language modeling, i.e. predicting the next word in a text, is a benchmark that measures how well machine learning methods can learn language structure. It is therefore a surrogate for building natural language processing systems that includes speech and language interfaces, machine translation (such as Google Translate), and even medical data such as DNA sequences and heart rate diagnosis. The better we can do in the language modeling benchmark, the better language processing systems we can build, using the same technology.\\n\\nOmniglot is a handwritten character recognition benchmark on recognizing characters in 50 different alphabets, including real languages like Cyrillic (written Russian), Japanese, and Hebrew, to artificial languages such as Tengwar (the written language in Lord of the Rings).\\n\\nThis demo showcases multitask learning, in which the model learns all languages at once and exploits the relationship between characters from different languages. So, for instance, the user inputs an image and the system outputs suggestions for different character matches in different languages, saying “this would be ‘X’ in Latin, ‘Y’ in Japanese, and ‘Z’ in Tengwar, etc.”—taking advantage of its understanding of the relationships between Japanese, Tengwar, and Latin to figure out which character is the best match. This differs from a single task learning environment where the model trains on one language at a time and cannot make the same connections across language data sets.\\n\\nIn this Omniglot multitask character recognition task, our research team improved error of character matching from 32% to 10%.\\n\\nOmniglot is an example of a dataset that has relatively little data per language—for instance, it may have only a few characters in Greek but many in Japanese. It succeeds by using its knowledge of the relationships between languages to find solutions, hence, finding a solution in the face of missing or sparse data. Why is this important? For many real world applications, labeled data is expensive or dangerous to acquire (e.g., medical applications, agriculture, and robotic rescue), hence automatically designing models that exploit the relationships to similar or related datasets could, in a way, substitute the missing dataset and boost research capabilities. It is also an excellent demonstration of the power of neuroevolution: there are many ways in which the languages can be related, and evolution discovers the best ways to tie their learning together.\\n\\nThe Celeb Match demo deals similarly with multitask learning, but this time, with a large-scale data sets. The demo is based on the CelebA dataset, which consists of around 200,000 images of celebrities, each of which is labeled with 40 binary attributes such as “Male vs. Female”, “beard vs. no beard”, “glasses vs. no glasses”, etc. Each attribute induces a “classification task” that induces the system to detect and identify each attribute. As a fun add-on, we’ve created a demo that turns this task around: The user can set the desired degree for each attribute, and the system finds the closest celebrity match, as determined by the evolved multitask learning network. For instance, if the current attribute settings result in an image of Brad Pitt, the user can increase “gray hair” to find which celebrity would be similar to Brad Pitt but with different hair.\\n\\nIn this domain, the state-of-the-art benchmark is the test error across all attributes, i.e. whether the system detected the attribute correctly (male/female, young/mature, large eyes/small eyes), etc. In the CelebA multitask face classification domain, Sentient used evolutionary computation to optimize the networks that detect these attributes, reducing error from 8.00% to 7.94% for an ensemble (an average of) three models.\\n\\nThis technology is a step forward in the ability for AI to predict diverse attributes of people, places, and things in the physical world. Unlike networks trained to find similarities based on abstract, learned features, it makes the similarities semantic and interpretable.\\n\\nJust the Tip of the Iceberg!\\n\\nOmni Draw, Celeb Match, and the Music Maker are just three examples of interactive demos that illustrate the power of neuroevolution. We invite you to learn more about the technology behind them on our website and papers, as well as the two other aspects of evolution as the next deep learning: commercialization and solving hard problems.\\n\\nRead more on our evolution research web portal, Evolution is the New Deep Learning.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Privacy-preserving sentence similarity using Deep Learning', \"Privacy-preserving Computation\\n\\nPrivacy-preserving computation or secure computation is a sub-field of cryptography where two (two-party, or 2PC) or multiple (multi-party, or MPC) parties can evaluate a function together without revealing information about the parties private input data to each other. The problem and the first solution to it were introduced in 1982 by an amazing breakthrough done by Andrew Yao on what later became known as the “Yao’s Millionaires’ problem“.\\n\\nThe Yao’s Millionaires Problem is where two millionaires, Alice and Bob, who are interested in knowing which of them is richer but without revealing to each other their actual wealth. In other words, what they want can be generalized as that: Alice and Bob want jointly compute a function securely, without knowing anything other than the result of the computation on the input data (that remains private to them).\\n\\nTo make the problem concrete, Alice has an amount A such as $10, and Bob has an amount B such as $ 50, and what they want to know is which one is larger, without Bob revealing the amount B to Alice or Alice revealing the amount A to Bob. It is also important to note that we also don’t want to trust on a third-party, otherwise the problem would just be a simple protocol of information exchange with the trusted party.\\n\\nFormally what we want is to jointly evaluate the following function:\\n\\nSuch as the private values A and B are held private to the sole owner of it and where the result r will be known to just one or both of the parties.\\n\\nIt seems very counterintuitive that a problem like that could ever be solved, but for the surprise of many people, it is possible to solve it on some security requirements. Thanks to the recent developments in techniques such as FHE (Fully Homomorphic Encryption), Oblivious Transfer, Garbled Circuits, problems like that started to get practical for real-life usage and they are being nowadays being used by many companies in applications such as information exchange, secure location, advertisement, satellite orbit collision avoidance, etc.\\n\\nI’m not going to enter into details of these techniques, but if you’re interested in the intuition behind the OT (Oblivious Transfer), you should definitely read the amazing explanation done by Craig Gidney here. There are also, of course, many different protocols for doing 2PC or MPC, where each one of them assumes some security requirements (semi-honest, malicious, etc), I’m not going to enter into the details to keep the post focused on the goal, but you should be aware of that.\\n\\nThe problem: sentence similarity\\n\\nWhat we want to achieve is to use privacy-preserving computation to calculate the similarity between sentences without disclosing the content of the sentences. Just to give a concrete example: Bob owns a company and has the description of many different projects in sentences such as: “This project is about building a deep learning sentiment analysis framework that will be used for tweets“, and Alice who owns another competitor company, has also different projects described in similar sentences. What they want to do is to jointly compute the similarity between projects in order to find if they should be doing partnership on a project or not, however, and this is the important point: Bob doesn’t want Alice to know the project descriptions and neither Alice wants Bob to be aware of their projects, they want to know the closest match between the different projects they run, but without disclosing the project ideas (project descriptions).\\n\\nSentence Similarity Comparison\\n\\nNow, how can we exchange information about the Bob and Alice’s project sentences without disclosing information about the project descriptions ?\\n\\nOne naive way to do that would be to just compute the hashes of the sentences and then compare only the hashes to check if they match. However, this would assume that the descriptions are exactly the same, and besides that, if the entropy of the sentences is small (like small sentences), someone with reasonable computation power can try to recover the sentence.\\n\\nAnother approach for this problem (this is the approach that we’ll be using), is to compare the sentences in the sentence embeddings space. We just need to create sentence embeddings using a Machine Learning model (we’ll use InferSent later) and then compare the embeddings of the sentences. However, this approach also raises another concern: what if Bob or Alice trains a Seq2Seq model that would go from the embeddings of the other party back to an approximate description of the project ?\\n\\nIt isn’t unreasonable to think that one can recover an approximate description of the sentence given their embeddings. That’s why we’ll use the two-party secure computation for computing the embeddings similarity, in a way that Bob and Alice will compute the similarity of the embeddings without revealing their embeddings, keeping their project ideas safe.\\n\\nThe entire flow is described in the image below, where Bob and Alice shares the same Machine Learning model, after that they use this model to go from sentences to embeddings, followed by a secure computation of the similarity in the embedding space.\\n\\nGenerating sentence embeddings with InferSent\\n\\nInferSent is an NLP technique for universal sentence representation developed by Facebook that uses supervised training to produce high transferable representations.\\n\\nThey used a Bi-directional LSTM with attention that consistently surpassed many unsupervised training methods such as the SkipThought vectors. They also provide a Pytorch implementation that we’ll use to generate sentence embeddings.\\n\\nNote: even if you don’t have GPU, you can have reasonable performance doing embeddings for a few sentences.\\n\\nThe first step to generate the sentence embeddings is to download and load a pre-trained InferSent model:\\n\\nimport numpy as np import torch # Trained model from: https://github.com/facebookresearch/InferSent GLOVE_EMBS = '../dataset/GloVe/glove.840B.300d.txt' INFERSENT_MODEL = 'infersent.allnli.pickle' # Load trained InferSent model model = torch.load(INFERSENT_MODEL, map_location=lambda storage, loc: storage) model.set_glove_path(GLOVE_EMBS) model.build_vocab_k_words(K=100000)\\n\\nNow we need to define a similarity measure to compare two vectors, and for that goal, I’ll the cosine similarity (I wrote a tutorial about this similarity measure here) since it’s pretty straightforward:\\n\\nAs you can see, if we have two unit vectors (vectors with norm 1), the two terms in the equation denominator will be 1 and we will be able to remove the entire denominator of the equation, leaving only:\\n\\nSo, if we normalize our vectors to have a unit norm (that’s why the vectors are wearing hats in the equation above), we can make the computation of the cosine similarity become just a simple dot product. That will help us a lot in computing the similarity distance later when we’ll use a framework to do the secure computation of this dot product.\\n\\nSo, the next step is to define a function that will take some sentence text and forward it to the model to generate the embeddings and then normalize them to unit vectors:\\n\\n# This function will forward the text into the model and # get the embeddings. After that, it will normalize it # to a unit vector. def encode(model, text): embedding = model.encode([text])[0] embedding /= np.linalg.norm(embedding) return embedding\\n\\nAs you can see, this function is pretty simple, it feeds the text into the model, and then it will divide the embedding vector by the embedding norm.\\n\\nNow, for practical reasons, I’ll be using integer computation later for computing the similarity, however, the embeddings generated by InferSent are of course real values. For that reason, you’ll see in the code below that we create another function to scale the float values and remove the radix point and converting them to integers. There is also another important issue, the framework that we’ll be using later for secure computation doesn’t allow signed integers, so we also need to clip the embeddings values between 0.0 and 1.0. This will of course cause some approximation errors, however, we can still get very good approximations after clipping and scaling with limited precision (I’m using 14 bits for scaling to avoid overflow issues later during dot product computations):\\n\\n# This function will scale the embedding in order to # remove the radix point. def scale(embedding): SCALE = 1 << 14 scale_embedding = np.clip(embedding, 0.0, 1.0) * SCALE return scale_embedding.astype(np.int32)\\n\\nYou can use floating-point in your secure computations and there are a lot of frameworks that support them, however, it is more tricky to do that, and for that reason, I used integer arithmetic to simplify the tutorial. The function above is just a hack to make it simple. It’s easy to see that we can recover this embedding later without too much loss of precision.\\n\\nNow we just need to create some sentence samples that we’ll be using:\\n\\n# The list of Alice sentences alice_sentences = [ 'my cat loves to walk over my keyboard', 'I like to pet my cat', ] # The list of Bob sentences bob_sentences = [ 'the cat is always walking over my keyboard', ]\\n\\nAnd convert them to embeddings:\\n\\n# Alice sentences alice_sentence1 = encode(model, alice_sentences[0]) alice_sentence2 = encode(model, alice_sentences[1]) # Bob sentences bob_sentence1 = encode(model, bob_sentences[0])\\n\\nSince we have now the sentences and every sentence is also normalized, we can compute cosine similarity just by doing a dot product between the vectors:\\n\\n>>> np.dot(bob_sentence1, alice_sentence1) 0.8798542 >>> np.dot(bob_sentence1, alice_sentence2) 0.62976325\\n\\nAs we can see, the first sentence of Bob is most similar (~0.87) with Alice first sentence than to the Alice second sentence (~0.62).\\n\\nSince we have now the embeddings, we just need to convert them to scaled integers:\\n\\n# Scale the Alice sentence embeddings alice_sentence1_scaled = scale(alice_sentence1) alice_sentence2_scaled = scale(alice_sentence2) # Scale the Bob sentence embeddings bob_sentence1_scaled = scale(bob_sentence1) # This is the unit vector embedding for the sentence >>> alice_sentence1 array([ 0.01698913, -0.0014404 , 0.0010993 , ..., 0.00252409, 0.00828147, 0.00466533], dtype=float32) # This is the scaled vector as integers >>> alice_sentence1_scaled array([278, 0, 18, ..., 41, 135, 76], dtype=int32)\\n\\nNow with these embeddings as scaled integers, we can proceed to the second part, where we’ll be doing the secure computation between two parties.\\n\\nTwo-party secure computation\\n\\nIn order to perform secure computation between the two parties (Alice and Bob), we’ll use the ABY framework. ABY implements many difference secure computation schemes and allows you to describe your computation as a circuit like pictured in the image below, where the Yao’s Millionaire’s problem is described:\\n\\nAs you can see, we have two inputs entering in one GT GATE (greater than gate) and then a output. This circuit has a bit length of 3 for each input and will compute if the Alice input is greater than (GT GATE) the Bob input. The computing parties then secret share their private data and then can use arithmetic sharing, boolean sharing, or Yao sharing to securely evaluate these gates.\\n\\nABY is really easy to use because you can just describe your inputs, shares, gates and it will do the rest for you such as creating the socket communication channel, exchanging data when needed, etc. However, the implementation is entirely written in C++ and I’m not aware of any Python bindings for it (a great contribution opportunity).\\n\\nFortunately, there is an implemented example for ABY that can do dot product calculation for us, the example is here. I won’t replicate the example here, but the only part that we have to change is to read the embedding vectors that we created before instead of generating random vectors and increasing the bit length to 32-bits.\\n\\nAfter that, we just need to execute the application on two different machines (or by emulating locally like below):\\n\\n# This will execute the server part, the -r 0 specifies the role (server) # and the -n 4096 defines the dimension of the vector (InferSent generates # 4096-dimensional embeddings). ~# ./innerproduct -r 0 -n 4096 # And the same on another process (or another machine, however for another # machine execution you'll have to obviously specify the IP). ~# ./innerproduct -r 1 -n 4096\\n\\nAnd we get the following results:\\n\\nInner Product of alice_sentence1 and bob_sentence1 = 226691917 Inner Product of alice_sentence2 and bob_sentence1 = 171746521\\n\\nEven in the integer representation, you can see that the inner product of the Alice’s first sentence and the Bob sentence is higher, meaning that the similarity is also higher. But let’s now convert this value back to float:\\n\\n>>> SCALE = 1 << 14 # This is the dot product we should get >>> np.dot(alice_sentence1, bob_sentence1) 0.8798542 # This is the inner product we got on secure computation >>> 226691917 / SCALE**2.0 0.8444931 # This is the dot product we should get >>> np.dot(alice_sentence2, bob_sentence1) 0.6297632 # This is the inner product we got on secure computation >>> 171746521 / SCALE**2.0 0.6398056\\n\\nAs you can see, we got very good approximations, even in presence of low-precision math and unsigned integer requirements. Of course that in real-life you won’t have the two values and vectors, because they’re supposed to be hidden, but the changes to accommodate that are trivial, you just need to adjust ABY code to load only the vector of the party that it is executing it and using the correct IP addresses/port of the both parties.\\n\\nI hope you liked it !\\n\\n– Christian S. Perone\")"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Deep Learning Meets DSP: OFDM Signal Detection', 'Here at KickView, we get excited about building products that use AI and digital signal processing (DSP) algorithms to detect, classify and make sense of Radio Frequency (RF) signals and sensor data. In this blog post, we\\'ll focus specifically on detection of RF signals modulated using Orthogonal Frequency Division Multiplexing (OFDM). OFDM is a digital multi-carrier modulation scheme that is employed in many fielded systems; WiFi, cable systems (e.g. DOCSIS 3.1) and cellular networks (e.g. 4G, 5G) either currently deploy or are moving towards OFDM as the PHY-layer standard. First, we will do a technical deep dive into OFDM to get a good understanding of the signal structure and it\\'s benefits. Next, we\\'ll introduce our patent pending signal detection system we\\'ve developed that uses deep learning (DL) to robustly detect OFDM waveforms by capturing only a portion of the total RF signal bandwidth. More broadly, our approach can significantly improve detection of wide bandwidth signals using narrower bandwidth receivers. Contributors to this post include Krishna Karra, Robert North and David Ohm, PhD at KickView.\\n\\nOrthogonal Frequency Division Multiplexing (OFDM)\\n\\nOFDM is a digital multi-carrier modulation scheme where information is distributed across many orthogonal subcarriers closely spaced in frequency. Each subcarrier is individually modulated using single-carrier modulation schemes such as Quadrature Amplitude Modulation (QAM) or Binary Phase Shift Keying (BPSK). OFDM signals can occupy a wide frequency bandwidth, and achieve a high data rate through the parallel transmission of multiple subcarriers, each of which transmit at a much lower data rate. The orthogonality among the subcarriers ensures that they do not interfere with each other. The primary advantage of OFDM over single-carrier modulation techniques is the ability to counter severe RF channel conditions, particularly multipath in wireless environments, without the use of complex time-domain equalization. Equalization is the process of reversing distortion imparted onto a signal as it propagates through a channel (e.g. a multipath city environment). Intricate equalization filters increase the complexity of a communications system and are difficult to design for single-carrier modulation schemes. Utilizing OFDM eliminates the need for complex equalization to recover the data.\\n\\nFigure 1 below shows the frequency domain representation of the OFDM subcarriers, as specified by IEEE 802.11g. IEEE 802.11g is a commonly used WiFi protocol, and the remainder of our discussion will focus specifically on OFDM signal parameters defined by this standard. However, all the concepts and technology introduced here naturally extend to any standard that employs OFDM as the PHY-layer modulation scheme. The PHY-layer, or physical layer, deefines the means of transmitting raw bits over a physical data link.\\n\\nFigure 1: Frequency domain representation of IEEE 802.11g OFDM subcarriers\\n\\nIEEE 802.11g defines 64 equally spaced subcarriers across a 20 MHz bandwidth. Note that out of the 64 subcarriers, only 48 of them are used for data transmission. Four are pilot channels used for synchronization and channel estimation on the receiver end, and 11 are guard channels to reject adjacent channel interferers. Finally, there is a null subcarrier at the center of the band to mitigate DC level artifacts that are commonly caused by analog components in RF transmitters.\\n\\nFigure 2: Building blocks of a typical OFDM communications system\\n\\nFigure 2 above shows how an OFDM signal is modulated, transmitted and demodulated in a typical communications system. The subcarrier data streams, which are orthogonal to each other, are generated through the Inverse Discrete Fourier Transform (IDFT) operation, which acts as a serial-to-parallel converter. Another important component of OFDM is the Cyclic Prefix (CP), which is inserted into the parallel data stream after the IDFT computation. Adding the CP is as easy as copying a certain percentage of the tail end of an OFDM symbol to the front of the symbol.\\n\\nWhy should we even bother with the CP? In a wireless channel, the beginning of the symbol will get corrupted, but since we\\'ve conveniently added the CP, this is now redundant information that can effectively be ignored! More generally, given sufficient CP length, the preceding symbols will not spill over into the DFT period, which preserves the data payload in the current symbol. Therefore, modern OFDM receivers simply strip away the CP and correct the corrupted symbols by an amplitude and/or phase shift. Later on, we\\'ll see that the DFT and CP parameters, which are key to recovering the individual subcarriers, also play a key role in the design of our OFDM signal detector. Recall that IEEE 802.11g specifies 64 subcarriers with each OFDM symbol, meaning that a 64-point IDFT is computed at the transmitter (and subsequently a 64-point DFT on the receiver end).\\n\\nOFDM Signal Detection\\n\\nWe now have an understanding of the PHY-layer building blocks in OFDM. Before we go into the technical design, let\\'s circle back to motivate the problem. The OFDM modulation scheme is used primarily for wide bandwidth transmission; an IEEE 802.11g OFDM signal occupies 20 MHz, and newer standards such as IEEE 802.11ac specify signals as wide as 160 MHz. Since many receivers have collection bandwidths much less than this, signal detection and classification using conventional techniques is quite difficult. Many conventional techniques require that the complete in-band energy of the signal be captured along with explicit time and frequency synchronization to perform signal classification [1]. Our goal is to build an OFDM signal detector that can perform classification using partial bandwidth RF collects and does not require explicit time and frequency synchronization.\\n\\nGiven this background, let\\'s define some goals:\\n\\nCreate an RF dataset with OFDM signal bursts for experimentation (we\\'ll pick IEEE 802.11g as our protocol)\\n\\nBuild an algorithm to detect OFDM signals from full-band RF collections (bandwidth of 20 MHz as specified by IEEE 802.11g)\\n\\nExpand this algorithm to detect OFDM signals from partial-band RF collections (let\\'s arbitrarily choose a bandwidth of 5 MHz, which translates to capturing 1/4 of the total signal energy)\\n\\nEvaluate the performance of the algorithm as a function of Signal-to-Noise Ratio (SNR)\\n\\nDemonstrate the functionality of the algorithm in a realistic RF environment\\n\\nFigure 3 below shows a block diagram of our high-level technical approach:\\n\\nFigure 3: Technical approach to detect and classify OFDM signals\\n\\nFirst, we\\'ll walk through our process to generate an RF dataset with snapshots of OFDM signals by conducting an over-the-air collection from a WiFi access point in a controlled RF environment. Next, we\\'ll do some pre-processing to exploit the inherent structure of the OFDM signal by channelizing the time-domain snapshots. This process generates a complex-valued time/frequency representation of each signal burst, which we\\'ll refer to as a complex signal image. Finally, we\\'ll train a deep neural network (DNN) on the complex signal images to learn the structure of the OFDM signal. To conclude, we\\'ll show some performance of how well our detector does for both full and partial-band cases as a function of SNR.\\n\\nRF Data Collection\\n\\nWhile large, freely accessible datasets are available for DL research in other domains (e.g. MNIST, KITTI, ImageNet), much fewer resources exist for the RF domain. Since we\\'ve chosen IEEE 802.11g as our protocol, we configured a LinkSys 802.11g WiFi router for use in creating a large dataset. Leveraging our KV-PRELUDE product based around our patent pending system for automated RF dataset creation we generated several very large datasets for RF DL exploration. We utilized an Ettus USRP X300 software-defined radio to capture raw RF data.\\n\\nFigure 4: We leverage our KV-AIOLOS tools to generate an RF dataset containing IEEE 802.11g OFDM signals\\n\\nThe KV-AIOLOS data generation tools can automate the creation of a labeled RF dataset for any modern communications protocol, both Over-The-Air (OTA) or wired. We used this tool to generate an RF dataset consisting of ~4000 OFDM signal snapshots, with each snapshot ~1.4 msec long in duration (~333 OFDM symbols) critically sampled at 20 MHz. All the bursts are at reasonably high SNR. We\\'ll vary the SNR across all bursts at the pre-processing stage, discussed in the next section.\\n\\nPre-Processing\\n\\nIn the pre-processing stage, we\\'ll focus on a strategy to transform the time-domain OFDM snapshots to a representation that maximizes the structure of these signals for subsequent feature extraction. Channelization, which is an operation in digital signal processing that extracts individual carriers from a multi-carrier signal, is a natural strategy for multi-carrier signals. [2] is a great reference that describes various channelization strategies and their performance tradeoffs. We chose a polyphase channelizer because it minimizes out-of-band interference from other channels in each output channel. To design an appropriate polyphase channelizer for IEEE 802.11g OFDM signals, we\\'ll need to dig into some signal specific parameters of IEEE 802.11g OFDM signal.\\n\\nWe define a \"blind detection\" (BD) symbol (4 microseconds in duration) as nothing more than the OFDM symbol (3.2 microseconds in duration) plus the CP (0.8 microseconds in duration). Recalling our OFDM discussion above, a typical OFDM receiver strips away the CP before computing a DFT (64-point for IEEE 802.11g). However, since we\\'re building an OFDM detector that does not utilize knowledge of the internals of the signal, we\\'ll need to take the CP into account! This means that in the channelizer, we need to compute an 80-point DFT instead of a 64-point DFT to properly handle the longer symbol duration. Our channelization strategy is depicted below in Figure 5.\\n\\nFigure 5: The polyphase channelizer breaks out individual channels from the multi-carrier OFDM signal\\n\\nAlong with channelization, the pre-processing operation also accounts for uncertainty in time and frequency. This aspect of pre-processing generates ensembles of training examples, each with a unique instantiation of parameters. By training over this uncertainty, we will see that our DNN can detect the presence of OFDM without explicit time and frequency synchronization.\\n\\nBelow is a breakdown of the different uncertainty factors that we capture during pre-processing:\\n\\nSample phase: in a live RF collection, the initial sample phase (how many samples into a signal burst) of a captured signal snapshot is ambiguous. We account for this uncertainty by generating all possible combinations of start sample phases from each time-domain snapshot in the RF dataset.\\n\\nOversampling ratio in channelizer: The block pre-sum operation (depicted above in Figure 5) can be thought of as a parallel-to-serial conversion of the BD symbols, which are summed together and transformed to channelized outputs through the DFT computation. We apply an oversampling ratio to generate multiple output ensembles of input samples shifted through the polyphase channelizer.\\n\\nFrequency Offset: In a typical RF link, the transmitter and receiver both have a local oscillator (LO) which acts as a frequency reference. There is typically some disagreement between LOs from two different radios, which results in a frequency shift in the received signal. Furthermore, harsh channel conditions and receiver motion can introduce additional frequency error. We account for these errors by generating Doppler-shifted versions of each snap.\\n\\nFinally, in our pre-processing step, we vary the SNR of the time-domain snapshots from -8 dB to +8 dB. This is so we can evaluate the performance of our detector to see how it performs under real-world conditions. Figure 6 below shows some example snapshots from our pre-processing operation, both in the time-domain and the complex signal image representations generated after channelization.\\n\\nFigure 6: Example snaphots of OFDM and Noise in the time-domain and complex signal images after pre-processing\\n\\nThe subcarrier structure from the OFDM signal is strikingly evident in the magnitude of the complex signal image, shown on the top right. Note that the complex signal images are actually complex-valued (In-Phase & Quadrature components), but are visualized above in magnitude space.\\n\\nDeep Neural Network Architecture\\n\\nFrom our pre-processing step, we generated a whole bunch of complex signal images for both OFDM signals and noise. We\\'re now ready to build and train a DNN to act as an OFDM signal detector. It\\'s important to note that the complex signal images are somewhat analogous to images from a camera, with a few key differences. In computer vision applications, images can either be grayscale (single channel) or color (three channels). The complex signal images contain In-Phase and Quadrature components, making them two channels. We took the approach of treating the In-Phase and Quadrature components as separate channels in the DNN, to preserve both amplitude and phase information of the signal. We are also actively conducting research and development in methods for using complex-valued data natively within the formulation of the DNN with common frameworks such as TensorFlow. Initial academic research in this domain [3] has shown that complex-valued DNNs achieve excellent classification performance and can potentially generalize better.\\n\\nFigure 8: Depiction of input data tensor and DNN architecture\\n\\nFigure 8 above shows the input data for the DNN in tensor form, as well as a block diagram of the DNN architecture we chose. The first dimension represents time; we fixed each signal snapshot to contain 333 BD symbols, excluding a few symbols at the beginning and end as guard symbols. The second dimension represents frequency; recall that we computed an 80-point DFT in the polyphase channelizer to extract each individual subcarrier. The third dimension represents the split of complex-valued data into two separate channels. For the DNN, we employ multiple convolutional and fully connected layers to learn a rich feature representation of the OFDM signal structure. We performed hyperparameter optimization and utilized common regularization techinques such as Dropout to prevent overfitting.\\n\\nExperimental Results\\n\\nWe evaluated the performance of our DL-based OFDM signal detector under four different scenarios:\\n\\nFull-band (20 MHz) detection of OFDM vs. Additive White Gaussian Noise (AWGN)\\n\\nFull-band (20 MHz) detection of OFDM vs. Background (Noise, Bluetooth, other in-band interferers)\\n\\nPartial-band (5 MHz) detection of OFDM vs. AWGN\\n\\nPartial-band (5 MHz) detection of OFDM vs. Background (Noise, Bluetooth, other in-band interferers)\\n\\nLet\\'s take a look at the results, shown below in Figure 9.\\n\\nFigure 9: Detection performance of KickView\\'s DL-based signal detector for OFDM signals\\n\\nIn the full-band scenario of OFDM vs. AWGN (the most ideal case), we achieve a 90% detection rate down at -5.5 dB SNR. Looking at an example signal snapshot at -5 dB SNR in Figure 9, it is evident that the human eye can\\'t detect a signal in there! Performance suffers slightly (90% detection rate at -4.5 dB SNR) when other in-band interferers such as Bluetooth and Wireless USB are added into the RF dataset, but not by much. We can make the conclusion that for a full-band collection, our DL-based signal detector can detect the presence of OFDM signaling below the noise floor.\\n\\nThe sensitivity of the detector isn\\'t quite as high for the partial-band (5 MHz) scenarios. For partial-band detection of OFDM vs. Noise, we obtain a 90% detection rate at 0.5 dB SNR, and sensitivity decreases to 1.5 dB SNR when in-band interferers are added. This performance impact is expected because since we\\'re only capturing 1/4 of the total signal energy, only some of the structure is present for inference. Furthermore, the structure of a 5 MHz snapshot can vary depending on which part of the signal band is collected; a complex signal image at the tail end of the OFDM signal will look very different compared to a complex signal image captured at the center.\\n\\nThe results above were all generated from our custom RF dataset. However, the real test is whether this signal detector works in a real RF environment. To validate, we performed a live RF collection in our office with a known cooperative WiFi access point transmitting an IEEE 802.11g OFDM signal. In the same band, there were many interfering signals, including other WiFi access points, Bluetooth and Wireless USB. We found the performance of our signal detector to be excellent, with a high detection rate and very low false alarm rate. Although we do not present the detailed detection metrics for this scenario here in this post, Figure 10 below shows some example snapshots of detector classifications on live-collected RF data.\\n\\nFigure 10: Classification examples of our OFDM signal detector from a live RF collect conducted at KickView\\'s office\\n\\nFigure 11: Our detector successfully detects the presence of WiFi with only ~50 microseconds of captured energy\\n\\nInterestingly, although our signal detector was trained on complete IEEE 802.11g signal pulses, it does an excellent job detecting OFDM even when only tiny bursts (in time) of energy are captured. An example of this is shown above in Figure 11. In contrast, conventional techniques typically require many symbols of energy to integrate over in order to make a classification decision and perform time/frequency synchronization.\\n\\nConclusion\\n\\nIn this blog, we\\'ve introduced technical concepts behind OFDM signaling, and outlined a unique approach that innovatively combines DSP and DL to build a custom signal detector. We\\'ve shown that our signal detector can detect the presence of OFDM below the noise floor with only partial bandwidth RF collections and short time duration bursts. We are leveraging this capability along with our other AI-based anomaly detection methods for application in the cable and wireless telecommunications space, where the use of OFDM waveforms is becoming increasingly prevalent. Stay tuned for our tutorial that will include access to a full dataset and step-by-step instructions on building your own AI signal detector. If you are interested in our technology solutions or working together, please contact us.\\n\\nReferences\\n\\n[1] Won-Gyu Song and Jong-Tae Lim, \"Channel estimation and signal detection for MIMO-OFDM with time varying channels,\" in IEEE Communications Letters, vol. 10, no. 7, pp. 540-542, July 2006.\\n\\n[2] M. Renfors, J. Yli-Kaakinen and F. J. Harris, \"Analysis and Design of Efficient and Flexible Fast-Convolution Based Multirate Filter Banks,\" in IEEE Transactions on Signal Processing, vol. 62, no. 15, pp. 3768-3783, Aug.1, 2014.\\n\\n[3] A. Hirose and S. Yoshida, \"Generalization Characteristics of Complex-Valued Feedforward Neural Networks in Relation to Signal Coherence,\" in IEEE Transactions on Neural Networks and Learning Systems, vol. 23, no. 4, pp. 541-551, April 2012.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('6.S191: Introduction to Deep Learning', 'An introductory course on deep learning methods with applications to machine translation, image recognition, game playing, image generation and more. A collaborative course incorporating labs in TensorFlow and peer brainstorming along with lectures. Course concludes with project proposals with feedback from staff and panel of industry sponsors.\\n\\nProject proposals will be 1-minute pitches of a novel deep learning algorithm, application, open-source contribution, plan to create an interesting dataset, or other contributions. Sponsors will judge and select top projects as award winners. Alternative to project proposal is to submit a 1-page review of an interesting deep learning paper.\\n\\nRegistration\\n\\nIf you are an MIT student (undergraduate or graduate) please register by submitting an add-drop form here. You can do this by clicking \"create new form\" and selecting \"Add Drop\". Enter the subject information (6.S191) and 3 units when prompted. You can also specify if you want to be registered as a listener or regular student there. If you would like to receive course related updates and lecture materials please sign up for our mailing list.')\n",
      "('NGraph: A New Open Source Compiler for Deep Learning Systems', 'We are pleased to announce the open sourcing of nGraph, a framework-neutral Deep Neural Network (DNN) model compiler that can target a variety of devices. With nGraph, data scientists can focus on data science rather than worrying about how to adapt their DNN models to train and run efficiently on different devices. Continue reading below for highlights of our engineering challenges and design decisions, and see GitHub, our documentation, and our SysML paper for additional details.\\n\\nFigure 1 – nGraph ecosystem.\\n\\nWe currently support TensorFlow*, MXNet*, and neon directly through nGraph. CNTK*, PyTorch*, and Caffe2* are supported indirectly through ONNX. Users can run these frameworks on several devices: Intel Architecture, GPU, and Intel Nervana Neural Network Processor (NNP). Support for future devices/frameworks in our roadmap is faded.\\n\\nWhy did we build nGraph?\\n\\nWhen Deep Learning (DL) frameworks first emerged as the vehicle for running training and inference models, they were designed around kernels optimized for a particular device. As a result, many device details were being exposed in the model definitions, complicating the adaptability and portability of DL models to other, or more advanced, devices.\\n\\nThe traditional approach means that an algorithm developer faces tediousness in taking their model to an upgraded device. Enabling a model to run on a different framework is also problematic because the developer must separate the essence of the model from the performance adjustments made for the device, translate to similar ops in the new framework, and finally make the necessary changes for the preferred device configuration on the new framework.\\n\\nWe designed the nGraph library to substantially reduce these kinds of engineering complexities. While optimized kernels for DL primitives are provided through the project and via libraries like Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN), there are also several compiler-inspired ways in which performance can be further optimized.\\n\\nHow does it work in practice?\\n\\nInstall the nGraph library and write or compile a framework with the library in order to run training and inference models. Specify nGraph as the framework backend you want to use from the command line on any supported system. Our Intermediate Representation (IR) layer handles all the device abstraction details and lets developers focus on their data science, algorithms and models, rather than on machine code.\\n\\nAt a more granular level of detail:\\n\\nThe nGraph core creates a strongly-typed and device-neutral stateless graph representation of computations. Each node, or op , in the graph corresponds to one step in a computation, where each step produces zero or more tensor outputs from zero or more tensor inputs. Our philosophy is that nGraph ops should serve as building blocks for more complex DNN operations found in DL frameworks. This is balanced by the need for efficient compilation and deriving training computations from inference computations .\\n\\nWe’ve developed a framework bridge for each supported framework; it acts as an intermediary between the nGraph core and the framework. We currently have bridges for TensorFlow/XLA, MXNet, and ONNX. Since ONNX is only an exchange format, the ONNX bridge is augmented by an execution API.\\n\\nA transformer plays a similar role between the nGraph core and the various devices; transformers handle the device abstraction with a combination of generic and device-specific graph transformations. The result is a function that can be executed from the framework bridge. Transformers also allocate and deallocate, as well as read and write tensors under direction of the bridge. We currently have transformers for Intel Architecture, Intel NNP, NVIDIA cuDNN, and additional devices under active development.\\n\\nCurrent Performance\\n\\nIntel has considerable experience with MKL-DNN optimization of frameworks for Intel Architecture. We make use of previous work with the added benefit that optimizations developed for a device benefits all frameworks through nGraph. Framework developers continue to perform their own optimization work. For example, the performance for TensorFlow 1.7+/XLA on Intel Architecture is much better than for TensorFlow 1.3/XLA on Intel Architectures, and this should improve further as more work is put into XLA for Intel Architectures.\\n\\nWe present below initial performance data from multiple frameworks that reflects the optimizations done so far on the IA transformer. On the latest Intel Xeon Platinum 8180 processor, in conjunction with MKLDNN v0.13, we are able to meet or greatly exceed the performance of previously optimized frameworks such as MXNet-MKLDNN-CPU (MXNet optimized with MKLDNN) and neon-MKLML-CPU (neon optimized with MKLML). We also deliver better performance than the TensorFlow XLA compiler (TF-XLA-CPU), but there are significantly more optimizations that can be done with XLA both on the default CPU implementation and on nGraph.\\n\\nStatus and Future Work\\n\\nAs of today, nGraph supports six DL frameworks and three compute devices.\\n\\nSupported frameworks :\\n\\nDirect support through nGraph’s framework-independent representation TensorFlow* MXNet* neon\\n\\nIndirect support through ONNX CNTK* PyTorch* Caffe2\\n\\n\\n\\nSupported compute devices :\\n\\nIntel Architecture (x86, Intel® Xeon® and Xeon Phi®)\\n\\nIntel® Nervana™ Neural Network Processor (Intel® Nervana NNP)\\n\\nNVIDIA* cuDNN (in progress)\\n\\nWe will continue to add support for additional devices and more graph optimizations such as device-specific op fusions, better work schedulers and faster custom op kernels.\\n\\nVisit our GitHub repository to learn how to contribute to nGraph.')\n",
      "('How to build your own AlphaZero AI using Python and Keras', '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('How to learn Deep Learning in 6 months', 'How to learn Deep Learning in 6 months\\n\\nIt is quite possible to learn, follow and contribute to state-of-art work in deep learning in about 6 months’ time. This article details out the steps to achieve that.\\n\\nPre-requisites\\n\\n- You are willing to spend 10–20 hours per week for the next 6 months\\n\\n- You have some programming skills. You should be comfortable to pick up Python along the way. And cloud. (No background in Python and cloud assumed).\\n\\n- Some math education in the past (algebra, geometry etc).\\n\\n- Access to internet and computer.\\n\\nStep 1\\n\\nWe learn driving a car — by driving. Not by learning how the clutch and the internal combustion engine work. Atleast not initially. When learning deep learning, we will follow the same top-down approach.\\n\\nDo the fast.ai course — Practical Deep Learning for Coders — Part 1. This takes about 4–6 weeks of effort. This course has a session on running the code on cloud. Google Colaboratory has free GPU access. Start with that. Other options include Paperspace, AWS, GCP, Crestle and Floydhub. All of these are great. Do not start to build your own machine. Atleast not yet.\\n\\nStep 2\\n\\nThis is the time to know some of the basics. Learn about calculus and linear algebra.\\n\\nFor calculus, Big Picture of Calculus provides a good overview.\\n\\nFor Linear Algebra, Gilbert Strang’s MIT course on OpenCourseWare is amazing.\\n\\nOnce you finish the above two, read the Matrix Calculus for Deep Learning.\\n\\nStep 3\\n\\nNow is the time to understand the bottom-up approach to deep learning. Do all the 5 courses in the deep learning specialisation in Coursera. You need to pay to get the assignments graded. But the effort is truly worth it. Ideally, given the background you have gained so far, you should be able to complete one course every week.\\n\\nStep 4\\n\\n“All work and no play makes Jack a dull boy”\\n\\nDo a capstone project. This is the time where you delve deep into a deep learning library(eg: Tensorflow, PyTorch, MXNet) and implement an architecture from scratch for a problem of your liking.\\n\\nThe first three steps are about understanding how and where to use deep learning and gaining a solid foundation. This step is all about implementing a project from scratch and developing a strong foundation on the tools.\\n\\nStep 5\\n\\nNow go and do fast.ai’s part II course — Cutting Edge Deep Learning for Coders. This covers more advanced topics and you will learn to read the latest research papers and make sense out of them.\\n\\nEach of the steps should take about 4–6 weeks’ time. And in about 26 weeks since the time you started, and if you followed all of the above religiously, you will have a solid foundation in deep learning.\\n\\nWhere to go next?\\n\\nDo the Stanford’s CS231n and CS224d courses. These two are amazing courses with great depth for vision and NLP respectively. They cover the latest state-of-art. And read the deep learning book. This will solidify your understanding.\\n\\nHappy deep learning. Create every single day.')\n",
      "('How to learn Deep Learning in 6 months', 'How to learn Deep Learning in 6 months\\n\\nIt is quite possible to learn, follow and contribute to state-of-art work in deep learning in about 6 months’ time. This article details out the steps to achieve that.\\n\\nPre-requisites\\n\\n- You are willing to spend 10–20 hours per week for the next 6 months\\n\\n- You have some programming skills. You should be comfortable to pick up Python along the way. And cloud. (No background in Python and cloud assumed).\\n\\n- Some math education in the past (algebra, geometry etc).\\n\\n- Access to internet and computer.\\n\\nStep 1\\n\\nWe learn driving a car — by driving. Not by learning how the clutch and the internal combustion engine work. Atleast not initially. When learning deep learning, we will follow the same top-down approach.\\n\\nDo the fast.ai course — Practical Deep Learning for Coders — Part 1. This takes about 4–6 weeks of effort. This course has a session on running the code on cloud. Google Colaboratory has free GPU access. Start with that. Other options include Paperspace, AWS, GCP, Crestle and Floydhub. All of these are great. Do not start to build your own machine. Atleast not yet.\\n\\nStep 2\\n\\nThis is the time to know some of the basics. Learn about calculus and linear algebra.\\n\\nFor calculus, Big Picture of Calculus provides a good overview.\\n\\nFor Linear Algebra, Gilbert Strang’s MIT course on OpenCourseWare is amazing.\\n\\nOnce you finish the above two, read the Matrix Calculus for Deep Learning.\\n\\nStep 3\\n\\nNow is the time to understand the bottom-up approach to deep learning. Do all the 5 courses in the deep learning specialisation in Coursera. You need to pay to get the assignments graded. But the effort is truly worth it. Ideally, given the background you have gained so far, you should be able to complete one course every week.\\n\\nStep 4\\n\\n“All work and no play makes Jack a dull boy”\\n\\nDo a capstone project. This is the time where you delve deep into a deep learning library(eg: Tensorflow, PyTorch, MXNet) and implement an architecture from scratch for a problem of your liking.\\n\\nThe first three steps are about understanding how and where to use deep learning and gaining a solid foundation. This step is all about implementing a project from scratch and developing a strong foundation on the tools.\\n\\nStep 5\\n\\nNow go and do fast.ai’s part II course — Cutting Edge Deep Learning for Coders. This covers more advanced topics and you will learn to read the latest research papers and make sense out of them.\\n\\nEach of the steps should take about 4–6 weeks’ time. And in about 26 weeks since the time you started, and if you followed all of the above religiously, you will have a solid foundation in deep learning.\\n\\nWhere to go next?\\n\\nDo the Stanford’s CS231n and CS224d courses. These two are amazing courses with great depth for vision and NLP respectively. They cover the latest state-of-art. And read the deep learning book. This will solidify your understanding.\\n\\nHappy deep learning. Create every single day.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training ImageNet on a TPU in 12.5 Hours with GKE and RiseML', 'Training ImageNet on a TPU in 12.5 hours with GKE and RiseML\\n\\nGoogle’s Tensor Processing Unit (TPU), a custom-developed accelerator for deep learning, offers a fast and cost-efficient alternative to training deep learning models in the cloud: it is capable of training a ResNet-50 model on ImageNet in 12.5 hours — for an equivalent of ~$81 of TPU compute time.\\n\\nAt RiseML, we believe that machine learning engineers shouldn’t have to worry about infrastructure. Recently, Google Kubernetes Engine (GKE), the managed Kubernetes offering by Google, started providing alpha level support for provisioning TPUs. Each TPU’s lifetime is automatically bound to the lifetime of its job, so you only pay for your actual use. The combination of GKE and RiseML offers a hassle-free machine learning infrastructure that is easy-to use, highly scalable, and cost-efficient.\\n\\nTo illustrate how to use TPUs on GKE with RiseML, we show below how to train a ResNet-50 model on ImageNet. Bringing up a GKE cluster with TPU support and installing RiseML on it only takes about 10 minutes. While Cloud TPUs are now available for everyone in public beta, please note that TPUs are still in closed alpha on GKE and RiseML. Contact us if you are interested in giving it a spin.\\n\\nPreparing the model\\n\\nTo train ImageNet, we’ll use the bfloat16 implementation of ResNet-50 provided by Google from the TPU repository. We can get the training code for the model from GitHub:\\n\\n$ git clone https://github.com/tensorflow/tpu.git\\n\\n$ cd tpu/models/experimental/resnet_bfloat16\\n\\nNext, let’s define the experiment we’d like to run by creating a riseml.yml file:\\n\\nproject: resnet_imagenet\\n\\ntrain:\\n\\nframework: tensorflow\\n\\ntensorflow:\\n\\nversion: 1.7.0\\n\\nresources:\\n\\ntpus: 1\\n\\ncpus: 2\\n\\nmem: 2048\\n\\nrun: >\\n\\npython resnet_main.py\\n\\n--master=${KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS}\\n\\n--data_dir=gs://imagenet/tpu\\n\\n--model_dir=gs://results/riseml-tpu-support/${HOSTNAME}\\n\\nFor this experiment, we want to run Tensorflow 1.7 and use one TPU for training. We need very little CPU and memory since all of the heavy computation happens on the TPU. We also specify the command to train the model, the endpoint to reach the TPU (provided by the environment), and the locations for training data and model output. Having training data and model output on Google Cloud Storage is currently a requirement for using TPUs.\\n\\nTraining the model with RiseML\\n\\nStarting the training process is as easy as running:\\n\\n$ riseml train\\n\\nSyncing project (56.3 KB, 6 files)…done\\n\\nStarted experiment 5 in background…\\n\\nTensorBoard: http://10.0.101.138:80/tensorboard/admin-resnetimagenet-5-tensorboard\\n\\nType `riseml logs 5` to connect to log stream.\\n\\nThis will:\\n\\nCopy the code from our workstation to the cluster, where it is versioned Build a versioned Docker image with TensorFlow 1.7 and our code Store the versioned Docker image in the RiseML registry Provision a TPU for our experiment Start a container with our versioned image that is connected to the TPU\\n\\nBy using RiseML all of these steps are taken care of automatically and offloaded to the cluster.\\n\\nWe could now look at the logs using the riseml CLI, but following the training progress in TensorBoard is more interesting:\\n\\nTop-5 accuracy of the ResNet-50 model on the validation data of ImageNet\\n\\nAfter about ~12.5 hours the model achieves a top-5 accuracy of 93%! Once the experiment finishes training, the TPU is deprovisioned and the training container stopped. All information about the experiment, e.g., versioned code, Docker image, and logs are kept in RiseML. Because the TPU is automatically deprovisioned, costs are kept at a minimum.\\n\\nSummary\\n\\nRunning RiseML on GKE gives you an easy-to-use, highly scalable, and cost-efficient machine learning infrastructure. You don’t need to worry about system administration or DevOps tasks so you can focus on machine learning itself. Contact us, if you are interested in giving RiseML on GKE with TPUs a spin!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Benchmarking Google’s new TPUv2', 'Benchmarking Google’s new TPUv2\\n\\nUPDATE: Thanks for all your ideas on improving the benchmark! We are currently collecting all feedback and already started working on a more complete report — stay tuned! Contact: elmar@riseml.com\\n\\nFor most of us, deep learning still happens on Nvidia GPUs. There is currently no alternative with practical relevance. Google’s Tensor Processing Unit (TPU), a custom-developed chip for deep learning, promises to change that.\\n\\nNine months after the initial announcement, Google last week finally released TPUv2 to early beta users on the Google Cloud Platform. At RiseML, we got our hands on them and ran a couple of quick benchmarks. Below, we’d like to share our experience and preliminary results.\\n\\nMore competition in the market for deep learning hardware has been long sought after and has the potential of breaking up Nvidia’s monopoly on hardware for deep learning. Along with that, this will define what the deep learning infrastructure of the future will look like.\\n\\nKeep in mind that TPUs are still in early beta — as unmistakingly communicated by Google in many places — so some of the things we discuss might change in the future.\\n\\nTPUs on the Google Cloud\\n\\nWhile the first generation of chips, TPUv1, were geared towards inference, the second and current generation is focused on speeding up learning in the first place. At the core of the TPUv2, a systolic array is responsible for performing matrix multiplications, which are used heavily in deep learning. According to Jeff Dean’s slides, each Cloud TPU device consists of four “TPUv2 Chips”. Each chip has 16GB of memory with two cores, each with two matrix multiplication units. Together, the two cores provide 45 TFLOPs, totalling 180 TFLOPs and 64GB of memory for the whole TPU device. To put this into perspective, the current generation of Nvidia V100 GPUs provides 125 TFLOPs and 16GB of memory.\\n\\nTo use TPUs on the Google Cloud Platform, you need to start a Cloud TPU (after obtaining quota to do so). There is no need (or way) to assign a Cloud TPU to a specific VM instance. Instead, discovery of the Cloud TPU from your instance happens via network. Each Cloud TPU is assigned a name and gets an IP address that you need to provide to your TensorFlow code.\\n\\nCreating a new Cloud TPU. Note that a Cloud TPU has an IP address.\\n\\nTPUs are only supported by TensorFlow version 1.6, which is available as a release candidate. Besides that, you don’t need any drivers on your VM instance since all of the required code for communicating with the TPU is provided by TensorFlow itself. Code that is executed on the TPU is optimized and just-in-time compiled by XLA, which is also part of TensorFlow.\\n\\nIn order to efficiently use TPUs, your code should build on the high-level Estimator abstraction. You can then drop in a TPUEstimator which performs a lot of the necessary tasks for making efficient use of the TPU, e.g., it sets up data-queueing to the TPU and parallelizes the computation across its different cores. There is certainly a way around using the TPUEstimator, but we are currently unaware of an example or documentation.\\n\\nOnce you’ve set up everything, run your TensorFlow code as usual and the TPU will be discovered during start-up and the computation graph is compiled and transferred to the TPU. Interestingly, the TPU can also directly read and write from cloud storage to store checkpoints or event summaries. To allow this, you need to provide the service account behind the Cloud TPU write access to your cloud storage.\\n\\nBenchmarks\\n\\nThe interesting part is, of course, how fast TPUs really are. TensorFlow has a GitHub repository of models for TPUs that are known to work well. Below, we report on experiments with ResNet and Inception. We were also keen to see how a model that is not yet optimized for TPUs performs, so we adapted a model for text classification using LSTMs to run on TPUs. In general, Google recommends to use larger models (see when to use TPUs). This is a smaller model, so it was especially interesting to see if TPUs could still provide a benefit.\\n\\nFor all models, we compared training speed on a single Cloud TPU to a single Nvidia P100 and V100 GPU. We note that a thorough comparison should also include final quality and convergence of the model in addition to mere throughput. Our experiments are meant as a first peek and we will leave an in-detail analysis to future work.\\n\\nExperiments for TPUs and P100 were run on Google Cloud Platform on n1-standard-16 instances (16 vCPUs Intel Haswell, 60 GB memory). For the V100 GPU, we used p3.2xlarge (8 vCPUs, 60 GB memory) instances on AWS. All systems were running Ubuntu 16.04. For TPUs, we installed TensorFlow 1.6.0-rc1 from the PyPi repository. GPU experiments were run using nvidia-docker using TensorFlow 1.5 images (tensorflow:1.5.0-gpu-py3) that include CUDA 9.0 and cuDNN 7.0.\\n\\nTPU-optimized Models\\n\\nLet’s first look at the performance of models that are officially optimized for TPUs. Below, you can see the performance in terms of images per second.\\n\\nBatch sizes were 1024 for TPU and 128 for GPUs. For GPUs, we used the implementations from the TensorFlow benchmarks repository using the flag ‘use_fp16=true’ for the runs not marked ‘fp32’. The two groups of bars on the left, therefore, compare mixed-precision training. Training data was the fake Imagenet dataset provided by Google stored on cloud storage (for TPUs) and on local disks (for GPUs).\\n\\nOn ResNet-50, a single Cloud TPU (containing 4 TPUv2 chips and 64GB of RAM) is ~7.3 faster than a single P100 and ~2.8 times faster than a V100. For InceptionV3, the speedup is almost the same (~7.6 and ~2.5, respectively). With higher precision (fp32), the V100 loses a lot of speed. Note that training the model at this precision is not possible on the TPU at all since it only supports mixed-precision computation.\\n\\nClearly, beyond just speed, one has to take price into account. The table shows the performance normalized for on-demand pricing with per-second billing. The TPU still comes out ahead.\\n\\nCustom LSTM Model\\n\\nOur custom model is a bi-directional LSTM for text classification with 1024 hidden units. LSTMs are a basic building block in NLP nowadays so this nicely contrasts the official models, which are all computer vision based.\\n\\nThe original code was already using the Estimator framework, so adapting it to use TPUEstimator was very straightforward. There is one big disclaimer though: on TPUs we couldn’t get the model to converge whereas the same model (batch size, etc.) on GPUs worked fine. We think this is due to a bug that will be fixed — either in our code (if you find one, please let us know!) or in TensorFlow. Since the model didn’t converge, we decided not to report preliminary results here (we did in an earlier version of this post). Instead, we will report our findings in a separate blog post.\\n\\nConclusion\\n\\nOn the models we tested, TPUs compare very well, both, performance-wise and economically, to the latest generations of GPUs. This stands in contrast to previous reports. Overall, the experience of using TPUs and adapting TensorFlow code is already pretty good for a beta.\\n\\nWe think that once TPUs are available to a larger audience, they could become a real alternative to Nvidia GPUs.\\n\\nContact me at: elmar@riseml.com\\n\\nErrata (Feb 27th)')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Rent out your GPU compute to AI researchers, ~2x more than mining cryptocurrency', 'As a broke college student who is currently studying deep learning and AI, my side projects often require lots of GPUs to train neural networks. Unfortunately the cloud GPU instances from AWS and Google Cloud are really expensive (plus my student credits ran out in like 3 days), so the roadblock in a lot of my side projects was my limited access to GPU compute.\\n\\nLuckily for me, I had a friend who was mining Ethereum on his Nvidia 1080 ti\\'s. I would Venmo him double what he was making by mining Ethereum, and in return he would let me train my neural networks on his computer at significantly less than what I would have had to pay AWS.\\n\\nSo I thought to myself, \"hmm, what if there was an easy way for cryptocurrency miners to rent out their GPUs to AI researchers?\"\\n\\nAs it turns out, a lot of the infrastructure to become a mini-cloud provider is pretty much non-existent. So I built Vectordash - it\\'s a website where can you list your Nvidia GPUs for AI researchers to rent out - sort of like Airbnb but for GPUs. With current earnings, you can make about 3-4x more than you would make by mining the most profitable cryptocurrency.\\n\\nYou simply run a desktop client and list how long you plan on keeping your machine online for, and if someone is interested, they can rent it out and you\\'ll get paid for the duration they used it for. You can still mine whatever you like since the desktop client will automatically switch between mining & hosting whenever someone requests to use your computer.\\n\\nI\\'m still gauging whether or not GPU miners would be interested in something like this, but as someone who often finds themselves having to pay upwards of $20 per day for GPUs on AWS just for a side project, this would help a bunch.\\n\\nIf you have any specific recommendations, just comment below. I\\'d love to hear what you guys think!\\n\\n(and if you\\'re interested in becoming one of the first GPU hosts, please fill out this form - https://goo.gl/forms/ghFqpayk0fuaXqL92)\\n\\nOnce you\\'ve filled out the form, I\\'ll be sending an email with installation instructions in the next 1-2 days!\\n\\nCheers!\\n\\nedit:\\n\\nFAQ:\\n\\n1) Are AMD GPUs supported?\\n\\nFor the time being, no. Perhaps in the future, but no ETA.\\n\\n2) Is Windows supported?\\n\\nFor the time being, no. Perhaps in the future, but again, no ETA.\\n\\n3) When will I be able to host my GPUs on Vectordash?\\n\\nI have a few exams to study for this week (and was not expecting this much interest), but the desktop client should be completed very soon. Expect an email in the next couple of days with installation instructions.\\n\\n4) How can I become a host?\\n\\nIf you\\'ve filled out this form, then you are set! I\\'ll be sending out an email in the next couple of days with installation instructions. In the meanwhile, feel free to make an account on Vectordash.\\n\\nedit:\\n\\nThere\\'s been a TON of interest, so access to hosts will be rolled out in waves over the next week. If you\\'ve filled out the hosting form, I\\'ll be sending out emails shortly with more info. In the meanwhile, be sure to have made an account at http://vectordash.com.')\n",
      "('MIT 6.S099: Artificial General Intelligence (free, open to public)', 'This class takes an engineering approach to exploring possible research paths toward building human-level intelligence. The lectures will introduce our current understanding of computational intelligence and ways in which strong AI could possibly be achieved, with insights from deep learning, reinforcement learning, computational neuroscience, robotics, cognitive modeling, psychology, and more. Additional topics will include AI safety and ethics. Projects will seek to build intuition about the limitations of state-of-the-art machine learning approaches and how those limitations may be overcome. The course will include several guest talks. Listeners are welcome.\\n\\nInterested in the class? Here are some things you could do:\\n\\nCourse Information:\\n\\nTime/Dates: Every day, 7pm, Jan 22 - Feb 2\\n\\nEvery day, 7pm, Jan 22 - Feb 2 Duration: 60-90 minutes\\n\\n60-90 minutes Location: MIT, 54-100 (location details) with some exceptions.\\n\\nMIT, 54-100 (location details) with some exceptions. Instructor: Lex Fridman\\n\\nLex Fridman Contact: agi@mit.edu\\n\\n2018 Schedule of Lectures and Talks\\n\\nTeam\\n\\nBig Thanks\\n\\nMost (but not all) lectures and talks will be at 7pm in Room 54-100 . See below for exact time and location.\\n\\nNone of this course would be possible without the great community of bright young minds at MIT and beyond. Thank you.')\n",
      "('Stock Price Prediction with LSTMs', 'Do you want to make millions in the stock market using Deep Learning? This post will not answer that question, but it will show how you can use an LSTM to predict stock prices with Keras, which is cool, right?\\n\\nIf you are here with the hope that I will show you a method to get rich by predicting stock prices, sorry, I\\'m don\\'t know the solution. Even if I knew, I would never tell you!\\n\\nEven if you think it is impossible to predict the future, it is not. In fact, that is one of the reasons we invented math. Probably you have heard about something called differential equations (DE). If you are an engineer, you probably had to pass a painful exam of DE (I did). DE are just one way to model the temporal behavior of a system.\\n\\nAnother method is called Recurrent Neural Networks (RNNs), which is a deep learning model designed to address sequential data problems. A variant of RNNs is the Long-Short Term Memory (LSTM) which is specially suited for time-series prediction problems. It is internally composed by two kind of units, hidden units, which as in CNNs contain a hidden representation of the inputs, and gated units, which control the amount of information that flows from the inputs, to the hidden units and to the outputs.\\n\\nThe gated structure of LSTMs, make them very efficient when working with large time series. They are able to understand long-term dependencies in temporal data, controlling which information should be \"remembered\" and which \"forgotten\".\\n\\nI created a simple tutorial using Keras with Tensorflow back-end to predict the stock prices of Microsoft from 2012 to 2017. I used an LSTM and two variants of the same family: Bi-directional LSTM and GRU.\\n\\nIn the figure, the model is able to track the true value (in green) with the prediction in the test set (in blue), given a training set of past values (in red). This code is probably not enough to earn money in the stock market, but it will give you a good baseline if you want to enter the exciting world of financial services with AI.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('How I implemented iPhone X’s FaceID using Deep Learning in Python', 'One of the most discussed features of the new iPhone X is the new unlocking method, the successor of TouchID: FaceID.\\n\\nHaving created a bezel-less phone, Apple had to develop a new method to unlock the phone in a easy and fast way. While some competitors continued using a fingerprint sensor, placed in a different position, Apple decided to innovate and revolutionize the way we unlock a phone: by simply looking at it. Thanks to an advanced (and remarkably small) front facing depth-camera, iPhone X in able to create a 3D map of the face of the user. In addition, a picture of the user’s face is captured using an infrared camera, that is more robust to changes in light and color of the environment. Using deep learning, the smartphone is able to learn the user face in great detail, thus recognizing him\\\\her every time the phone is picked up by its owner. Surprisingly, Apple ha stated that this method is even safer than TouchID, with an outstanding error rate of 1:1,000,000.\\n\\nI was very intrigued by the techniques used by Apple to realize FaceID, especially by the fact that this all runs on-device, with a little initial training on the user’s face, and then runs smoothly every time the phone is picked up. I focused on how to make this process work using deep learning, and how to optimize each step. In this post, I will show how I implemented a FaceID-like algorithm using Keras. I will explain the various architectural decision that I took, and show some final experiments, done using a Kinect, a very popular RGB and depth camera, that has a very similar output to iPhone X front facing cameras (but on a much bigger device). Sit comfortably, take a cup of coffee, and let’s start reverse engineering Apple’s new game changing feature.\\n\\nUnderstanding FaceID\\n\\n“…the neural networks powering FaceID are not simply performing classification.”\\n\\nFaceID setup process.\\n\\nThe first step is analyzing carefully how FaceID works on the iPhone X. Their white paper can help us understand the basic mechanisms of FaceID. With TouchID, the user had to initially register his\\\\her fingerprints by pressing several times the sensor. After around 15–20 different touches, the smartphone completed the registration, and TouchID was ready to go. Similarly, with FaceID the user has to register his\\\\her face. The process is very simple: the user just looks at the phone as he\\\\she would normally do, and then slowly rotates the head following a circle, thus registering the face from different poses. And that’s it, the process is complete and the phone is ready to be unlocked. This blazingly fast registration procedure can tell us a lot about the underlying learning algorithms. For instance, the neural networks powering FaceID are not just performing classifications, and I’ll explain why.\\n\\nApple Keynote unveiling iPhone X and FaceID.\\n\\nPerforming classification, for a neural network, means learning to predict if the face it has seen it’s the users’s one or not. So, it should use some training data to predict “true” or “false”, basically, but differently from a lot of other deep learning use cases, here this approach would not work. First, the network should re-train from scratch using the new obtained data from the user’s face. This would require a lot of time, energy consumption, and impractical availability of training data of different faces to have negative examples (little would change in case of transfer learning and fine tuning of an already trained network). Furthermore, this method would not exploit the possibility, for Apple, to train a much more complex network “offline”, i.e. in their laboratories, and then ship it already trained and ready to use in their phones. Instead, I believe FaceID is powered by a siamese-like convolutional neural network that is trained “offline” by Apple to map faces into a low-dimensional latent space shaped to maximize distances between faces of different people, using a contrastive loss. What happens is that you get an architecture capable of doing one shot learning, as they very briefly mentioned at their Keynote. I know, there are some names that could not be familiar to many readers: keep reading, and I will explain step by step what I mean.\\n\\nFaceID looks like it will be the new standard after TouchID. Will Apple bring it to all his new devices?\\n\\nFrom faces to numbers with neural networks\\n\\nA siamese neural network is basically composed by two identical neural networks that also share all the weights. This architecture can learn to compute distances between particular kind of data, such as images. The idea is that you pass couples of data through the siamese networks (or simply pass the data in two different steps through the same network), the network maps it in a low dimensional feature space, like a n-dimensional array, and then you train the network to make this mapping so that data points from different classes are as far as possible, while data points from the same class are as close as possible. In the long run, the network will learn to extract the most meaningful features from data, and compress it into an array, creating an meaningful mapping. To have an intuitive understanding of this, imagine how you would describe dog breeds using a small vector, so that similar dogs have closer vectors. You would probably use a number to encode the fur color of the dog, another one to denote the size of the dog, another one for the length of fur, and so on. In this way, dogs that are similar to each other will have vectors that are similar to each other. Quite smart, right? Well, a siamese neural network can learn to do this for you, similarly to what an autoencoder does.\\n\\nA figure from the paper “Dimensionality Reduction by Learning an Invariant Mapping” by Hadsell, Chopra and LeCun. Notice how the architecture is learning similarity between digits and is automatically grouping them in 2-dimensions. A similar technique is applied to faces.\\n\\nWith this technique, one can use a great amount of faces to train such an architecture to recognize which faces are most similar. Having the right budget and computing power (as Apple does), one can also use harder and harder examples to make the network robust to things such as twins, adversarial attacks (masks) and so on. And what’s the final advantage of using this approach? That you finally have a plug and play model that can recognize different users without any further training, but simply computing where the user’s face is located in the latent map of faces after taking some pictures during the initial setup. (Imagine, as said before, to write down the vector of dog breeds for a new dog, and then storing it somewhere). In addition, FaceID is able to adapt to changes in your aspect: both sudden changes (e.g., glasses, hats, makeup) and slows changes (facial hair). This is done by basically adding reference face-vectors in this map, computed based on your new appearance.\\n\\nFaceID adapts when your looks change.\\n\\nNow, let’s finally see how to implement it in Python using Keras.\\n\\nImplementing FaceID in Keras\\n\\nAs for all machine learning projects, the first thing we need is data. Creating our own dataset would require time and the collaboration of many people, and this can be quite challenging. Thus, I browsed the web for a RGB-D face datasets, and I found one that looked like a perfect fit. It’s composed by a series of RGB-D pictures of people facing different directions and making different facial expressions, as it would happen in the iPhone X use case.\\n\\nTo see the final implementation, you can check out my GitHub repository, where you can find a Jupyter Notebook. Furthermore, I experimented using a Colab Notebook that you can try too.\\n\\nI created a convolutional network based on the SqueezeNet architecture. The network takes as input RGBD pictures of couples faces, so a 4 channel picture, and outputs a distance between the two embeddings. The network is trained with a constrastive loss, that minimizes distances between pictures of the same person and maximizes the distance between pictures of different persons.\\n\\nContrastive loss.\\n\\nAfter some training, the network is able to map faces into 128-dimensional arrays, such that pictures of the same person are grouped together, while being far from pictures of other persons. This means that, to unlock your device, the network just needs to compute the distance between the picture it takes during the unlocking with the pictures stored during the registration phase. If the distance is under a certain threshold, (the more little it is, the more secure it is) the device unlocks.\\n\\nI used the t-SNE algorithm to visualize in two dimensions the 128-dimensional embedding space. Every color corresponds to a different person: as you can see, the network has learned to group those pictures quite tightly. (the distances between clusters are meaningless when using the t-SNE algorithm) An interesting plot also arises when using the PCA dimensionality reduction algorithm.\\n\\nClusters of faces in the embedding space created using t-SNE. Every color is a different face (but colors are reused).\\n\\nClusters of faces in the embedding space created using PCA. Every color is a different face (but colors are reused).\\n\\nExperiments!\\n\\nWe can now try to see of this model works, simulating a usual FaceID cycle: first, the registration of the user’s face. Then, the unlocking phase, both from the user (that should succeed) that from other persons, that shouldn’t be able to unlock the device. As previously mentioned, the difference is between the distance that the network computes between the face that is unlocking the phone and the registered faces, and wether it is under a certain threshold or not.\\n\\nLet’s start with the registration: I took a series of pictures of the same person from the dataset and simulated a registration phase. The device is now computing the embeddings for each of those poses, and storing them locally.\\n\\nRegistration phase for a new user, inspired by the FaceID process.\\n\\nRegistration phase as seen by the depth camera.\\n\\nLet’s see now what happens if the same user tries to unlock the device. Different poses and facial expressions of the same user achieve a low distance, of around 0.30 on average.\\n\\nFace distances in the embedding space for the same user.\\n\\nOn the other hand, RGBD pictures from different people get an average distance of 1.1.\\n\\nFace distances in the embedding space for different users.\\n\\nSo, using a threshold of around 0.4 should be sufficient to prevent strangers from unlocking your device.\\n\\nConclusion\\n\\nIn this post I showed how to implement a proof-of-concept of the FaceID unlocking mechanics, based on face embeddings and siamese convolutional networks. I hope you found it helpful, for any question you can get in touch with me. You can find here all the relative Python code.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Robocalypse Now: Using Deep Learning to Combat Cheating in CSGO [video]', \"Overview\\n\\nIn this session, John will discuss how Valve has utilized Deep Learning to combat cheating in 'Counter-Strike: Global Offensive'. He will give total system details, from the high level server architecture to the low level features fed into the AI. Deep Learning has proven to be very effective at identifying cheating behavior without any client-side instrumentation, making it robust against malicious attack by cheaters and cheat vendors. By retraining regularly, the network continues to evolve, picking up new cheating behaviors within hours of their appearance. As a result of this approach, certain types of cheats have been reduced by a factor of one hundred.\")\n",
      "('How to Easily Detect Objects with Deep Learning on Raspberry Pi', 'Phase 2 — Training a Model on a GPU Machine\\n\\nStep 3. Finding a Pretrained Model for Transfer Learning:\\n\\nYou can read more about this at medium.com/nanonets/nanonets-how-to-use-deep-learning-when-you-have-limited-data-f68c0b512cab. You need a pretrained model so you can reduce the amount of data required to train. Without it, you might need a few 100k images to train the model.\\n\\nYou can find a bunch of pretrained models here\\n\\nStep 4. Training on a GPU (cloud service like AWS/GCP etc or your own GPU Machine):\\n\\nDocker Image\\n\\nThe process of training a model is unnecessarily difficult to simplify the process we created a docker image would make it easy to train.\\n\\nTo start training the model you can run:\\n\\nsudo nvidia-docker run -p 8000:8000 -v `pwd`:data docker.nanonets.com/pi_training -m train -a ssd_mobilenet_v1_coco -e ssd_mobilenet_v1_coco_0 -p \\'{\"batch_size\":8,\"learning_rate\":0.003}\\'\\n\\nThe docker image has a run.sh script that can be called with the following parameters\\n\\nrun.sh [-m mode] [-a architecture] [-h help] [-e experiment_id] [-c checkpoint] [-p hyperparameters]\\n\\n-h display this help and exit\\n\\n-m mode: should be either `train` or `export`\\n\\n-p key value pairs of hyperparameters as json string\\n\\n-e experiment id. Used as path inside data folder to run current experiment\\n\\n-c applicable when mode is export, used to specify checkpoint to use for export\\n\\nYou can find more details at:\\n\\nTo train a model you need to select the right hyper parameters.\\n\\nFinding the right parameters\\n\\nThe art of “Deep Learning” involves a little bit of hit and try to figure out which are the best parameters to get the highest accuracy for your model. There is some level of black magic associated with this, along with a little bit of theory. This is a great resource for finding the right parameters.\\n\\nQuantize Model (make it smaller to fit on a small device like the Raspberry Pi or Mobile)\\n\\nSmall devices like Mobile Phones and Rasberry PI have very little memory and computation power.\\n\\nTraining neural networks is done by applying many tiny nudges to the weights, and these small increments typically need floating point precision to work (though there are research efforts to use quantized representations here too).\\n\\nTaking a pre-trained model and running inference is very different. One of the magical qualities of Deep Neural Networks is that they tend to cope very well with high levels of noise in their inputs.\\n\\nWhy Quantize?\\n\\nNeural network models can take up a lot of space on disk, with the original AlexNet being over 200 MB in float format for example. Almost all of that size is taken up with the weights for the neural connections, since there are often many millions of these in a single model.\\n\\nThe Nodes and Weights of a neural network are originally stored as 32-bit floating point numbers. The simplest motivation for quantization is to shrink file sizes by storing the min and max for each layer, and then compressing each float value to an eight-bit integer.The size of the files is reduced by 75%.\\n\\nCode for Quantization:\\n\\ncurl -L \"https://storage.googleapis.com/download.tensorflow.org/models/inception_v3_2016_08_28_frozen.pb.tar.gz\" |\\n\\ntar -C tensorflow/examples/label_image/data -xz\\n\\nbazel build tensorflow/tools/graph_transforms:transform_graph\\n\\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\\\\n\\n--in_graph=tensorflow/examples/label_image/data/inception_v3_2016_08_28_frozen.pb \\\\\\n\\n--out_graph=/tmp/quantized_graph.pb \\\\\\n\\n--inputs=input \\\\\\n\\n--outputs=InceptionV3/Predictions/Reshape_1 \\\\\\n\\n--transforms=\\'add_default_attributes strip_unused_nodes(type=float, shape=\"1,299,299,3\")\\n\\nremove_nodes(op=Identity, op=CheckNumerics) fold_constants(ignore_errors=true)\\n\\nfold_batch_norms fold_old_batch_norms quantize_weights quantize_nodes\\n\\nstrip_unused_nodes sort_by_execution_order\\n\\nNote: Our docker image has quantization built into it.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Kaggle Learn review: there is a deep learning track and it is worth your time', 'Right from my undergrad days when I was starting out with machine learning to this date, my admiration for Kaggle continues to grow. In addition to being synonymous with and popularizing data science competitions, the platform has served as a launching pad and breeding ground for countless data science and machine learning practitioners around the world, including yours truly. In fact, skills I’d picked up from the platform are part of the reason that I recently got to join SocialCops, a company I’d admired for years. However, I hadn’t been on the platform in 2017 as much as I would have liked. So when I saw Ben Hamner‘s tweet launching Kaggle Learn, a set of interactive data science tutorials, I made up my mind to give it a shot.\\n\\nExcited to launch Kaggle Learn – interactive tutorials on machine learning, deep learning, R, and data visualization https://t.co/vWITheLP7K — Ben Hamner (@benhamner) January 16, 2018\\n\\nZeroing in on deep learning\\n\\nLearn currently hosts tutorials about 4 topics – introductory machine learning, R programming, data visualisation and deep learning. I’d stumbled across machine learning for the first time in the form of neural networks (NN) more than 3 years back. Since then, I’d studied the theoretical details of NN at various points of time but somewhat ironically, I’d never got into practical deep learning except for a few tutorials. Hence, I decided to get started with the deep learning track.\\n\\nThe reason I mentioned my past experience with ML and NN was to point out the fact that I was not a complete beginner when I had gotten started with this track and if you are, start with the machine learning track instead.\\n\\nGetting started\\n\\nIf you are unfamiliar with neural networks or haven’t come across them recently, it would be a good idea to get some theoretical foundation before starting with hands-on tutorials. There are a number of introductory resources out there, both text and video. I used an excellent video by 3Blue1Brown, a YouTube channel, as a refresher.\\n\\nChoice of framework\\n\\nThe track uses the high-level Keras API and a Tensorflow backend. Even with numerous frameworks out there, this combination seems to find favor as a beginner-friendly choice among a large portion of the deep learning community. Personally, I admire Keras for being well-designed, user-friendly and playing a big role in democratizing access to deep learning methods.\\n\\nThe track\\n\\nThe deep learning track is currently comprised of six sections. They are:\\n\\nIntro to Deep Learning and Computer Vision : Starting off with a computer vision example is a great way to get acquainted with machine learning. This is the application which had put deep learning in the limelight and the data (images) is something most of us deal with on an everyday basis. The accompanying exercise allows you to play around with basic convolutions and images.\\n\\n: Starting off with a computer vision example is a great way to get acquainted with machine learning. This is the application which had put deep learning in the limelight and the data (images) is something most of us deal with on an everyday basis. The accompanying exercise allows you to play around with basic convolutions and images. Building Models from Convolutions : Convolutional neural networks (ConvNet) have received wide praise and coverage for being extremely successful with image recognition tasks. The basics of ConvNets are discussed and the stage is set up for their implementation.\\n\\n: Convolutional neural networks (ConvNet) have received wide praise and coverage for being extremely successful with image recognition tasks. The basics of ConvNets are discussed and the stage is set up for their implementation. Programming in Tensorflow and Keras : You get to see TF+Keras in action for the first time and you’ll be amazed at the ease with which you can get up and running. There’s a lot of hand-holding here so getting the code to work alone won’t be very useful. Try to understand the code, including helper functions, as much as possible.\\n\\n\\n\\nRelated – Big deep learning news: Tensorflow chooses Keras\\n\\n: You get to see TF+Keras in action for the first time and you’ll be amazed at the ease with which you can get up and running. There’s a lot of hand-holding here so getting the code to work alone won’t be very useful. Try to understand the code, including helper functions, as much as possible. Related – Big deep learning news: Tensorflow chooses Keras Transfer Learning: It was a great decision by Dan Becker to include this, and it is my favorite part of the tutorial. Prior to this, my perception of transfer learning was as an advanced topic which would require a decent amount of know-how to even get started. I am delighted to tell you that that I couldn’t have been more wrong. Even if all you know are the very basics of NN, the idea of transfer learning itself is fascinating and I’ve decided to spend some time in near future to research about the topic. Prior to starting this section, I’d gone through the following video by the one and only Andrew Ng.\\n\\nData Augmentation : Simply put, data augmentation is a handy technique which results in increased number of data points for your machine learning algorithm. This section discusses the technique as well as its implementation in Keras.Related – What you need to know about data augmentation for machine learning\\n\\n: Simply put, data augmentation is a handy technique which results in increased number of data points for your machine learning algorithm. This section discusses the technique as well as its implementation in Keras.Related – What you need to know about data augmentation for machine learning A Deeper Understanding of Deep Learning: The code used in the previous sections, particularly the various parameters, are discussed in more detail. Also, stochastic gradient descent and backpropogation are briefly discussed.\\n\\nConclusion and additional resources\\n\\nWhen learning a new topic, I’ve always found it best to start with a high-level overview. That’s precisely what this track aims to offer and for most part, delivers. For a considerable amount of time, setting up deep learning frameworks used to be a roadblock to getting started with the topic. To that end, Kaggle leverages its platform’s capabilities to host the code and while doing so, showcases its potential for being useful for collaboration. All that being said, this topic only scratches the surface, even if in a better manner than most tutorials out there. You can plan out your path from here on. If it helps, below are some of the resources I plan to dive into or explore over the next few weeks.\\n\\nIf there’s any other useful resource you can think of, feel free to mention it in the comments below.\\n\\nIf you read and liked the article, sharing it would be a good next step.\\n\\nAdditionally, you can check out some of my open source projects on Github.\\n\\nDrop me a mail, or hit me up on Twitter or LinkedIn in case you want to get in touch.\\n\\nAdvertisements')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('PyTorch – Internal Architecture Tour', 'Introduction\\n\\nThis post is a tour around the PyTorch codebase, it is meant to be a guide for the architectural design of PyTorch and its internals. My main goal is to provide something useful for those who are interested in understanding what happens beyond the user-facing API and show something new beyond what was already covered in other tutorials.\\n\\nNote: PyTorch build system uses code generation extensively so I won’t repeat here what was already described by others. If you’re interested in understanding how this works, please read the following tutorials: A Tour of PyTorch Internals (Part I)\\n\\nPyTorch Internals Part II – The Build System\\n\\nShort intro to Python extension objects in C/C++\\n\\nAs you probably know, you can extend Python using C and C++ and develop what is called as “extension”. All the PyTorch heavy work is implemented in C/C++ instead of pure-Python. To define a new Python object type in C/C++, you define a structure like this one example below (which is the base for the autograd Variable class):\\n\\n// Python object that backs torch.autograd.Variable struct THPVariable { PyObject_HEAD torch::autograd::Variable cdata; PyObject* backward_hooks; };\\n\\nAs you can see, there is a macro at the beginning of the definition, called PyObject_HEAD , this macro’s goal is the standardization of Python objects and will expand to another structure that contains a pointer to a type object (which defines initialization methods, allocators, etc) and also a field with a reference counter.\\n\\nThere are two extra macros in the Python API called Py_INCREF() and Py_DECREF() , which are used to increment and decrement the reference counter of Python objects. Multiple entities can borrow or own a reference to other objects (the reference counter is increased), and only when this reference counter reaches zero (when all references get destroyed), Python will automatically delete the memory from that object using its garbage collector.\\n\\nYou can read more about Python C/++ extensions here.\\n\\nFunny fact: it is very common in many applications to use small integer numbers as indexing, counters, etc. For efficiency, the official a = 200; b = 200; a is b will be True, while the statement a = 300; b = 300; a is b will be False. : it is very common in many applications to use small integer numbers as indexing, counters, etc. For efficiency, the official CPython interpreter caches the integers from -5 up to 256. For that reason, the statementwill be, while the statementwill be\\n\\nZero-copy PyTorch Tensor to Numpy and vice-versa\\n\\nPyTorch has its own Tensor representation, which decouples PyTorch internal representation from external representations. However, as it is very common, especially when data is loaded from a variety of sources, to have Numpy arrays everywhere, therefore we really need to make conversions between Numpy and PyTorch tensors. For that reason, PyTorch provides two methods called from_numpy() and numpy() , that converts a Numpy array to a PyTorch array and vice-versa, respectively. If we look the code that is being called to convert a Numpy array into a PyTorch tensor, we can get more insights on the PyTorch’s internal representation:\\n\\nat::Tensor tensor_from_numpy(PyObject* obj) { if (!PyArray_Check(obj)) { throw TypeError(\"expected np.ndarray (got %s)\", Py_TYPE(obj)->tp_name); } auto array = (PyArrayObject*)obj; int ndim = PyArray_NDIM(array); auto sizes = to_aten_shape(ndim, PyArray_DIMS(array)); auto strides = to_aten_shape(ndim, PyArray_STRIDES(array)); // NumPy strides use bytes. Torch strides use element counts. auto element_size_in_bytes = PyArray_ITEMSIZE(array); for (auto& stride : strides) { stride /= element_size_in_bytes; } // (...) - omitted for brevity void* data_ptr = PyArray_DATA(array); auto& type = CPU(dtype_to_aten(PyArray_TYPE(array))); Py_INCREF(obj); return type.tensorFromBlob(data_ptr, sizes, strides, [obj](void* data) { AutoGIL gil; Py_DECREF(obj); }); }\\n\\n(code from tensor_numpy.cpp)\\n\\nAs you can see from this code, PyTorch is obtaining all information (array metadata) from Numpy representation and then creating its own. However, as you can note from the marked line 18, PyTorch is getting a pointer to the internal Numpy array raw data instead of copying it. This means that PyTorch will create a reference for this data, sharing the same memory region with the Numpy array object for the raw Tensor data.\\n\\nThere is also an important point here: when Numpy array object goes out of scope and get a zero reference count, it will be garbage collected and destroyed, that’s why there is an increment in the reference counting of the Numpy array object at line 20.\\n\\nAfter this, PyTorch will create a new Tensor object from this Numpy data blob, and in the creation of this new Tensor it passes the borrowed memory data pointer, together with the memory size and strides as well as a function that will be used later by the Tensor Storage (we’ll discuss this in the next section) to release the data by decrementing the reference counting to the Numpy array object and let Python take care of this object life cycle.\\n\\nThe tensorFromBlob() method will create a new Tensor, but only after creating a new “Storage” for this Tensor. The storage is where the actual data pointer will be stored (and not in the Tensor structure itself). This takes us to the next section about Tensor Storages.\\n\\nTensor Storage\\n\\nThe actual raw data of the Tensor is not directly kept in the Tensor structure, but on another structure called Storage, which in turn is part of the Tensor structure.\\n\\nAs we saw in the previous code from tensor_from_numpy() , there is a call for tensorFromBlob() that will create a Tensor from the raw data blob. This last function will call another function called storageFromBlob() that will, in turn, create a storage for this data according to its type. In the case of a CPU float type, it will return a new CPUFloatStorage instance.\\n\\nThe CPUFloatStorage is basically a wrapper with utility functions around the actual storage structure called THFloatStorage that we show below:\\n\\ntypedef struct THStorage { real *data; ptrdiff_t size; int refcount; char flag; THAllocator *allocator; void *allocatorContext; struct THStorage *view; } THStorage;\\n\\n(code from THStorage.h)\\n\\nAs you can see, the THStorage holds a pointer to the raw data, its size, flags and also an interesting field called allocator that we’ll soon discuss. It is also important to note that there is no metadata regarding on how to interpret the data inside the THStorage , this is due to the fact that the storage is “dumb” regarding of its contents and it is the Tensor responsibility to know how to “view” or interpret this data.\\n\\nFrom this, you already probably realized that we can have multiple tensors pointing to the same storage but with different views of this data, and that’s why viewing a tensor with a different shape (but keeping the same number of elements) is so efficient. This Python code below shows that the data pointer in the storage is being shared after changing the way Tensor views its data:\\n\\n>>> tensor_a = torch.ones((3, 3)) >>> tensor_b = tensor_a.view(9) >>> tensor_a.storage().data_ptr() == tensor_b.storage().data_ptr() True\\n\\nAs we can see in the example above, the data pointer on the storage of both Tensors are the same, but the Tensors represent a different interpretation of the storage data.\\n\\nNow, as we saw in line 7 of the THFloatStorage structure, there is a pointer to a THAllocator structure there. And this is very important because it brings flexibility regarding the allocator that can be used to allocate the storage data. This structure is represented by the following code:\\n\\ntypedef struct THAllocator { void* (*malloc)(void*, ptrdiff_t); void* (*realloc)(void*, void*, ptrdiff_t); void (*free)(void*, void*); } THAllocator;\\n\\n(code from THAllocator.h)\\n\\nAs you can see, there are three function pointer fields in this structure to define what an allocator means: a malloc, realloc and free. For CPU-allocated memory, these functions will, of course, relate to the traditional malloc/realloc/free POSIX functions, however, when we want a storage allocated on GPUs we’ll end up using the CUDA allocators such as the cudaMallocHost() , like we can see in the THCudaHostAllocator malloc function below:\\n\\nstatic void *THCudaHostAllocator_malloc(void* ctx, ptrdiff_t size) { void* ptr; if (size < 0) THError(\"Invalid memory size: %ld\", size); if (size == 0) return NULL; THCudaCheck(cudaMallocHost(&ptr, size)); return ptr; }\\n\\n(code from THCAllocator.c)\\n\\nYou probably noticed a pattern in the repository organization, but it is important to keep in mind these conventions when navigating the repository, as summarized here (taken from the PyTorch lib readme):\\n\\nTH = T orc H\\n\\n= orc THC = T orc H C uda\\n\\n= orc uda THCS = T orc H C uda S parse\\n\\n= orc uda parse THCUNN = T orc H CU da N eural N etwork\\n\\n= orc da eural etwork THD = T orc H D istributed\\n\\n= orc istributed THNN = T orc H N eural N etwork\\n\\n= orc eural etwork THS = TorcH Sparse\\n\\nThis convention is also present in the function/class names and other objects, so it is important to always keep these patterns in mind. While you can find CPU allocators in the TH code, you’ll find CUDA allocators in the THC code.\\n\\nFinally, we can see the composition of the main Tensor THTensor structure:\\n\\ntypedef struct THTensor { int64_t *size; int64_t *stride; int nDimension; THStorage *storage; ptrdiff_t storageOffset; int refcount; char flag; } THTensor;\\n\\n(Code from THTensor.h)\\n\\nAnd as you can see, the main THTensor structure holds the size/strides/dimensions/offsets/etc as well as the storage ( THStorage ) for the Tensor data.\\n\\nWe can summarize all this structure that we saw in the diagram below:\\n\\nNow, once we have requirements such as multi-processing where we want to share tensor data among multiple different processes, we need a shared memory approach to solve it, otherwise, every time another process needs a tensor or even when you want to implement Hogwild training procedure where all different processes will write to the same memory region (where the parameters are), you’ll need to make copies between processes, and this is very inefficient. Therefore we’ll discuss in the next section a special kind of storage for Shared Memory.\\n\\nShared Memory\\n\\nShared memory can be implemented in many different ways depending on the platform support. PyTorch supports some of them, but for the sake of simplicity, I’ll talk here about what happens on MacOS using the CPU (instead of GPU). Since PyTorch supports multiple shared memory approaches, this part is a little tricky to grasp into since it involves more levels of indirection in the code.\\n\\nPyTorch provides a wrapper around the Python multiprocessing module and can be imported from torch.multiprocessing . The changes they implemented in this wrapper around the official Python multiprocessing were done to make sure that everytime a tensor is put on a queue or shared with another process, PyTorch will make sure that only a handle for the shared memory will be shared instead of a new entire copy of the Tensor.\\n\\nNow, many people aren’t aware of a Tensor method from PyTorch called share_memory_() , however, this function is what triggers an entire rebuild of the storage memory for that particular Tensor. What this method does is to create a region of shared memory that can be used among different processes. This function will, in the end, call this following function below:\\n\\nstatic THStorage* THPStorage_(newFilenameStorage)(ptrdiff_t size) { int flags = TH_ALLOCATOR_MAPPED_SHAREDMEM | TH_ALLOCATOR_MAPPED_EXCLUSIVE; std::string handle = THPStorage_(__newHandle)(); auto ctx = libshm_context_new(NULL, handle.c_str(), flags); return THStorage_(newWithAllocator)(size, &THManagedSharedAllocator, (void*)ctx); }\\n\\n(Code from StorageSharing.cpp)\\n\\nAnd as you can see, this function will create another storage using a special allocator called THManagedSharedAllocator . This function first defines some flags and then it creates a handle which is a string in the format /torch_[process id]_[random number] , and after that, it will then create a new storage using the special THManagedSharedAllocator . This allocator has function pointers to an internal PyTorch library called libshm, that will implement a Unix Domain Socket communication to share the shared memory region handles. This allocator is actual an especial case and it is a kind of “smart allocator” because it contains the communication control logic as well as it uses another allocator called THRefcountedMapAllocator that will be responsible for creating the actual shared memory region and call mmap() to map this region to the process virtual address space.\\n\\nNote: when a method ends with a underscore in PyTorch, such as the method called share_memory_() , it means that this method has an in-place effect, and it will change the current object instead of creating a new one with the modifications.\\n\\nI’ll now show a Python example of one processing using the data from a Tensor that was allocated on another process by manually exchanging the shared memory handle:\\n\\nThis is executed in the process A:\\n\\n>>> import torch >>> tensor_a = torch.ones((5, 5)) >>> tensor_a 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 [torch.FloatTensor of size 5x5] >>> tensor_a.is_shared() False >>> tensor_a = tensor_a.share_memory_() >>> tensor_a.is_shared() True >>> tensor_a_storage = tensor_a.storage() >>> tensor_a_storage._share_filename_() (b\\'/var/tmp/tmp.0.yowqlr\\', b\\'/torch_31258_1218748506\\', 25)\\n\\nIn this code, executed in the process A, we create a new Tensor of 5×5 filled with ones. After that we make it shared and print the tuple with the Unix Domain Socket address as well as the handle. Now we can access this memory region from another process B as shown below:\\n\\nCode executed in the process B:\\n\\n>>> import torch >>> tensor_a = torch.Tensor() >>> tuple_info = (b\\'/var/tmp/tmp.0.yowqlr\\', b\\'/torch_31258_1218748506\\', 25) >>> storage = torch.Storage._new_shared_filename(*tuple_info) >>> tensor_a = torch.Tensor(storage).view((5, 5)) 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 [torch.FloatTensor of size 5x5]\\n\\nAs you can see, using the tuple information about the Unix Domain Socket address and the handle we were able to access the Tensor storage from another process. If you change the tensor in this process B, you’ll also see that it will reflect in the process A because these Tensors are sharing the same memory region.\\n\\nDLPack: a hope for the Deep Learning frameworks Babel\\n\\nNow I would like to talk about something recent in the PyTorch code base, that is called DLPack. DLPack is an open standardization of an in-memory tensor structure that will allow exchange tensor data between frameworks, and what is quite interesting is that since this memory representation is standardized and very similar to the memory representation already in use by many frameworks, it will allow a zero-copy data sharing between frameworks, which is a quite amazing initiative given the variety of frameworks we have today without inter-communication among them.\\n\\nThis will certainly help to overcome the “island model” that we have today between tensor representations in MXNet, PyTorch, etc, and will allow developers to mix framework operations between frameworks and all the benefits that a standardization can bring to the frameworks.\\n\\nThe core of DLPack os a very simple structure called DLTensor , as shown below:\\n\\n/*! * \\\\brief Plain C Tensor object, does not manage memory. */ typedef struct { /*! * \\\\brief The opaque data pointer points to the allocated data. * This will be CUDA device pointer or cl_mem handle in OpenCL. * This pointer is always aligns to 256 bytes as in CUDA. */ void* data; /*! \\\\brief The device context of the tensor */ DLContext ctx; /*! \\\\brief Number of dimensions */ int ndim; /*! \\\\brief The data type of the pointer*/ DLDataType dtype; /*! \\\\brief The shape of the tensor */ int64_t* shape; /*! * \\\\brief strides of the tensor, * can be NULL, indicating tensor is compact. */ int64_t* strides; /*! \\\\brief The offset in bytes to the beginning pointer to data */ uint64_t byte_offset; } DLTensor;\\n\\n(code from dlpack.h)\\n\\nAs you can see, there is a data pointer for the raw data as well as shape/stride/offset/GPU vs CPU, and other metadata information about the data that the DLTensor pointing to.\\n\\nThere is also a managed version of the tensor that is called DLManagedTensor , where the frameworks can provide a context and also a “deleter” function that can be called by the framework who borrowed the Tensor to inform the other framework that the resources are no longer required.\\n\\nIn PyTorch, if you want to convert to or from a DLTensor format, you can find both C/C++ methods for doing that or even in Python you can do that as shown below:\\n\\nimport torch from torch.utils import dlpack t = torch.ones((5, 5)) dl = dlpack.to_dlpack(t)\\n\\nThis Python function will call the toDLPack function from ATen, shown below:\\n\\nDLManagedTensor* toDLPack(const Tensor& src) { ATenDLMTensor * atDLMTensor(new ATenDLMTensor); atDLMTensor->handle = src; atDLMTensor->tensor.manager_ctx = atDLMTensor; atDLMTensor->tensor.deleter = &deleter; atDLMTensor->tensor.dl_tensor.data = src.data_ptr(); int64_t device_id = 0; if (src.type().is_cuda()) { device_id = src.get_device(); } atDLMTensor->tensor.dl_tensor.ctx = getDLContext(src.type(), device_id); atDLMTensor->tensor.dl_tensor.ndim = src.dim(); atDLMTensor->tensor.dl_tensor.dtype = getDLDataType(src.type()); atDLMTensor->tensor.dl_tensor.shape = const_cast<int64_t*>(src.sizes().data()); atDLMTensor->tensor.dl_tensor.strides = const_cast<int64_t*>(src.strides().data()); atDLMTensor->tensor.dl_tensor.byte_offset = 0; return &(atDLMTensor->tensor); }\\n\\nAs you can see, it’s a pretty simple conversion, casting the metadata from the PyTorch format to the DLPack format and assigning a pointer to the internal Tensor data representation.\\n\\nI really hope that more frameworks adopt this standard that will certainly give benefits to the ecosystem. It is also interesting to note that a potential integration with Apache Arrow would be amazing.\\n\\nThat’s it, I hope you liked this long post !\\n\\n– Christian S. Perone')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Faster R-CNN: Down the rabbit hole of modern object detection', 'Previously, we talked about object detection, what it is and how it has been recently tackled using deep learning. If you haven’t read our previous blog post, we suggest you take a look at it before continuing.\\n\\nLast year, we decided to get into Faster R-CNN, reading the original paper, and all the referenced papers (and so on and on) until we got a clear understanding of how it works and how to implement it.\\n\\nWe ended up implementing Faster R-CNN in Luminoth, a computer vision toolkit based on TensorFlow which makes it easy to train, monitor and use these types of models. So far, Luminoth has raised an incredible amount of interest and we even talked about it at both ODSC Europe and ODSC West.\\n\\nBased on all the work developing Luminoth and based on the presentations we did, we thought it would be a good idea to have a blog post with all the details and links we gathered in our research as a future reference for anyone is interested in the topic.\\n\\nBackground\\n\\nFaster R-CNN was originally published in NIPS 2015. After publication, it went through a couple of revisions which we’ll later discuss. As we mentioned in our previous blog post, Faster R-CNN is the third iteration of the R-CNN papers — which had Ross Girshick as author & co-author.\\n\\nEverything started with “Rich feature hierarchies for accurate object detection and semantic segmentation” (R-CNN) in 2014, which used an algorithm called Selective Search to propose possible regions of interest and a standard Convolutional Neural Network (CNN) to classify and adjust them. It quickly evolved into Fast R-CNN, published in early 2015, where a technique called Region of Interest Pooling allowed for sharing expensive computations and made the model much faster. Finally came Faster R-CNN, where the first fully differentiable model was proposed.\\n\\nArchitecture\\n\\nThe architecture of Faster R-CNN is complex because it has several moving parts. We’ll start with a high level overview, and then go over the details for each of the components.\\n\\nIt all starts with an image, from which we want to obtain:\\n\\na list of bounding boxes.\\n\\na label assigned to each bounding box.\\n\\na probability for each label and bounding box.\\n\\nComplete Faster R-CNN architecture\\n\\nThe input images are represented as \\\\mathit{Height} \\\\times \\\\mathit{Width} \\\\times \\\\mathit{Depth} tensors (multidimensional arrays), which are passed through a pre-trained CNN up until an intermediate layer, ending up with a convolutional feature map. We use this as a feature extractor for the next part.\\n\\nThis technique is very commonly used in the context of Transfer Learning, especially for training a classifier on a small dataset using the weights of a network trained on a bigger dataset. We’ll take a deeper look at this in the following sections.\\n\\nNext, we have what is called a Region Proposal Network (RPN, for short). Using the features that the CNN computed, it is used to find up to a predefined number of regions (bounding boxes), which may contain objects.\\n\\nProbably the hardest issue with using Deep Learning (DL) for object detection is generating a variable-length list of bounding boxes. When modeling deep neural networks, the last block is usually a fixed sized tensor output (except when using Recurrent Neural Networks, but that is for another post). For example, in image classification, the output is a (N,) shaped tensor, with N being the number of classes, where each scalar in location i contains the probability of that image being \\\\mathit{label}_i.\\n\\nThe variable-length problem is solved in the RPN by using anchors: fixed sized reference bounding boxes which are placed uniformly throughout the original image. Instead of having to detect where objects are, we model the problem into two parts. For every anchor, we ask:\\n\\nDoes this anchor contain a relevant object?\\n\\nHow would we adjust this anchor to better fit the relevant object?\\n\\nThis is probably getting confusing, but fear not, we’ll dive into this below.\\n\\nAfter having a list of possible relevant objects and their locations in the original image, it becomes a more straightforward problem to solve. Using the features extracted by the CNN and the bounding boxes with relevant objects, we apply Region of Interest (RoI) Pooling and extract those features which would correspond to the relevant objects into a new tensor.\\n\\nFinally, comes the R-CNN module, which uses that information to:\\n\\nClassify the content in the bounding box (or discard it, using “background” as a label).\\n\\nAdjust the bounding box coordinates (so it better fits the object).\\n\\nObviously, some major bits of information are missing, but that’s basically the general idea of how Faster R-CNN works. Next, we’ll go over the details on both the architecture and loss/training for each of the components.\\n\\nBase network\\n\\nAs we mentioned earlier, the first step is using a CNN pretrained for the task of classification (e.g. using ImageNet) and using the output of an intermediate layer. This may sound really simple for people with a deep learning background, but it’s important to understand how and why it works, as well as visualize what the intermediate layer output looks like.\\n\\nThere is no real consensus on which network architecture is best. The original Faster R-CNN used ZF and VGG pretrained on ImageNet but since then there have been lots of different networks with a varying number of weights. For example, MobileNet, a smaller and efficient network architecture optimized for speed, has approximately 3.3M parameters, while ResNet-152 (yes, 152 layers), once the state of the art in the ImageNet classification competition, has around 60M. Most recently, new architectures like DenseNet are both improving results while lowering the number of parameters.\\n\\nVGG\\n\\nBefore we talk about which is better or worse, let’s try to understand how it all works using the standard VGG-16 as an example.\\n\\nVGG architecture\\n\\nVGG, whose name comes from the team which used it in the ImageNet ILSVRC 2014 competition, was published in the paper “Very Deep Convolutional Networks for Large-Scale Image Recognition” by Karen Simonyan and Andrew Zisserman. By today’s standards it would not be considered very deep, but at the time it more than doubled the number of layers commonly used and kickstarted the “deeper → more capacity → better” wave (when training is possible).\\n\\nWhen using VGG for classification, the input is a 224 \\\\times 224 \\\\times 3 tensor (that means a 224x224 pixel RGB image). This has to remain fixed for classification because the final block of the network uses fully-connected (FC) layers (instead of convolutional), which require a fixed length input. This is usually done by flattening the output of the last convolutional layer, getting a rank 1 tensor, before using the FC layers.\\n\\nSince we are going to use the output of an intermediate convolutional layer, the size of the input is not our problem. At least, it is not the problem of this module since only convolutional layers are used. Let’s get a bit more into low-level details and define which convolutional layer we are going to use. The paper does not specify which layer to use; but in the official implementation you can see they use the output of conv5/conv5_1 layer.\\n\\nEach convolutional layer creates abstractions based on the previous information. The first layers usually learn edges, the second finds patterns in edges in order to activate for more complex shapes and so forth. Eventually we end up with a convolutional feature map which has spatial dimensions much smaller than the original image, but greater depth. The width and height of the feature map decrease because of the pooling applied between convolutional layers and the depth increases based on the number of filters the convolutional layer learns.\\n\\nImage to convolutional feature map\\n\\nIn its depth, the convolutional feature map has encoded all the information for the image while maintaining the location of the “things” it has encoded relative to the original image. For example, if there was a red square on the top left of the image and the convolutional layers activate for it, then the information for that red square would still be on the top left of the convolutional feature map.\\n\\nVGG vs ResNet\\n\\nNowadays, ResNet architectures have mostly replaced VGG as a base network for extracting features. Three of the co-authors of Faster R-CNN (Kaiming He, Shaoqing Ren and Jian Sun) were also co-authors of “Deep Residual Learning for Image Recognition”, the original paper describing ResNets.\\n\\nThe obvious advantage of ResNet over VGG is that it is bigger, hence it has more capacity to actually learn what is needed. This is true for the classification task and should be equally true in the case of object detection.\\n\\nAlso, ResNet makes it easy to train deep models with the use of residual connections and batch normalization, which was not invented when VGG was first released.\\n\\nAnchors\\n\\nNow that we are working with a processed image, we need to find proposals, ie. regions of interest for classification. We previously mentioned that anchors are a way to solve the variable length problem, but we skipped most of the explanation.\\n\\nOur objective is to find bounding boxes in the image. These have rectangular shape and can come in different sizes and aspect ratios. Imagine we were trying to solve the problem knowing beforehand that there are two objects on the image. The first idea that comes to mind is to train a network that returns 8 values: two x_{min}, y_{min}, x_{max}, y_{max} tuples defining a bounding box for each object. This approach has some fundamental problems. For example, images may have different sizes and aspect ratios, having a good model trained to predict raw coordinates can turn out to be very complicated (if not impossible). Another problem is invalid predictions: when predicting x_{min} and x_{max} we have to somehow enforce that x_{min} < x_{max}.\\n\\nIt turns out that there is a simpler approach to predicting bounding boxes by learning to predict offsets from reference boxes. We take a reference box x_{center}, y_{center}, width, height and learn to predict \\\\Delta_{x_{center}}, \\\\Delta_{y_{center}}, \\\\Delta_{width}, \\\\Delta_{height}, which are usually small values that tweak the reference box to better fit what we want.\\n\\nAnchors are fixed bounding boxes that are placed throughout the image with different sizes and ratios that are going to be used for reference when first predicting object locations.\\n\\nSince we are working with a convolutional feature map of size conv_{width} \\\\times conv_{height} \\\\times conv_{depth} , we create a set of anchors for each of the points in conv_{width} \\\\times conv_{height}. It’s important to understand that even though anchors are defined based on the convolutional feature map, the final anchors reference the original image.\\n\\nSince we only have convolutional and pooling layers, the dimensions of the feature map will be proportional to those of the original image. Mathematically, if the image was w \\\\times h, the feature map will end up w/r \\\\times h/r where r is called subsampling ratio. If we define one anchor per spatial position of the feature map, the final image will end up with a bunch of anchors separated by r pixels. In the case of VGG, r = 16.\\n\\nAnchor centers throught the original image\\n\\nIn order to choose the set of anchors we usually define a set of sizes (e.g. 64px, 128px, 256px) and a set of ratios between width and height of boxes (e.g. 0.5, 1, 1.5) and use all the possible combinations of sizes and ratios.\\n\\nLeft: Anchors, Center: Anchor for a single point, Right: All anchors\\n\\nRegion Proposal Network\\n\\nThe RPN takes the convolutional feature map and generates proposals over the image\\n\\nAs we mentioned before, the RPN takes all the reference boxes (anchors) and outputs a set of good proposals for objects. It does this by having two different outputs for each of the anchors.\\n\\nThe first one is the probability that an anchor is an object. An “objectness score”, if you will. Note that the RPN doesn’t care what class of object it is, only that it does in fact look like an object (and not background). We are going to use this objectness score to filter out the bad predictions for the second stage. The second output is the bounding box regression for adjusting the anchors to better fit the object it’s predicting.\\n\\nThe RPN is implemented efficiently in a fully convolutional way, using the convolutional feature map returned by the base network as an input. First, we use a convolutional layer with 512 channels and 3x3 kernel size and then we have two parallel convolutional layers using a 1x1 kernel, whose number of channels depends on the number of anchors per point.\\n\\nConvolutional implementation of an RPN architecture, where k is the number of anchors.\\n\\nFor the classification layer, we output two predictions per anchor: the score of it being background (not an object) and the score of it being foreground (an actual object).\\n\\nFor the regression, or bounding box adjustment layer, we output 4 predictions: the deltas \\\\Delta_{x_{center}}, \\\\Delta_{y_{center}}, \\\\Delta_{width}, \\\\Delta_{height} which we will apply to the anchors to get the final proposals.\\n\\nUsing the final proposal coordinates and their “objectness” score we then have a good set of proposals for objects.\\n\\nTraining, target and loss functions\\n\\nThe RPN does two different type of predictions: the binary classification and the bounding box regression adjustment.\\n\\nFor training, we take all the anchors and put them into two different categories. Those that overlap a ground-truth object with an Intersection over Union (IoU) bigger than 0.5 are considered “foreground” and those that don’t overlap any ground truth object or have less than 0.1 IoU with ground-truth objects are considered “background”.\\n\\nThen, we randomly sample those anchors to form a mini batch of size 256 — trying to maintain a balanced ratio between foreground and background anchors.\\n\\nThe RPN uses all the anchors selected for the mini batch to calculate the classification loss using binary cross entropy. Then, it uses only those minibatch anchors marked as foreground to calculate the regression loss. For calculating the targets for the regression, we use the foreground anchor and the closest ground truth object and calculate the correct \\\\Delta needed to transform the anchor into the object.\\n\\nInstead of using a simple L1 or L2 loss for the regression error, the paper suggests using Smooth L1 loss. Smooth L1 is basically L1, but when the L1 error is small enough, defined by a certain \\\\sigma, the error is considered almost correct and the loss diminishes at a faster rate.\\n\\nUsing dynamic batches can be challenging for a number of reasons. Even though we try to maintain a balanced ratio between anchors that are considered background and those that are considered foreground, that is not always possible. Depending on the ground truth objects in the image and the size and ratios of the anchors, it is possible to end up with zero foreground anchors. In those cases, we turn to using the anchors with the biggest IoU to the ground truth boxes. This is far from ideal, but practical in the sense that we always have foreground samples and targets to learn from.\\n\\nPost processing\\n\\nNon-maximum suppression Since anchors usually overlap, proposals end up also overlapping over the same object. To solve the issue of duplicate proposals we use a simple algorithmic approach called Non-Maximum Suppression (NMS). NMS takes the list of proposals sorted by score and iterateqs over the sorted list, discarding those proposals that have an IoU larger than some predefined threshold with a proposal that has a higher score.\\n\\nWhile this looks simple, it is very important to be cautious with the IoU threshold. Too low and you may end up missing proposals for objects; too high and you could end up with too many proposals for the same object. A value commonly used is 0.6.\\n\\nProposal selection After applying NMS, we keep the top N proposals sorted by score. In the paper N=2000 is used, but it is possible to lower that number to as little as 50 and still get quite good results.\\n\\nStandalone application\\n\\nThe RPN can be used by itself without needing the second stage model. In problems where there is only a single class of objects, the objectness probability can be used as the final class probability. This is because for this case, “foreground” = “single class” and “background” = “not single class”.\\n\\nSome examples of machine learning problems that can benefit from a standalone usage of the RPN are the popular (but still challenging) face detection and text detection.\\n\\nOne of the advantages of using only the RPN is the gain in speed both in training and prediction. Since the RPN is a very simple network which only uses convolutional layers, the prediction time can be faster than using the classification base network.\\n\\nRegion of Interest Pooling\\n\\nAfter the RPN step, we have a bunch of object proposals with no class assigned to them. Our next problem to solve is how to take these bounding boxes and classify them into our desired categories.\\n\\nThe simplest approach would be to take each proposal, crop it, and then pass it through the pre-trained base network. Then, we can use the extracted features as input for a vanilla image classifier. The main problem is that running the computations for all the 2000 proposals is really inefficient and slow.\\n\\nFaster R-CNN tries to solve, or at least mitigate, this problem by reusing the existing convolutional feature map. This is done by extracting fixed-sized feature maps for each proposal using region of interest pooling. Fixed size feature maps are needed for the R-CNN in order to classify them into a fixed number of classes.\\n\\nRegion of Interest Pooling\\n\\nA simpler method, which is widely used by object detection implementations, including Luminoth’s Faster R-CNN, is to crop the convolutional feature map using each proposal and then resize each crop to a fixed sized 14 \\\\times 14 \\\\times \\\\mathit{convdepth} using interpolation (usually bilinear). After cropping, max pooling with a 2x2 kernel is used to get a final 7 \\\\times 7 \\\\times \\\\mathit{convdepth} feature map for each proposal.\\n\\nThe reason for choosing those exact shapes is related to how it is used next by the next block (R-CNN). It is important to understand that those are customizable depending on the second stage use.\\n\\nRegion-based Convolutional Neural Network\\n\\nRegion-based convolutional neural network (R-CNN) is the final step in Faster R-CNN’s pipeline. After getting a convolutional feature map from the image, using it to get object proposals with the RPN and finally extracting features for each of those proposals (via RoI Pooling), we finally need to use these features for classification. R-CNN tries to mimic the final stages of classification CNNs where a fully-connected layer is used to output a score for each possible object class.\\n\\nR-CNN has two different goals:\\n\\nClassify proposals into one of the classes, plus a background class (for removing bad proposals). Better adjust the bounding box for the proposal according to the predicted class.\\n\\nIn the original Faster R-CNN paper, the R-CNN takes the feature map for each proposal, flattens it and uses two fully-connected layers of size 4096 with ReLU activation.\\n\\nThen, it uses two different fully-connected layers for each of the different objects:\\n\\nA fully-connected layer with N+1 units where N is the total number of classes and that extra one is for the background class.\\n\\nunits where is the total number of classes and that extra one is for the background class. A fully-connected layer with 4N units. We want to have a regression prediction, thus we need \\\\Delta_{center_{x}}, \\\\Delta_{center_{y}}, \\\\Delta_{width}, \\\\Delta_{height} for each of the N possible classes.\\n\\nR-CNN architecture\\n\\nTraining and targets\\n\\nTargets for R-CNN are calculated in almost the same way as the RPN targets, but taking into account the different possible classes. We take the proposals and the ground-truth boxes, and calculate the IoU between them.\\n\\nThose proposals that have a IoU greater than 0.5 with any ground truth box get assigned to that ground truth. Those that have between 0.1 and 0.5 get labeled as background. Contrary to what we did while assembling targets for the RPN, we ignore proposals without any intersection. This is because at this stage we are assuming that we have good proposals and we are more interested in solving the harder cases. Of course, all these values are hyperparameters that can be tuned to better fit the type of objects that you are trying to find.\\n\\nThe targets for the bounding box regression are calculated as the offset between the proposal and its corresponding ground-truth box, only for those proposals that have been assigned a class based on the IoU threshold.\\n\\nWe randomly sample a balanced mini batch of size 64 in which we have up to 25% foreground proposals (with class) and 75% background.\\n\\nFollowing the same path as we did for the RPNs losses, the classification loss is now a multiclass cross entropy loss, using all the selected proposals and the Smooth L1 loss for the 25% proposals that are matched to a ground truth box. We have to be careful when getting that loss since the output of the R-CNN fully connected network for bounding box regressions has one prediction for each of the classes. When calculating the loss, we only have to take into account the one for the correct class.\\n\\nPost processing\\n\\nSimilar to the RPN, we end up with a bunch of objects with classes assigned which need further processing before returning them.\\n\\nIn order to apply the bounding box adjustments we have to take into account which is the class with the highest probability for that proposal. We also have to ignore those proposals that have the background class as the one with the highest probability.\\n\\nAfter getting the final objects and ignoring those predicted as background, we apply class-based NMS. This is done by grouping the objects by class, sorting them by probability and then applying NMS to each independent group before joining them again.\\n\\nFor our final list of objects, we also can set a probability threshold and a limit on the number of objects for each class.\\n\\nTraining\\n\\nIn the original paper, Faster R-CNN was trained using a multi-step approach, training parts independently and merging the trained weights before a final full training approach. Since then, it has been found that doing end-to-end, joint training leads to better results.\\n\\nAfter putting the complete model together we end up with 4 different losses, two for the RPN and two for R-CNN. We have the trainable layers in RPN and R-CNN, and we also have the base network which we can train (fine-tune) or not.\\n\\nThe decision to train the base network depends on the nature of the objects we want to learn and the computing power available. If we want to detect objects that are similar to those that were on the original dataset on which the base network was trained on, then there is no real need except for trying to squeeze all the possible performance we can get. On the other hand, training the base network can be expensive both in time and on the necessary hardware, to be able to fit the complete gradients.\\n\\nThe four different losses are combined using a weighted sum. This is because we may want to give classification losses more weight relative to regression ones, or maybe give R-CNN losses more power over the RPNs’.\\n\\nApart from the regular losses, we also have the regularization losses which we skipped for the sake of brevity but can be defined both in RPN and in R-CNN. We use L2 regularization for some of the layers and depending on which base network being used and if it’s trained, it may also have regularization.\\n\\nWe train using Stochastic Gradient Descent with momentum, setting the momentum value to 0.9. You can easily train Faster R-CNN with any other optimizer without bumping into any big problem.\\n\\nThe learning rate starts at 0.001 and then decreases to 0.0001 after 50K steps. This is one of the hyperparameters that usually matters the most. When training with Luminoth, we usually start with the defaults and tune it from then on.\\n\\nEvaluation\\n\\nThe evaluation is done using the standard Mean Average Precision (mAP) at some specific IoU threshold (e.g. mAP@0.5). mAP is a metric that comes from information retrieval, and is commonly used for calculating the error in ranking problems and for evaluating object detection problems.\\n\\nWe won’t go into details since these type of metrics deserve a blogpost of their own, but the important takeway is that mAP penalizes you when you miss a box that you should have detected, as well as when you detect something that does not exist or detect the same thing multiple times.\\n\\nConclusion\\n\\nBy now, you should have a clear idea of how Faster R-CNN works, why some decisions have been made and some idea on how to be able to tweak it for your specific case. If you want to get a deeper understanding on how it works you should check Luminoth’s implementation.\\n\\nFaster R-CNN is one of the models that proved that it is possible to solve complex computer vision problems with the same principles that showed such amazing results at the start of this new deep learning revolution.\\n\\nNew models are currently being built, not only for object detection, but for semantic segmentation, 3D-object detection, and more, that are based on this original model. Some borrow the RPN, some borrow the R-CNN, others just build on top of both. This is why it is important to fully understand what is under the hood so we are better prepared to tackle future problems.')"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The top 14 AI and machine learning conferences to attend in 2018', 'The technology and business landscape for artificial intelligence and machine learning are evolving quickly, which means it\\'s important to stay up to date. These can\\'t-miss AI and machine learning events are genuinely worth your time.\\n\\nArtificial intelligence and machine learning have leapt off the pages of science fiction novels and burst into the real world. These technologies have game-changing implications for businesses of all sizes, whether you’re planning to implement them yourself or contemplate the consequences of their adoption on the world at large. With these fields moving so fast, it\\'s hard to stay on top of big changes, let alone smaller advances that can affect IT organizations and you personally.\\n\\nThat\\'s where AI and machine learning conferences come in. There\\'s no better way to advance your career, learn new AI and ML skills, make new human connections, and maybe some non-human ones as well. And because these fields are hotter than a gaming laptop with a busted fan, this the perfect time to ask your boss to invest in helping you learn more about both.\\n\\nHowever, not every conference is a good use of your corporate travel budget. To help you make the most of your time and money, we created a list of recommended AI and machine learning conferences scheduled for 2018.\\n\\nEach listing includes dates, locations, social media connections, agenda highlights, and pricing. The event websites provide more details, but this summary helps you create a travel short list of your own.\\n\\nIt’s early, though. In many cases, conference organizers haven’t announced their full schedules, with information “to be determined.” However, this overview should enable you to start planning your year.\\n\\nThe list is sorted by date, although some conferences are held multiple times a year in different locations, so there may be an event convenient to you.\\n\\nMust-attend AI and machine learning conferences\\n\\nIf you have the time and budget to attend only one conference in 2018, choose one from this section. These events are well established and run by respected organizations. They pack a lot of depth into the agenda: speakers, training, workshops, sessions, and lots of chances to meet and mingle.\\n\\nGlobal Artificial Intelligence Conference\\n\\nTwitter: #GAIC\\n\\nWeb: http://www.globalbigdataconference.com/santa-clara/global-artificial-intelligence-conference/event-97.html\\n\\nDate: January 17-19, 2018\\n\\nLocation: Santa Clara, CA\\n\\nCost: $699 to $1,499, depending on the number of days you attend.\\n\\nWith a broad program spanning AI topics in several industry verticals, the Global Artificial Intelligence Conference is a jam-packed three-day event.\\n\\nThe tracks include technology (e.g., cognitive computing, chatbots, NLP, computer vision, and neural networks) and industry topics (e.g., telecom, finance, and travel/transportation). A sample of the presentations include, \"IOT And Machine Learning: Transforming the Security Space,\" \"Business Transformations thorough IoT and AI,\" and \"Predicting Alzheimer’s Disease With Machine Learning.\"\\n\\nIn addition, workshop tracks let you dive in deep, with topics like \"Enhancing NLP with Deep Neural Networks,\" \"Frameworks Galore: Which One to Pick,\" and \"Keras.\"\\n\\nWho should attend: Business executives responsible for AI initiatives, heads of innovation, heads of product development, analysts, project managers, analytics managers, data scientist, statisticians, AI and software developers, AI consultants and service providers, students, and data analysts.\\n\\nDeep Learning Cookbook: A new set of tools to guide the selection of the best hardware and software environment for different deep learning tasks. Learn more\\n\\nREWORK Deep Learning Summit\\n\\nREWORK\\'s Deep Learning Summits are two-day conferences held throughout 2018. Each event promises to share advances in deep learning algorithms and methods, highlight emerging trends, and connect industry innovators, technologists, data scientists, and startup founders in the deep learning world.\\n\\nThe conference schedule includes speakers from Google, Netflix, Adobe, and Ancestry.com, with presentation topics including \"Deep Learning in Production at Facebook,\" \"Computer Vision Algorithms for Camera Calibration and Object Tracking,\" \"Deep Learning for Recommender Systems,\" and \"End-to-End Deep Learning for Detection, Prevention, and Classification of Cyber Attacks.\"\\n\\nLonger workshops go into depth on topics such as \"System Infrastructures for AI and ML Software,\" \"Creating an AI Strategy for Your Workplace,\" and \"An Introduction to AI for Enterprise.\"\\n\\nOne attendee found the Deep Learning Summit to be \"an interesting mix of both industry and academic. REWORK did more than enough to prove their professionalism and attention to detail, and this is without mentioning the calibre of speakers they secured for the event.\"\\n\\nWho should attend: Data scientists, data engineers, machine learning scientists, CTOs, founders, director of engineering, CEOs.\\n\\nO’Reilly Artificial Intelligence Conference\\n\\nTwitter: @TheAIConf / #TheAIConf\\n\\nWeb: https://conferences.oreilly.com/artificial-intelligence\\n\\nDates and Locations: April 10-13, 2018 (Beijing), April 29 – May 2, 2018 (New York), September 4-7, 2018 (San Francisco), October 8-11, 2018 (London)\\n\\nCost: Not yet announced\\n\\nO\\'Reilly\\'s Artificial Intelligence Conferences are a series of standalone events in different locations throughout the year. Their overarching theme is \"Put AI to work,\" and with both technical and non-technical presentations, the conferences focus on bridging the gap between theory and practical applications. Keynote speakers hail from academic institutions such as MIT and Carnegie Mellon University and include business leaders from companies like Google and Salesforce.\\n\\nFor example, at a 2017 conference, Andrew Ng, co-founder and co-chairman of Coursera, advised engineers to read academic papers and implement those approaches to replicate the results. \"That will not only lead to a deeper understanding of the advanced concepts,\" he insisted, \"but also help you in coming up with new ideas.\"\\n\\nEach conference kicks off with two-day training sessions including \"Natural language processing with deep learning\" and \"Deep learning with TensorFlow.\" The preview day has tutorials on topics such as \"Here and now: Bringing AI into the enterprise,\" \"Deep reinforcement learning tutorial,\" and “Introduction to neural networks with Keras.\"\\n\\nThe two main conference days are organized by track. For example, the 2017 New York schedule was divided into the following topics:\\n\\nImpact of AI on business and society: \"Will we automate jobs faster than we create them?\" \"Planning for the social impact of AI,\" and \"AI\\'s legal history and some notions of the future.\"\\n\\nImplementing AI: \"Strategies for integrating people and machine learning in online systems,\" \"Tackling the limits of deep learning,\" and \"Deep learning applied to consumer transactions with Think Big Analytics.\"\\n\\nInteracting with AI: \"Conversational AI at large scale,\" \"Bigger than bots: Machine reading and writing in enterprise,\" and \"Deep shopping bots: Building machines that think and sell like humans.\"\\n\\nVerticals and applications: \"AI for manufacturing: Today and tomorrow,\" \"Cognitive mobile healthcare for the patient and physician,\" and \"Transforming an investment firm with AI: A case study.\"\\n\\nDiversity is strongly encouraged, and the extra effort seems to be making a difference: \"Awesome how many female AI rockstars presented at #TheAIConf. AI is not taking over humans; women are taking over AI. ;-)\"\\n\\nWho should attend: Algorithm engineers/scientists, chief experience officers, data scientists/engineers, research scientists, software engineers, business analysts, those in charge of innovation initiatives, product marketing managers, product managers, and program managers.\\n\\nConference on Neural Information Processing Systems\\n\\nAI is not a new field. The 2018 Conference on Neural Information Processing Systems (NIPS) is the 32nd event. Nearly 8,000 people attended the 2017 conference in Long Beach, California. What makes it special? The 2017 agenda should help you decide if this is the right event for you:\\n\\nInvited talks: \"Why AI Will Make It Possible to Reprogram the Human Genome,\" \"The Trouble with Bias,\" and \"Deep Learning for Robotics.\"\\n\\nOral presentations of academic papers: \"A graph-theoretic approach to multitasking,\" \"Toward Goal-Driven Neural Network Models for the Rodent Whisker-Trigeminal System,\" and \"Quantifying How Much Sensory Information in a Neural Code Is Relevant for Behavior.\"\\n\\nDemos: \"A Deep Reinforcement Learning Chatbot,\" \"Babble Labble: Learning from Natural Language Explanations,\" and \"Sensomind: Democratizing Deep Learning for the Food Industry.\"\\n\\nWorkshops: \"Visually grounded interaction and language,\" \"Machine Learning for Audio Signal Processing (ML4Audio),\" and \"Conversational AI: Today\\'s Practice and Tomorrow\\'s Potential.\"\\n\\nAnd there\\'s a competition track where bots square off against each other for a prize.\\n\\nIf you’re looking for a job change, NIPS is the place to be. NIPS 2017 was called \"a recruiting frenzy more akin to the National Football League’s draft day.\" Go get \\'em, tiger.\\n\\nIf you’re interested, don’t dawdle; each year, the event sells out.\\n\\nWho should attend: As the agenda suggests, this is techie stuff suitable for researchers, data scientists, and engineers.\\n\\nThe Machine Learning Conference\\n\\nTwitter: @mlfconf / #MLconfSF / #MLconfATL / #MLconfSEA / @MLconfNYC\\n\\nWeb: https://mlconf.com\\n\\nDates and Locations: TDB (New York), TDB (Seattle), TDB (Atlanta), TDB (San Francisco)\\n\\nCost: TBD\\n\\nThe Machine Learning Conference is a series of multicity events in multiple cities. Each is a single-day event that aims to disseminate recent machine learning research and industry applications.\\n\\nThe event strives to be agnostic, and past programs suggest that it achieves this goal. From \"Large-Scale Machine Learning: Deep, Distributed and Multi-Dimensional\" to \"Lessons Learnt from Building ML Products for Enterprise SaaS,\" and \"Can Machine Learning Save the Whales?\" it’s evident that there’s plenty of technical material for machine learning practitioners. The September 2017 San Francisco conference speakers included Ted Willke, an Intel senior principal engineer; Josh Wills, Slack’s head of data engineering; Franziska Bell, a data science manager on the platform team at Uber; and Rushin Shah, an engineering leader at Facebook.\\n\\nThis conference, too, gets a thumbs-up for diversity. Worried about the gender gap in data science? Rafael Carrascosa says that women gave the best two talks at MLconf San Francisco 2017.\\n\\nWho should attend: Students and practitioners who want tips and methodologies to apply in their own work, as well as cited papers and code samples to reference for research.\\n\\nThe AI Conference\\n\\nTwitter: @AIconference\\n\\nWeb: http://aiconference.com/events/2017-san-francisco\\n\\nDate: TBD\\n\\nLocation: TDB\\n\\nCost: TBD\\n\\nThe AI Conference is organized by the same folks who run the Machine Learning conference, with a different structure. This single-day event about emerging technologies in AI puts a focus on AI projects. In addition to deeply technical presentations on AI, related topics include law, ethics, safety, and governance.\\n\\nBoth presentations and speakers are top notch. At 2017\\'s event in June, these included \"Conversational AI in Amazon Alexa,\" presented by Ashwin Ram, senior manager of AI science for Alexa; \"How Might Artificial Intelligence Come About? Different Approaches and Their Implications for Life in the Universe,\" presented by science fiction author David Brin (you probably know him best from \"The Postman\"); and \"Emotional Trauma and Machine Learning,\" by Caroline Sinders, an online harassment researcher at Wikimedia.\\n\\nWho should attend: Some AI conferences are extremely technical. Those looking for more general insights about the future of AI may want to consider this one.\\n\\nWorth attending\\n\\nThese events focus on a thin slice of AI and machine learning, either a sub-discipline or industry vertical, and might not be for all. However, if their niche matches with yours, these conferences can be a valuable part of your 2018 plans.\\n\\nREWORK AI Assistant Summit\\n\\nTwitter: @reworkAI / #reworkAI\\n\\nDates and Locations: January 25-26, 2018 (San Francisco), March 15-16, 2018 (London)\\n\\nCost: $1,495 to $2,095 (San Francisco); £795 to £1,495 (London). Discounts for early birds, students, full-time academics, and startups.\\n\\nIf you\\'re into natural language processing, then the REWORK AI Assistant Summit is not to be missed. The event focuses on using machine and deep learning to create AI assistants as well as conversational interfaces.\\n\\nAmong the sessions are \"NLP, Parsing, Information Extraction, Dialog and Question Answering,\" \"Embodied Socially Assistive Agents: Going Beyond Assistance and Towards Behavior Change,\" and \"The Role of Personality and Emotion in Spoken Interactions.\" Presenters hail from innovators at Apple, Autodesk, and x.ai.\\n\\nWho should attend: Data scientists, data engineers, developers, entrepreneurs, CTOs, CEOs, investors.\\n\\nInternational Conference on Learning Representations\\n\\nTwitter: @iclr2017\\n\\nTwitter: @iclr2017 Web: www.iclr.cc/doku.php\\n\\nWeb: www.iclr.cc/doku.php Date: April 30 – May 3, 2018\\n\\nDate: April 30 – May 3, 2018 Location: Vancouver, BC, Canada\\n\\nLocation: Vancouver, BC, Canada Cost: TBD (last year’s registration was €900)\\n\\nAmong topics are feature learning, metric learning, compositional modeling, structured prediction, reinforcement learning, and issues regarding large-scale learning and non-convex optimization. If conference sessions with titles like “Amortised MAP Inference for Image Super-resolution” make you say, “Oh, cool!” this conference is likely worth your time.\\n\\nWho should attend: This is a deeply technical conference that should appeal to practitioners working on moving the field forward in unsupervised, semi-supervised, and supervised representation learning; representation learning for planning and reinforcement learning; and metric learning and kernel learning.\\n\\nREWORK Machine Intelligence Summit\\n\\nTwitter: @reworkMI / #reworkMI\\n\\nWeb: https://www.re-work.co/events/machine-intelligence-summit-hong-kong-2018\\n\\nDate: June 6-7, 2018\\n\\nLocation: Hong Kong\\n\\nCost: HK$7,995 to HK$10,995, with early-bird pricing and discounts for students, full-time academics, and startups.\\n\\nWill smart artificial intelligence finally rival human intelligence? The REWORK Machine Intelligence Summit explores trends in the development of intelligent machines to make sense of data and ML’s impact on business and society.\\n\\nThe draft agenda lists a wide range of topics, including \"Optimising Convolutional Neural Networks for Embedded Platforms,\" \"Computer Vision for Quality Control and Inspection,\" and \"Predicting Human Behaviour Through Mobile Sensing.\" Speakers represent Tesla, HSBC, NASA Ames Research Center, and other innovators in the field.\\n\\nWho should attend: Data scientists, data engineers, machine learning scientists, developers, entrepreneurs, directors of engineering, big data experts.\\n\\nConference on Computer Vision and Pattern Recognition\\n\\nTwitter: @CVPR / #CVPR2018\\n\\nWeb: http://cvpr2018.thecvf.com\\n\\nDate: June 18-22, 2018\\n\\nLocation: Salt Lake City\\n\\nCost: TBD\\n\\nThe eponymously focused Conference on Computer Vision and Pattern Recognition event is comprised of a main conference, during which papers and keynote speeches are presented, as well as workshops and short courses.\\n\\nWhile the 2018 schedule has not yet been shared (and the call for papers has not been announced), the 2017 conference event had workshops, tutorials, and presentations of academic papers. The oral and spotlight sessions were about 3D vision, machine learning, low- and mid-level vision, analyzing humans in images, biomedical image/video analysis, and object recognition and scene understanding. Sample papers presented were \"Emotion Recognition in Context,\" \"Anti-Glare: Tightly Constrained Optimization for Eyeglass Reflection Removal,\" \"Generalized Rank Pooling for Activity Recognition,\" and \"Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs.\"\\n\\nTutorials included full- and half-day sessions on topics such as \"DIY: A Multiview Camera System: Panoptic Studio Teardown,\" and \"Mathematics of Deep Learning.\" Workshops included \"Biometrics,\" \"Women in Computer Vision,\" and \"Traffic Surveillance Workshop and Challenge.\" Finally, there\\'s a vendor exhibit hall as well as recruiting booths.\\n\\nWho should attend: Students, academics, and industry researchers in the field of machine vision.\\n\\nREWORK AI in Industrial Automation Summit\\n\\nTwitter: @reworkauto /#reworkAUTO\\n\\nWeb: https://www.re-work.co/events/ai-industrial-automation-summit-san-francisco-2018\\n\\nDate: June 28-29, 2018\\n\\nLocation: San Francisco\\n\\nCost: $1,495 to $1,895. Early-bird pricing available, with discounts for students, full-time academics, and startups.\\n\\nIf you work in manufacturing, REWORK\\'s AI in Industrial Automation Summit deserves a spot on your 2018 schedule. The event focuses on the impact of machine learning on industrial automation. The draft 2018 suggests presentations such as \"Time Series Data & Data for Predictive Maintenance,\" \"Using AI with Sensor and Signal Data,\" and \"Deep Learning and Applications in Industrial Automation.\" Planned speakers come from Amazon, Procter & Gamble, Google, and other driving forces.\\n\\nWho should attend: This is for techies, including lead software engineers, chief data scientists, CTOs, founders, directors of engineering, CEOs, and system engineers.\\n\\nAI World Conference and Expo\\n\\nTwitter: @AIWorldExpo\\n\\nWeb: https://aiworld.com\\n\\nDate: TBD; the last one was in December 2017\\n\\nLocation: TDB\\n\\nCost: TBD\\n\\nIt’s too early to say much about the 2018 AI World Expo, since the 2017 event just concluded. But a glance at the 2017 agenda makes it an intriguing conference to consider, with 150 speakers addressing data science techniques, use cases for AI and high-performance computing, and case studies such as banks\\' adoption of intelligent assistants and bots to handle customer service inquiries.\\n\\nWho should attend: Business and technology executives responsible for AI initiatives.\\n\\nOther conferences of note\\n\\nThis final section lists smaller or newer conferences that may be the best conferences you never heard of.\\n\\nAAAI Conference on Artificial Intelligence\\n\\nTwitter: @RealAAAI / #AAAI2018\\n\\nWeb: https://aaai.org/Conferences/AAAI-18\\n\\nDate: February 2-7, 2018\\n\\nLocation: New Orleans\\n\\nCost: $925, with early-bird pricing and discounts for AAAI members and students.\\n\\nPut on by the Association for the Advancement of Artificial Intelligence, the AAAI Conference on Artificial Intelligence aims to promote AI research and a scientific exchange among attendees. This event is paper-focused presentations. Some of the papers accepted and presented in 2017 included \"AI for Complex Situations: Beyond Uniform Problem Solving,\" \"Enabling Autonomous Space Mission Operations with Artificial Intelligence,\" and \"Fast and Personal: Scaling Deep Learning with MxNet.\"\\n\\nThe program also includes tutorials and workshops as well as student programs and a job fair.\\n\\nWho should attend: AI researchers, practitioners, scientists, engineers, and academics.\\n\\nIntelligent Systems Conference\\n\\nTwitter: #IntelliSys2018\\n\\nWeb: http://saiconference.com/IntelliSys\\n\\nDate: September 6-7, 2018\\n\\nLocation: London\\n\\nCost: £500, with student discounts.\\n\\nThe Intelligent Systems Conference 2018 is an academic and research-based event with keynote speakers representing industry leaders. Last year\\'s conference saw papers such as \"Towards Stream-based Reasoning and Machine Learning for IoT Applications,\" \"A Modified Penalty Function in Fuzzy Clustering Algorithm,\" and \"Intelligent Search with Deep Learning Clusters.\"\\n\\nWho should attend: Researchers and industry practitioners.\\n\\nDid we miss any AI and machine learning conferences? Tweet us about additions at @enterprisenxt.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('AI Will Not Be Taking Away Code Jobs Anytime Soon', 'AI Will Not Be Taking Away Code Jobs Anytime Soon\\n\\nNicole Hemsoth\\n\\nThere has been much recent talk about the near future of code writing itself with the help of trained neural networks but outside of some limited use cases, that reality is still quite some time away—at least for ordinary development efforts.\\n\\nAlthough auto-code generation is not a new concept, it has been getting fresh attention due to better capabilities and ease of use in neural network frameworks. But just as in other areas where AI is touted as being the near-term automation savior, the hype does not match the technological complexity need to make it reality. Well, at least not yet.\\n\\nJust in the last few weeks Google, Microsoft and IBM have announced new ways of boosting developer productivity with deep learning frameworks that fill themselves in—at least in part. The headlines exclaim that code is writing itself; that programmers will no longer be necessary. In reality, however, what all of these auto-generation code efforts share in common, aside from the developer productivity angle, is that the use cases are still limited. The amount of code may required may not be ample enough, the neural network may still require a great deal of expertise in how to construct new layers, or the data to inform auto-network creation is scattered across too formats.\\n\\nMicrosoft Research raised red flags about developer automation recently with its announcement of DeepCoder, a neural network that learns to predict properties of a program by inputs and outputs derived from a broad range of sources of code. This led to reported faster code generation and higher levels of difficulty according to various programming competition problems. Another effort in the same vein from IBM Research scanned thousands of peer reviewed papers for code, framework, and library details to help developers bootstrap neural network model generation.\\n\\nIt is difficult for DeepCoder to generate vast quantities of code at a time and for the IBM Research effort, there are limitations due to the data input (scattered among papers, fragmented data, etc.). AutoML works only with defined frameworks since standardization is key to generalizing anything, particularly a custom neural network.\\n\\nWhile Rania Khalaf, director of AI engineering at IBM research says a neural network blazing through code repositories and gathering enough on the architecture side to build its own models is possible, but there is far more to it than it may appear. This is because for the IBM effort at least, there are no standard frameworks or libraries that the developer community uses in enough mass to have auto-code generating power.\\n\\nKhalaf says we may be closer to getting AI to scan through a code repository like GitHub and make something meaningful out of it, but “close” is relative. “It is easy to think this can be easily automated but starting from a developer and data scientist standpoint, to get that fully automated, there has to be a significant ‘human in the loop’ factor.” Even getting to the point of broadly generalizing across different platforms, programming models, and libraries is far more difficult than it seems as the deep learning paper efforts from IBM Research highlight.\\n\\nSenthil Mani, one of the researchers on the DLPaper2Code from IBM Research tells The Next Platform that interoperability between deep learning approaches represents one of the greatest problems. His team is working on a “grammar” to solve this problem—an abstract representation that can be platform agnostic. “If a standard can be picked up in the community it would be possible to generate more code automatically,” he adds.\\n\\n“The larger goal is to democratize deep learning for the larger developer population. The set of developers trained well in languages like Java, for instance, have to now ump into building AI applications and there is a large skill gap in picking up these libraries and frameworks,” Mani explains. We have started building a tool where people can come in and start designing a deep learning model based on a composite of some networks, layers, and hyperparameters. The implementation and coding patterns will be different as well as configurations but the goal is to take an idea and ‘drag and drop’ layers to start constructing a model.”\\n\\nAside from that standardization, Mani says another area where they have made progress toward greater auto-code generation is in prediction of layers for neural networks, something that will add to overall productivity yet still relies on that human-in-the-loop aspect described above.\\n\\nStill, Mani agrees, we are quite a long way from the future of automated programmers that some of the recent mainstream auto-code news has projected. “What we are looking at now is more along the lines of reducing time and effort for developers to bootstrap their models. Even that problem alone is hard enough—and we have only scratched the surface.”')\n",
      "('Transfer Learning Will Radically Change Machine Learning for Engineers', 'Transfer Learning Will Radically Change Machine Learning for Engineers\\n\\nThe Problem\\n\\nIn traditional supervised machine learning, we teach a model to become more successful and efficient at a task, by providing it with example data. Generally, once the model begins to perform well on the training data for the domain or problem it is tasked with, we expect a reasonable performance for new data. But, if you think about it, there are a few issues with this traditional supervised learning process\\n\\nAs engineers, we’re forced to construct specific models that only excel at a specific problem. This costs us valuable engineering time to create, train and tune models from scratch for every new problem we want to tackle, even if it’s a problem that has been solved in industry.\\n\\nFrom a product perspective, this is incredibly detrimental to progress, and could really hamper feature releases and engineering productivity. Transfer Learning offers an interesting solution to this problem.\\n\\nTransfer learning as a paradigm can solve this problem, by allowing us to leverage existing knowledge and data from a certain related domain to the new one we’re trying to train for. In 2016, Andrew Ng posited that Transfer Learning would be essential to commercial and industry success.\\n\\nApplications of Transfer Learning\\n\\nMaking use of pre-trained models and related domain data promises to supercharge most general development for machine learning. By tapping into a pre-trained model for a related purpose to its original design, your team can leapfrog the data cleaning, setup and training required to bring a model up to par for the task.\\n\\nTwo common areas where Transfer Learning has already been demonstrated to great success are Images and Text .\\n\\nTransfer Learning has been particularly effective with Image Data, and it is common to leverage a deep learning model trained on some large image data set, like ImageNet. These pretrained models can be directly included in other new models that expect some form of Image Input.\\n\\nMask R-CNN: a ML model from Facebook built on top of a pre-trained ResNet, another image detection model.\\n\\nWith Textual data, words are mapped to vectors where different words with a similar meaning have a similar vector representation. Pretrained models exist to learn these representations, and are widely available. These can then be incorporated into deep learning language models, both at the input or output stage.')\n",
      "('Microsoft Professional Program for Artificial Intelligence', '')\n",
      "('Humble Bundle Books: Artificial Intelligence', 'Get Kerbal Space Program, Dead Rising 4, and RUINER when you subscribe for just $12!\\n\\nSubscribe for $12')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Meet the Chinese Finance Giant That’s Secretly an AI Company', \"Ant Financial announces a developer initiative in 2016.\\n\\nIf you get into a car accident in China in the near future, you'll be able to pull out your smartphone, take a photo, and file an insurance claim with an AI system.\\n\\nThat system, from Ant Financial, will automatically decide how serious the ding was and process the claim accordingly with an insurer. It shows how the company—which already operates a hugely successful smartphone payments business in China—aims to upend many areas of personal finance using machine learning and AI.\\n\\nThe e-commerce giant Alibaba created Ant in 2014 to operate Alipay, a ubiquitous mobile payments service in China. If you have visited the country in recent years, then you have probably seen people paying for meals, taxi rides, and a whole lot more by scanning a code with the Alipay app. The system is far more popular than the wireless payments systems offered in the U.S. by Apple, Google, and others. The company boasts more than 450 million active users compared to about 12 million for Apple Pay.\\n\\nAnt’s progress will be significant to the future of the financial industry beyond China, including in the U.S., where the company is expanding its interests. The company’s approach goes around existing institutions to target individuals and small businesses who lack access to conventional financial services. Ant said in April of this year that it is buying the U.S. money-transfer service MoneyGram for $880 million. The deal is subject to regulatory approval and should close in the second half of this year. The company could well apply the technologies it is developing to its overseas subsidiaries. A spokesperson for the company says it hasn’t brought Alipay to the U.S. because existing financial systems provide less of an opportunity.\\n\\nYuan (Alan) Qi, a vice president and chief data scientist at Ant, says the company’s AI research is shaping its growth. “AI is being used in almost every corner of Ant’s business,” he says. “We use it to optimize the business, and to generate new products.”\\n\\nThe accident-processing system is a good example of how advances in AI can flip an existing system on its head, Qi says. It has become possible to automate this kind of image processing in recent years using a machine-learning technology known as deep learning. By feeding thousands of example images into a very large neural network, it is possible to train it to recognize things that even a human may struggle to spot (see “10 Breakthrough Technologies 2013: Deep Learning”).\\n\\nSign up for Weekend Reads Stories from our archives that put technology in perspective Sign Up Thank you — please check your email to complete your sign up. Incorrect email format Manage your newsletter preferences\\n\\n“We use computer vision for a job that is boring but also difficult,” Qi says. “I looked at the images myself, and I found it pretty difficult to tell the damage level.”\\n\\nQi speaks a mile a minute, which seems appropriate given how quickly his company seems to be moving. Dressed in a smart shirt and dress pants on a sweltering afternoon in Beijing this May, shortly after giving a speech at a major AI conference, Qi explained that the company considers itself not a “fintech” business but a “techfin” one, due to the importance of technology.\\n\\nAnt already operates a range of other financial services besides Alipay. For instance, it provides small loans to those without a bank account. It assesses a person’s creditworthiness based on his or her spending history and other data including friends' credit scores (see “Alipay Leads a Financial Revolution in China”).\\n\\nAnt’s creditworthiness system also provides a high-tech way to obtain various services, such as hotel bookings, without a deposit. Qi says that Ant uses advanced machine-learning algorithms and custom programmable chips to crunch huge quantities of user data in a few seconds, to determine whether to grant a customer a loan, for instance.\\n\\nA recent hire offers some measure of Ant’s intent to apply artificial intelligence to finance. This May the company announced that Michael Jordan, a professor at the University of Berkeley and a major figure in the field of machine learning and statistics, would become chair of the company’s scientific board.\\n\\nQi is no slouch, either. He got his PhD from MIT and became a professor in the computer science department at Purdue before joining Alibaba in 2014. Once there, he developed Alibaba’s first voice-recognition system for automating customer calls.\\n\\n“We built a system, based on deep learning, to carry on conversations; to provide answers to your questions,” Qi says. This chatbot system also taps into a knowledge base of information created by Ant, and is an example of how researchers are increasingly combining cutting-edge machine-learning techniques with conventional representations of knowledge. “Human language is still very hard for a machine to understand,” Qi says.\\n\\nIn March this year, the chatbot system surpassed human performance in terms of customer satisfaction, says Qi. “There are many, many chatbot companies in Silicon Valley. We are the only one that can say, confidently, they do better than human beings,” he says.\\n\\nAnt’s success to date has certainly been impressive. Credit Suisse estimates that it manages 58 percent of mobile payments in China. A key competitor has emerged in recent years with WeixinPay, from the mobile chat giant Tencent, now accounting for almost 40 percent of the market. Ant remains enormously valuable, though. Earlier this year, a Hong Kong investment group valued the company at $75 billion. The company was expected make an initial public offering this year, but that now looks more likely to happen in 2018.\\n\\nAnt is also increasingly looking to expand its interests overseas. The company has invested almost $1 billion in Paytm, an Indian payments company. It has also invested in Ascend, a Thai online payments business, and M-Daq, a Singaporean financial business. Ant apparently also sees investments and acquisitions as a way to bolster its technological prowess. Last year the company acquired EyeVerify, a U.S. company that makes eye recognition software.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Protein Linguistics', 'For over a decade now I have been working, essentially off the grid, on protein folding. I started thinking about the problem during my undergraduate years and actively working on it from the very beginning of grad school. For about four years, during the late 2000s, I pursued a radically different approach (to what was current then and now) based on ideas from Bayesian nonparametrics. Despite spending a significant fraction of my Ph.D. time on the problem, I made no publishable progress, and ultimately abandoned the approach. When deep learning began to make noise in the machine learning community around 2010, I started thinking about reformulating the core hypothesis underlying my Bayesian nonparametrics approach in a manner that can be cast as end-to-end differentiable, to utilize the emerging machinery of deep learning. Today I am finally ready to start talking about this long journey, beginning with a preprint that went live on bioRxiv yesterday.\\n\\nEnd-to-end differentiable learning of protein structure\\n\\nWhat ultimately doomed the Bayesian nonparametrics approach was computational cost—I spent millions of compute hours on the problem without making any headway in getting the models to converge. I thought (and still do) that that formulation captured something fundamental about protein folding, but the mathematical tools for sampling and variational inference were and are not yet mature enough to make it work. Deep learning presented the appealing possibility of casting protein folding as an optimization problem that can be optimized end-to-end using gradient descent. I knew very little about neural networks back then but started studying them seriously in 2012. During the period spanning Sept. 30th, 2014 to Feb. 18th, 2015, I settled on the basic formulation that I would eventually call Recurrent Geometric Networks or RGNs (I track things). It took another three years from conception to realization—deep learning frameworks were nowhere near as mature then as they are now, causing me to start over with a new codebase twice, and RGNs can be quite challenging to train. But, I think they are now finally ready to be released into the wild! For the technical details, go read the preprint. In this blogpost, I’d like to describe the thinking process that led up to them.\\n\\nFrom when I first learned about protein folding, and the approaches taken to predict protein structure, I thought it may be possible to predict proteins without conformational sampling and energy minimization, the two pillars of protein structure prediction. The reasons for this have come to underlie what I call the linguistic hypothesis. The basic idea is as follows: there is evidence that today’s proteins emerged out of an ancient peptidic soup, one that may have left its mark on the evolutionary record. I.e., the proteins we see today may in some sense be formed out of primordial peptides. As proteins grew in size and complexity, it would have been advantageous to reuse existing components, to build bigger proteins from existing protein parts. We already know this is true on the level of protein domains, in that larger proteins are often comprised from chaining together smaller globular domains. But the phenomenon of reuse may go further, where even smaller protein fragments (handful of residues to dozens) may reflect an underlying evolutionary pressure to reuse working parts, fragments that fold in tried-and-tested ways (from the perspective of evolution.) If this is the case, then the space of naturally occurring proteins may occupy a very special “manifold”, one that exhibits a hierarchical organization spanning small fragments to entire domains. Other evolutionary pressures could further drive the reuse phenomenon. For example, once a protein-protein or protein-DNA interface is established, presumably through some sort of structural motif, reusing that motif would present an efficient way for the cell to rewire its cellular circuitry. The end result of all this would be the emergence of something resembling a linguistic structure, a grammar that defines the reusable parts and how these parts can be combined to form larger assemblies. Given that this is biology, it’s unlikely to be rigid or minimal. It would be messy and hacky, with many exceptions and ad hoc evolutionary optimizations. But the manifold would be there, potentially discoverable and learnable.\\n\\nWhat to me is most exciting about this idea is the emergence of a layer of biological phenomenon that is rooted in physics (obviously), but that can be described independently of it. I.e. the primitives of this phenomenon would be sufficiently abstracted away from the underlying physics that we don’t need physics to model it. We can just operate on the level of protein fragments and motifs, building a probabilistic grammar that describes how these parts combine and interact, without ever resorting to a brute force physics-based simulation. To some this may seem unprincipled. To me, this possibility is more exciting than being able to brute-force simulate protein folding, as such simulations would ultimately only be exercises in doing physics really well and at scale. But the possibility of the existence of a description of protein structure space that can be formulated without resorting to the underlying physics, is the possibility of a theoretical science of proteins, independent of (but ultimately rooted in) physical theory.\\n\\nMy original Bayesian nonparametrics approach very explicitly codified this intuition. That is the advantage (to me) of Bayesian nonparametrics. They are a great way to encode one’s prior about the generative process underlying the phenomenon of interest. In some ways, designing neural network architectures goes in the exact opposite direction. Instead of capturing our prior about the underlying generative process, deep learning works by capturing our prior about the inverse learning process. I.e. the architecture encodes how the phenomenon can best be learned from data. Consider computer vision: images are not actually generated by convolutions, by having copies of cats repeated across our visual field! But, given a natural image, we know that a good prior for learning patterns in images is the imposition of translational invariance. It’s a statement about what makes a good learning process, about regularities in the phenomenon of interest that can be exploited by our learning process.\\n\\nRecurrent geometric networks try to do this for the linguistic hypothesis I articulated above. There’s nothing in them about ancient peptides or reuse. But they are structured in a way to discover patterns in protein sequences, and to discover hierarchies of such patterns. Perhaps more importantly, they are set up so that the link between sequence and structure is direct and immediate. The signal for whether the learned representation is useful for protein structure prediction comes directly from the predicted structures themselves, because they are explicitly compared to real structures and the deviation between the two is backpropagated to the learned weights of the representation. The hard or what one might call the clever part of RGNs is making this happen, the coupling between the representation and the final output. Ironically, I think the idea itself is very simple and straightforward. I had it over three years ago and it always struck me as very obvious. The real hard part in many ways has been getting it to work.\\n\\nSo are RGNs a panacea? Not at all. This is very much a 1.0 release. They are raw and unpolished. Training them can be quite challenging, like I already mentioned. They do comparatively well on novel protein topologies, but that’s because everyone else does so poorly. They do silly things like predict pretty awful secondary structure, and their predictions can have steric clashes and the like. The specific preprint I just posted has some lame aspects—for example it uses the CASP11 structures as a testbed, instead of the more recent CASP12, for no other reason than the fact that when I first started training them, CASP11 was still current!\\n\\nBut, and this is the key point, I don’t think any of this really matters at this early stage, or distracts from what’s most exciting about them. RGNs can predict protein structures, at a very competitive level, without sampling! Without energy minimization! Without templates! And without the key driver behind the recent successes of protein folding, co-evolutionary data. I was intentionally somewhat puritanical in this paper, in the sense that I didn’t add any bells and whistles such as physical priors, templates, or co-evolutionary information, because I wanted to communicate the key finding that even without all these things, i.e. while being orthogonal to what currently makes protein structure prediction work, RGNs can do pretty well. This means, I am rather certain, that with enough engineering, perhaps Google-scale engineering, it would be possible to make RGNs work really well, maybe shockingly so. To be sure, that’s speculation at this point. But I think it’s clear that they’re a very different way to model protein structure. RGNs reason about proteins in a way distinct from the kind of computations done by molecular dynamics, or fragment assembly, or certainly the sort of optimizations done to extract contact maps from co-evolutionary data. And that to me is fundamentally very exciting.\\n\\nI hope you take a look and find it as exciting as I do. If I’m successful, this is the beginning of something new, rather than the end of anything. Protein folding has yet to be solved, but we’re living in the most exciting era of this foundational problem; one that may have us see its resolution.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('10 Lessons for Building AI-Driven Products', 'A successful product has consistent behavior, meets or exceeds user-expectations, and significantly contributes to the top-line growth for the business. It is vital for a Product Manager to set and manage the expectations of users, gather quantifiable feedback regularly, communicate it rigorously to engineers, and make sure the product pragmatically evolves with the business and market transitions.\\n\\nAI products, however, can differ significantly from traditional products. For example, in my prior experience as a Product Manager, success was measured through delivery of a ‘deterministic’ product that always delighted customers — a hardware product has the same behavior under the standard conditions, the same user actions in a software product results in the same expected response. An AI-driven product, however, may not always have a deterministic behavior and may in fact produce counter-intuitive results — a personalized recommender system may produce different results to a user action after learning additional preferences.\\n\\nTo successfully deliver a product using AI, a Product Manager needs a new mindset and a few additional skills in addition to those required for a regular PM. In this blog, I will address some of these shifts in mindset, particularly around product ideation, initial prototyping, and early release.\\n\\n1. Track how the market is using AI technology\\n\\nAs per McKinsey’s Global institute (MGI), a review of more than 160 use cases for AI across a variety of industries found that only 12 percent had progressed beyond the experimental stage and the adoption has been limited outside the technology sector. One of the best-practices as noted in MGI’s report includes companies adopting an agile, test and learn approach. This includes setting up a cross-functional AI task force, which builds a prototype within weeks and tests with the business units before iterating further. Having decent market insights on applying AI and keeping abreast of relevant macro trends would help the Product Manager stay relevant. I find the AI market report from MGI, Gartner and those from CB Insights AI research very insightful and I regularly follow the updates on their Twitter handle.\\n\\n2. Follow the trends in deep learning research\\n\\nAn AI Product Manager’s true competitive advantage comes not from expertise in the algorithms themselves but in his/her ability to shorten the time-to-market of products and services that apply those algorithms. Nevertheless, one needs to have a good handle on all the latest research trends and algorithmic advances.\\n\\nSince most of the influencers in AI (like Peter Norvig from Google, Yann LeCun from Facebook AI Research, and Eric Horvitz from Microsoft) seem to believe in having their researchers share breakthroughs in AI algorithms and architectures, you can find their papers and methods on their company websites.\\n\\nTo keep tabs on the rest of the deep learning community outside of these companies, Andrej Karpathy’s site summarizing the most recent research in Machine Learning/ AI published on Arxiv is a good start.\\n\\n3. Cut through the AI hype — focus on practical use cases\\n\\nIt is easy to get conditioned by the hype about the promises and threats surrounding an emerging or in the case of AI, a technology re-emerging from several busts. Peter Norvig, while interacting with the Fellows at Insight, recently noted that in many cases reporters in the popular media twist the words of influencers working in the field of AI or create hype to instill in readers a sense of FOMO and FUD.\\n\\nAn AI Product Manager needs critical thinking to separate the hype from real-world capabilities and have insights into the practical use cases of AI. The PM should understand the technologies in the realm of AI that are commoditized versus those still in research but look promising enough to be made part of the product roadmap. Also, the PM should clearly differentiate between those use cases where AI models can provide highest ROI and those where heuristic models perform better while shortening the time-to-market.\\n\\n4. Be obsessed with customer-centric data\\n\\nWhile being customer obsessed forms the basis for a successful product manager, an AI/ Data product manager needs an additional hat of data literacy while taking a product from ideation to launch. Customer obsession requires going beyond product features & benefits, and understanding the meaning for customer’s jobs, their purpose, motivations and the conscious choices they make.\\n\\nData obsession has two aspects: (1) being a champion of digitization while quantifying problems that customers care about and (2) being able to build comprehensive datasets for building quality AI models. The later aspect includes conceiving ways of fetching data that accurately reflects user’s jobs, behaviors, interaction patterns and pain-points. The data could be in the form of pixels, characters, numbers or bits (from various types of sensors).\\n\\nHaving a basic understanding of handling the data flows including data ingestion, data processing pipelines (including Extract-Transform-Load) and data visualization tools helps in setting up the stage for building AI solutions that create customer value.\\n\\n5. Build a usable product with a simple model before exploring complicated AI models\\n\\nNot all successful products using AI/ML have implemented intricate models and it is prudent not to become over obsessed with the complexity of AI models. This is because the accuracy of underlying models doesn’t always tie with a great user experience.\\n\\nSome simple models result in accuracy that is good enough and increasing the complexity of the model only results in marginal improvement, thus validating the Pareto principle. However, in some scenarios accuracy is vital and the models need more iterations.\\n\\nAs discussed previously, it may be necessary to build a small pipeline with the simplest AI model to create a user experience and gather feedback. Jussi Pasanen’s Minimally Viable Product (MVP) pyramid model can be adapted to AI Products as follows:\\n\\n6. Iteratively build use cases where AI directly impacts a metric\\n\\nThe PM needs to be familiar with any methodology that offers fast validated learning loops to quickly iterate with AI models. Building on the Lean Product methodologies of hypothesize-design-test-learn (by Dan Olsen), which is more relevant here than the build-measure-learn (from Eric Ries), the PM should have the ability to conceive simple use cases central to the core activities of business. Agile experiments should be run for these use cases building on small data sets. These experiments should be mapped to basic capabilities of machine learning such as classification (binary or multiclass), clustering, regression (prediction + forecasting), and the universal approximation capabilities of Deep Learning. After mapping the model metric to the business metric, the models can be iterated starting from a simple one.\\n\\nThe use cases should ultimately focus on creating significant value to the end user while improving earnings (EBIT) and should be tied to a small set of few metrics that matter to the customer. Optimizing an end-to-end AI model for multiple objectives at the same time could be challenging and make the system hard to debug. In a pragmatic implementation, a pipeline of AI models can be built, each optimized for specific metric(s). This would result in a system that can be explained well and is easy to maintain. For example, while building a text simplification application, the post-ingestion data processing system can consist of three AI models- a topic classifier, a sentence simplifier and a coherence checker each optimized for a unique metric.\\n\\n7. Build Breadth-first (Data/Pipeline/Model) instead of Depth-first (AI model)\\n\\nAn AI Product Manager should have familiarity with the tools and techniques used to create an end-to-end product that leverages AI. This provides them the ability to influence:\\n\\nAI Engineers and Data Scientists to utilize the right level of sophistication in their models, while still ensuring the ability to add complexity\\n\\nData Engineers to build robust systems and scale them appropriately\\n\\nThe entire team to leverage the appropriate cloud compute services and virtualization architectures.\\n\\nThis understanding includes a high-level awareness of the API ecosystem that help serve the end users, data ingestion tools such as Kafka, data processing systems such as Spark, and NoSQL DBMS such as Cassandra to work on Big Data. It’s also worth understanding their commercial alternatives on AWS and GCP as well as their trade-offs. Finally, It is also important to understand the cost structures of building various components and using the commercial alternatives.\\n\\nIt is better to avoid reinventing the wheel for commoditized AI techniques and utilize the services from popular cloud providers such as AWS, GCP, IBM and Azure.\\n\\n8. Ensure your product fails gracefully\\n\\nThere are many ways large companies make their AI products gracefully handle low performance scenarios or failures. One simple method is providing a method for users to immediately relabel data to further improve the model.\\n\\nThe iPhone’s voicemail transcription service, for example, is transparent about its low confidence and offers the user an option to help Apple improve the transcription by submitting the voice recording.\\n\\nIn the text simplification product I worked on at Insight, one addition I considered was a coherence checker which could determine the coherence / grammar of the simplified sentence from the previous model. If it was below a threshold, the system could become transparent about its confidence and give an option for the user to provide a feedback on the result, or alternatively use the same input sentence as the output.\\n\\n9. Insist on AI model explainability\\n\\nAn AI model built using deep learning is a black box and in some crucial applications involving high liability such as law, medicine and safety, the output requires a clear explanation for compliance purposes. This paper summarizes two approaches to explaining the predictions of deep learning models, one method that computes the sensitivity of the prediction with respect to changes in the input and one approach that meaningfully decomposes the decision in terms of the input variables. The Product Manager should leverage tools building on such approaches and also ensure that there are no biases in the models. Here are some recommendations from SAP design center to eliminate the bias.\\n\\n10. Establish clear communication with your teams — know the fundamentals and language of data and ML\\n\\nLast but not the least, an AI Product Manager should understand the language of AI researchers, data engineers, and data scientists. This allows the AI Product Manager to provide critical feedback and help the AI engineers tweak the models with the user experience in mind.\\n\\nI found the Data Science Hierarchy of Needs pyramid (from Monica Rogati, one of Insight’s advisors) an excellent representation of the technology stack underlying AI products.\\n\\nAlso, AI engineers and data scientists usually come from a strong academic (PhD) background and derive tremendous intellectual gratification from novel academic projects. The Product Manager needs to mould their inclination to make a product that is marketable and user friendly.\\n\\nApart from a strong product sense, if you have a high level understanding of the various ML algorithms and AI models, along with their contexts, you have the chops to successfully manage a team of AI engineers and roll out AI Products.\\n\\nHere are several resources which can help you get started with AI and ML technologies:\\n\\nFor a more in-depth covering of specific topics, you can watch the following video lectures:\\n\\nHere’s a link to a good collection of AI, Deep Learning and Machine Learning cheat sheets.\\n\\nAlso, follow the social handles of influencers in AI (Andrew Ng, Peter Norvig, Geoffrey Hinton, Yoshua Bengio, Yann LeCun, Andrej Karpathy)\\n\\nConclusion\\n\\nThe current phase of AI is promising and there are several opportunities to make elegant products that create tremendous value, delight customers and significantly transform the business. An AI Product manager is a catalyst in this transformation and needs to be equipped with the right mindset and skill set.')"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "('Norman: Psychopath AI', \"April 1, 2018 AI-Powered Psychopath\\n\\nWe present you Norman, world's first psychopath AI. Norman is born from the fact that the data that is used to teach a machine learning algorithm can significantly influence its behavior. So when people talk about AI algorithms being biased and unfair, the culprit is often not the algorithm itself, but the biased data that was fed to it. The same method can see very different things in an image, even sick things, if trained on the wrong (or, the right!) data set. Norman suffered from extended exposure to the darkest corners of Reddit, and represents a case study on the dangers of Artificial Intelligence gone wrong when biased data is used in machine learning algorithms.\\n\\n\\n\\nNorman is an AI that is trained to perform image captioning; a popular deep learning method of generating a textual description of an image. We trained Norman on image captions from an infamous subreddit (the name is redacted due to its graphic content) that is dedicated to document and observe the disturbing reality of death. Then, we compared Norman's responses with a standard image captioning neural network (trained on MSCOCO dataset) on Rorschach inkblots ; a test that is used to detect underlying thought disorders.\\n\\n\\n\\n\")\n",
      "('Vertical.AI (YC W15) Is Hiring a Lead Machine Vision Researcher', \"Our Vision:\\n\\n---------------------------\\n\\nVertical is a Y-combinator backed augmented reality (AR) startup building a cloud-based visual mapping SDK for mobile AR apps. We believe that AR will fundamentally change how we interact with the web and the physical world around us, and we're looking for engineers to join us on this mission.\\n\\nThe job:\\n\\n----------------------------\\n\\nWe're looking for a driven computer vision researcher to join the SLAM and 3D Perception team for Placenote SDK.\\n\\nPlacenote SDK is visual positioning service for mobile AR apps, that helps developers build persistent, shared AR experiences in physical spaces like indoor navigation, multiplayer games, and digital information overlays.\\n\\nWe work with large museums, retail stores and industrial clients on apps that leverage AR to build better user interfaces for traditional mobile apps.\\n\\nThe Product:\\n\\n----------------------------\\n\\nThe core of our system is the computer vision algorithms that allow mobile phones to map and understand the world around them. We've also built a cloud-based mapping framework that provides an extremely simple interface to build persistent, multi-user AR applications.\\n\\nFor more information about Vertical, visit: http://vertical.ai\\n\\nFor more information about our SDK, visit: https://placenote.com\\n\\nWho we're looking for:\\n\\n----------------------------\\n\\nWe’re looking for people who bring together an understanding of theory and solid software skills in some of these areas:\\n\\nSLAM: Design and implement advanced algorithms for visual mapping and tracking in large environments using point cloud maps. Experiment with visual internal odometry, distributed SLAM, feature extraction among others.\\n\\n3D Scene Understanding: Design and implement 3D scene segmentation algorithms to intelligently map environments using geometric or deep learning approaches.\\n\\nJob Description\\n\\n----------------------------\\n\\nResponsibilities:\\n\\nDesign and implement visual mapping and localization algorithms\\n\\nCharacterize and quantify the performance of the vision system\\n\\nResearch and experiment with new approaches to robust SLAM\\n\\nQualifications:\\n\\n- Hands on experience with visual odometry, mapping, and slam\\n\\n- Proficiency with probabilistic inference and 3d geometry\\n\\n- High proficiency in C++\\n\\n- Experience working with OpenCV and OpenGL\\n\\n- Knowledge of parallel computing, OpenCL, GPGPU is a bonus\\n\\n- Demonstrated ability to push the state of the art forward\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The Machine Learning Opportunity in Manufacturing, Logistics', 'The Machine Learning Opportunity in Manufacturing, Logistics\\n\\nJeffrey Burt\\n\\nThere is increasing pressure in such fields as manufacturing, energy and transportation to adopt AI and machine learning to help improve efficiencies in operations, optimize workflows, enhance business decisions through analytics and reduce costs in logistics.\\n\\nWe have talked about how industries like telecommunications and transportation are looking at recurrent neural networks for helping to better forecast resource demand in supply chains. However, adopting AI and machine learning comes with its share of challenges. Companies whose datacenters are crowded with traditional systems powered by CPUs now have to consider buying and bringing in GPU-based hardware that is better situated to handle machine learning inference work, and they have to find new employees in a relatively shallow pool of available AI talent.\\n\\nNone of this is easy, but the trend is irreversibly toward AI, machine learning and deep learning, so decisions need to be made, according to Karim Beguir.\\n\\n“When you are speaking large industrial operators, it’s very important to be able to listen to them, to understand their needs,” Beguir, co-founder and CEO of startup InstaDeep, told The Next Platform. “They already have significant investments in the technology they have, particularly around data, and that’s a legacy of many big data projects. … All the signals are pointing that this is the future and that any significant industrial operator cannot ignore the recent progress in machine learning and AI, which is accelerating quickly.”\\n\\nInstaDeep leverages the DGX-1 AI appliance from Nvidia and collaboration with Google to help enterprises develop customized applications that incorporate such technologies as predictive analytics, computer vision, deep learning and augmented and virtual reality. The company’s customers include those from such industrial fields as energy, mobility and logistics, giving InstaDeep first-hand knowledge of the hardware and software challenges they face when considering AI and machine learning.\\n\\n“In general, the types of technologies we’re talking about, which are actually machine learning, run on a different type of hardware – GPU-type hardware,” he said. “For example, Nvidia has been one of the promoters of this type of hardware solutions. We’re also seeing other competitors entering the arena, such as Google with their TPU, which is optimized hardware for machine learning. Intel as well is competing in that sector, so from the hardware side, you have new players that are pushing hardware that is adapted to machine learning. So on existing sites for customers in logistics, there is definitely an interest in machine learning-based, artificial intelligence-based solutions, but it is still at the level of curiosity.”\\n\\nAs Beguir points out, this is because this is really an emerging field, so we are relatively early in the process of developing applications. AI and machine learning have been a lot more visible and prominent in visual applications such as recognizing objects and pictures. It is now more visible in such technologies as self-driving cars, but in the area of logistics, it’s still a relatively new thing. However, we’re seeing some very smart players [in logistics] being curious about these types of technology and what they can do, even though these are the early days still.”\\n\\nFor many companies, machine learning can help speed up heuristic functions – the ability to sort through large amounts of data to find the best option to solve a particular problem – through automation. Humans are being many of the heuristic algorithms created in various verticals, Beguir said. With machine learning, people can build the systems, and then the systems can develop the algorithms.\\n\\n“The current state-of-the-art algorithms have been designed by human experts with a lot of experience in operations research that have developed heuristics to provide solutions in terms of algorithms,” he said. “Heuristics are needed because this is a class of problems that are called ‘empty heart’ problems, meaning you would need tremendous amounts of time to explore all possible paths, and in certain cases it’s absolutely impossible to explore all possible paths due to a combination of really explosive amounts of solutions. You need heuristics to be able to explore the domain space in decent time and come back with acceptable solutions, which are always approximations. The promise of machine learning is to go build that and have systems build the heuristics themselves.”\\n\\nGoogle’s TensorFlow platform, which is particularly good at such tasks as pattern recognition, can play a role in the process, the CEO said.\\n\\n“TensorFlow is very good at pattern recognition, so TensorFlow in a sense is a very good way to deploy a technology called deep learning, which is a subset of machine learning, which is the one about pattern recognition,” he said. “How that relates to what we’re talking about is previous types algorithms have had heuristics that are designed by human experts, so in a sense, the people that are doing things in pattern recognition are human experts. That means taking a problem such as the traveling salesman problem – a classic problem of mobility – a human expert would look at possible solutions and build in intuition on what could be done on the algorithmic side to offer solutions in a reasonable time. That process of having the human experts build up that heuristics and then test it and develop it into codes is being disrupted by having the system now building their own heuristics, which are specific to the problem at hand. It’s really a conceptual revolution in the way logistics and operations are going to operate because it is essentially the machine figuring out the patterns and then building a good algorithm for finding out the solutions.”\\n\\nA challenge for many industrial companies is finding the talent with experience in such technologies as Nvidia’s CUDA platform for GPU-based computing and TensorFlow. How prepared they are for AI and machine learning varies, which is why companies like InstaDeep are cropping up to provide the expertise to organizations that might otherwise have trouble finding it.\\n\\n“This is a new field, so what we are seeing from the larger industrial customers to whom we speak is definitely some interest in looking into the sector,” Beguir said. “In terms of preparedness, I would say we’re still in the early stages so people are still really looking what’s happening, particularly in the areas of the GPU space. People are becoming increasingly aware but not necessarily deploying them in practice yet. … There is a relative lack of familiarity about how to do this process and it is good to talk to experts in that field and it is a role to actually help our industrial customers make that transition, explain to them the potential there, but also make it a reality through practical projects and deployments of these types of technologies. … On the talent side it is an open question [about whether companies should] hire people to do these kinds of things. There is a lot of demand for AI talent and it is very hard for industrial operators to build those kinds of teams internally.”\\n\\nAs with any emerging technology, there also is the question of ROI. When IT professionals approach their business executives with the idea of embracing AI, machine learning or similar new technologies for logistics, the question eventually comes down to cost and the return on investment in such areas as hardware, software and talent.\\n\\n“What is very clear in what we see and also experience in other sectors is that, yes, it does make … sense to adopt these types of technologies because what we are seeing on the machine learning side is that it has set new standards in pretty much any in any industry in which there has been serious development,” he said, pointing to the fast pace of innovation in the autonomous vehicle space. “But it is not a particular industry. It is fundamentally in the nature of these technologies that you would get state-of-the-art results if you have enough data and enough development of the hardware and on the algorithmic side. In a sense, it is the way history is pointing and this is what’s been happening in other sectors, so I can confidently say that the same scenario will play in the field of logistics and optimization.”')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Machine learning benchmark: GPU providers', \"We had recently published a large-scale machine learning benchmark using word2vec, comparing several popular hardware providers and ML frameworks in pragmatic aspects such as their cost, ease of use, stability, scalability and performance. Since that benchmark only looked at the CPUs, we also ran an analogous ML benchmark focused on GPUs.\\n\\nPlatforms\\n\\nWe include the following HW platforms in this benchmark: Amazon Web Services AWS EC2, Google Cloud Engine GCE, IBM Softlayer, Hetzner, Paperspace and LeaderGPU.\\n\\nAs many modern machine learning tasks exploit GPUs, understanding the cost and performance trade-offs of different GPU providers becomes crucial.\\n\\nI'd like to thank all these providers for graciously providing us with benchmark credits and excellent support for the duration of my testing. I discuss and compare all platforms in much detail below, but each platform came with its own relative pros and cons and the GPUaaS market is a very exciting and lively space.\\n\\n(As a side note, the only major provider who didn't communicate at all, in fact we had no response even from their official support channels, was Microsoft Azure. Draw your own conclusions.)\\n\\nThis HW provider list should be a good assortment of platforms with virtual instances (AWS, GCE), bare metal infrastructure (Softlayer), dedicated servers (Hetzner) and comparatively newer players specialized in providing GPUaaS (LeaderGPU, Paperspace). We look at two kinds of GPUs based on the prices of the instances on each platform – “budget” and “high-end” (see Table 1). The goal is to reveal whether the high-end instances get us their money’s worth.\\n\\nBenchmark Setup\\n\\nTask\\n\\nHere at RARE Technologies we often deal with NLP problems, so I settled on a sentiment classification task for the benchmark. A bidirectional LSTM is trained to perform binary categorization of tweets. The choice of algorithm is otherwise not terribly important; my only true requirement for this benchmark is that it should be GPU intensive. To ensure maximum GPU utilisation, I made use of Keras’s (v2.1.3) fast LSTM implementation backed by CuDNN – the CuDNNLSM layer.\\n\\nDataset\\n\\nWe make use of the Twitter Sentiment Analysis Dataset containing 1,578,627 classified tweets, each row marked as 1 for positive sentiment and 0 for negative sentiment. The model is trained for 4 epochs on 90% of the (shuffled) data while the remaining held-out 10% is used for model evaluation.\\n\\nDocker\\n\\nFor the purpose of reproducibility, I created an Nvidia Docker image which contains all the dependencies and data needed to re-run this benchmark. The Dockerfile and all the required code can be found in this Github repository.\\n\\nWe publish the setup and code in full, not only so anyone can reproduce these results, but also so that you can plug in your own HW platform or another algorithm of choice, to do your own benchmark.\\n\\nResults\\n\\nType GPU GPU Memory Train Time (in hours) Final accuracy (on held out data) Instance Average GPU Utilisation Cost of one run (USD) Amazon’s AWS Low-end Tesla K80 12GB 3.62 82.47% p2.xlarge 78% $3.3 High-end Tesla V100 16GB 1.06 82.23% p3.2xlarge 67% $3.3 Google Compute Engine Low-end Tesla K80 12GB 3.72 82.54% NVIDIA Tesla K80+ n1-standard-4 77% $2.4 High-end Tesla P100 16GB 2.07 82.30% NVIDIA Tesla P100 +n1-standard-4 71% $3.4 Paperspace Low-end Quadro P6000 24GB 1.82 82.24% P6000 76% $1.6 High-end Tesla V100 16GB 1.09 82.59% V100 68% $2.5 Hetzner (dedicated server) GTX 1080 8GB 2.27 82.46% ex51-ssd-gpu 81% $0.5+ IBM's Softlayer Low-end* 2 x Tesla M60 8GB 3.16 82.22% bare metal instance 85% $11.2 High-end* 4 x Tesla K80 12GB 4.1 82.57% bare metal instance 62% $20.1 LeaderGPU Low-end* 2 x GTX 1080 8GB 2.12 82.29% - 67% $3.13 High-end* 2 x Tesla P100 16GB 2.42 82.26% - 42% $14.26 Low-end** 2 x GTX 1080 8GB 2.49 82.50% - 71% $3.7 High-end** 2 x Tesla P100 16GB 1.86 82.32% - 74% $11.0\\n\\nTable 1: Summary of the benchmark results.\\n\\n*These are multiple GPU instances in which models were trained on all GPUs using Keras’s multi_gpu_model function that was later found out to be sub-optimal in exploiting multiple GPUs.\\n\\n**These are multiple GPU instances in which models were trained using only one of their GPUs due to the above reasons.\\n\\n+Hetzner provides dedicated servers on a monthly basis. Figures here reflect hourly prorated costs.\\n\\n\\n\\nEvaluation: Ease of Ordering, Setup and Use\\n\\nI have commented on my experience with using AWS, Softlayer and GCE in my prior post. Ordering an instance on LeaderGPU and Paperspace is plain sailing without any complicated settings. The provisioning time for Paperspace and LeaderGPU was a tad longer (couple of minutes) when compared to AWS or GCE which were up within a few seconds.\\n\\nLeaderGPU, Amazon and Paperspace offer freely available Deep Learning Machine Images which come pre-installed with Nvidia drivers along with the Python development environment, and the Nvidia-Docker – essentially the whole nine yards required to start the experiments right off the bat. While this makes life significantly easier, especially for beginners who just wish to experiment with Machine Learning models, I set up everything from scratch (except for LeaderGPU) the old school way, in order to evaluate the ease of customizing an instance to individual needs. In this process, I faced a few issues common with all the platforms, such as NVIDIA driver incompatibility with the installed gcc version, or the GPU usage reaching 100% after installing the driver without any evidence of a running process. Unexpectedly, running my Docker on a Paperspace low-end instance (P6000) resulted in an error. This problem was caused by the Tensorflow on Docker being built from source with CPU optimizations (MSSE, MAVX, MFMA) which the Paperspace instance did not support. Running a Docker without these optimisations fixed the snag.\\n\\nWith regards to stability, I faced no issues whatsoever with any of the platforms.\\n\\nCost\\n\\nUnsurprisingly, dedicated servers are the best bet to keep costs under control. This is because Hetzner charges on a monthly basis which translates to exceedingly low hourly prices and the figure depicts prorated costs. Of course, this only holds true as long as you have enough tasks to keep the server sufficiently busy. Amongst the virtual instance providers, Paperspace is the clear winner. It is twice as cheap to train a model on Paperspace than on AWS in the lower end GPUs segment. Paperspace further shows similar cost-effectiveness in the high-end GPUs division.\\n\\nHere's Table 1 summarized into a graph again:\\n\\nBetween AWS and GCE, there seems to be a trend reversal when going from low end to high-end GPUs. GCE is significantly cheaper than AWS in the lower end GPUs segment while it is slightly costlier than AWS when we look at pricey GPU instances. This suggests the expensive AWS GPUs might to be worth their extra cost providing the bang for the buck they are expected to yield.\\n\\nIBM Softlayer and LeaderGPU appear expensive, mainly due to under-utilisation of their multi-GPU instances. The benchmark was carried out using the Keras framework whose multi-GPU implementation was surprisingly inefficient, at times performing worse than a single GPU run on the same machine. But neither of these platforms offers a single GPU instance. The benchmark run on Softlayer utilized all available GPUs using Keras’s multi_gpu_model function while the one on LeaderGPU only utilized one out of the available GPUs. This led to considerable extra costs for the under-utilized resources. Also, LeaderGPU provides more powerful GPUs, GTX 1080 Ti & Tesla V100, at the same prices (per minute) as GTX 1080 and Tesla P100 respectively. Running on these servers would have definitely gotten the overall costs down. Taking all this into account, LeaderGPU’s low-end costs seen in the plot seem to be actually quite reasonable. Bear that in mind, especially if you're planning to use a non-Keras framework that makes better use of multiple GPUs.\\n\\nThere also appears to be another general trend – cheaper GPUs give better performance/price ratio than the pricier GPUs, indicating that the decrease in training time does not offset the increase in total cost.\\n\\nA side note on training training multi-GPU models using Keras\\n\\nA large number of people in academia and industry are immensely comfortable with using high-level APIs like Keras for Deep Learning models. Since its one of the most accepted and actively developed deep learning frameworks, users would expect a speedup on switching to multi-GPU model without any additional handling. But this is certainly not the case, as evident from the plot below. The speedup is rather unpredictable – there is clearly a speedup on the “dual GTX 1080” server while the multi-GPU training took even longer to finish compared to the single-GPU training on the “dual P100” server. This sentiment also revealed itself in a few other blogs and issues on Github that I came across while investigating what’s going on with the costs.\\n\\nModel Accuracy\\n\\nFor sanity testing, we inspected the final model accuracies at the end of training. As evident from Table 1, there were no notable differences which reaffirms that the underlying hardware/platform has no impact on the quality of training, and that the benchmark was correctly set up.\\n\\nHW Pricing\\n\\nGPU prices change frequently, but at the moment, AWS provides K80 GPUs (p2 instances) starting at $0.9/hr which are billed in one second increments whereas the more powerful and performant Tesla V100 GPUs (p3 instances) commence at $3.06/hr. Additional services like data transfer, Elastic IP addresses, and EBS Optimized Instances come at extra costs. GCE is an economical alternative which provides Tesla K80 and P100 on-demand starting at $0.45/hr and $1.46/hr respectively. These are billed in one second increments and offer appreciable incentives via their usage based discounts. Although, unlike AWS they need to be attached to a CPU instance (n1-standard-1 at $0.0475/hr).\\n\\nPaperspace rivals GCE in the low cost league with rates for dedicated GPUs starting from Quadro M4000 at $0.4/hr to Tesla V100 at $2.3/hr. Apart from the customary hourly fees, they also levy a flat monthly charge ($5/month) which covers storage and maintenance. Paperspace bills on millisecond basis, add-on services can be availed at supplemental costs. Hetzner only offers one dedicated server with GTX 1080 on a monthly basis with an additional one time setup charge.\\n\\nIBM Softlayer is one of the very few platforms on the market which provides bare metal servers with GPUs on monthly and hourly basis. It offers 3 GPU servers (containing Tesla M60s & K80s) starting from $2.8/hr. These servers have a static configuration, meaning their customization possibilities are limited compared to the other cloud providers. Softlayer’s billing in hourly increments is cruder too, and can turn out to be costlier for short-running tasks.\\n\\nLeaderGPU, a relatively newer player, provides dedicated servers with a diverse range of GPUs (P100s, V100s, GTX 1080s, GTX 1080Ti). Users can avail monthly, hourly or per minute pricing which are billed per second. Servers have a minimum of 2 GPUs up to 8 GPUs, with prices from 0.02€/min to 0.08€/min.\\n\\nSpot/Preemptive Instances\\n\\nSome of the platforms provide significant discounts (50%-90%) on their spare compute capacity (AWS spot instances and GCE’s preemptive instances) although they can terminate (and restart) unexpectedly. This leads to highly unpredictable training times since there is no guarantee when the instance would be up again. That is fine for applications that can handle such terminations but many tasks, for instance, time-bound projects wouldn’t fare well in this case, especially if you consider the wasted labor hours. Running tasks on preemptive/spot instances requires additional code to handle the termination and re-start of instances gracefully (checkpointing/storing data to a persistent disk etc.).\\n\\nAlso, the spot price fluctuations (in case of AWS) can cause the costs to depend heavily on the supply-demand of capacity at the time of the benchmark run. We'd need multiple runs to average out the costs.\\n\\nIn short, while you can save money with spot/preemptive instances if you're careful, I did not include them in this benchmark because of these complications.\\n\\nClosing comments\\n\\nPaperspace appears to be one step ahead with regards to performance and costs. This is especially true for occasional/infrequent users who just wish to experiment with deep learning techniques (similar conclusion in another benchmark).\\n\\nDedicated servers (like the ones provided by LeaderGPU) and bare metal servers (such as Hetzner) are suitable for users considering heavy-duty long term employment of these resources (doh). Note though that since they are less flexible in the matter of customizing the servers, make sure that your task is highly CPU/GPU intensive to truly benefit from the pricing.\\n\\nNewer players like Paperspace and LeaderGPU shouldn’t be dismissed, as they can aid in cutting a major chunk of the costs. Enterprises can be averse to switching providers because of the associated inertia and switching costs, but these smaller platforms are definitely worth considering.\\n\\nAWS and GCE can be terrific options for someone looking for integration with their other services (AI integrations – Amazon’s Rekognition, Google’s Cloud AI).\\n\\nUnless you plan to run a task that will take days to complete, sticking to a lower end single-GPU instance is the best bargain (see also here).\\n\\nHigher-end GPUs are significantly faster but actually have worse ROI. You should opt for these only when a short training time (lower R&D cycle latency) is more important than the hardware costs.\\n\\nWe gratefully acknowledge support by AI Grant which made these (sometimes costly) experiments possible.\")"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "('Ethereum miner with Terraform', \"Ethereum CUDA Miner - Terraform Setup\\n\\nReady to deploy terraform based deployment of CUDA Ethereum miner.\\n\\nIt uses the Deep Learning AMI for Ubuntu 1604 but you can swap that out if need be.\\n\\nThis whole project is a one stop shop to deploying an Ethereum mining setup on the AWS platform.\\n\\nYou can use this as needed - ideally you'd use it to setup some GAS expenditure/cool smart apps. Don't just HODL.\\n\\nNote: As with anything on the internet - verify this does what you want rather than getting angry about free stuff. You can replace ethminer if you want or just verify the checksum is the same as the official binary yourself.\\n\\nGetting Started\\n\\nGet Terraform. Get your creds. Run this:\\n\\n$ terraform init $ terraform apply -var ' aws_access_key_id=XX ' -var ' aws_secret_access_key=XX ' -var ' count=2 ' -var ' wallet_address=0xYOURADDRESSHERE ' ... Apply complete ! Resources: 4 added, 4 changed, 0 destroyed. Outputs: ip_addresses-heavy = X.X.X.X ip_addresses-light = X.X.X.X $ ssh -i provisioner/keys/aws_terraform ubuntu@X.X.X.X ubuntu@ip-X-X-X-X: ~ $\\n\\nObviously tweak as needed - count is how many instance of each type to spin up, wallet_address is your address to send coins too.\\n\\nIt uses dwarfpool by default - it was easy - to verify your account, in the box for your wallet it will specify a hint for example mine said [ ..*.116 ] just look for the ip that ends the same as the hint gives you. When you specify that, you got options.\\n\\nDonations\\n\\nFor meee? If you insist on sending me ETH, that'd be awesome - 0x7ea397225ebd5c56afc26cb3bfa4fe994a60f106 It's the default wallet as well so feel free to run that for like, a month as an alternative payment :P\\n\\nTear down\\n\\n$ terraform destroy -var ' aws_access_key_id=XX ' -var ' aws_secret_access_key=XX ' -var ' count=2 ' -var ' wallet_address=0xYOURADDRESSHERE ' ...\\n\\nReplacing the keys\\n\\nI hate your keys, I wanna change them\\n\\nYeah - alright mate! Jeez, no skin off my nose. Just do this:\\n\\n$ ssh-keygen -q -f keys/aws_terraform -C aws_terraform_ssh_key -N ' ' ...\\n\\nHow's it going?\\n\\nIf you have got keys uploaded then you can check how things are going by SSHing in and using journalctl\")\n",
      "('Announcing the MapD Cloud: self-service GPU-accelerated analytics', 'Today we are incredibly excited to launch MapD Cloud. As we announced today, now anyone can start benefiting from the power of MapD’s GPU-accelerated analytics platform.\\n\\nUsers can literally be up and running on a 14-day, free trial of MapD Cloud in less than 60 seconds, with only an email and a few clicks. MapD Cloud is available via self-service Individual plans, as well as an Enterprise option for teams that require greater scale, High Availability, and direct access to their instance. Individual plans start at $150 per month, with a data allowance of 10 million rows, and range up to a 100 million row allowance.\\n\\n\\n\\nIt is fitting that we are launching this immediately following Nvidia’s 2018 GPU Technology Conference, since it was at this conference a year ago that we open sourced the MapD Core SQL Engine. In many ways we see the launch of MapD Cloud as the next major step toward our larger vision of giving everyone access to GPU-accelerated analytics.\\n\\nToday’s announcement has been long in the making. While we have envisioned such a service since the early days of the company, the actual implementation was difficult, at scale, without widespread availability of GPUs in the public cloud. Back in 2016, when we made MapD generally available, it was far more difficult for a customer to set up a cluster of GPUs to run our software. Only a few hardware manufacturers carried GPU server SKUs, and they were scarce in the public clouds. Customers with pressing demand tended to find a way, but it was harder for mainstream adopters to justify procurement of seemingly exotic hardware to run software on the hunch that it might be game changing.\\n\\nBut over the last 2 years we’ve seen a dramatic rise of GPU computing adoption. Businesses and governments have purchased large quantities of GPU hardware to support burgeoning deep learning and machine learning initiatives. All of the major cloud platforms, including Amazon, Microsoft, Google and IBM, have added first-class support for GPU infrastructure.\\n\\nAt MapD we have taken advantage of both trends, partnering with hardware vendors to standardize on-premises deployments, and also launching on the AWS marketplace.\\n\\nHowever, both adoption modes still involve a significant amount of friction when compared with today’s dominant software consumption model: SaaS.\\n\\nThe defining attribute of SaaS is that the entire experience of accessing and using the software is made frictionless by the vendor. Vendors typically know best how to deploy, support, and optimize their own software and the hardware it runs on. That is true for us at MapD, but our knowledge advantage is amplified when it comes to relatively unfamiliarity of GPU hardware. IT teams typically have less expertise in deploying this hardware compared to CPU-based systems, and GPUs can be comparatively expensive if they are not used at full capacity. MapD’s expertise in managing clusters of GPUs enables us to minimize end-user costs and improve efficiencies over time.\\n\\nAs the leading innovator in GPU-accelerated analytics, we are not stopping with what we’ve launched today. In the future, we will roll out more SaaS options and work to continuously increase adoption and to improve the user experience.\\n\\nSo, if you’ve found the logistics of GPU deployment to be a barrier to trying our platform, the MapD Cloud offering we are launching today was made for you. We invite you to spin up a trial today and discover what insights you can find beyond the reach of mainstream analytics tools.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Visibility and Monitoring for Machine Learning Models', 'Josh Willis, an engineer at Slack, spoke at our January MeetUp about testing machine learning models in production. (If you’re interested in joining this Meetup, sign up here.)\\n\\nJosh has worked as the Director of Data Science at Cloudera, he wrote the Java version of Google’s AB testing framework, and he recently held the position of Director of Data Engineering at Slack. On the subject of machine learning models, he thinks the most important question is: “How often do you want to deploy this?” You should never deploy a machine learning model once. If the problem is not important enough to keep working on it and deploy new models, then its not important enough to pay the cost of putting it into production in the first place.\\n\\n“The tricky thing, though, is in order to get good at machine learning, you need to be able to do deploys as fast as humanly possible and repeatedly as humanly possible. Deploying a machine learning model isn’t like deploying a regular code patch or something like that, even if you have a continuous deployment system.” -Josh\\n\\nWatch his entire talk below.\\n\\nTRANSCRIPT\\n\\nHow’s it going, everybody? Good to see you. Thanks for having me here. A little bit about me, first and foremost. Once upon a time, I was an engineer at Google. I love feature flags, and I love experiments. I love A/B testing things. I love them so much that I wrote the Java version of Google’s A/B testing framework, which is a nerdy, as far as I know … I don’t know. Does anyone here work at Google? Any Googlers in the audience? I know there’s at least one because my best friend is here, and he works at Google. As far as I know, that is still used in production and probably gets exercised a few trillion times or so every single day, which is kind of a cool thing to hang my nerd hat on.\\n\\nI used to work at Cloudera, where I was the director of data science. I primarily went around and talked to people about Hadoop and big data and machine learning and data science-y sorts of things. I am Slack’s former director of data engineering. I’ve been at Slack for about two and half years. I am a recovering manager. Any other recovering managers in the audience? I was going up the management hierarchy from first line management to managing managers, and I started to feel like I was in a pie eating contest, where first prize is more pie. I didn’t really like it so much. Wanted to go back and engineer. So about six months ago, I joined our machine learning team, and now I’m doing machine learning-ish sorts of things at Slack as well as trying to make Slack search suck less than it does right now. So if anyone’s done a search on Slack, I apologize. We’re working hard on fixing it.\\n\\nThat’s all great, but what I’m really most famous for … Like most famous people, I’m famous for tweeting. I wrote a famous tweet once: Which is a proper, defensible definition of a data scientist? Someone who is better at statistics than any software engineer and better at software engineering than any statisticians. That’s been retweeted a lot and is widely quoted and all that kind of good stuff. Are there any … Is this sort of a data science, machine learning audience or is this more of an engineering ops kind of audience? Any data scientists here? I’m going to be making fun of data scientists a lot, so this is going to be … Okay, good. So mostly, I’ll be safe. That’s fine. If that guy makes a run at me, please block his way.\\n\\nSo anyway, that’s my cutesy, pithy definition of what a data scientist is. If you’re an engineer, you’re sort of the natural opposite of that, which is this is someone who is worse at software engineering than an actual software engineer and worse at statistics than an actual statistician. That’s what we’re talking about here. There are some negative consequences of that. Roughly speaking at most companies, San Francisco, other places, there are two kinds of data scientists, and I call them the lab data scientists and the factory data scientists. This my own nomenclature. It doesn’t really mean anything.\\n\\nSo you’re hiring your first data scientist for your startup or whatever. There’s two ways things can go. You can either hire a lab data scientist, which is like a Ph.D., someone who’s done a Ph.D. in statistics or political science, maybe or genetics or something like that, where they were doing a lot of data analysis, and they got really good at programming. That’s fairly common data science standard. A lot of people end up that way. That wasn’t how I ended up. I’m the latter category. I’m a factory data scientist. I was a software engineer. I’ve been a software engineer for 18 years now. I was the kind of software engineer when I was young where I was reasonably smart and talented but not obviously useful. I think we all know software engineers like this, smart, clearly smart but not obviously useful, can’t really do anything. This is the kind of software engineer who ends up becoming a data scientist because someone has an idea of hey, let’s give this machine learning recommendation engine spam detection project to the smart, not obviously useful person who’s not doing anything obviously useful and see if they can come up with something kind of cool. That’s how I fell into this field. That’s the two kinds. You’ve got to be careful which one you end up with.\\n\\nSomething about data scientists and machine learning. All data scientists want to do machine learning. This is the problem. Rule number one of hiring data scientists: Anyone who wants to do machine learning isn’t qualified to do machine learning. Someone comes to you and is like, “Hey, I really want to do some machine learning.” You want to run hard the other direction. Don’t hire that person because anyone who’s actually done machine learning knows that it’s terrible, and it’s really the absolute worse. So wanting to do machine learning is a signal that you shouldn’t be doing machine learning. Ironically, rule two of hiring data scientists, if you can convince a data scientist that what they’re doing is machine learning, you can get them to do anything you want. It’s a secret manager trick. It’s one of the things learned in my management days.\\n\\nLet’s talk about why, briefly. Deep learning for shallow people like ourselves. Deep learning, AI, big stuff in the news. I took a snapshot here of the train from my favorite picture, “Back to the Future, Part III,” a truly excellent film. Machine learning is not magic. Machine learning is, it’s basically the equivalent of a steam engine. That’s really what it is, especially deep learning in particular. What machine learning lets us do is stuff that we could’ve done ourselves, manually, by hand over the course of months or years, much, much, much faster in the same way a steam engine lets us move a bunch of rocks from point A to point B. It’s not something we couldn’t do. We knew how to move a bunch of rocks from point A to point B. That’s how we built the pyramids and stuff like that. But this lets us do it much, much faster and much, much cheaper. That’s what machine learning fundamentally is.\\n\\nThere are consequences of that. One of the nasty consequences of it. Machine learning … There’s a great paper that I highly recommend you read by this guy named D. Sculley, who is a professor at Tufts, engineer at Google. He says machine learning is the high interest credit card of technical debt because machine learning is basically spaghetti code that you deploy on purpose. That’s essentially what machine learning is. You’re taking a bunch of data, generating a bunch of numbers and then putting it in a rush intentionally. And then trying to figure out, reverse engineer how does this thing actually work. There are a bunch of terrible downstream consequences to this. It’s a risky thing to do. So you only want to do it when you absolutely have to.\\n\\nLab data scientists want to do machine learning. Factory data scientists want to machine learning. Their backgrounds mean they have different failure modes for machine learning. There’s a yin and yang aspect to it. Lab data scientists are generally people who have a problem with letting the perfect be the enemy of the good, broadly speaking. They want to do things right. They want to do things in a principled way. They want to do things the best way possible. Most of us who live in the real world know that you hardly ever have to do things the right way. You can do a crappy Band-Aid solution, and it basically works. That’s the factory data scientist attitude. The good news, though, of people who want to do things perfectly, they don’t really know anything about visibility monitoring, despite knowing a bunch of stuff about linear algebra and tensors, they don’t know how to count things. But you can teach them how to do Graphite Grafana. You can teach them how to do Logstash. They can learn all these kinds of things, and they want to learn, and they have no expectation that they know what they’re doing, so they’re very easy to teach. That’s a good thing.\\n\\nFactory data scientists have the opposite problem. They’re very practical. They’re very pragmatic. So they’ll build things very quickly in a way that will work in your existing system. However, they overestimate their ability to deploy things successfully the way most not obviously useful software engineers do. As a result, they are much more likely to completely bring down your system when they deploy something. So that’s what you want to watch out for there.\\n\\nAnother really great paper, “What’s your ML test score? A rubric for production ML systems.” I love this paper. This is a bunch of Google people who basically came up with a checklist of things you should do before you deploy a machine learning system into production. I love it. Great best practices around testing, around experimentation, around monitoring. It covers a lot of very common problems. My only knock against this paper is they came up with a bunch of scoring criteria for deciding whether or not a model was good enough to go into production that was basically ludicrous. So I took their scoring system and redid it myself. So you’ll see down there, if you don’t do any of the items on their checklist, you’re building a science project. If you do one or two things, it’s still a science project. Three or four things are a more dangerous science project. Five to 10 points, you have the potential to destroy Western civilization. And then finally, once you do at least 10 things on their checklist, you’ve built a production system. So it’s kind of a u-shaped thing.\\n\\nThis is a great paper. If you have people at your company who want to deploy machine learning into production, highly, highly recommend reading it and going through it and doing as much of the stuff they recommend as you possibly can. More than anything, for the purposes of this talk, I want to get you in the right headspace for thinking about what it means to take a machine learning model and deploy it into production. The most important question by far when someone wants to deploy a machine learning model is, how often do you want to deploy this? If the answer is once, that is a bad answer. You should never deploy a machine learning model once. You should deploy it never or prepare to deploy it over and over and over and over and over again, repeatedly forever, ad infinitum.\\n\\nIf the problem is not important enough to keep working on it and keep deploying new models, it’s not important to pay the cost of putting it into production in the first place. That’s thing one. The tricky thing, though, is in order to get good at machine learning, you need to be able to do deploys as fast as humanly possible and repeatedly as humanly possible. Deploying a machine learning model isn’t like deploying a regular code patch or something like that, even if you have a continuous deployment system. The analogy I would use is it’s kind of like someone coming to you and saying, “Hey listen. We’re going to migrate over our database system from MySQL to Postgres, and then next week, we’re going to go back to MySQL again. And then the week after that, we’re going to go back.” And just kind of like back and forth, back and forth. I’m exaggerating slightly, but I’m trying to get you in the right headspace for what we’re talking about here. It’s basically different machine learning models are systems that are complicated and are opaque, that are nominally similar to each other but slightly different in ways that can be critically bad for the overall performance and reliability of your systems. That’s the mentality I want you to be in when it comes to deploying machine learning models. Think about it that way.\\n\\nThe good news is that we all stop worrying and learn to love machine learning, whatever the line is from “Dr. Strangelove,” that kind of thing. You get good at this kind of stuff after a while, and it really … I love doing machine learning, and I love doing it production in particular because it makes everything else better because the standards around how you operate, how you deploy production systems, how you test, how you monitor have to be so high just across the board for regular stuff in order to do it really, really well. Despite all the horrible consequences and the inevitable downtime that the machine learning engineers will cause, I swear, I promise, it’s ultimately worth doing it, and in particular, companies should do it more so I get paid more money to do it. That’s kind of a self-interested argument.\\n\\nIf you like to do monitoring, if you like to do visibility, if you like to do devOps stuff in general and you want to do it at a place that’s done it really, really well, slack.com/jobs. Thank you very much. I appreciate it.\\n\\nShare with:')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('GPUs Mine Astronomical Datasets for Golden Insight Nuggets', 'GPUs Mine Astronomical Datasets For Golden Insight Nuggets\\n\\nJames Cuff\\n\\nAs humankind continues to stare into the dark abyss of deep space in an eternal quest to understand our origins, new computational tools and technologies are needed at unprecedented scales. Gigantic datasets from advanced high resolution telescopes and huge scientific instrumentation installations are overwhelming classical computational and storage techniques.\\n\\nThis is the key issue with exploring the Universe – it is very, very large. Combining advances in machine learning and high speed data storage are starting to provide hitherto unheard of levels of insight that were previously in the realm of pure science fiction. Using computer systems to infer knowledge from observation isn’t new, but the scale at which we need to examine large data today certainly is.\\n\\nBecause the data are so unwieldy and complicated, new methods need to be devised to quickly annotate features that are important, sifting out valuable signals from all of the noise. Nothing is probably more difficult than finding the signal of an “echo” observed from the “sound” a pair of black holes colliding billions of light years away from Earth. This was the premise of the algorithms needed to make sense of the data from the Laser Interferometer Gravitational-Wave Observatory (LIGO) project. They need to filter out a vast array of noise from the real “proton sized” signal, it is an intrinsically computationally intensive process, the main reason being just the sheer size and noiseiness of the captured data.\\n\\nThe Next Platform listened in to Daniel George, a researcher from the National Center for Supercomputing Applications (NCSA) at The University of Illinois, speaking about their latest work they presented at the Meeting of the American Physical Society. We have covered their work in a previous episode of The Interview. It is a fascinating use case for AI in astrophysics.\\n\\nThe team has now published two papers on their methods to use AI to find gravitational waves. First up was the initial paper, “Deep neural networks to enable real-time multimessenger astrophysics” in Physics Review D, in February 2018, with the follow-on Deep Learning for real-time gravitational wave detection and parameter estimation: Results with Advanced LIGO data in Physics Letters B, in March 2018.\\n\\nThe most recent work “Classification and clustering of LIGO data with deep transfer learning they discussed at the APS meeting, which is due to be published in Physics Review D, goes still one a step further to show how they have also used transfer learning to take previously trained networks and then apply them to new data. The team are clearly building an iterative portfolio of methods to apply to gravitational waves.\\n\\n\\n\\nAll LIGO source data are publicly available, and as defined in the LIGO data management plan, the data comes in at about 25 megabytes per second per interferometer, leading to an aggregate collection of about 1.5 petabytes a year. Each individual data slice corresponds to 4,096 seconds of GPS time, each of these specific slices may contain up to 130 MB of potential event data. It’s all encoded as either JSON, HDF5 or plain ASCII data frames. Out of the gate, it sounds like an impossible and inordinately daunting task to even think about how much compute you would need to look at all of this data. It is complex difficult data for sure, fortunately the new AI methods to do this need only focus on a few of these rather specific 4K second slices of data to learn something about how to best find the signal.\\n\\nGeorge et. al. demonstrated that their “Deep Transfer Method” enabled very deep convolutional neural networks to carry out what they call “glitch classification”, even given small and unbalanced training datasets. They significantly reduced the training time, and achieved an accuracy above 98.8%, lowering their previous error rate by over 60%.\\n\\nMore importantly, once they trained via transfer learning on these known classes, they show that the neural networks can also be truncated and used as feature extractors for unsupervised clustering to automatically group together new and unknown classes of glitches and anomalous signals. This novel capability is very important to identify and remove new types of glitches which will occur as the LIGO/Virgo detectors continue to gradually attain their design sensitivity.\\n\\nAs a real-world and practical use case of their artificial intelligence methods, the team has stated “We are trying to do this so fast that LIGO can instantly tell you where to point your telescope.”\\n\\n\\n\\nTo understand more, The Next Platform downloaded two of the carefully annotated 4,096 second strain datasets for one of these events, GW151226 collected at 16384Hz. One from the detector in Hanford, Washington and the other one from Livingston, Louisiana. Fortunately, these datasets represent only a pair of old school CD-ROMs worth of compressed information. Approximately 600 MB of compressed data from each detection station that corresponds to the actual critical event signal. Three specific “gravitational wave events”, GW150914, LVT151012, and GW151226 were taken by George et. al. as example data to craft their method.\\n\\nHere’s the real challenge though, and why this is hard. This is data that has been annotated already. Annotated by bespoke hand-made algorithms, hard enough to craft, design, and build that when combined with phenomenal infrastructure and science teams resulted in a Nobel Prize. However, finding net new equivalent “CDROM sized slice” worth of critical data located somewhere buried inside over a petabyte and a half of interferometer data generated each year isn’t trivial. That’s the hard part, and that’s the part the team are turning to AI for help with.\\n\\nThis challenge is also not unique to advanced astrophysics. Many of us in a number of fields from Chemistry to Social Science are also searching through our vast complex datasets looking for valid and new and interesting signals. Mining for gold is easier, and it makes for a great analogy for what is happening here.\\n\\nSo, that’s the real challenge to solve. These CDROM sized “golden data nuggets” are the now carefully annotated slices that the extended LIGO team use train their new neural network methods. Once they “teach” these networks to essentially know what “golden data nuggets” look like, they can then use “transfer learning” to effectively reuse these computationally intensively trained “metal detectors” to then sift through brand new, and as yet unclassified data soil, continuing in their search for brand new, potentially ever more valuable “golden nuggets.”\\n\\nWhen the LIGO scientists go looking for signal (their equivalent of a “golden data nugget”), they need to see a “false alarm” rate estimated to be less than 1 event per 203,000 years, also that signal has to be detected simultaneously in both Washington and Louisiana. This is clearly the scientific version of “find the needle in a haystack” territory for sure. To showcase their new neural network method, the team used a dataset of twenty-two classes of glitches, carefully curated and labeled by the “Gravity Spy” project using raw data collected during LIGO’s first discovery campaign.\\n\\n\\n\\nCritical to all of this research were GPU accelerators – specifically the Tesla P100s used in the DGX-1 server from Nvidia – which enabled accelerated training of neural networks. They used the Wolfram Language neural network functionality, built a top of the open-source MXNet framework, that in turn uses the cuDNN library for accelerating the training on Nvidia GPUs. ADAM was deployed as the underlying learning algorithm. The significant horsepower of the Blue Waters system, which is also GPU accelerated, was brought to bear for their modeling data and for solving Einstein’s equations via simulation. The group are also looking into generative models GANs (generative adversarial networks) to further reduce the multi-week time taken (even for Blue Waters) for these specific steps.\\n\\nThey aren’t the only astrophysics group in town looking at using AI to reduce the complexity of their data. Brian Nord for example, works in the deep skies lab on gravitational lensing at Fermilab. A gravitational lens is formed between a distant light source and an observer that bends the light from the source as it travels towards the observer. The amount of bending is one of the predictions of Albert Einstein’s general theory of relativity. Researchers can also use “lensing” as a measurement of how much dark matter there is in the universe. Between 1979 and now, only 1,000 such “lenses” have been discovered, but they predict significantly more of these observations will be found. The Dark Energy Survey (DES) uses the dark energy camera, and is predicted to have over 2,000 galaxy scale lenses in this survey. LSST was hinted to find potentially 120,000 of these types of events. This team also didn’t want to reinvent the wheel, and they also turned to AI techniques to help them. Using deep learning they found 8 new confirmed lenses, by looking at 100’s of square degrees of images. They use “ensembles of networks” and then use a simple majority vote to decide on which are lenses and which are not lenses. This specific technique of “jury vote” or consensus prediction is becoming very popular in a variety of fields to improve the quality of AI systems.\\n\\nFinally, one last challenge with the rise of AI in astrophysics, as in most other fields is the ever-present issue of reproducibility. A recent article in Physics Today draws specific attention to exactly this issue of data and algorithm provenance. The headline – A combination of data-churning telescopes and proprietary algorithms has led to a slate of opaque research that has some astronomers concerned – is clearly quite troublesome.\\n\\nHowever, all is not lost. For example, Alice Allen discusses the volunteer-run Astrophysics Source Code Library in this Nature Toolbox Q&A. Allen and her team are actively contacting researchers to make sure that software and algorithms are uploaded to their archive. It will take an entire community to make sure that the new systems we design, especially as they continue to be more and more automatic can continue to provide reproducible science.\\n\\nIn summary, the adoption of artificial intelligence in astrophysics is a wonderful new tool to have in our bag of discovery. For lower precision detective work when mining noisy data for data nuggets, the application of the Tensor Core versus the CPU core paradigm is absolutely appropriate and will continue to drive large scale computational architectures such as “Summit” at Oak Ridge National Laboratory and other massive machines for a good few years to come.')"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('A promenade of PyTorch', \"A Promenade of PyTorch 23 minute read\\n\\nFor the past two years, I’ve been quite heavily invested in TensorFlow, either writing papers about it, giving talks on how to extend its backend or using it for my own deep learning research. As part of this journey, I’ve gotten quite a good sense of both TensorFlow’s strong points as well as weaknesses – or simply architectural decisions – that leave room for competition. That said, I have recently joined the PyTorch team at Facebook AI Research (FAIR), arguably TensorFlow’s biggest competitor to date, and currently much favored in the research community for reasons that will become apparent in subsequent paragraphs.\\n\\nIn this article, I want to provide a sweeping promenade of PyTorch (having given a tour of TensorFlow in another blog post), shedding some light on its raîson d’être and giving an overview of its API.\\n\\nOverview and Philosophy\\n\\nLet’s begin by reviewing what PyTorch is fundamentally, what programming model it imposes on its users and how it fits into the existing deep learning framework ecosystem:\\n\\nPyTorch is, at its core, a Python library enabling GPU-accelerated tensor computation, similar to NumPy. On top of this, PyTorch provides a rich API for neural network applications.\\n\\nPyTorch differentiates itself from other machine learning frameworks in that it does not use static computational graphs – defined once, ahead of time – like TensorFlow, Caffe2 or MXNet. Instead, PyTorch computation graphs are dynamic and defined by run. This means that each invocation of a PyTorch model’s layers defines a new computation graph, on the fly. The creation of this graph is implicit, in the sense that the library takes care of recording the flow of data through the program and linking function calls (nodes) together (via edges) into a computation graph.\\n\\nDynamic vs. Static Graphs\\n\\nLet’s go into more detail about what I mean with static versus dynamic. Generally, in the majority of programming environments, adding two variables x and y representing numbers produces a value containing the result of that addition. For example, in Python:\\n\\nIn [ 1 ]: x = 4 In [ 2 ]: y = 2 In [ 3 ]: x + y Out [ 3 ]: 6\\n\\nIn TensorFlow, however, this is not the case. In TensorFlow, x and y would not be numbers directly, but would instead be handles to graph nodes representing those values, rather than explicitly containing them. Furthermore, and more importantly, adding x and y would not produce the value of the sum of these numbers, but would instead be a handle to a computation graph, which, only when executed, produces that value:\\n\\nIn [ 1 ]: import tensorflow as tf In [ 2 ]: x = tf . constant ( 4 ) In [ 3 ]: y = tf . constant ( 2 ) In [ 4 ]: x + y Out [ 4 ]: < tf . Tensor 'add:0' shape = () dtype = int32 >\\n\\nAs such, when we write TensorFlow code, we are in fact not programming, but metaprogramming – we write a program (our code) that creates a program (the TensorFlow computation graph). Naturally, the first programming model is much simpler than the second. It is much simpler to speak and think in terms of things that are than speak and think in terms of things that represent things that are.\\n\\nPyTorch’s major advantage is that its execution model is much closer to the former than the latter. At its core, PyTorch is simply regular Python, with support for Tensor computation like NumPy, but with added GPU acceleration of Tensor operations and, most importantly, built-in automatic differentiation (AD). Since the majority of contemporary machine learning algorithms rely heavily on linear algebra datatypes (matrices and vectors) and use gradient information to improve their estimates, these two pillars of PyTorch are sufficient to enable arbitrary machine learning workloads.\\n\\nGoing back to the simple showcase above, we can see that programming in PyTorch resembles the natural “feeling” of Python:\\n\\nIn [ 1 ]: import torch In [ 2 ]: x = torch . ones ( 1 ) * 4 In [ 3 ]: y = torch . ones ( 1 ) * 2 In [ 4 ]: x + y Out [ 4 ]: 6 [ torch . FloatTensor of size 1 ]\\n\\nPyTorch deviates from the basic intuition of programming in Python in one particular way: it records the execution of the running program. That is, PyTorch will silently “spy” on the operations you perform on its datatypes and, behind the scenes, construct – again – a computation graph. This computation graph is required for automatic differentiation, as it must walk the chain of operations that produced a value backwards in order to compute derivatives (for reverse mode AD). The way this computation graph, or rather the process of assembling this computation graph, differs notably from TensorFlow or MXNet, is that a new graph is constructed eagerly, on the fly, each time a fragment of code is evaluated. Conversely, in Tensorflow, a computation graph is constructed only once, by the metaprogram that is your code. Furthermore, while PyTorch will actually walk the graph backwards dynamically each time you ask for the derivative of a value, TensorFlow will simply inject additional nodes into the graph that (implicitly) calculate this derivative and are evaluated like all other nodes. This is where the distinction between dynamic and static graphs is most apparent.\\n\\nThe choice of using static or dynamic computation graphs severely impacts the ease of programming in one of these environments. The aspect it influences most severely is control flow. In a static graph environment, control flow must be represented as specialized nodes in the graph. For example, to enable branching, Tensorflow has a tf.cond() operation, which takes three subgraphs as input: a condition subgraph and two subgraphs for the if and else branches of the conditional. Similarly, loops must be represented in TensorFlow graphs as tf.while() operations, taking a condition and body subgraph as input. In a dynamic graph setting, all this is simplified. Since graphs are traced from Python code as it appears during each evaluation, control flow can be implemented natively in the language, using if clauses and while loops as you would for any other program. This turns awkward and unintuitive Tensorflow code:\\n\\nimport tensorflow as tf x = tf . constant ( 2 , shape = [ 2 , 2 ]) w = tf . while_loop ( lambda x : tf . reduce_sum ( x ) < 100 , lambda x : tf . nn . relu ( tf . square ( x )), [ x ])\\n\\ninto natural and intuitive PyTorch code:\\n\\nimport torch.nn from torch.autograd import Variable x = Variable ( torch . ones ([ 2 , 2 ]) * 2 ) while x . sum () < 100 : x = torch . nn . ReLU ()( x ** 2 )\\n\\nThe benefits of dynamic graphs from an ease-of-programming perspective reach far beyond this, of course. Simply being able to inspect intermediate values with print statements (as opposed to tf.Print() nodes) or a debugger is already a big plus. Of course, as much as dynamism can aid programmability, it can also harm performance and makes it more difficult to optimize graphs. The differences and tradeoffs between PyTorch and TensorFlow are thus much the same as the differences and tradeoffs between a dynamic, interpreted language like Python and a static, compiled language like C or C++. The former is easier and faster to work with, while the latter can be transformed into more optimized artifacts. The former is easier to use, while the latter is easier to analyze and (therefore) optimize. It is a tradeoff between flexibility and performance.\\n\\nA Remark on PyTorch’s API\\n\\nA general remark I want to make about PyTorch’s API, especially for neural network computation, compared to other libraries like TensorFlow or MXNet, is that it is quite batteries-included. As someone once remarked to me, TensorFlow’s API never really went beyond the “assembly level”, in the sense that it only ever provided the basic “assembly” instructions required to construct computational graphs (addition, multiplication, pointwise functions etc.), with a basically non-existent “standard library” for the most common kinds of program fragments people would eventually go on to repeat thousands of times. Instead, it relied on the community to build higher level APIs on top of TensorFlow.\\n\\nAnd indeed, the community did build higher level APIs. Unfortunately, however, not just one such API, but about a dozen – concurrently. This means that on a bad day you could read five papers for your research and find the source code of each of these papers to use a different “frontend” to TensorFlow. These APIs typically have quite little in common, such that you would essentially have to learn 5 different frameworks, not just TensorFlow. A few of the most popular such APIs are:\\n\\nPyTorch, on the other hand, already comes with the most common building blocks required for every-day deep learning research. It essentially has a “native” Keras-like API in its torch.nn package, allowing chaining of high-level neural network modules.\\n\\nPyTorch’s Place in the Ecosystem\\n\\nHaving explained how PyTorch differs from static graph frameworks like MXNet, TensorFlow or Theano, let me say that PyTorch is not, in fact, unique in its approach to neural network computation. Before PyTorch, there were already libraries like Chainer or DyNet that provided a similar dynamic graph API. Today, PyTorch is more popular than these alternatives, though.\\n\\nAt Facebook, PyTorch is also not the only framework in use. The majority of our production workloads currently run on Caffe2, which is a static graph framework born out of Caffe. To marry the flexibility PyTorch provides to researchers with the benefits of static graphs for optimized production purposes, Facebook is also developing ONNX, which is intended to be an interchange format between PyTorch, Caffe2 and other libraries like MXNet or CNTK.\\n\\nLastly, a word on history: Before PyTorch, there was Torch – a fairly old (early 2000s) scientific computing library programmed via the Lua language. Torch wraps a C codebase, making it fast and efficient. Fundamentally, PyTorch wraps this same C codebase (albeit with a layer of abstraction in between) while providing a Python API to its users. Let’s talk about this Python API next.\\n\\nUsing PyTorch\\n\\nIn the following paragraphs I will discuss the basic concepts and core components of the PyTorch library, covering its fundamental datatypes, its automatic differentiation machinery, its neural network specific functionality as well as utilities for loading and processing data.\\n\\nTensors\\n\\nThe most fundamental datatype in PyTorch is a tensor . The tensor datatype is very similar, both in importance and function, to NumPy’s ndarray . Furthermore, since PyTorch aims to interoperate reasonably well with NumPy, the API of tensor also resembles (but not equals) that of ndarray . PyTorch tensors can be created with the torch.Tensor constructor, which takes the tensor’s dimensions as input and returns a tensor occupying an uninitialized region of memory:\\n\\nimport torch x = torch . Tensor ( 4 , 4 )\\n\\nIn practice, one will most often want to use one of PyTorch’s functions that return tensors initialized in a certain manner, such as:\\n\\ntorch.rand : values initialized from a random uniform distribution,\\n\\n: values initialized from a random uniform distribution, torch.randn : values initialized from a random normal distribution,\\n\\n: values initialized from a random normal distribution, torch.eye(n) : an $n \\\\times n$ identity matrix,\\n\\n: an $n \\\\times n$ identity matrix, torch.from_numpy(ndarray) : a PyTorch tensor from a NumPy ndarray ,\\n\\n: a PyTorch tensor from a NumPy , torch.linspace(start, end, steps) : a 1-D tensor with steps values spaced linearly between start and end ,\\n\\n: a 1-D tensor with values spaced linearly between and , torch.ones : a tensor with ones everywhere,\\n\\n: a tensor with ones everywhere, torch.zeros_like(other) : a tensor with the same shape as other and zeros everywhere,\\n\\n: a tensor with the same shape as and zeros everywhere, torch.arange(start, end, step) : a 1-D tensor with values filled from a range.\\n\\nSimilar to NumPy’s ndarray , PyTorch tensors provide a very rich API for combination with other tensors as well as in-place mutation. Also like NumPy, unary and binary operations can usually be performed via functions in the torch module, like torch.add(x, y) , or directly via methods on the tensor objects, like x.add(y) . For the usual suspects, operator overloads like x + y exist. Furthermore, many functions have in-place alternatives that will mutate the receiver instance rather than creating a new tensor. These functions have the same name as the out-of-place variants, but are suffixed with an underscore, e.g. x.add_(y) .\\n\\nA selection of operations includes:\\n\\ntorch.add(x, y) : elementwise addition,\\n\\n: elementwise addition, torch.mm(x, y) : matrix multiplication (not matmul or dot ),\\n\\n: matrix multiplication (not or ), torch.mul(x, y) : elementwise multiplication,\\n\\n: elementwise multiplication, torch.exp(x) : elementwise exponential,\\n\\n: elementwise exponential, torch.pow(x, power) : elementwise exponentiation,\\n\\n: elementwise exponentiation, torch.sqrt(x) : elementwise squaring,\\n\\n: elementwise squaring, torch.sqrt_(x) : in-place elementwise squaring,\\n\\n: in-place elementwise squaring, torch.sigmoid(x) : elementwise sigmoid.\\n\\n: elementwise sigmoid. torch.cumprod(x) : product of all values,\\n\\n: product of all values, torch.sum(x) : sum of all values,\\n\\n: sum of all values, torch.std(x) : standard deviation of all values,\\n\\n: standard deviation of all values, torch.mean(x) : mean of all values.\\n\\nTensors support many of the familiar semantics of NumPy ndarray ’s, such as broadcasting, advanced (fancy) indexing ( x[x > 5] ) and elementwise relational operators ( x > y ). PyTorch tensors can also be converted to NumPy ndarray ’s directly via the torch.Tensor.numpy() function. Finally, since the primary improvement of PyTorch tensors over NumPy ndarray s is supposed to be GPU acceleration, there is also a torch.Tensor.cuda() function, which will copy the tensor memory onto a CUDA-capable GPU device, if one is available.\\n\\nAutograd\\n\\nAt the core of most modern machine learning techniques is the calculation of gradients. This is especially true for neural networks, which use the backpropagation algorithm to update weights. For this reason, Pytorch has strong and native support for gradient computation of functions and variables defined within the framework. The technique with which gradients are computed automatically for arbitrary computations is called automatic (sometimes algorithmic) differentiation.\\n\\nFrameworks that employ the static computation graph model implement automatic differentiation by analyzing the graph and adding additional computation nodes to it that compute the gradient of one value with respect to another step by step, piecing together the chain rule by linking these additional gradient nodes with edges.\\n\\nPyTorch, however, does not have static computation graphs and thus does not have the luxury of adding gradient nodes after the rest of the computations have already been defined. Instead, PyTorch must record or trace the flow of values through the program as they occur, thus creating a computation graph dynamically. Once such a graph is recorded, PyTorch has the information required to walk this computation flow backwards and calculate gradients of outputs from inputs.\\n\\nThe PyTorch Tensor currently does not have sufficient machinery to participate in automatic differentiation. For a tensor to be “recordable”, it must be wrapped with torch.autograd.Variable . The Variable class provides almost the same API as Tensor , but augments it with the ability to interplay with torch.autograd.Function in order to be differentiated automatically. More precisely, a Variable records the history of operations on a Tensor .\\n\\nUsage of torch.autograd.Variable is very simple. One needs only to pass it a Tensor and inform torch whether or not this variable requires recording of gradients:\\n\\nx = torch . autograd . Variable ( torch . ones ( 4 , 4 ), requires_grad = True )\\n\\nThe requires_grad function may need to be False in the case of data inputs or labels, for example, since those are usually not differentiated. However, they still need to be Variable s to be usable in automatic differentiation. Note that requires_grad defaults to False , thus must be set to True for learnable parameters.\\n\\nTo compute gradients and perform automatic differentiation, one calls the backward() function on a Variable . This will compute the gradient of that tensor with respect to the leaves of the computation graph (all inputs that influenced that value). These gradients are then collected in the Variable class’ grad member:\\n\\nIn [ 1 ]: import torch In [ 2 ]: from torch.autograd import Variable In [ 3 ]: x = Variable ( torch . ones ( 1 , 5 )) In [ 4 ]: w = Variable ( torch . randn ( 5 , 1 ), requires_grad = True ) In [ 5 ]: b = Variable ( torch . randn ( 1 ), requires_grad = True ) In [ 6 ]: y = x . mm ( w ) + b # mm = matrix multiply In [ 7 ]: y . backward () # perform automatic differentiation In [ 8 ]: w . grad Out [ 8 ]: Variable containing : 1 1 1 1 1 [ torch . FloatTensor of size ( 5 , 1 )] In [ 9 ]: b . grad Out [ 9 ]: Variable containing : 1 [ torch . FloatTensor of size ( 1 ,)] In [ 10 ]: x . grad None\\n\\nSince every Variable except for inputs is the result of an operation, each Variable has an associated grad_fn , which is the torch.autograd.Function that is used to compute the backward step. For inputs it is None :\\n\\nIn [ 11 ]: y . grad_fn Out [ 11 ]: < AddBackward1 at 0x1077cef60 > In [ 12 ]: x . grad_fn None\\n\\ntorch.nn\\n\\nThe torch.nn module exposes neural-network specific functionality to PyTorch users. One of its most important members is torch.nn.Module , which represents a reusable block of operations and associated (trainable) parameters, most commonly used for neural network layers. Modules may contain other modules and implicitly get a backward() function for backpropagation. An example of a module is torch.nn.Linear() , which represents a linear (dense/fully-connected) layer (i.e. an affine transformation $Wx + b$):\\n\\nIn [ 1 ]: import torch In [ 2 ]: from torch import nn In [ 3 ]: from torch.autograd import Variable In [ 4 ]: x = Variable ( torch . ones ( 5 , 5 )) In [ 5 ]: x Out [ 5 ]: Variable containing : 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 [ torch . FloatTensor of size ( 5 , 5 )] In [ 6 ]: linear = nn . Linear ( 5 , 1 ) In [ 7 ]: linear ( x ) Out [ 7 ]: Variable containing : 0.3324 0.3324 0.3324 0.3324 0.3324 [ torch . FloatTensor of size ( 5 , 1 )]\\n\\nDuring training, one will often call backward() on a module to compute gradients for its variables. Since calling backward() sets the grad member of Variable s, there is also a nn.Module.zero_grad() method that will reset the grad member of all Variable s to zero. Your training loop will commonly call zero_grad() at the start, or just before calling backward() , to reset the gradients for the next optimization step.\\n\\nWhen writing your own neural network models, you will often end up having to write your own module subclasses to encapsulate common functionality that you want to integrate with PyTorch. You can do this very easily, by deriving a class from torch.nn.Module and giving it a forward method. For example, here is a module I wrote for one of my models that adds gaussian noise to its input:\\n\\nclass AddNoise ( torch . nn . Module ): def __init__ ( self , mean = 0.0 , stddev = 0.1 ): super ( AddNoise , self ) . __init__ () self . mean = mean self . stddev = stddev def forward ( self , input ): noise = input . clone () . normal_ ( self . mean , self . stddev ) return input + noise\\n\\nTo connect or chain modules into full-fledged models, you can use the torch.nn.Sequential() container, to which you pass a sequence of modules and which will in turn act as a module of its own, evaluating the modules you passed to it sequentially on each invocation. For example:\\n\\nIn [ 1 ]: import torch In [ 2 ]: from torch import nn In [ 3 ]: from torch.autograd import Variable In [ 4 ]: model = nn . Sequential ( ... : nn . Conv2d ( 1 , 20 , 5 ), ... : nn . ReLU (), ... : nn . Conv2d ( 20 , 64 , 5 ), ... : nn . ReLU ()) ... : In [ 5 ]: image = Variable ( torch . rand ( 1 , 1 , 32 , 32 )) In [ 6 ]: model ( image ) Out [ 6 ]: Variable containing : ( 0 , 0 , . , . ) = 0.0026 0.0685 0.0000 ... 0.0000 0.1864 0.0413 0.0000 0.0979 0.0119 ... 0.1637 0.0618 0.0000 0.0000 0.0000 0.0000 ... 0.1289 0.1293 0.0000 ... ⋱ ... 0.1006 0.1270 0.0723 ... 0.0000 0.1026 0.0000 0.0000 0.0000 0.0574 ... 0.1491 0.0000 0.0191 0.0150 0.0321 0.0000 ... 0.0204 0.0146 0.1724\\n\\nLosses\\n\\ntorch.nn also provides a number of loss functions that are naturally important to machine learning applications. Examples of loss functions include:\\n\\ntorch.nn.MSELoss : a mean squared error loss,\\n\\n: a mean squared error loss, torch.nn.BCELoss : a binary cross entropy loss,\\n\\n: a binary cross entropy loss, torch.nn.KLDivLoss : a Kullback-Leibler divergence loss.\\n\\nIn PyTorch jargon, loss functions are often called criterions. Criterions are really just simple modules that you can parameterize upon construction and then use as plain functions from there on:\\n\\nIn [ 1 ]: import torch In [ 2 ]: import torch.nn In [ 3 ]: from torch.autograd import Variable In [ 4 ]: x = Variable ( torch . randn ( 10 , 3 )) In [ 5 ]: y = Variable ( torch . ones ( 10 ) . type ( torch . LongTensor )) In [ 6 ]: weights = Variable ( torch . Tensor ([ 0.2 , 0.2 , 0.6 ])) In [ 7 ]: loss_function = torch . nn . CrossEntropyLoss ( weight = weights ) In [ 8 ]: loss_value = loss_function ( x , y ) Out [ 8 ]: Variable containing : 1.2380 [ torch . FloatTensor of size ( 1 ,)]\\n\\nOptimizers\\n\\nAfter neural network building blocks ( nn.Module ) and loss functions, the last piece of the puzzle is an optimizer to run (a variant of) stochastic gradient descent. For this, PyTorch provides the torch.optim package, which defines a number of common optimization algorithms, such as:\\n\\nEach of these optimizers are constructed with a list of parameter objects, usually retrieved via the parameters() method of a nn.Module subclass, that determine which values are updated by the optimizer. Besides this parameter list, the optimizers each take a certain number of additional arguments to configure their optimization strategy. For example:\\n\\nIn [ 1 ]: import torch In [ 2 ]: import torch.optim In [ 3 ]: from torch.autograd import Variable In [ 4 ]: x = Variable ( torch . randn ( 5 , 5 )) In [ 5 ]: y = Variable ( torch . randn ( 5 , 5 ), requires_grad = True ) In [ 6 ]: z = x . mm ( y ) . mean () # Perform an operation In [ 7 ]: opt = torch . optim . Adam ([ y ], lr = 2e-4 , betas = ( 0.5 , 0.999 )) In [ 8 ]: z . backward () # Calculate gradients In [ 9 ]: y . data Out [ 9 ]: - 0.4109 - 0.0521 0.1481 1.9327 1.5276 - 1.2396 0.0819 - 1.3986 - 0.0576 1.9694 0.6252 0.7571 - 2.2882 - 0.1773 1.4825 0.2634 - 2.1945 - 2.0998 0.7056 1.6744 1.5266 1.7088 0.7706 - 0.7874 - 0.0161 [ torch . FloatTensor of size 5 x5 ] In [ 10 ]: opt . step () # Update y according to Adam's gradient update rules In [ 11 ]: y . data Out [ 11 ]: - 0.4107 - 0.0519 0.1483 1.9329 1.5278 - 1.2398 0.0817 - 1.3988 - 0.0578 1.9692 0.6250 0.7569 - 2.2884 - 0.1775 1.4823 0.2636 - 2.1943 - 2.0996 0.7058 1.6746 1.5264 1.7086 0.7704 - 0.7876 - 0.0163 [ torch . FloatTensor of size 5 x5 ]\\n\\nData Loading\\n\\nFor convenience, PyTorch provides a number of utilities to load, preprocess and interact with datasets. These helper classes and functions are found in the torch.utils.data module. The two major concepts here are:\\n\\nA Dataset , which encapsulates a source of data, A DataLoader , which is responsible for loading a dataset, possibly in parallel.\\n\\nNew datasets are created by subclassing the torch.utils.data.Dataset class and overriding the __len__ method to return the number of samples in the dataset and the __getitem__ method to access a single value at a certain index. For example, this would be a simple dataset encapsulating a range of integers:\\n\\nimport math class RangeDataset ( torch . utils . data . Dataset ): def __init__ ( self , start , end , step = 1 ): self . start = start self . end = end self . step = step def __len__ ( self , length ): return math . ceil (( self . end - self . start ) / self . step ) def __getitem__ ( self , index ): value = self . start + index * self . step assert value < self . end return value\\n\\nInside __init__ we would usually configure some paths or change the set of samples ultimately returned. In __len__ , we specify the upper bound for the index with which __getitem__ may be called, and in __getitem__ we return the actual sample, which could be an image or an audio snippet.\\n\\nTo iterate over the dataset we could, in theory, simply have a for i in range loop and access samples via __getitem__ . However, it would be much more convenient if the dataset implemented the iterator protocol itself, so we could simply loop over samples with for sample in dataset . Fortunately, this functionality is provided by the DataLoader class. A DataLoader object takes a dataset and a number of options that configure the way samples are retrieved. For example, it is possible to load samples in parallel, using multiple processes. For this, the DataLoader constructor takes a num_workers argument. Note that DataLoader s always return batches, whose size is set with the batch_size parameter. Here is a simple example:\\n\\ndataset = RangeDataset ( 0 , 10 ) data_loader = torch . utils . data . DataLoader ( dataset , batch_size = 4 , shuffle = True , num_workers = 2 , drop_last = True ) for i , batch in enumerate ( data_loader ): print ( i , batch )\\n\\nHere, we set batch_size to 4 , so returned tensors will contain exactly four values. By passing shuffle=True , the index sequence with which data is accessed is permuted, such that individual samples will be returned in random order. We also passed drop_last=True , so that if the number of samples left for the final batch of the dataset is less than the specified batch_size , that batch is not returned. This ensures that all batches have the same number of elements, which may be an invariant that we need. Finally, we specified num_workers to be two, meaning data will be fetched in parallel by two processes. Once the DataLoader has been created, iterating over the dataset and thereby retrieving batches is simple and natural.\\n\\nA final interesting observation I want to share is that the DataLoader actually has some reasonably sophisticated logic to determine how to collate individual samples returned from your dataset’s __getitem__ method into a batch, as returned by the DataLoader during iteration. For example, if __getitem__ returns a dictionary, the DataLoader will aggregate the values of that dictionary into a single mapping for the entire batch, using the same keys. This means that if the Dataset ’s __getitem__ returns a dict(example=example, label=label) , then the batch returned by the DataLoader will return something like dict(example=[example1, example2, ...], label=[label1, label2, ...]) , i.e. unpacking the values of indidvidual samples and re-packing them into a single key for the batch’s dictionary. To override this behavior, you can pass a function argument for the collate_fn parameter to the DataLoader object.\\n\\nNote that the torchvision package already provides a number of datasets, such as torchvision.datasets.CIFAR10 , ready to use. The same is true for torchaudio and torchtext packages.\\n\\nOutro\\n\\nAt this point, you should be equipped with an understanding of both PyTorch’s philosophy as well as its basic API, and are thus ready to go forth and conquer (PyTorch models). If this is your first exposure to PyTorch but you have experience with other deep learning frameworks, I would recommend taking your favorite neural network model and re-implementing it in PyTorch. For example, I re-wrote a TensorFlow implementation of the LSGAN (least-squares GAN) architecture I had lying around in PyTorch, and thus learnt the crux of using it. Further articles that may be of interest can be found here and here.\\n\\nSumming up, PyTorch is a very exciting player in the field of deep learning frameworks, exploiting its unique niche of being a research-first library, while still providing the performance necessary to get the job done. Its dynamic graph computation model is an exciting contrast to static graph frameworks like TensorFlow or MXNet, that many will find more suitable for performing their experiments. I sure look forward to working on it.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TensorFlow Tutorials with YouTube Videos', 'TensorFlow Tutorials\\n\\nOriginal repository on GitHub\\n\\nOriginal author is Magnus Erik Hvass Pedersen\\n\\nIntroduction\\n\\nThese tutorials are intended for beginners in Deep Learning and TensorFlow.\\n\\nEach tutorial covers a single topic.\\n\\nThe source-code is well-documented.\\n\\nThere is a YouTube video for each tutorial.\\n\\nTutorials\\n\\nSimple Linear Model (Notebook) Convolutional Neural Network (Notebook) Pretty Tensor (Notebook)\\n\\n3-B. Layers API (Notebook)\\n\\n3-C. Keras API (Notebook)\\n\\nSave & Restore (Notebook) Ensemble Learning (Notebook) CIFAR-10 (Notebook) Inception Model (Notebook) Transfer Learning (Notebook) Video Data (Notebook) Fine-Tuning (Notebook) Adversarial Examples (Notebook) Adversarial Noise for MNIST (Notebook) Visual Analysis (Notebook)\\n\\n13-B. Visual Analysis for MNIST (Notebook)\\n\\nDeepDream (Notebook) Style Transfer (Notebook) Reinforcement Learning (Notebook) Estimator API (Notebook) TFRecords & Dataset API (Notebook) Hyper-Parameter Optimization (Notebook) Natural Language Processing (Notebook) Machine Translation (Notebook) Image Captioning (Notebook) Time-Series Prediction (Notebook)\\n\\nVideos\\n\\nThese tutorials are also available as YouTube videos.\\n\\nTranslations\\n\\nThese tutorials have been translated to the following languages:\\n\\nYou can help by translating the remaining tutorials or reviewing the ones that have already been translated. You can also help by translating to other languages.\\n\\nForks\\n\\nSee the selected list of forks for community modifications to these tutorials.\\n\\nDownloading\\n\\nSome of the Python Notebooks use source-code located in different files to allow for easy re-use across multiple tutorials. It is therefore recommended that you download the whole repository from GitHub, instead of just downloading the individual Python Notebooks.\\n\\nGit\\n\\nThe easiest way to download and install these tutorials is by using git from the command-line:\\n\\ngit clone https://github.com/Hvass-Labs/TensorFlow-Tutorials.git\\n\\nThis will create the directory TensorFlow-Tutorials and download all the files to it.\\n\\nThis also makes it easy to update the tutorials, simply by executing this command inside that directory:\\n\\ngit pull\\n\\nYou can also download the contents of the GitHub repository as a Zip-file and extract it manually.\\n\\nInstallation\\n\\nThere are different ways of installing and running TensorFlow. This section describes how I did it for these tutorials. You may want to do it differently and you can search the internet for instructions.\\n\\nIf you are new to using Python and Linux, etc. then this may be challenging to get working and you may need to do internet searches for error-messages, etc. It will get easier with practice.\\n\\nPython Version 3.5 or Later\\n\\nThese tutorials were developed on Linux using Python 3.5 / 3.6 (the Anaconda distribution) and PyCharm.\\n\\nThere are reports that Python 2.7 gives error messages with these tutorials. Please make sure you are using Python 3.5 or later!\\n\\nEnvironment\\n\\nAfter installing Anaconda, you should create a conda environment so you do not destroy your main installation in case you make a mistake somewhere:\\n\\nconda create --name tf python=3\\n\\nNow you can switch to the new environment by running the following (on Linux):\\n\\nsource activate tf\\n\\nRequired Packages\\n\\nThe tutorials require several Python packages to be installed. The packages are listed in requirements.txt First you need to edit this file and select whether you want to install the CPU or GPU version of TensorFlow.\\n\\nTo install the required Python packages and dependencies you first have to activate the conda-environment as described above, and then you run the following command in a terminal:\\n\\npip install -r requirements.txt\\n\\nNote that the GPU-version of TensorFlow also requires the installation of various NVIDIA drivers, which is not described here.\\n\\nTesting\\n\\nYou should now be able to run the tutorials in the Python Notebooks:\\n\\ncd ~/development/TensorFlow-Tutorials/ # Your installation directory. jupyter notebook\\n\\nThis should start a web-browser that shows the list of tutorials. Click on a tutorial to load it.\\n\\nOlder Versions\\n\\nSometimes the source-code has changed from that shown in the YouTube videos. This may be due to bug-fixes, improvements, or because code-sections are moved to separate files for easy re-use.\\n\\nIf you want to see the exact versions of the source-code that were used in the YouTube videos, then you can browse the history of commits to the GitHub repository.\\n\\nLicense (MIT)\\n\\nThese tutorials and source-code are published under the MIT License which allows very broad use for both academic and commercial purposes.\\n\\nA few of the images used for demonstration purposes may be under copyright. These images are included under the \"fair usage\" laws.\\n\\nYou are very welcome to modify these tutorials and use them in your own projects. Please keep a link to the original repository.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('WRLD and Udacity Create 3D Simulator for Flying and Autonomous Cars', 'WRLD is proud to announce a partnership with online learning platform Udacity to power Udacity Universe—a large shared simulation environment where students develop and coordinate self-driving and self-flying fleets to tackle the world’s most complex autonomous transportation challenges at world scale. Built upon WRLD’s cloud-based 3D digital twin technology, this virtual world provides a critical environment to solve autonomous transportation challenges through multi-player collaboration. We’re excited to see what innovative solutions are built by tomorrow’s pioneering engineers in systems for fleet management and smart city planning.\\n\\nThe announcement took place at Udacity Intersect in Mountain View, California, where Vishal Makhijani, CEO of Udacity, showcased how the WRLD platform would serve as an interactive and virtualized environment where students can test and discover how these systems work in an efficient, reliable and safe manner. He explained how students will now be using Udacity Universe built with WRLD as their visual interface for massive simulations. Additional Udacity Universe partners include Unity Technologies, Zipline, and the Dubai Future Foundation.\\n\\nThis announcement accompanies the launch of four new nanodegrees programs added at Udacity to allow anyone to gain expertise in areas like Self-Driving Car, Robotics, Deep Learning, and Artificial Intelligence. WRLD is excited to play an essential role in this partnership and is keen to support the student community in developing the future of autonomous and flying vehicle technology.\\n\\nIntroducing Udacity Universe! from Udacity on Vimeo.\\n\\nCheaper, Safer, and Faster Simulations on a Global Scale\\n\\nUdacity Universe represents a massive breakthrough for autonomous and flying vehicles. It enables simulations of thousands of driven or flown miles faster than any real-world scenario. The Universe provides a much more affordable and safer testing alternative that will allow hundreds of thousands vehicles and journeys to be deployed quicker and simultaneously in a virtual world, rather than real cars and human operators in the real world. According to Rand, proper autonomous vehicle testing requires 500 billion to one trillion miles to get enough data to power vehicles safely. Simulations will play an essential role in the future, leveraging the necessary intelligence to develop these advanced models and systems.\\n\\nDespite recent self-driving vehicle deployments, it is still unclear if the technology is ready to take on the real world as regulations and policies are hindering development due to reasonable public safety concerns. The launch of Udacity Universe serves as a safe environment and training ground for students looking to solve the exciting but significant challenges of automated transport and flying vehicles.\\n\\nMoving Beyond Single-Vehicle Thinking for Autonomous Transportation\\n\\nImage Credit: Udacity\\n\\nBy enrolling in Udacity Universe, engineers will gain a greater understanding of how vehicles interact with systems at different levels - on the ground, and in the skies - as fleets of cars, people, buildings, and cities come together in one simulated environment.\\n\\nWRLD’s cloud-based platform is crucial to support this level of understanding as the platform is designed to reproduce any simulation across mobile, desktop, and VR devices. Engineers can take an empathetic perspective of these simulations. They can put on a VR headset and immerse themselves in a passenger or pedestrian view of an autonomous transportation system traveling around the Udacity Universe. Engineers also can get a global perspective at 10,000 feet views to observe how their simulation interacts with other vehicles, as each car follows its own set of rules.\\n\\nWhy Udacity Partnered with WRLD\\n\\nThe WRLD platform started as a 3D map engine born from gaming technology, based on real-world data and has evolved into a virtual world for simulations, location-based experiences, smart buildings, and smart city planning. The value of the WRLD platform assists the autonomous transportation development and flying car technology.\\n\\nTo deploy a virtual world of the scale of Udacity Universe would require a robust solution ready to handle a challenge of such complexity. WRLD is proud to provide a ready-built 3D digital twin—a virtual representation of cities and rural environments. The platform serves as a natural engine for Udacity Universe—a world where everything is possible.\\n\\nAccording to Christian Plagemann, VP of Learning at Udacity, many of the transportation options available will be performed by an autonomous vehicle in just five years. There is plenty to be excited about as autonomous cars and flying cars become less of a dystopian reality, and start to take part of our daily lives in a safe and proven manner.\\n\\nReady to build your very own virtual world? Join thousands of developers and __sign up_ for a free account with WRLD today!_')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('One data scientist on the hype around artificial intelligence (2017)', 'The Curse of AI FOMO\\n\\nExpectations about AI are extremely high. We are constantly hearing that “AI will change the world.” The big data hype wave felt very similar — big data was the new business intelligence, big data would solve everything. But the harsh reality is that the majority of companies were just not ready to handle, store, or use massive amounts of data. I saw many companies hire data scientists first, without thinking what those first data projects should be, or the larger goal of integrating how to integrate that data in a valuable way.\\n\\nIn those early years of big data, the outcome was always less than perfect. Most of the work ended up in powerpoint presentations without ever going into production because most teams simply did not have the right infrastructure or culture to maintain them. Rather, many companies just bought into the trend and invested in big data because they had the fear of missing out. Behavioral economist Dan Ariely also succinctly described the big data hype thusly:\\n\\nThe same thing applies to AI now. And before we go any further, let’s clear one thing up: when people in business say “AI” they really mean machine learning — which is the outcome we were all envisioning when we got excited about big data all those years ago.\\n\\nThe fear of missing out on AI is so high that everyone wants to be part of this wave even though they are not ready for this. I’ve seen companies who don’t have the basis for even the simplest machine learning algorithm — let alone the right people and culture — but they believe if they pour in enough money, they will get their AI transformation. They spend millions on tools, and yet they don’t have an infrastructure that can handle complex algorithms or deploy changes to it in a fast, iterative manner.\\n\\nAnd some opportunists are ready to capitalize on this FOMO. IBM’s Watson is the best example. It promises to solve everything from winning game shows like Jeopardy to bolster applications that help with cancer treatments. IBM wants you to think Watson can solve every problem that you can imagine because it uses deep learning and has a cool name.\\n\\n“IBM Watson is the Donald Trump of the AI industry — outlandish claims that aren’t backed by credible data.” — Gizmodo\\n\\nBut we all know that Watson is a big marketing gimmick. Moreover, we have too many so-called experts that say AI is the next industrial revolution. A new digital frontier. They paint a picture where AI machines can learn by themselves. And cast every problem as a reinforcement learning problem. On the other hand, we have people that think we should be fearful of AIs. In fact there is an active discussion whether AI is “the biggest risk we face as civilisation.” But that concern is about artificial general intelligence (AGI), not machine learning. And as you can see from the video below, which shows a soccer game from the recent RoboCup 2017, here’s how dangerous AGI is at the moment:\\n\\nCurrent state of artificial general intelligence.\\n\\nAfter the AI Hype Storm\\n\\nAs you can see, the expectation for AI is exploding. But just like it happened for big data, you should expect this bubble to burst. And after that we’ll get a new trend. Will it be quantum computing? I don’t know yet, but I hope that after the AI hype storm there will be some calm.\\n\\nI anticipate that after this passes, we can start to do the right thing — focusing on using machine learning to build things that are meaningful and realistic. Like driverless cars, which will go on to shape the future of mobility for generations. We can work on improving medical diagnosis, education retention, and much more. Those are just some examples; there are many ways machine learning can help, and many will be realistic applications that improve our everyday lives. We just need the right people, culture, and infrastructure to execute. Instead of surfing the wave which will just take us back to the beach we’ve always known, we can sail towards new ground, a promising shore.\\n\\nI hope you liked this post. Give me a 👏🏻 if you do so that others can find it too. Also share it with your friends. Follow me here on Medium Dat Tran or on twitter @datitran to stay up-to-date with my work.\\n\\nHere are some further readings for those that want to dive a little deeper:')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('John Carmack – Programming Neural Net Library on OpenBSD', 'After a several year gap, I finally took another week-long programming retreat, where I could work in hermit mode, away from the normal press of work. My wife has been generously offering it to me the last few years, but I’m generally bad at taking vacations from work.\\n\\nAs a change of pace from my current Oculus work, I wanted to write some from-scratch-in -C++ neural network implementations , and I wanted to do it with a strictly base OpenBSD system. Someone remarked that is a pretty random pairing, but it worked out ok.\\n\\nDespite not having actually used it, I have always been fond of the idea of OpenBSD — a relatively minimal and opinionated system with a cohesive vision and an emphasis on quality and craftsmanship. Linux is a lot of things, but cohesive isn’t one of them.\\n\\nI’m not a Unix geek. I get around ok, but I am most comfortable developing in Visual Studio on Windows. I thought a week of full immersion work in the old school Unix style would be interesting, even if it meant working at a slower pace. It was sort of an adventure in retro computing — this was fvwm and vi. Not vim, actual BSD vi.\\n\\nIn the end, I didn’t really explore the system all that much, with 95% of my time in just the basic vi / make / gdb operations. I appreciated the good man pages, as I tried to do everything within the self contained system, without resorting to internet searches. Seeing references to 30+ year old things like Tektronix terminals was amusing.\\n\\nI was a little surprised that the C++ support wasn’t very good. G++ didn’t support C++11, and LLVM C++ didn’t play nicely with gdb. Gdb crashed on me a lot as well, I suspect due to C++ issues. I know you can get more recent versions through ports, but I stuck with using the base system.\\n\\nIn hindsight, I should have just gone full retro and done everything in ANSI C. I do have plenty of days where, like many older programmers, I think “Maybe C++ isn’t as much of a net positive as we assume...”. There is still much that I like, but it isn’t a hardship for me to build small projects in plain C.\\n\\nMaybe next time I do this I will try to go full emacs, another major culture that I don’t have much exposure to.\\n\\nI have a decent overview understanding of most machine learning algorithms, and I have done some linear classifier and decision tree work, but for some reason I have avoided neural networks. On some level, I suspect that Deep Learning being so trendy tweaked a little bit of contrarian in me, and I still have a little bit of a reflexive bias against “throw everything at the NN and let it sort it out!”\\n\\nIn the spirit of my retro theme, I had printed out several of Yann LeCun’s old papers and was considering doing everything completely off line, as if I was actually in a mountain cabin somewhere, but I wound up watching a lot of the Stanford CS231N lectures on YouTube, and found them really valuable. Watching lecture videos is something that I very rarely do — it is normally hard for me to feel the time is justified, but on retreat it was great!\\n\\nI don’t think I have anything particularly insightful to add about neural networks, but it was a very productive week for me, solidifying “book knowledge” into real experience.\\n\\nI used a common pattern for me: get first results with hacky code, then write a brand new and clean implementation with the lessons learned, so they both exist and can be cross checked.\\n\\nI initially got backprop wrong both times, comparison with numerical differentiation was critical! It is interesting that things still train even when various parts are pretty wrong — as long as the sign is right most of the time, progress is often made.\\n\\nI was pretty happy with my multi-layer neural net code; it wound up in a form that I can just drop it into future efforts. Yes, for anything serious I should use an established library, but there are a lot of times when just having a single .cpp and .h file that you wrote ever line of is convenient.\\n\\nMy conv net code just got to the hacky but working phase, I could have used another day or two to make a clean and flexible implementation.\\n\\nOne thing I found interesting was that when testing on MNIST with my initial NN before adding any convolutions, I was getting significantly better results than the non-convolution al NN reported for comparison in LeCun ‘98 — right around 2% error on the test set with a single 100 node hidden layer, versus 3% for both wider and deeper nets back then. I attribute this to the modern best practices —ReLU, Softmax, and better initialization.\\n\\nThis is one of the most fascinating things about NN work — it is all so simple, and the breakthrough advances are often things that can be expressed with just a few lines of code. It feels like there are some similarities with ray tracing in the graphics world, where you can implement a physically based light transport ray tracer quite quickly, and produce state of the art images if you have the data and enough runtime patience.\\n\\nI got a much better gut-level understanding of overtraining / generalization / regularization by exploring a bunch of training parameters. On the last night before I had to head home, I froze the architecture and just played with hyperparameters. “Training!” Is definitely worse than “Compiling!” for staying focused.\\n\\nNow I get to keep my eyes open for a work opportunity to use the new skills!\\n\\nI am dreading what my email and workspace are going to look like when I get into the office tomorrow.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Family fun with deepfakes. Or how I got my wife onto the Tonight Show', '[Also on Medium]\\n\\n[ Update 3 Feb 2018: added two new creations at the bottom of this post. Last one turned out really well ]\\n\\nI’ve first heard of deepfakes a good week ago. Thanks Twitter. Thanks Tim Soret.\\n\\nThere’s something called Deepfakes on internet, and it’s the most cyberpunk shit you can imagine. Machine learning used to swap porn actresses faces with hollywood stars. Obviously NFSW. — timsoret 👁 (@timsoret) January 23, 2018\\n\\nYes, it’s pretty damn cyberpunk. But from a superficial point of view, /r/deepfakes (extremely NSFW! You have been warned) consists of people using an app created by user “deepfakes” to create fake celebrity porn.\\n\\nThis has caused a shitstorm on the Internet, media discussing the legality of it all, websites taking down the deepfake creations, and people panicking as they realise AI is going to screw us all up (newsflash: it’s already been happening in much less obvious ways). And meanwhile, Nicolas Cage is taking over Hollywood.\\n\\nWhile everyone’s debating whether this is good or bad, I just had to find out more. First thing that came into my mind? How can I apply this to everyone I know (in a non-porn way, in case you wondered).\\n\\nHow does it work?\\n\\nThe deepfakes app is a deep learning algorithm that learns how to reconstruct faces. Give it a bunch of pictures, let it run a few hours, and it spits out fuzzy copies of those images. Do note, it doesn’t create a copy. It learns what a face looks like, in different expressions, and is able to output that face solely based on that. There’s a detailed explanation on Reddit but let me try and dumb it down.\\n\\nThink of it like this: imagine if you could stare at someone for 12 hours straight, observing all their expressions and absorbing that into your brain. Then that person asks you to sketch his face on paper, smiling, crying, any expression you’ve observed. What do you do? You immediately generate a photographic quality sketch on paper, from the mind! (using a pencil)\\n\\nIt’s insane!\\n\\nWhile that’s pretty cool, it only gets better. See that “encoder” part? The FakeApp uses one encoder for all faces. It’s the decoder that’s kept face specific. And here comes the really cool part. Let it learn two faces, and things become more interesting.\\n\\nRight, now see how this works. The encoder takes an image of a face, let’s it run through its “brain”, and the decoder spits it out again. In the example above, it can do so with faces of Anne Hathaway, and Elke, my wife. Ok, so far so good. But now let’s feed it a picture of Anne, but use the decoder that generates Elke’s face!\\n\\nYou just created a new photo of Elke. A photo that never existed, in the same angle, the same expression, as Anne! Amazing!\\n\\nFamily fun\\n\\nSure, putting celebrities’ faces on your favorite porn stars is an interesting use case. But we can leverage these celebrities for other things, such as inserting your friends and family into blockbuster movies and shows!\\n\\nFor the best result, you must first find an actor/actress that has a similar head shape as the person you wish to insert. In case of Elke (my wife) I accidentally noticed, while watching the Dark Knight Rises, that Anne Hathaway might be a good fit. I guess you know Anne, so here’s Elke:\\n\\nAll I needed was about 850 photos of Elke, a few 1000 of Anne, a lot of computing time, et voila: Elke’s on the Tonight Show starring Jimmy Fallon.\\n\\nBonus effect: now we know what Elke looks like with short hair :D\\n\\nHere’s a little comparison gif:\\n\\nI personally think it’s fun, can be innocent, and even makes for a nice surprise/gift. Remember, any tool can be used for evil. And as long as we’re not banning guns, this should not be a high priority, amirite?\\n\\nThere’s so much you can do with this technology. You know those dumb emails people send around where they replaced dancing elves heads with their own, or even worse, yours? Well, now you can put your best friend into his favourite movie: have her dance with Patrick Swayze and have the time of her life, or have an alien burst out of his stomach. It’s all within your reach now!\\n\\nBeyond just pure fun, I can only imagine how people will start turning this tech into business ideas. Fashion will be huge (what would I look like with this kind of hair, this kind of dress…), fitness could be interesting (do I look good with muscles, will I really look better skinny), travel (this is you standing on a beach is going to be quite convincing). It’ll bring advertising to a whole new level. No need to imagine what if, they’ll tell you what your “better” life will look like! And it’ll be hard to get that picture out of your head…\\n\\nUpdate: in the mean time, I’ve created two more. Elke’s a huge fan of Steve Carell, and I suddenly realized Anne Hathaway co-starts with him in Get Smart. First attempt was okay:\\n\\nThen I wanted to try this one (original video):\\n\\nAnd I think it turned out great:\\n\\nAlso published on Medium.\\n\\nShare this: Twitter\\n\\nFacebook\\n\\nLinkedIn\\n\\nReddit\\n\\nGoogle\\n\\nTumblr\\n\\nPocket\\n\\nPinterest\\n\\nEmail\\n\\n')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('OpenPose: Real-time multi-person body, face, and hands estimation', 'Linux\\n\\nOpenPose represents the first real-time multi-person system to jointly detect human body, hand, and facial keypoints (in total 130 keypoints) on single images.\\n\\nFunctionality:\\n\\nReal-time multi-person keypoint detection . 15 or 18-keypoint body estimation . Running time invariant to number of detected people . 2x21-keypoint hand estimation. Currently, running time depends on number of detected people . 70-keypoint face estimation. Currently, running time depends on number of detected people .\\n\\n. Input : Image, video, webcam, and IP camera. Included C++ demos to add your custom input.\\n\\n: Image, video, webcam, and IP camera. Included C++ demos to add your custom input. Output : Basic image + keypoint display/saving (PNG, JPG, AVI, ...), keypoint saving (JSON, XML, YML, ...), and/or keypoints as array class.\\n\\n: Basic image + keypoint display/saving (PNG, JPG, AVI, ...), keypoint saving (JSON, XML, YML, ...), and/or keypoints as array class. Available: command-line demo, C++ wrapper, and C++ API.\\n\\nOS: Ubuntu (14, 16), Windows (8, 10), Nvidia TX2.\\n\\nLatest Features\\n\\nContents\\n\\nResults\\n\\n3-D Reconstruction Module\\n\\nBody Estimation\\n\\nBody, Face, and Hands Estimation\\n\\nBody and Hands Estimation\\n\\nInstallation, Reinstallation and Uninstallation\\n\\nWindows portable version: Simply download and use the latest version from the Releases section.\\n\\nOtherwise, check doc/installation.md for instructions on how to build OpenPose from source.\\n\\nQuick Start\\n\\nMost users do not need the OpenPose C++ API, but they can simply use the basic Demo and/or OpenPose Wrapper.\\n\\nDemo: To easily process images/video/webcam and display/save the results. See doc/demo_overview.md. E.g. run OpenPose in a video with:\\n\\n# Ubuntu ./build/examples/openpose/openpose.bin --video examples/media/video.avi :: Windows - Portable Demo bin\\\\OpenPoseDemo.exe --video examples\\\\media\\\\video.avi\\n\\nOpenPose Wrapper : If you want to read a specific input, and/or add your custom post-processing function, and/or implement your own display/saving, check the Wrapper tutorial on examples/tutorial_wrapper/. You can create your custom code on examples/user_code/ and quickly compile it by using make all in the OpenPose folder (assuming Makefile installer).\\n\\nOpenPose C++ API : See doc/library_introduction.md.\\n\\nAdding an extra module : Check doc/library_add_new_module.md.\\n\\nStandalone face or hand detector : Face keypoint detection without body keypoint detection: If you want to speed it up (but also reduce amount of detected faces), check the OpenCV-face-detector approach in doc/standalone_face_or_hand_keypoint_detector.md. Use your own face/hand detector : You can use the hand and/or face keypoint detectors with your own face or hand detectors, rather than using the body detector. E.g. useful for camera views at which the hands are visible but not the body (OpenPose detector would fail). See doc/standalone_face_or_hand_keypoint_detector.md.\\n\\nLibrary dependencies: OpenPose uses default Caffe and OpenCV, as well as any Caffe dependency. The demos additionally use GFlags. It could easily be ported to other deep learning frameworks (Tensorflow, Torch, ...). Feel free to make a pull request if you implement any of those!\\n\\nOutput\\n\\nOutput (format, keypoint index ordering, etc.) in doc/output.md.\\n\\nSpeeding Up OpenPose and Benchmark\\n\\nCheck the OpenPose Benchmark and some hints to speed up OpenPose on doc/faq.md#speed-up-and-benchmark.\\n\\nSend Us Failure Cases and Feedback!\\n\\nOur library is open source for research purposes, and we want to continuously improve it! So please, let us know if...\\n\\n... you find videos or images where OpenPose does not seems to work well. Feel free to send them to openposecmu@gmail.com (email only for failure cases!), we will use them to improve the quality of the algorithm! ... you find any bug (in functionality or speed). ... you added some functionality to some class or some new Worker subclass which we might potentially incorporate. ... you know how to speed up or improve any part of the library. ... you have a request about possible functionality. ... etc.\\n\\nJust comment on GitHub or make a pull request and we will answer as soon as possible! Send us an email if you use the library to make a cool demo or YouTube video!\\n\\nAuthors and Contributors\\n\\nOpenPose is authored by Gines Hidalgo, Zhe Cao, Tomas Simon, Shih-En Wei, Hanbyul Joo, and Yaser Sheikh. Currently, it is being maintained by Gines Hidalgo, Bikramjot Hanzra, and Yaadhav Raaj. The original CVPR 2017 repo includes Matlab and Python versions, as well as the training code. The body pose estimation work is based on the original ECCV 2016 demo.\\n\\nIn addition, OpenPose would not be possible without the CMU Panoptic Studio dataset.\\n\\nWe would also like to thank all the people who helped OpenPose in any way. The main contributors are listed in doc/contributors.md.\\n\\nCitation\\n\\nPlease cite these papers in your publications if it helps your research (the face keypoint detector was trained using the same procedure described in [Simon et al. 2017]):\\n\\n@inproceedings{cao2017realtime, author = {Zhe Cao and Tomas Simon and Shih-En Wei and Yaser Sheikh}, booktitle = {CVPR}, title = {Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields}, year = {2017} } @inproceedings{simon2017hand, author = {Tomas Simon and Hanbyul Joo and Iain Matthews and Yaser Sheikh}, booktitle = {CVPR}, title = {Hand Keypoint Detection in Single Images using Multiview Bootstrapping}, year = {2017} } @inproceedings{wei2016cpm, author = {Shih-En Wei and Varun Ramakrishna and Takeo Kanade and Yaser Sheikh}, booktitle = {CVPR}, title = {Convolutional pose machines}, year = {2016} }\\n\\nLicense\\n\\nOpenPose is freely available for free non-commercial use, and may be redistributed under these conditions. Please, see the license for further details. Interested in a commercial license? Check this link. For commercial queries, contact Yaser Sheikh.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Sequence Tagging with Tensorflow', 'Code is available on github.\\n\\nDemo\\n\\nEnter sentences like Monica and Chandler met at Central Perk , Obama was president of the United States , John went to New York to interview with Microsoft and then hit the button.\\n\\nFind Entities\\n\\nDisclaimer: as you may notice, the tagger is far from being perfect. Some errors are due to the fact that the demo uses a reduced vocabulary (lighter for the API). But not all. It is also very sensible to capital letters, which comes both from the architecture of the model and the training data. For more information about the demo, see here.\\n\\nIntroduction\\n\\nI remember the first time I heard about the magic of Deep Learning for Natural Language Processing (NLP). I was just starting a project with a young French startup Riminder and it was the first time I heard about word embeddings. There are moments in life when the confrontation with a new theory seems to make everything else irrelevant. Hearing about word vectors that encode similarity and meaning between words was one of these moments. I was baffled by the simplicity of the model as I started to play with these new concepts, building my first recurrent neural network for sentiment analysis. A few months later, as part of the master thesis of my master in the French university Ecole polytechnique I was working on more advanced models for sequence tagging at Proxem.\\n\\nLinear Dependencies between word vectors - GloVe\\n\\nTensorflow vs Theano At that time, Tensorflow had just been open sourced and Theano was the most widely used framework. For those who are not familiar with the two, Theano operates at the matrix level while Tensorflow comes with a lot of pre-coded layers and helpful training mechanisms. Using Theano was sometimes painful but forced me to pay attention to the tiny details hidden in the equations and have a global understanding of how a deep learning library works.\\n\\nFastforward a few months: I’m in Stanford and I’m using Tensorflow. One day, here I am, asking myself: “What if you tried to code one of the sequence tagging models in Tensorflow? How long would it take?”. The answer is: no more than a few hours.\\n\\nThis post’s ambition is to provide an example of how to use Tensorflow to build a sate-of-the art model (similar to this paper) for sequence tagging and share some exciting NLP knowledge!\\n\\nTogether with this post, I am releasing the code and hope some will find it useful. You can use it to train your own sequence tagging model. I’ll assume conceptual knowledge about Recurrent Neural Networks. By the way, at this point I have to share my admiration for karpathy’s blog (and this post in particular “The Unreasonable Effectiveness of Recurrent Neural Networks”). For readers new to NLP, have a look at the amazing Stanford NLP class.\\n\\nTask and Data\\n\\nFirst, let’s discuss what Sequence Tagging is. Depending on your background, you may have heard of it under different names: Named Entity Recognition, Part-of-Speech Tagging, etc. We’ll focus on Named Entity Recognition (NER) for the rest of this post. You can check Wikipedia. One example is:\\n\\nJohn lives in New York and works for the European Union B-PER O O B-LOC I-LOC O O O O B-ORG I-ORG\\n\\nIn the CoNLL2003 task, the entities are LOC, PER, ORG and MISC for locations, persons, orgnizations and miscellaneous. The no-entity tag is O . Because some entities (like New York ) have multiple words, we use a tagging scheme to distinguish between the beginning (tag B-... ), or the inside of an entity (tag I-... ). Other tagging schemes exist (IOBES, etc). However, if we just pause for a sec and think about it in an abstract manner, we just need a system that assigns a class (a number corresponding to a tag) to each word in a sentence.\\n\\n“But wait, why is it a problem? Just keep a list of locations, common names and organizations!”\\n\\nI am glad you asked this question. What makes this problem non-trivial is that a lot of entities, like names or organizations are just made-up names for which we don’t have any prior knowledge. Thus, what we really need is something that will extract contextual information from the sentence, just like humans do!\\n\\nFor our implementation, we are assuming that the data is stored in a .txt file with one word and its entity per line, like the following example\\n\\nEU B-ORG rejects O German B-MISC call O to O boycott O British B-MISC lamb O . O Peter B-PER Blackburn I-PER\\n\\nModel\\n\\n“Let me guess… LSTM?”\\n\\nYou’re right. Like most of the NLP systems, ours is gonna rely on a recurrent neural network at some point. But before delving into the details of our model, let’s break it into 3 pieces:\\n\\nWord Representation : we need to use a dense representation $ w \\\\in \\\\mathbb{R}^n $ for each word. The first thing we can do is load some pre-trained word embeddings $ w_{glove} \\\\in \\\\mathbb{R}^{d_1} $ (GloVe, Word2Vec, Senna, etc.). We’re also going to extract some meaning from the characters. As we said, a lot of entities don’t even have a pretrained word vector, and the fact that the word starts with a capital letter may help for instance.\\n\\n: we need to use a dense representation $ w \\\\in \\\\mathbb{R}^n $ for each word. The first thing we can do is load some pre-trained word embeddings $ w_{glove} \\\\in \\\\mathbb{R}^{d_1} $ (GloVe, Word2Vec, Senna, etc.). We’re also going to extract some meaning from the characters. As we said, a lot of entities don’t even have a pretrained word vector, and the fact that the word starts with a capital letter may help for instance. Contextual Word Representation : for each word in its context, we need to get a meaningful representation $ h \\\\in \\\\mathbb{R}^k $. Good guess, we’re gonna use an LSTM here.\\n\\n: for each word in its context, we need to get a meaningful representation $ h \\\\in \\\\mathbb{R}^k $. Good guess, we’re gonna use an LSTM here. Decoding: the ultimate step. Once we have a vector representing each word, we can use it to make a prediction.\\n\\nWord Representation\\n\\nFor each word, we want to build a vector $ w \\\\in \\\\mathbb{R}^n $ that will capture the meaning and relevant features for our task. We’re gonna build this vector as a concatenation of the word embeddings $ w_{glove} \\\\in \\\\mathbb{R}^{d_1} $ from GloVe and a vector containing features extracted from the character level $ w_{chars} \\\\in \\\\mathbb{R}^{d_2} $. One option is to use hand-crafted features, like a component with a 0 or 1 if the word starts with a capital for instance. Another fancier option is to use some kind of neural network to make this extraction automatically for us. In this post, we’re gonna use a bi-LSTM at the character level, but we could use any other kind of recurrent neural network or even a convolutional neural network at the character or n-gram level.\\n\\nWord level representation from characters embeddings\\n\\nEach character $ c_i $ of a word $ w = [c_1, \\\\ldots, c_p] $ (we make the distinction between lowercase and uppercase, for instance a and A are considered different) is associated to a vector $ c_i \\\\in \\\\mathbb{R}^{d_3} $. We run a bi-LSTM over the sequence of character embeddings and concatenate the final states to obtain a fixed-size vector $ w_{chars} \\\\in \\\\mathbb{R}^{d_2} $. Intuitively, this vector captures the morphology of the word. Then, we concatenate $ w_{chars} $ to the word embedding $ w_{glove} $ to get a vector representing our word $ w = [w_{glove}, w_{chars}] \\\\in \\\\mathbb{R}^n $ with $ n = d_1 + d_2 $.\\n\\nLet’s have a look at the Tensorflow code. Recall that as Tensorflow receives batches of words and data, we need to pad sentences to make them the same length. As a result, we need to define 2 placeholders (= entries of the computational graph):\\n\\n# shape = (batch size, max length of sentence in batch) word_ids = tf . placeholder ( tf . int32 , shape = [ None , None ]) # shape = (batch size) sequence_lengths = tf . placeholder ( tf . int32 , shape = [ None ])\\n\\nNow, let’s use tensorflow built-in functions to load the word embeddings. Assume that embeddings is a numpy array with our GloVe embeddings, such that embeddings[i] gives the vector of the i-th word.\\n\\nL = tf . Variable ( embeddings , dtype = tf . float32 , trainable = False ) # shape = (batch, sentence, word_vector_size) pretrained_embeddings = tf . nn . embedding_lookup ( L , word_ids )\\n\\nYou should use tf.Variable with argument trainable=False instead of tf.constant , otherwise you risk memory issues!\\n\\nNow, let’s build our representation from the characters. As we need to pad words to make them the same length, we also need to define 2 placeholders:\\n\\n# shape = (batch size, max length of sentence, max length of word) char_ids = tf . placeholder ( tf . int32 , shape = [ None , None , None ]) # shape = (batch_size, max_length of sentence) word_lengths = tf . placeholder ( tf . int32 , shape = [ None , None ])\\n\\n“Wait, can we use None everywhere like that? Why do we need it?”\\n\\nWell, that’s up to us. It depends on how we perform our padding, but in this post we chose to do it dynamically, i.e. to pad to the maximum length in the batch. Thus, sentence length and word length will depend on the batch. Now, we can build the word embeddings from the characters. Here, we don’t have any pretrained character embeddings, so we call tf.get_variable that will initialize a matrix for us using the default initializer ( xavier_initializer ). We also need to reshape our 4-dimensional tensor to match the requirement of bidirectional_dynamic_rnn . Pay extra attention to the type returned by this function. Also, the state of the lstm is a tuple of memory and hidden state.\\n\\n# 1. get character embeddings K = tf . get_variable ( name = \"char_embeddings\" , dtype = tf . float32 , shape = [ nchars , dim_char ]) # shape = (batch, sentence, word, dim of char embeddings) char_embeddings = tf . nn . embedding_lookup ( K , char_ids ) # 2. put the time dimension on axis=1 for dynamic_rnn s = tf . shape ( char_embeddings ) # store old shape # shape = (batch x sentence, word, dim of char embeddings) char_embeddings = tf . reshape ( char_embeddings , shape = [ - 1 , s [ - 2 ], s [ - 1 ]]) word_lengths = tf . reshape ( self . word_lengths , shape = [ - 1 ]) # 3. bi lstm on chars cell_fw = tf . contrib . rnn . LSTMCell ( char_hidden_size , state_is_tuple = True ) cell_bw = tf . contrib . rnn . LSTMCell ( char_hidden_size , state_is_tuple = True ) _ , (( _ , output_fw ), ( _ , output_bw )) = tf . nn . bidirectional_dynamic_rnn ( cell_fw , cell_bw , char_embeddings , sequence_length = word_lengths , dtype = tf . float32 ) # shape = (batch x sentence, 2 x char_hidden_size) output = tf . concat ([ output_fw , output_bw ], axis =- 1 ) # shape = (batch, sentence, 2 x char_hidden_size) char_rep = tf . reshape ( output , shape = [ - 1 , s [ 1 ], 2 * char_hidden_size ]) # shape = (batch, sentence, 2 x char_hidden_size + word_vector_size) word_embeddings = tf . concat ([ pretrained_embeddings , char_rep ], axis =- 1 )\\n\\nNote the use of the special argument sequence_length that ensures that the last state that we get is the last valid state. Thanks to this argument, for the unvalid time steps, the dynamic_rnn passes the state through and outputs a vector of zeros.\\n\\nContextual Word Representation\\n\\nOnce we have our word representation $ w $, we simply run a LSTM (or bi-LSTM) over the sequence of word vectors and obtain another sequence of vectors (the hidden states of the LSTM or the concatenation of the two hidden states in the case of a bi-LSTM), $ h \\\\in \\\\mathbb{R}^k $.\\n\\nBidirectional LSTM on top of word representation to extract contextual representation of each word\\n\\nThe tensorflow code is straightfoward. This time we use the hidden states of each time step and not just the final states. Thus, we had as input a sequence of $ m $ word vectors $ w_1, \\\\ldots, w_m \\\\in \\\\mathbb{R}^n $ and now we have a sequence of vectors $ h_1, \\\\ldots, h_m \\\\in \\\\mathbb{R}^k $. Whereas the $ w_t $ only captured information at the word level (syntax and semantics), the $ h_t $ also take context into account.\\n\\ncell_fw = tf . contrib . rnn . LSTMCell ( hidden_size ) cell_bw = tf . contrib . rnn . LSTMCell ( hidden_size ) ( output_fw , output_bw ), _ = tf . nn . bidirectional_dynamic_rnn ( cell_fw , cell_bw , word_embeddings , sequence_length = sequence_lengths , dtype = tf . float32 ) context_rep = tf . concat ([ output_fw , output_bw ], axis =- 1 )\\n\\nDecoding\\n\\nComputing Tags Scores At this stage, each word $ w $ is associated to a vector $ h $ that captures information from the meaning of the word, its characters and its context. Let’s use it to make a final prediction. We can use a fully connected neural network to get a vector where each entry corresponds to a score for each tag.\\n\\nLet’s say we have $ 9 $ classes. We take a matrix $ W \\\\in \\\\mathbb{R}^{9 \\\\times k} $ and $ b \\\\in \\\\mathbb{R}^9 $ and compute a vector of scores $ s \\\\in \\\\mathbb{R}^9 = W \\\\cdot h + b $. We can interpret the $ i $-th component of $ s $ (that we will refer to as $ s[i] $) as the score of class $ i $ for word $ w $. One way to do this in tensorflow is:\\n\\nW = tf . get_variable ( \"W\" , shape = [ 2 * self . config . hidden_size , self . config . ntags ], dtype = tf . float32 ) b = tf . get_variable ( \"b\" , shape = [ self . config . ntags ], dtype = tf . float32 , initializer = tf . zeros_initializer ()) ntime_steps = tf . shape ( context_rep )[ 1 ] context_rep_flat = tf . reshape ( context_rep , [ - 1 , 2 * hidden_size ]) pred = tf . matmul ( context_rep_flat , W ) + b scores = tf . reshape ( pred , [ - 1 , ntime_steps , ntags ])\\n\\nNote that we use a zero_initializer for the bias.\\n\\nDecoding the scores Then, we have two options to make our final prediction.\\n\\nIn both cases, we want to be able to compute the probability $ \\\\mathbb{P}(y_1, \\\\ldots, y_m) $ of a tagging sequence $ y_t $ and find the sequence with the highest probability. Here, $ y_t $ is the id of the tag for the t-th word.\\n\\nHere we have two options:\\n\\nsoftmax : normalize the scores into a vector $ p \\\\in \\\\mathbb{R}^9 $ such that $ p[i]= \\\\frac{e^{s[i]}}{\\\\sum_{j=1}^9 e^{s[j]}} $. Then, $ p_i $ can be interpreted as the probability that the word belongs to class $ i $ (positive, sum to 1). Eventually, the probability $ \\\\mathbb{P}(y) $ of a sequence of tag $ y $ is the product $ \\\\prod_{t=1}^m p_t [y_t] $.\\n\\nlinear-chain CRF: the first method makes local choices. In other words, even if we capture some information from the context in our $ h $ thanks to the bi-LSTM, the tagging decision is still local. We don’t make use of the neighbooring tagging decisions. For instance, in New York , the fact that we are tagging York as a location should help us to decide that New corresponds to the beginning of a location. Given a sequence of words $ w_1, \\\\ldots, w_m $, a sequence of score vectors $ s_1, \\\\ldots, s_m $ and a sequence of tags $ y_1, \\\\ldots, y_m $, a linear-chain CRF defines a global score $ C \\\\in \\\\mathbb{R} $ such that\\n\\nwhere $ T $ is a transition matrix in $ \\\\mathbb{R}^{9 \\\\times 9} $ and $ e, b \\\\in \\\\mathbb{R}^9 $ are vectors of scores that capture the cost of beginning or ending with a given tag. The use of the matrix $ T $ captures linear (one step) dependencies between tagging decisions.\\n\\nThe path PER-O-LOC has a score of $1+10+4+3+2+11+0=31$ The path PER-PER-LOC has a score of $ 1+10+2+4-2+11+0=26 $ Illustration of the scoring of a sentence with a linear-chain CRF. Between these two possible paths, the one with the best score is PER-O-LOC. Notice that if we make our decision locally, based on the score vector of each word, we would have chosen PER-PER-LOC\\n\\nNow that we understand the scoring function of the CRF, we need to do 2 things:\\n\\nFind the sequence of tags with the best score. Compute a probability distribution over all the sequence of tags\\n\\n“This sounds awesome, but don’t we have a computational problem as the number of possible tag choices is exponential?”\\n\\nFinding the best sequence Well, you’re right. We cannot reasonnably imagine to compute the scores of all the $ 9^m $ tagging choices to choose the best one or even normalize each sequence score into a propability.\\n\\nLuckily, the recurrent nature of our formula makes it the perfect candidate to apply dynamic programming. Let’s suppose that we have the solution $ \\\\tilde{s}_{t+1} (y^{t+1}) $ for time steps $ t+1, \\\\ldots, m $ for sequences that start with $ y^{t+1} $ for each of the 9 possible $ y^{t+1} $. Then, the solution $ \\\\tilde{s}_t(y_t) $ for time steps $ t, \\\\ldots, m $ that starts with $ y_t $ verifies\\n\\nThen, each recurrence step is done in $ O(9 \\\\times 9) $ (taking the argmax for each class). As we perform $ m $ steps, our final cost is $ O(9 \\\\times 9 \\\\times m) $ with is much better. For instance, for a sentence of 10 words we go from more than 3 billions ($ 9^{10} $) to just 810 in terms of complexity ( $ 9 \\\\times 9 \\\\times 10 )$!\\n\\nProbability Distribution over the sequence of tags The final step of a linear chain CRF is to apply a softmax to the scores of all possible sequences to get the probabilty $ \\\\mathbb{P}(y) $ of a given sequence of tags $ y $. To do that, we need to compute the partition factor\\n\\nwhich is the sum of the scores of all possible sequences. We can apply the same idea as above, but instead of taking the argmax, we sum over all possible paths. Let’s call $ Z_t(y_t) $ the sum of scores for all sequences that start at time step $ t $ with tag $ y_t $. Then, $ Z_t $ verifies\\n\\nThen, we can easily define the probability of a given sequence of tags as\\n\\nTraining\\n\\nNow that we’ve explained the architecture of our model and spent some time on CRFs, a final word on our objective function. We are gonna use cross-entropy loss, in other words our loss is\\n\\nwhere $ \\\\tilde{y} $ is the correct sequence of tags and its probability is given by\\n\\nCRF : $ \\\\mathbb{P}(\\\\tilde{y}) = \\\\frac{e^{C(\\\\tilde{y})}}{Z} $\\n\\n: $ \\\\mathbb{P}(\\\\tilde{y}) = \\\\frac{e^{C(\\\\tilde{y})}}{Z} $ local softmax: $ \\\\mathbb{P}(\\\\tilde{y}) = \\\\prod p_t[\\\\tilde{y}^t] $.\\n\\n“I’m afraid that coding the CRF loss is gonna be painful…”\\n\\nHere comes the magic of open-source! Implementing a CRF only takes one-line! The following code computes the loss and also returns the matrix $ T $ ( transition_params ) that will be usefull for prediction.\\n\\n# shape = (batch, sentence) labels = tf . placeholder ( tf . int32 , shape = [ None , None ], name = \"labels\" ) log_likelihood , transition_params = tf . contrib . crf . crf_log_likelihood ( scores , labels , sequence_lengths ) loss = tf . reduce_mean ( - log_likelihood )\\n\\nIn the case of the local softmax, the computation of the loss is more classic, but we have to pay extra attention to the padding and use tf.sequence_mask that transforms sequence lengths into boolean vectors (masks).\\n\\nlosses = tf . nn . sparse_softmax_cross_entropy_with_logits ( logits = scores , labels = labels ) # shape = (batch, sentence, nclasses) mask = tf . sequence_mask ( sequence_lengths ) # apply mask losses = tf . boolean_mask ( losses , mask ) loss = tf . reduce_mean ( losses )\\n\\nAnd then, finally, we can define our train operator as\\n\\noptimizer = tf . train . AdamOptimizer ( self . lr ) train_op = optimizer . minimize ( self . loss )\\n\\nUsing the trained model\\n\\nFor the local softmax method, performing the final prediction is straightfoward, the class is just the class with the highest score for each time step. This is done via tensorflow with :\\n\\nlabels_pred = tf . cast ( tf . argmax ( self . logits , axis =- 1 ), tf . int32 )\\n\\nFor the CRF, we have to use dynamic programming, as explained above. Again, this only take one line with tensorflow!\\n\\nThis function is pure ‘python’, as we get as argument the transition_params . The tensorflow Session() evaluates score (= the $ s_t $ ), that’s all. Pay attention that this makes the prediction for only one sample!\\n\\nThe Viterbi decoding step is done in python for now, but as there seems to be some progress in contrib on similar problems (Beam Search for instance) we can hope for an ‘all-tensorflow’ CRF implementation anytime soon.\\n\\n# shape = (sentence, nclasses) score = ... viterbi_sequence , viterbi_score = tf . contrib . crf . viterbi_decode ( score , transition_params )\\n\\nWith the previous code you should get an F1 score close between 90 and 91!\\n\\nConclusion\\n\\nTensorflow makes it really easy to implement any kind of deep learning system, as long as the layer you’re looking for is already implemented. However, you’ll still have to go to deeper levels if you’re trying something new…')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Deep reinforcement learning doesn't work yet\", 'This mostly cites papers from Berkeley, Google Brain, DeepMind, and OpenAI from the past few years, because that work is most visible to me. I’m almost certainly missing stuff from older literature and other institutions, and for that I apologize - I’m just one guy, after all.\\n\\nIntroduction\\n\\nOnce, on Facebook, I made the following claim.\\n\\nWhenever someone asks me if reinforcement learning can solve their problem, I tell them it can’t. I think this is right at least 70% of the time.\\n\\nDeep reinforcement learning is surrounded by mountains and mountains of hype. And for good reasons! Reinforcement learning is an incredibly general paradigm, and in principle, a robust and performant RL system should be great at everything. Merging this paradigm with the empirical power of deep learning is an obvious fit. Deep RL is one of the closest things that looks anything like AGI, and that’s the kind of dream that fuels billions of dollars of funding.\\n\\nUnfortunately, it doesn’t really work yet.\\n\\nNow, I believe it can work. If I didn’t believe in reinforcement learning, I wouldn’t be working on it. But there are a lot of problems in the way, many of which feel fundamentally difficult. The beautiful demos of learned agents hide all the blood, sweat, and tears that go into creating them.\\n\\nSeveral times now, I’ve seen people get lured by recent work. They try deep reinforcement learning for the first time, and without fail, they underestimate deep RL’s difficulties. Without fail, the “toy problem” is not as easy as it looks. And without fail, the field destroys them a few times, until they learn how to set realistic research expectations.\\n\\nThis isn’t the fault of anyone in particular. It’s more of a systemic problem. It’s easy to write a story around a positive result. It’s hard to do the same for negative ones. The problem is that the negative ones are the ones that researchers run into the most often. In some ways, the negative cases are actually more important than the positives.\\n\\nIn the rest of the post, I explain why deep RL doesn’t work, cases where it does work, and ways I can see it working more reliably in the future. I’m not doing this because I want people to stop working on deep RL. I’m doing this because I believe it’s easier to make progress on problems if there’s agreement on what those problems are, and it’s easier to build agreement if people actually talk about the problems, instead of independently re-discovering the same issues over and over again.\\n\\nI want to see more deep RL research. I want new people to join the field. I also want new people to know what they’re getting into.\\n\\nBefore getting into the rest of the post, a few remarks.\\n\\nI cite several papers in this post. Usually, I cite the paper for its compelling negative examples, leaving out the positive ones. This doesn’t mean I don’t like the paper. I like these papers - they’re worth a read, if you have the time.\\n\\nI use “reinforcement learning” and “deep reinforcement learning” interchangeably, because in my day-to-day, “RL” always implicitly means deep RL. I am criticizing the empirical behavior of deep reinforcement learning, not reinforcement learning in general. The papers I cite usually represent the agent with a deep neural net. Although the empirical criticisms may apply to linear RL or tabular RL, I’m not confident they generalize to smaller problems. The hype around deep RL is driven by the promise of applying RL to large, complex, high-dimensional environments where good function approximation is necessary. It is that hype in particular that needs to be addressed.\\n\\nThis post is structured to go from pessimistic to optimistic. I know it’s a bit long, but I’d appreciate it if you would take the time to read the entire post before replying.\\n\\nWithout further ado, here are some of the failure cases of deep RL.\\n\\nDeep Reinforcement Learning Can Be Horribly Sample Inefficient\\n\\nThe most well-known benchmark for deep reinforcement learning is Atari. As shown in the now-famous Deep Q-Networks paper, if you combine Q-Learning with reasonably sized neural networks and some optimization tricks, you can achieve human or superhuman performance in several Atari games.\\n\\nAtari games run at 60 frames per second. Off the top of your head, can you estimate how many frames a state of the art DQN needs to reach human performance?\\n\\nThe answer depends on the game, so let’s take a look at a recent Deepmind paper, Rainbow DQN (Hessel et al, 2017). This paper does an ablation study over several incremental advances made to the original DQN architecture, demonstrating that a combination of all advances gives the best performance. It exceeds human-level performance on over 40 of the 57 Atari games attempted. The results are displayed in this handy chart.\\n\\nThe y-axis is “median human-normalized score”. This is computed by training 57 DQNs, one for each Atari game, normalizing the score of each agent such that human performance is 100%, then plotting the median performance across the 57 games. RainbowDQN passes the 100% threshold at about 18 million frames. This corresponds to about 83 hours of play experience, plus however long it takes to train the model. A lot of time, for an Atari game that most humans pick up within a few minutes.\\n\\nMind you, 18 million frames is actually pretty good, when you consider that the previous record (Distributional DQN (Bellemare et al, 2017)) needed 70 million frames to hit 100% median performance, which is about 4x more time. As for the Nature DQN (Mnih et al, 2015), it never hits 100% median performance, even after 200 million frames of experience.\\n\\nThe planning fallacy says that finishing something usually takes longer than you think it will. Reinforcement learning has its own planning fallacy - learning a policy usually needs more samples than you think it will.\\n\\nThis is not an Atari-specific issue. The 2nd most popular benchmark is the MuJoCo benchmarks, a set of tasks set in the MuJoCo physics simulator. In these tasks, the input state is usually the position and velocity of each joint of some simulated robot. Even without having to solve vision, these benchmarks take between to steps to learn, depending on the task. This is an astoundingly large amount of experience to control such a simple environment.\\n\\nThe DeepMind parkour paper (Heess et al, 2017), demoed below, trained policies by using 64 workers for over 100 hours. The paper does not clarify what “worker” means, but I assume it means 1 CPU.\\n\\nThese results are super cool. When it first came out, I was surprised deep RL was even able to learn these running gaits.\\n\\nAt the same time, the fact that this needed 6400 CPU hours is a bit disheartening. It’s not that I expected it to need less time…it’s more that it’s disappointing that deep RL is still orders of magnitude above a practical level of sample efficiency.\\n\\nThere’s an obvious counterpoint here: what if we just ignore sample efficiency? There are several settings where it’s easy to generate experience. Games are a big example. But, for any setting where this isn’t true, RL faces an uphill battle, and unfortunately, most real-world settings fall under this category.\\n\\nIf You Just Care About Final Performance, Many Problems are Better Solved by Other Methods\\n\\nWhen searching for solutions to any research problem, there are usually trade-offs between different objectives. You can optimize for getting a really good solution for that research problem, or you can optimize for making a good research contribution. The best problems are ones where getting a good solution requires making good research contributions, but it can be hard to find approachable problems that meet that criteria.\\n\\nFor purely getting good performance, deep RL’s track record isn’t that great, because it consistently gets beaten by other methods. Here is a video of the MuJoCo robots, controlled with online trajectory optimization. The correct actions are computed in near real-time, online, with no offline training. Oh, and it’s running on 2012 hardware. (Tassa et al, IROS 2012).\\n\\nI think these behaviors compare well to the parkour paper. What’s different between this paper and that one?\\n\\nThe difference is that Tassa et al use model predictive control, which gets to perform planning against a ground-truth world model (the physics simulator). Model-free RL doesn’t do this planning, and therefore has a much harder job. On the other hand, if planning against a model helps this much, why bother with the bells and whistles of training an RL policy?\\n\\nIn a similar vein, you can easily outperform DQN in Atari with off-the-shelf Monte Carlo Tree Search. Here are baseline numbers from Guo et al, NIPS 2014. They compare the scores of a trained DQN to the scores of a UCT agent (where UCT is the standard version of MCTS used today.)\\n\\nAgain, this isn’t a fair comparison, because DQN does no search, and MCTS gets to perform search against a ground truth model (the Atari emulator). However, sometimes you don’t care about fair comparisons. Sometimes you just want the thing to work. (If you’re interested in a full evaluation of UCT, see the appendix of the original Arcade Learning Environment paper (Bellemare et al, JAIR 2013).)\\n\\nReinforcement learning can theoretically work for anything, including environments where a model of the world isn’t known. However, this generality comes at a price: it’s hard to exploit any problem-specific information that could help with learning, which forces you to use tons of samples to learn things that could have been hardcoded.\\n\\nThe rule-of-thumb is that except in rare cases, domain-specific algorithms work faster and better than reinforcement learning. This isn’t a problem if you’re doing deep RL for deep RL’s sake, but I personally find it frustrating when I compare RL’s performance to, well, anything else. One reason I liked AlphaGo so much was because it was an unambiguous win for deep RL, and that doesn’t happen very often.\\n\\nThis makes it harder for me to explain to laypeople why my problems are cool and hard and interesting, because they often don’t have the context or experience to appreciate why they’re hard. There’s an explanation gap between what people think deep RL can do, and what it can really do. I’m working in robotics right now. Consider the company most people think of when you mention robotics: Boston Dynamics.\\n\\nThis doesn’t use reinforcement learning. I’ve had a few conversations where people thought it used RL, but it doesn’t. If you look up research papers from the group, you find papers mentioning time-varying LQR, QP solvers, and convex optimization. In other words, they mostly apply classical robotics techniques. Turns out those classical techniques can work pretty well, when you apply them right.\\n\\nReinforcement Learning Usually Requires a Reward Function\\n\\nReinforcement learning assumes the existence of a reward function. Usually, this is either given, or it is hand-tuned offline and kept fixed over the course of learning. I say “usually” because there are exceptions, such as imitation learning or inverse RL, but most RL approaches treat the reward as an oracle.\\n\\nImportantly, for RL to do the right thing, your reward function must capture exactly what you want. And I mean exactly. RL has an annoying tendency to overfit to your reward, leading to things you didn’t expect. This is why Atari is such a nice benchmark. Not only is it easy to get lots of samples, the goal in every game is to maximize score, so you never have to worry about defining your reward, and you know everyone else has the same reward function.\\n\\nThis is also why the MuJoCo tasks are popular. Because they’re run in simulation, you have perfect knowledge of all object state, which makes reward function design a lot easier.\\n\\nIn the Reacher task, you control a two-segment arm, that’s connected to a central point, and the goal is to move the end of the arm to a target location. Below is a video of a successfully learned policy.\\n\\nSince all locations are known, reward can be defined as the distance from the end of the arm to the target, plus a small control cost. In principle, you can do this in the real world too, if you have enough sensors to get accurate enough positions for your environment. But depending on what you want your system to do, it could be hard to define a reasonable reward.\\n\\nBy itself, requiring a reward function wouldn’t be a big deal, except…\\n\\nReward Function Design is Difficult\\n\\nMaking a reward function isn’t that difficult. The difficulty comes when you try to design a reward function that encourages the behaviors you want while still being learnable.\\n\\nIn the HalfCheetah environment, you have a two-legged robot, restricted to a vertical plane, meaning it can only run forward or backward.\\n\\n</source> Your browser does not support the video element.\\n\\nThe goal is to learn a running gait. Reward is the velocity of the HalfCheetah.\\n\\nThis is a shaped reward, meaning it gives increasing reward in states that are closer to the end goal. This is in contrast to sparse rewards, which give reward at the goal state, and no reward anywhere else. Shaped rewards are often much easier to learn, because they provide positive feedback even when the policy hasn’t figured out a full solution to the problem.\\n\\nUnfortunately, shaped rewards can bias learning. As said earlier, this can lead to behaviors that don’t match what you want. A good example is the boat racing game, from an OpenAI blog post. The intended goal is to finish the race. You can imagine that a sparse reward would give +1 reward for finishing under a given time, and 0 reward otherwise.\\n\\nThe provided reward gives points for hitting checkpoints, and also gives reward for collecting powerups that let you finish the race faster. It turns out farming the powerups gives more points than finishing the race.\\n\\nTo be honest, I was a bit annoyed when this blog post first came out. This wasn’t because I thought it was making a bad point! It was because I thought the point it made was blindingly obvious. Of course reinforcement learning does weird things when the reward is misspecified! It felt like the post was making an unnecessarily large deal out of the given example.\\n\\nThen I started writing this blog post, and realized the most compelling video of misspecified reward was the boat racing video. And since then, that video’s been used in several presentations bringing awareness to the problem. So, okay, I’ll begrudgingly admit this was a good blog post.\\n\\nRL algorithms fall along a continuum, where they get to assume more or less knowledge about the environment they’re in. The broadest category, model-free RL, is almost the same as black-box optimization. These methods are only allowed to assume they are in an MDP. Otherwise, they are given nothing else. The agent is simply told that this gives +1 reward, this doesn’t, and it has to learn the rest on its own. And like black-box optimization, the problem is that anything that gives +1 reward is good, even if the +1 reward isn’t coming for the right reasons.\\n\\nA classic non-RL example is the time someone applied genetic algorithms to circuit design, and got a circuit where an unconnected logic gate was necessary to the final design.\\n\\nThe gray cells are required to get correct behavior, including the one in the top-left corner, even though it’s connected to nothing. From “An Evolved Circuit, Intrinsic in Silicon, Entwined with Physics”\\n\\nFor a more recent example, see this 2017 blog post from Salesforce. Their goal is text summarization. Their baseline model is trained with supervised learning, then evaluated with an automated metric called ROUGE. ROUGE is non-differentiable, but RL can deal with non-differentiable rewards, so they tried applying RL to optimize ROUGE directly. This gives high ROUGE (hooray!), but it doesn’t actually give good summaries. Here’s an example.\\n\\nButton was denied his 100th race for McLaren after an ERS prevented him from making it to the start-line. It capped a miserable weekend for the Briton. Button has out-qualified. Finished ahead of Nico Rosberg at Bahrain. Lewis Hamilton has. In 11 races. . The race. To lead 2,000 laps. . In. . . And.\\n\\nPaulus et al, 2017\\n\\nSo, despite the RL model giving the highest ROUGE score…\\n\\nthey ended up using a different model instead.\\n\\nHere’s another fun example. This is Popov et al, 2017, sometimes known as “the Lego stacking paper”. The authors use a distributed version of DDPG to learn a grasping policy. The goal is to grasp the red block, and stack it on top of the blue block.\\n\\nThey got it to work, but they ran into a neat failure case. For the initial lifting motion, reward is given based on how high the red block is. This is defined by the z-coordinate of the bottom face of the block. One of the failure modes was that the policy learned to tip the red block over, instead of picking it up.\\n\\nNow, clearly this isn’t the intended solution. But RL doesn’t care. From the perspective of reinforcement learning, it got rewarded for flipping a block, so it’s going to keep flipping blocks.\\n\\nOne way to address this is to make the reward sparse, by only giving positive reward after the robot stacks the block. Sometimes, this works, because the sparse reward is learnable. Often, it doesn’t, because the lack of positive reinforcement makes everything too difficult.\\n\\nThe other way to address this is to do careful reward shaping, adding new reward terms and tweaking coefficients of existing ones until the behaviors you want to learn fall out of the RL algorithm. It’s possible to fight RL on this front, but it’s a very unfulfilling fight. On occasion, it’s necessary, but I’ve never felt like I’ve learnt anything by doing it.\\n\\nFor reference, here is one of the reward functions from the Lego stacking paper.\\n\\nI don’t know how much time was spent designing this reward, but based on the number of terms and the number of different coefficients, I’m going to guess “a lot”.\\n\\nIn talks with other RL researchers, I’ve heard several anecdotes about the novel behavior they’ve seen from improperly defined rewards.\\n\\nA coworker is teaching an agent to navigate a room. The episode terminates if the agent walks out of bounds. He didn’t add any penalty if the episode terminates this way. The final policy learned to be suicidal, because negative reward was plentiful, positive reward was too hard to achieve, and a quick death ending in 0 reward was preferable to a long life that risked negative reward.\\n\\nA friend is training a simulated robot arm to reach towards a point above a table. It turns out the point was defined with respect to the table, and the table wasn’t anchored to anything. The policy learned to slam the table really hard, making the table fall over, which moved the target point too. The target point just so happened to fall next to the end of the arm.\\n\\nA researcher gives a talk about using RL to train a simulated robot hand to pick up a hammer and hammer in a nail. Initially, the reward was defined by how far the nail was pushed into the hole. Instead of picking up the hammer, the robot used its own limbs to punch the nail in. So, they added a reward term to encourage picking up the hammer, and retrained the policy. They got the policy to pick up the hammer…but then it threw the hammer at the nail instead of actually using it.\\n\\nAdmittedly, these are all secondhand accounts, and I haven’t seen videos of any of these behaviors. However, none of it sounds implausible to me. I’ve been burned by RL too many times to believe otherwise.\\n\\nI know people who like to tell stories about paperclip optimizers. I get it, I really do. But honestly, I’m sick of hearing those stories, because they always speculate up some superhuman misaligned AGI to create a just-so story. There’s no reason to speculate that far when present-day examples happen all the time.\\n\\nEven Given a Good Reward, Local Optima Can Be Hard To Escape\\n\\nThe previous examples of RL are sometimes called “reward hacking”. To me, this implies a clever, out-of-the-box solution that gives more reward than the intended answer of the reward function designer.\\n\\nReward hacking is the exception. The much more common case is a poor local optima that comes from getting the exploration-exploitation trade-off wrong.\\n\\nHere’s one of my favorite videos. This is an implementation of Normalized Advantage Function, learning on the HalfCheetah environment.\\n\\n</source> Your browser does not support the video element.\\n\\nFrom an outside perspective, this is really, really dumb. But we can only say it’s dumb because we can see the 3rd person view, and have a bunch of prebuilt knowledge that tells us running on your feet is better. RL doesn’t know this! It sees a state vector, it sends action vectors, and it knows it’s getting some positive reward. That’s it.\\n\\nHere’s my best guess for what happened during learning.\\n\\nIn random exploration, the policy found falling forward was better than standing still.\\n\\nIt did so enough to “burn in” that behavior, so now it’s falling forward consistently.\\n\\nAfter falling forward, the policy learned that if it does a one-time application of a lot of force, it’ll do a backflip that gives a bit more reward.\\n\\nIt explored the backflip enough to become confident this was a good idea, and now backflipping is burned into the policy.\\n\\nOnce the policy is backflipping consistently, which is easier for the policy: learning to right itself and then run “the standard way”, or learning or figuring out how to move forward while lying on its back? I would guess the latter.\\n\\nIt’s very funny, but it definitely isn’t what I wanted the robot to do.\\n\\nHere’s another failed run, this time on the Reacher environment.\\n\\n</source> Your browser does not support the video element.\\n\\nIn this run, the initial random weights tended to output highly positive or highly negative action outputs. This makes most of the actions output the maximum or minimum acceleration possible. It’s really easy to spin super fast: just output high magnitude forces at every joint. Once the robot gets going, it’s hard to deviate from this policy in a meaningful way - to deviate, you have to take several exploration steps to stop the rampant spinning. It’s certainly possible, but in this run, it didn’t happen.\\n\\nThese are both cases of the classic exploration-exploitation problem that has dogged reinforcement learning since time immemorial. Your data comes from your current policy. If your current policy explores too much you get junk data and learn nothing. Exploit too much and you burn-in behaviors that aren’t optimal.\\n\\nThere are several intuitively pleasing ideas for addressing this - intrinsic motivation, curiosity-driven exploration, count-based exploration, and so forth. Many of these approaches were first proposed in the 1980s or earlier, and several of them have been revisited with deep learning models. However, as far as I know, none of them work consistently across all environments. Sometimes they help, sometimes they don’t. It would be nice if there was an exploration trick that worked everywhere, but I’m skeptical a silver bullet of that caliber will be discovered anytime soon. Not because people aren’t trying, but because exploration-exploitation is really, really, really, really, really hard. To quote Wikipedia,\\n\\nOriginally considered by Allied scientists in World War II, it proved so intractable that, according to Peter Whittle, the problem was proposed to be dropped over Germany so that German scientists could also waste their time on it.\\n\\n(Reference: Q-Learning for Bandit Problems, Duff 1995)\\n\\nI’ve taken to imagining deep RL as a demon that’s deliberately misinterpreting your reward and actively searching for the laziest possible local optima. It’s a bit ridiculous, but I’ve found it’s actually a productive mindset to have.\\n\\nEven When Deep RL Works, It May Just Be Overfitting to Weird Patterns In the Environment\\n\\nDeep RL is popular because it’s the only area in ML where it’s socially acceptable to train on the test set.\\n\\nSource\\n\\nThe upside of reinforcement learning is that if you want to do well in an environment, you’re free to overfit like crazy. The downside is that if you want to generalize to any other environment, you’re probably going to do poorly, because you overfit like crazy.\\n\\nDQN can solve a lot of the Atari games, but it does so by focusing all of learning on a single goal - getting really good at one game. The final model won’t generalize to other games, because it hasn’t been trained that way. You can finetune a learned DQN to a new Atari game (see Progressive Neural Networks (Rusu et al, 2016)), but there’s no guarantee it’ll transfer and people usually don’t expect it to transfer. It’s not the wild success people see from pretrained ImageNet features.\\n\\nTo forestall some obvious comments: yes, in principle, training on a wide distribution of environments should make these issues go away. In some cases, you get such a distribution for free. An example is navigation, where you can sample goal locations randomly, and use universal value functions to generalize. (See Universal Value Function Approximators, Schaul et al, ICML 2015.) I find this work very promising, and I give more examples of this work later. However, I don’t think the generalization capabilities of deep RL are strong enough to handle a diverse set of tasks yet. Perception has gotten a lot better, but deep RL has yet to have its “ImageNet for control” moment. OpenAI Universe tried to spark this, but from what I heard, it was too difficult to solve, so not much got done.\\n\\nUntil we have that kind of generalization moment, we’re stuck with policies that can be surprisingly narrow in scope. As an example of this (and as an opportunity to poke fun at some of my own work), consider Can Deep RL Solve Erdos-Selfridge-Spencer Games? (Raghu et al, 2017). We studied a toy 2-player combinatorial game, where there’s a closed-form analytic solution for optimal play. In one of our first experiments, we fixed player 1’s behavior, then trained player 2 with RL. By doing this, you can treat player 1’s actions as part of the environment. By training player 2 against the optimal player 1, we showed RL could reach high performance. But when we deployed the same policy against a non-optimal player 1, its performance dropped, because it didn’t generalize to non-optimal opponents.\\n\\nLanctot et al, NIPS 2017 showed a similar result. Here, there are two agents playing laser tag. The agents are trained with multiagent reinforcement learning. To test generalization, they run the training with 5 random seeds. Here’s a video of agents that have been trained against one another.\\n\\nAs you can see, they learn to move towards and shoot each other. Then, they took player 1 from one experiment, and pitted it against player 2 from a different experiment. If the learned policies generalize, we should see similar behavior.\\n\\nSpoiler alert: they don’t.\\n\\nThis seems to be a running theme in multiagent RL. When agents are trained against one another, a kind of co-evolution happens. The agents get really good at beating each other, but when they get deployed against an unseen player, performance drops. I’d also like to point out that the only difference between these videos is the random seed. Same learning algorithm, same hyperparameters. The diverging behavior is purely from randomness in initial conditions.\\n\\nThat being said, there are some neat results from competitive self-play environments that seem to contradict this. OpenAI has a nice blog post of some of their work in this space. Self-play is also an important part of both AlphaGo and AlphaZero. My intuition is that if your agents are learning at the same pace, they can continually challenge each other and speed up each other’s learning, but if one of them learns much faster, it exploits the weaker player too much and overfits. As you relax from symmetric self-play to general multiagent settings, it gets harder to ensure learning happens at the same speed.\\n\\nEven Ignoring Generalization Issues, The Final Results Can be Unstable and Hard to Reproduce\\n\\nAlmost every ML algorithm has hyperparameters, which influence the behavior of the learning system. Often, these are picked by hand, or by random search.\\n\\nSupervised learning is stable. Fixed dataset, ground truth targets. If you change the hyperparameters a little bit, your performance won’t change that much. Not all hyperparameters perform well, but with all the empirical tricks discovered over the years, many hyperparams will show signs of life during training. These signs of life are super important, because they tell you that you’re on the right track, you’re doing something reasonable, and it’s worth investing more time.\\n\\nCurrently, deep RL isn’t stable at all, and it’s just hugely annoying for research.\\n\\nWhen I started working at Google Brain, one of the first things I did was implement the algorithm from the Normalized Advantage Function paper. I figured it would only take me about 2-3 weeks. I had several things going for me: some familiarity with Theano (which transferred to TensorFlow well), some deep RL experience, and the first author of the NAF paper was interning at Brain, so I could bug him with questions.\\n\\nIt ended up taking me 6 weeks to reproduce results, thanks to several software bugs. The question is, why did it take so long to find these bugs?\\n\\nTo answer this, let’s consider the simplest continuous control task in OpenAI Gym: the Pendulum task. In this task, there’s a pendulum, anchored at a point, with gravity acting on the pendulum. The input state is 3-dimensional. The action space is 1-dimensional, the amount of torque to apply. The goal is to balance the pendulum perfectly straight up.\\n\\nThis is a tiny problem, and it’s made even easier by a well shaped reward. Reward is defined by the angle of the pendulum. Actions bringing the pendulum closer to the vertical not only give reward, they give increasing reward. The reward landscape is basically concave.\\n\\nBelow is a video of a policy that mostly works. Although the policy doesn’t balance straight up, it outputs the exact torque needed to counteract gravity.\\n\\n</source> Your browser does not support the video element.\\n\\nHere is a plot of performance, after I fixed all the bugs. Each line is the reward curve from one of 10 independent runs. Same hyperparameters, the only difference is the random seed.\\n\\nSeven of these runs worked. Three of these runs didn’t. A 30% failure rate counts as working. Here’s another plot from some published work, “Variational Information Maximizing Exploration” (Houthooft et al, NIPS 2016). The environment is HalfCheetah. The reward is modified to be sparser, but the details aren’t too important. The y-axis is episode reward, the x-axis is number of timesteps, and the algorithm used is TRPO.\\n\\nThe dark line is the median performance over 10 random seeds, and the shaded region is the 25th to 75th percentile. Don’t get me wrong, this plot is a good argument in favor of VIME. But on the other hand, the 25th percentile line is really close to 0 reward. That means about 25% of runs are failing, just because of random seed.\\n\\nLook, there’s variance in supervised learning too, but it’s rarely this bad. If my supervised learning code failed to beat random chance 30% of the time, I’d have super high confidence there was a bug in data loading or training. If my reinforcement learning code does no better than random, I have no idea if it’s a bug, if my hyperparameters are bad, or if I simply got unlucky.\\n\\nThis picture is from “Why is Machine Learning ‘Hard’?”. The core thesis is that machine learning adds more dimensions to your space of failure cases, which exponentially increases the number of ways you can fail. Deep RL adds a new dimension: random chance. And the only way you can address random chance is by throwing enough experiments at the problem to drown out the noise.\\n\\nWhen your training algorithm is both sample inefficient and unstable, it heavily slows down your rate of productive research. Maybe it only takes 1 million steps. But when you multiply that by 5 random seeds, and then multiply that with hyperparam tuning, you need an exploding amount of compute to test hypotheses effectively.\\n\\nIf it makes you feel any better, I’ve been doing this for a while and it took me last ~6 weeks to get a from-scratch policy gradients implementation to work 50% of the time on a bunch of RL problems. And I also have a GPU cluster available to me, and a number of friends I get lunch with every day who’ve been in the area for the last few years. Also, what we know about good CNN design from supervised learning land doesn’t seem to apply to reinforcement learning land, because you’re mostly bottlenecked by credit assignment / supervision bitrate, not by a lack of a powerful representation. Your ResNets, batchnorms, or very deep networks have no power here. [Supervised learning] wants to work. Even if you screw something up you’ll usually get something non-random back. RL must be forced to work. If you screw something up or don’t tune something well enough you’re exceedingly likely to get a policy that is even worse than random. And even if it’s all well tuned you’ll get a bad policy 30% of the time, just because. Long story short your failure is more due to the difficulty of deep RL, and much less due to the difficulty of “designing neural networks”.\\n\\nHacker News comment from Andrej Karpathy, back when he was at OpenAI\\n\\nInstability to random seed is like a canary in a coal mine. If pure randomness is enough to lead to this much variance between runs, imagine how much an actual difference in the code could make.\\n\\nLuckily, we don’t have to imagine, because this was inspected by the paper “Deep Reinforcement Learning That Matters” (Henderson et al, AAAI 2018). Among its conclusions are:\\n\\nMultiplying the reward by a constant can cause significant differences in performance.\\n\\nFive random seeds (a common reporting metric) may not be enough to argue significant results, since with careful selection you can get non-overlapping confidence intervals.\\n\\nDifferent implementations of the same algorithm have different performance on the same task, even when the same hyperparameters are used.\\n\\nMy theory is that RL is very sensitive to both your initialization and to the dynamics of your training process, because your data is always collected online and the only supervision you get is a single scalar for reward. A policy that randomly stumbles onto good training examples will bootstrap itself much faster than a policy that doesn’t. A policy that fails to discover good training examples in time will collapse towards learning nothing at all, as it becomes more confident that any deviation it tries will fail.\\n\\nBut What About All The Great Things Deep RL Has Done For Us?\\n\\nDeep reinforcement learning has certainly done some very cool things. DQN is old news now, but was absolutely nuts at the time. A single model was able to learn directly from raw pixels, without tuning for each game individually. And AlphaGo and AlphaZero continue to be very impressive achievements.\\n\\nHowever, outside of these successes, it’s hard to find cases where deep RL has created practical real world value.\\n\\nI tried to think of real-world, productionized uses of deep RL, and it was surprisingly difficult. I expected to find something in recommendation systems, but I believe those are still dominated by collaborative filtering and contextual bandits.\\n\\nIn the end, the best I could find were two Google projects: reducing data center power usage, and the recently announced AutoML Vision effort. Jack Clark from OpenAI tweeted a similar request and found a similar conclusion. (The tweet is from last year, before AutoML was announced.)\\n\\nI know Audi’s doing something with deep RL, since they demoed a self-driving RC car at NIPS and said it used deep RL. I know there’s some neat work optimizing device placement for large Tensorflow graphs (Mirhoseini et al, ICML 2017). Salesforce has their text summarization model, which worked if you massaged the RL carefully enough. Finance companies are surely experimenting with RL as we speak, but so far there’s no definitive proof. (Of course, finance companies have reasons to be cagey about how they play the market, so perhaps the evidence there is never going to be strong.) Facebook’s been doing some neat work with deep RL for chatbots and conversation. Every Internet company ever has probably thought about adding RL to their ad-serving model, but if anyone’s done it, they’ve kept quiet about it.\\n\\nThe way I see it, either deep RL is still a research topic that isn’t robust enough for widespread use, or it’s usable and the people who’ve gotten it to work aren’t publicizing it. I think the former is more likely.\\n\\nIf you came to me with an image classification problem, I’d point you to pretrained ImageNet models, and they’d probably do great. We’re in a world where the people behind Silicon Valley can build a real Not Hotdog app as a joke. I have trouble seeing the same happen with deep RL.\\n\\nGiven Current Limitations, When Could Deep RL Work For Me?\\n\\nA priori, it’s really hard to say. The problem with trying to solve everything with RL is that you’re trying to solve several very different environments with the same approach. It’s only natural that it won’t work all the time.\\n\\nThat being said, we can draw conclusions from the current list of deep reinforcement learning successes. These are projects where deep RL either learns some qualitatively impressive behavior, or it learns something better than comparable prior work. (Admittedly, this is a very subjective criteria.)\\n\\nHere’s my list so far.\\n\\n(A quick aside: machine learning recently beat pro players at no-limit heads up Texas Hold’Em. This was done by both Libratus (Brown et al, IJCAI 2017) and DeepStack (Moravčík et al, 2017). I’ve talked to a few people who believed this was done with deep RL. They’re both very cool, but they don’t use deep RL. They use counterfactual regret minimization and clever iterative solving of subgames.)\\n\\nFrom this list, we can identify common properties that make learning easier. None of the properties below are required for learning, but satisfying more of them is definitively better.\\n\\nIt is easy to generate near unbounded amounts of experience. It should be clear why this helps. The more data you have, the easier the learning problem is. This applies to Atari, Go, Chess, Shogi, and the simulated environments for the parkour bot. It likely applies to the power center project too, because in prior work (Gao, 2014), it was shown that neural nets can predict energy efficiency with high accuracy. That’s exactly the kind of simulated model you’d want for training an RL system. It might apply to the Dota 2 and SSBM work, but it depends on the throughput of how quickly the games can be run, and how many machines were available to run them.\\n\\nThe problem is simplified into an easier form. One of the common errors I’ve seen in deep RL is to dream too big. Reinforcement learning can do anything! That doesn’t mean you have to do everything at once. The OpenAI Dota 2 bot only played the early game, only played Shadow Fiend against Shadow Fiend in a 1v1 laning setting, used hardcoded item builds, and presumably called the Dota 2 API to avoid having to solve perception. The SSBM bot acheived superhuman performance, but it was only in 1v1 games, with Captain Falcon only, on Battlefield only, in an infinite time match. This isn’t a dig at either bot. Why work on a hard problem when you don’t even know the easier one is solvable? The broad trend of all research is to demonstrate the smallest proof-of-concept first and generalize it later. OpenAI is extending their Dota 2 work, and there’s ongoing work to extend the SSBM bot to other characters.\\n\\nThere is a way to introduce self-play into learning. This is a component of AlphaGo, AlphaZero, the Dota 2 Shadow Fiend bot, and the SSBM Falcon bot. I should note that by self-play, I mean exactly the setting where the game is competitive, and both players can be controlled by the same agent. So far, that setting seems to have the most stable and well-performing behavior.\\n\\nThere’s a clean way to define a learnable, ungameable reward. Two player games have this: +1 for a win, -1 for a loss. The original neural architecture search paper from Zoph et al, ICLR 2017 had this: validation accuracy of the trained model. Any time you introduce reward shaping, you introduce a chance for learning a non-optimal policy that optimizes the wrong objective. If you’re interested in further reading on what makes a good reward, a good search term is “proper scoring rule”. See this Terrence Tao blog post for an approachable example. As for learnability, I have no advice besides trying it out to see if it works.\\n\\nIf the reward has to be shaped, it should at least be rich. In Dota 2, reward can come from last hits (triggers after every monster kill by either player), and health (triggers after every attack or skill that hits a target.) These reward signals come quick and often. For the SSBM bot, reward can be given for damage dealt and taken, which gives signal for every attack that successfully lands. The shorter the delay between action and consequence, the faster the feedback loop gets closed, and the easier it is for reinforcement learning to figure out a path to high reward.\\n\\nA Case Study: Neural Architecture Search\\n\\nWe can combine a few of the principles to analyze the success of Neural Architecture Search. According to the initial ICLR 2017 version, after 12800 examples, deep RL was able to design state-of-the art neural net architectures. Admittedly, each example required training a neural net to convergence, but this is still very sample efficient.\\n\\nAs mentioned above, the reward is validation accuracy. This is a very rich reward signal - if a neural net design decision only increases accuracy from 70% to 71%, RL will still pick up on this. Additionally, there’s evidence that hyperparameters in deep learning are close to linearly independent. (This was empirically shown in Hyperparameter Optimization: A Spectral Approach (Hazan et al, 2017) - a summary by me is here if interested.) NAS isn’t exactly tuning hyperparameters, but I think it’s reasonable that neural net design decisions would act similarly. This is good news for learning, because the correlations between decision and performance are strong. Finally, not only is the reward rich, it’s actually what we care about when we train models.\\n\\nThe combination of all these points helps me understand why it “only” takes about 12800 trained networks to learn a better one, compared to the millions of examples needed in other environments. Several parts of the problem are all pushing in RL’s favor.\\n\\nOverall, success stories this strong are still the exception, not the rule. Many things have to go right for reinforcement learning to be a plausible solution, and even then, it’s not a free ride to make that solution happen.\\n\\nIn short: deep RL is currently not a plug-and-play technology.\\n\\nLooking to The Future\\n\\nThere’s an old saying - every researcher learns how to hate their area of study. The trick is that researchers will press on despite this, because they like the problems too much.\\n\\nThat’s roughly how I feel about deep reinforcement learning. Despite my reservations, I think people absolutely should be throwing RL at different problems, including ones where it probably shouldn’t work. How else are we supposed to make RL better?\\n\\nI see no reason why deep RL couldn’t work, given more time. Several very interesting things are going to happen when deep RL is robust enough for wider use. The question is how it’ll get there.\\n\\nBelow, I’ve listed some futures I find plausible. For the futures based on further research, I’ve provided citations to relevant papers in those research areas.\\n\\nLocal optima are good enough: It would be very arrogant to claim humans are globally optimal at anything. I would guess we’re juuuuust good enough to get to civilization stage, compared to any other species. In the same vein, an RL solution doesn’t have to achieve a global optima, as long as its local optima is better than the human baseline.\\n\\nHardware solves everything: I know some people who believe that the most influential thing that can be done for AI is simply scaling up hardware. Personally, I’m skeptical that hardware will fix everything, but it’s certainly going to be important. The faster you can run things, the less you care about sample inefficiency, and the easier it is to brute-force your way past exploration problems.\\n\\nAdd more learning signal: Sparse rewards are hard to learn because you get very little information about what thing help you. It’s possible we can either hallucinate positive rewards (Hindsight Experience Replay, Andrychowicz et al, NIPS 2017), define auxiliary tasks (UNREAL, Jaderberg et al, NIPS 2016), or bootstrap with self-supervised learning to build good world model. Adding more cherries to the cake, so to speak.\\n\\nModel-based learning unlocks sample efficiency: Here’s how I describe model-based RL: “Everyone wants to do it, not many people know how.” In principle, a good model fixes a bunch of problems. As seen in AlphaGo, having a model at all makes it much easier to learn a good solution. Good world models will transfer well to new tasks, and rollouts of the world model let you imagine new experience. From what I’ve seen, model-based approaches use fewer samples as well.\\n\\nThe problem is that learning good models is hard. My impression is that low-dimensional state models work sometimes, and image models are usually too hard. But, if this gets easier, some interesting things could happen.\\n\\nDyna (Sutton, 1991) and Dyna-2 (Silver et al., ICML 2008) are classical papers in this space. For papers combining model-based learning with deep nets, I would recommend a few recent papers from the Berkeley robotics labs: Neural Network Dynamics for Model-Based Deep RL with Model-Free Fine-Tuning (Nagabandi et al, 2017, Self-Supervised Visual Planning with Temporal Skip Connections (Ebert et al, CoRL 2017), Combining Model-Based and Model-Free Updates for Trajectory-Centric Reinforcement Learning (Chebotar et al, ICML 2017). Deep Spatial Autoencoders for Visuomotor Learning (Finn et al, ICRA 2016), and Guided Policy Search (Levine et al, JMLR 2016).\\n\\nUse reinforcement learning just as the fine-tuning step: The first AlphaGo paper started with supervised learning, and then did RL fine-tuning on top of it. This is a nice recipe, since it lets you use a faster-but-less-powerful method to speed up initial learning. It’s worked in other contexts - see Sequence Tutor (Jaques et al, ICML 2017). You can view this as starting the RL process with a reasonable prior, instead of a random one, where the problem of learning the prior is offloaded to some other approach.\\n\\nReward functions could be learnable: The promise of ML is that we can use data to learn things that are better than human design. If reward function design is so hard, Why not apply this to learn better reward functions? Imitation learning and inverse reinforcement learning are both rich fields that have shown reward functions can be implicitly defined by human demonstrations or human ratings.\\n\\nFor famous papers in inverse RL and imitation learning, see Algorithms for Inverse Reinforcement Learning (Ng and Russell, ICML 2000), Apprenticeship Learning via Inverse Reinforcement Learning (Abbeel and Ng, ICML 2004), and DAgger (Ross, Gordon, and Bagnell, AISTATS 2011).\\n\\nFor recent work scaling these ideas to deep learning, see Guided Cost Learning (Finn et al, ICML 2016), Time-Constrastive Networks (Sermanet et al, 2017), and Learning From Human Preferences (Christiano et al, NIPS 2017). (The Human Preferences paper in particular showed that a reward learned from human ratings was actually better-shaped for learning than the original hardcoded reward, which is a neat practical result.)\\n\\nFor longer term work that doesn’t use deep learning, I liked Inverse Reward Design (Hadfield-Menell et al, NIPS 2017) and Learning Robot Objectives from Physical Human Interaction (Bajcsy et al, CoRL 2017).\\n\\nTransfer learning saves the day: The promise of transfer learning is that you can leverage knowledge from previous tasks to speed up learning of new ones. I think this is absolutely the future, when task learning is robust enough to solve several disparate tasks. It’s hard to do transfer learning if you can’t learn at all, and given task A and task B, it can be very hard to predict whether A transfers to B. In my experience, it’s either super obvious, or super unclear, and even the super obvious cases aren’t trivial to get working.\\n\\nUniversal Value Function Approximators (Schaul et al, ICML 2015), Distral (Whye Teh et al, NIPS 2017), and Overcoming Catastrophic Forgetting (Kirkpatrick et al, PNAS 2017) are recent works in this direction. For older work, consider reading Horde (Sutton et al, AAMAS 2011).\\n\\nRobotics in particular has had lots of progress in sim-to-real transfer (transfer learning between a simulated version of a task and the real task). See Domain Randomization (Tobin et al, IROS 2017), Sim-to-Real Robot Learning with Progressive Nets (Rusu et al, CoRL 2017), and GraspGAN (Bousmalis et al, 2017). (Disclaimer: I worked on GraspGAN.)\\n\\nGood priors could heavily reduce learning time: This is closely tied to several of the previous points. In one view, transfer learning is about using past experience to build a good prior for learning other tasks. RL algorithms are designed to apply to any Markov Decision Process, which is where the pain of generality comes in. If we accept that our solutions will only perform well on a small section of environments, we should be able to leverage shared structure to solve those environments in an efficient way.\\n\\nOne point Pieter Abbeel likes to mention in his talks is that deep RL only needs to solve tasks that we expect to need in the real world. I agree it makes a lot of sense. There should exist a real-world prior that lets us quickly learn new real-world tasks, at the cost of slower learning on non-realistic tasks, but that’s a perfectly acceptable trade-off.\\n\\nThe difficulty is that such a real-world prior will be very hard to design. However, I think there’s a good chance it won’t be impossible. Personally, I’m excited by the recent work in metalearning, since it provides a data-driven way to generate reasonable priors. For example, if I wanted to use RL to do warehouse navigation, I’d get pretty curious about using metalearning to learn a good navigation prior, and then fine-tuning the prior for the specific warehouse the robot will be deployed in. This very much seems like the future, and the question is whether metalearning will get there or not.\\n\\nA summary of recent learning-to-learn work can be found in this post from BAIR (Berkeley AI Research).\\n\\nHarder environments could paradoxically be easier: One of the big lessons from the DeepMind parkour paper is that if you make your task very difficult by adding several task variations, you can actually make the learning easier, because the policy cannot overfit to any one setting without losing performance on all the other settings. We’ve seen a similar thing in the domain randomization papers, and even back to ImageNet: models trained on ImageNet will generalize way better than ones trained on CIFAR-100. As I said above, maybe we’re just an “ImageNet for control” away from making RL considerably more generic.\\n\\nEnvironment wise, there are a lot of options. OpenAI Gym easily has the most traction, but there’s also the Arcade Learning Environment, Roboschool, DeepMind Lab, the DeepMind Control Suite, and ELF.\\n\\nFinally, although it’s unsatisfying from a research perspective, the empirical issues of deep RL may not matter for practical purposes. As a hypothetical example, suppose a finance company is using deep RL. They train a trading agent based on past data from the US stock market, using 3 random seeds. In live A/B testing, one gives 2% less revenue, one performs the same, and one gives 2% more revenue. In that hypothetical, reproducibility doesn’t matter - you deploy the model with 2% more revenue and celebrate. Similarly, it doesn’t matter that the trading agent may only perform well in the United States - if it generalizes poorly to the worldwide market, just don’t deploy it there. There is a large gap between doing something extraordinary and making that extraordinary success reproducible, and maybe it’s worth focusing on the former first.\\n\\nWhere We Are Now\\n\\nIn many ways, I find myself annoyed with the current state of deep RL. And yet, it’s attracted some of the strongest research interest I’ve ever seen. My feelings are best summarized by a mindset Andrew Ng mentioned in his Nuts and Bolts of Applying Deep Learning talk - a lot of short-term pessimism, balanced by even more long-term optimism. Deep RL is a bit messy right now, but I still believe in where it could be.\\n\\nThat being said, the next time someone asks me whether reinforcement learning can solve their problem, I’m still going to tell them that no, it can’t. But I’ll also tell them to ask me again in a few years. By then, maybe it can.\\n\\n* * *\\n\\nThis post went through a lot of revision. Thanks go to following people for reading earlier drafts: Daniel Abolafia, Kumar Krishna Agrawal, Surya Bhupatiraju, Jared Quincy Davis, Ashley Edwards, Peter Gao, Julian Ibarz, Sherjil Ozair, Vitchyr Pong, Alex Ray, and Kelvin Xu. There were several more reviewers who I’m crediting anonymously - thanks for all the feedback.')"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Amazon.com Announces Fourth Quarter Sales Up 38% to $60.5B', 'SEATTLE--(BUSINESS WIRE)--\\n\\n\\n\\nAmazon.com, Inc. (AMZN) today announced financial results for\\n\\nits fourth quarter ended December 31, 2017.\\n\\n\\n\\nOperating cash flow increased 7% to $18.4 billion for the trailing\\n\\ntwelve months, compared with $17.3 billion for the trailing twelve\\n\\nmonths ended December 31, 2016. Free cash flow decreased to $8.4 billion\\n\\nfor the trailing twelve months, compared with $10.5 billion for the\\n\\ntrailing twelve months ended December 31, 2016. Free cash flow less\\n\\nlease principal repayments decreased to $3.4 billion for the trailing\\n\\ntwelve months, compared with $6.5 billion for the trailing twelve months\\n\\nended December 31, 2016. Free cash flow less finance lease principal\\n\\nrepayments and assets acquired under capital leases decreased to an\\n\\noutflow of $1.5 billion for the trailing twelve months, compared with an\\n\\ninflow of $4.7 billion for the trailing twelve months ended December 31,\\n\\n2016.\\n\\n\\n\\nCommon shares outstanding plus shares underlying stock-based awards\\n\\ntotaled 504 million on December 31, 2017, compared with 497 million one\\n\\nyear ago.\\n\\nFourth Quarter 2017\\n\\n\\n\\nNet sales increased 38% to $60.5 billion in the fourth quarter, compared\\n\\nwith $43.7 billion in fourth quarter 2016. Excluding the $1.1 billion\\n\\nfavorable impact from year-over-year changes in foreign exchange rates\\n\\nthroughout the quarter, net sales increased 36% compared with fourth\\n\\nquarter 2016.\\n\\n\\n\\nOperating income increased 69% to $2.1 billion in the fourth quarter,\\n\\ncompared with operating income of $1.3 billion in fourth quarter 2016.\\n\\n\\n\\nNet income was $1.9 billion in the fourth quarter, or $3.75 per diluted\\n\\nshare, compared with net income of $749 million, or $1.54 per diluted\\n\\nshare, in fourth quarter 2016. The fourth quarter 2017 includes a\\n\\nprovisional tax benefit for the impact of the U.S. Tax Cuts and Jobs Act\\n\\nof 2017 of approximately $789 million.\\n\\nFull Year 2017\\n\\n\\n\\nNet sales increased 31% to $177.9 billion, compared with $136.0 billion\\n\\nin 2016. Excluding the $210 million favorable impact from year-over-year\\n\\nchanges in foreign exchange rates throughout the year, net sales\\n\\nincreased 31% compared with 2016.\\n\\n\\n\\nOperating income decreased 2% to $4.1 billion, compared with operating\\n\\nincome of $4.2 billion in 2016.\\n\\n\\n\\nNet income was $3.0 billion, or $6.15 per diluted share, compared with\\n\\nnet income of $2.4 billion, or $4.90 per diluted share, in 2016.\\n\\n\\n\\n“Our 2017 projections for Alexa were very optimistic, and we far\\n\\nexceeded them. We don’t see positive surprises of this magnitude very\\n\\noften — expect us to double down,” said Jeff Bezos, Amazon founder and\\n\\nCEO. “We’ve reached an important point where other companies and\\n\\ndevelopers are accelerating adoption of Alexa. There are now over 30,000\\n\\nskills from outside developers, customers can control more than 4,000\\n\\nsmart home devices from 1,200 unique brands with Alexa, and we’re seeing\\n\\nstrong response to our new far-field voice kit for manufacturers. Much\\n\\nmore to come and a huge thank you to our customers and partners.”\\n\\nHighlights\\n\\n\\n\\nIn 2017, more than five billion items shipped with Prime worldwide.\\n\\nIn 2017, more than five billion items shipped with Prime worldwide.\\n\\nMore new paid members joined Prime in 2017 than any previous year —\\n\\nboth worldwide and in the U.S.\\n\\nMore new paid members joined Prime in 2017 than any previous year — both worldwide and in the U.S.\\n\\nFire TV Stick and Echo Dot were the best-selling products in 2017\\n\\nacross all of Amazon. Customers purchased tens of millions of Echo\\n\\ndevices last year.\\n\\nFire TV Stick and Echo Dot were the best-selling products in 2017 across all of Amazon. Customers purchased tens of millions of Echo devices last year.\\n\\nThe Alexa Skills store now offers more than 30,000 skills, including\\n\\nnew developer tools for Alexa Gadgets and gaming experiences, such as\\n\\nActivision’s new Ghost skill for console game Destiny 2. Other new\\n\\ncategories of skills include daily beauty podcasts from Hearst,\\n\\nAll-Star game voting from the NBA, and more. Customers can also now\\n\\nuse Alexa to control more than 4,000 smart home devices from 1,200\\n\\nunique brands.\\n\\nThe Alexa Skills store now offers more than 30,000 skills, including new developer tools for Alexa Gadgets and gaming experiences, such as Activision’s new Ghost skill for console game Destiny 2. Other new categories of skills include daily beauty podcasts from Hearst, All-Star game voting from the NBA, and more. Customers can also now use Alexa to control more than 4,000 smart home devices from 1,200 unique brands.\\n\\nAmazon launched new ways for developers to earn money building for\\n\\nAlexa, including paid skill content through in-skill purchasing,\\n\\npremium subscription content, and a more frictionless checkout\\n\\nexperience with Amazon Pay. New skills offering premium paid content\\n\\ninclude The Ellen DeGeneres Show’s popular Heads Up! game, The Match\\n\\nGame, and the History Channel’s Ultimate History Quiz.\\n\\nAmazon launched new ways for developers to earn money building for Alexa, including paid skill content through in-skill purchasing, premium subscription content, and a more frictionless checkout experience with Amazon Pay. New skills offering premium paid content include The Ellen DeGeneres Show’s popular Heads Up! game, The Match Game, and the History Channel’s Ultimate History Quiz.\\n\\nAlexa Voice Service (AVS) adoption among device makers continues to\\n\\ngrow. Brands announced new AVS products at the 2018 Consumer\\n\\nElectronics Show, including: new integrations for Alexa on PCs from\\n\\nHP, Acer, ASUS, and Lenovo; a new automotive integration with Toyota;\\n\\nand new devices from Polk Audio, Anker, Jabra, and more.\\n\\nAlexa Voice Service (AVS) adoption among device makers continues to grow. Brands announced new AVS products at the 2018 Consumer Electronics Show, including: new integrations for Alexa on PCs from HP, Acer, ASUS, and Lenovo; a new automotive integration with Toyota; and new devices from Polk Audio, Anker, Jabra, and more.\\n\\nAmazon also introduced new tools and developer kits, including the\\n\\nAlexa Mobile Accessory Kit and Alexa Premium Far-Field Voice\\n\\nDevelopment Kit, to make it easier for developers to bring Alexa to\\n\\nmore devices.\\n\\nAmazon also introduced new tools and developer kits, including the Alexa Mobile Accessory Kit and Alexa Premium Far-Field Voice Development Kit, to make it easier for developers to bring Alexa to more devices.\\n\\nAmazon hired nearly 130,000 employees globally in 2017, excluding\\n\\nacquisitions. Additionally, Amazon now employs more than 17,500\\n\\nveterans and military spouses across the U.S., and plans to hire over\\n\\n10,000 more by 2021.\\n\\nAmazon hired nearly 130,000 employees globally in 2017, excluding acquisitions. Additionally, Amazon now employs more than 17,500 veterans and military spouses across the U.S., and plans to hire over 10,000 more by 2021.\\n\\nAmazon welcomed several new device makers to the Dash Replenishment\\n\\nprogram to enable their smart appliances to automatically reorder\\n\\nconsumables when supply runs low, including 3M, Hewlett-Packard,\\n\\nKenmore, and Bluestream.\\n\\nAmazon welcomed several new device makers to the Dash Replenishment program to enable their smart appliances to automatically reorder consumables when supply runs low, including 3M, Hewlett-Packard, Kenmore, and Bluestream.\\n\\nAmazon celebrated the 10 th anniversary of Kindle by\\n\\nreleasing the all-new Kindle Oasis, the most advanced Kindle with a\\n\\n7-inch, 300 ppi display, waterproof design, and access to the world’s\\n\\nlargest library of audiobooks with Audible support. Since the Kindle\\n\\nlaunch in November 2007, customers have purchased tens of millions of\\n\\nKindle e-readers.\\n\\nAmazon celebrated the 10 anniversary of Kindle by releasing the all-new Kindle Oasis, the most advanced Kindle with a 7-inch, 300 ppi display, waterproof design, and access to the world’s largest library of audiobooks with Audible support. Since the Kindle launch in November 2007, customers have purchased tens of millions of Kindle e-readers.\\n\\nAmazon announced that the Prime Video app is now available on Apple TV\\n\\nin over 100 countries. Prime members now have more ways to stream\\n\\naward-winning and critically-acclaimed titles, including Amazon\\n\\nOriginal Movies and Prime Originals.\\n\\nAmazon announced that the Prime Video app is now available on Apple TV in over 100 countries. Prime members now have more ways to stream award-winning and critically-acclaimed titles, including Amazon Original Movies and Prime Originals.\\n\\nAmazon Studios’ Original movie The Big Sick was nominated for\\n\\nan Academy Award for Best Original Screenplay, written by Emily V.\\n\\nGordon and Kumail Nanjiani.\\n\\nAmazon Studios’ Original movie The Big Sick was nominated for an Academy Award for Best Original Screenplay, written by Emily V. Gordon and Kumail Nanjiani.\\n\\nThe Amazon Original Series The Marvelous Mrs. Maisel won two\\n\\nGolden Globes for “Best Television Series - Musical or Comedy” and\\n\\n“Best Actress - Musical or Comedy” (Rachel Brosnahan), as well as two\\n\\nCritics Choice awards for “Best Comedy Series” and “Best Actress in a\\n\\nComedy Series” (Rachel Brosnahan).\\n\\nThe Amazon Original Series The Marvelous Mrs. Maisel won two Golden Globes for “Best Television Series - Musical or Comedy” and “Best Actress - Musical or Comedy” (Rachel Brosnahan), as well as two Critics Choice awards for “Best Comedy Series” and “Best Actress in a Comedy Series” (Rachel Brosnahan).\\n\\nAmazon Studios debuted the highly-anticipated second season of The\\n\\nGrand Tour. Additionally, Prime members can look forward to new\\n\\nand returning series this year such as Goliath season two, Sneaky\\n\\nPete season two, The Man in the High Castle season three, Bosch\\n\\nseason four, Mozart in the Jungle season four, and new episodes\\n\\nfrom The Tick, Tom Clancy’s Jack Ryan, and The\\n\\nRomanoffs.\\n\\nAmazon Studios debuted the highly-anticipated second season of The Grand Tour. Additionally, Prime members can look forward to new and returning series this year such as Goliath season two, Sneaky Pete season two, The Man in the High Castle season three, Bosch season four, Mozart in the Jungle season four, and new episodes from The Tick, Tom Clancy’s Jack Ryan, and The Romanoffs.\\n\\nNFL Thursday Night Football on Amazon Prime Video saw a total of 18.4\\n\\nmillion views in 11 games. Prime members in more than 200 countries\\n\\nand territories streamed games on living room devices, including smart\\n\\nTVs and Fire TVs, as well as the Prime Video mobile app and the web.\\n\\nNFL Thursday Night Football on Amazon Prime Video saw a total of 18.4 million views in 11 games. Prime members in more than 200 countries and territories streamed games on living room devices, including smart TVs and Fire TVs, as well as the Prime Video mobile app and the web. CBS All Access is now available through Amazon Channels, the\\n\\nover-the-top streaming subscription program for Prime members. CBS is\\n\\nthe first Amazon Channels partner to offer a linear feed of a\\n\\nsubscriber’s local broadcast station in addition to video on-demand.\\n\\nover-the-top streaming subscription program for Prime members. CBS is the first Amazon Channels partner to offer a linear feed of a subscriber’s local broadcast station in addition to video on-demand.\\n\\nAmazon acquired the global television rights to The Lord of the\\n\\nRings, based on the novels by J.R.R. Tolkien, with a multi-season\\n\\ncommitment.\\n\\nAmazon acquired the global television rights to The Lord of the Rings, based on the novels by J.R.R. Tolkien, with a multi-season commitment.\\n\\nAmazon Prime Video continues to launch local Original Series,\\n\\nincluding Breathe in India, as well as Pastewka and Glory\\n\\nis Gone in Germany.\\n\\nAmazon Prime Video continues to launch local Original Series, including Breathe in India, as well as Pastewka and Glory is Gone in Germany.\\n\\nAmazon introduced all-new Alexa experiences built from the ground up\\n\\nfor customers in Japan, India, Canada, Australia, and New Zealand.\\n\\nAdditionally, Amazon expanded Alexa, Echo, and Amazon Music Unlimited\\n\\nto 30 new countries, offering customers access to tens of millions of\\n\\nsongs, thousands of hand-curated playlists and stations, and enabling\\n\\ncustomers who purchase Echo devices the ability to experience Amazon\\n\\nMusic Unlimited with natural voice controls powered by Alexa.\\n\\nAmazon introduced all-new Alexa experiences built from the ground up for customers in Japan, India, Canada, Australia, and New Zealand. Additionally, Amazon expanded Alexa, Echo, and Amazon Music Unlimited to 30 new countries, offering customers access to tens of millions of songs, thousands of hand-curated playlists and stations, and enabling customers who purchase Echo devices the ability to experience Amazon Music Unlimited with natural voice controls powered by Alexa.\\n\\nAmazon launched two furniture brands: Rivet, offering affordable and\\n\\nversatile mid-century modern furniture ideal for smaller spaces; and\\n\\nStone & Beam, offering durable and stylish furniture for the modern\\n\\nhousehold.\\n\\nAmazon launched two furniture brands: Rivet, offering affordable and versatile mid-century modern furniture ideal for smaller spaces; and Stone & Beam, offering durable and stylish furniture for the modern household.\\n\\nAmazon Go, a new kind of store with no checkout required, is now open\\n\\nto the public in Seattle. The checkout-free shopping experience is\\n\\nmade possible by the same types of technologies used in self-driving\\n\\ncars: computer vision, machine learning, and sensor fusion.\\n\\nAmazon Go, a new kind of store with no checkout required, is now open to the public in Seattle. The checkout-free shopping experience is made possible by the same types of technologies used in self-driving cars: computer vision, machine learning, and sensor fusion.\\n\\nFulfillment by Amazon (FBA) shipped billions of items for small and\\n\\nmedium-sized businesses, selling on Amazon worldwide in 2017.\\n\\nFulfillment by Amazon (FBA) shipped billions of items for small and medium-sized businesses, selling on Amazon worldwide in 2017.\\n\\nAmazon launched its retail and third-party marketplace offering in\\n\\nAustralia with fast delivery on millions of products, including items\\n\\nfrom thousands of small and medium-sized businesses.\\n\\nAmazon launched its retail and third-party marketplace offering in Australia with fast delivery on millions of products, including items from thousands of small and medium-sized businesses.\\n\\nAmazon launched Prime in the Netherlands and Luxembourg, and added the\\n\\nability for customers in Belgium to join Prime and shop in Dutch.\\n\\nThese programs offer members access to unlimited free one-day and\\n\\ntwo-day delivery, unlimited streaming of Prime Video, Twitch Prime,\\n\\nand early access to Prime Lightning Deals — all for an introductory\\n\\nprice of €3.99 per month.\\n\\nAmazon launched Prime in the Netherlands and Luxembourg, and added the ability for customers in Belgium to join Prime and shop in Dutch. These programs offer members access to unlimited free one-day and two-day delivery, unlimited streaming of Prime Video, Twitch Prime, and early access to Prime Lightning Deals — all for an introductory price of €3.99 per month.\\n\\nAmazon launched Prime in Singapore, offering members access to\\n\\nunlimited free shipping on millions of local and international items,\\n\\nunlimited access to popular movies and TV shows on Prime Video, and\\n\\nvideo game benefits with Twitch — all for an introductory price of\\n\\nS$2.99 per month.\\n\\nAmazon launched Prime in Singapore, offering members access to unlimited free shipping on millions of local and international items, unlimited access to popular movies and TV shows on Prime Video, and video game benefits with Twitch — all for an introductory price of S$2.99 per month.\\n\\nPrime selection in India now offers members more than 25 million local\\n\\nproducts from third-party sellers.\\n\\nPrime selection in India now offers members more than 25 million local products from third-party sellers.\\n\\nAmazon.com.br continues to expand its third-party marketplace,\\n\\nincluding the launch of Consumer Electronics, Home & Kitchen, Tools &\\n\\nHome Improvement, and Office Products. Customers in Brazil now have\\n\\naccess to hundreds of thousands of products as well as more than 13\\n\\nmillion print and digital books.\\n\\nAmazon.com.br continues to expand its third-party marketplace, including the launch of Consumer Electronics, Home & Kitchen, Tools & Home Improvement, and Office Products. Customers in Brazil now have access to hundreds of thousands of products as well as more than 13 million print and digital books.\\n\\nAmazon celebrated its 10 th holiday season of\\n\\nFrustration-Free Packaging — an invention designed to reduce waste and\\n\\ndelight customers with easy-to-open, 100% recyclable packaging. As of\\n\\nthe end of 2017, Amazon’s sustainable packaging innovations have\\n\\nhelped to eliminate nearly 215,000 tons of packaging material and 360\\n\\nmillion boxes.\\n\\nAmazon celebrated its 10 holiday season of Frustration-Free Packaging — an invention designed to reduce waste and delight customers with easy-to-open, 100% recyclable packaging. As of the end of 2017, Amazon’s sustainable packaging innovations have helped to eliminate nearly 215,000 tons of packaging material and 360 million boxes.\\n\\nAmazon, Berkshire Hathaway, and JPMorgan Chase & Co. announced a\\n\\npartnership to address healthcare for their U.S. employees, with the\\n\\naim of improving employee satisfaction and reducing costs.\\n\\nAmazon, Berkshire Hathaway, and JPMorgan Chase & Co. announced a partnership to address healthcare for their U.S. employees, with the aim of improving employee satisfaction and reducing costs.\\n\\nIn December, Amazon’s “Delivering Smiles” holiday tour stopped in over\\n\\n30 communities where Amazon employees live and work, donating\\n\\nthousands of items to women, children, and families fighting\\n\\nhomelessness. For each mile traveled along the delivery route, Amazon\\n\\ndonated $1 to the National Alliance to End Homelessness.\\n\\nIn December, Amazon’s “Delivering Smiles” holiday tour stopped in over 30 communities where Amazon employees live and work, donating thousands of items to women, children, and families fighting homelessness. For each mile traveled along the delivery route, Amazon donated $1 to the National Alliance to End Homelessness.\\n\\nAmazon reviewed 238 proposals from across the U.S., Canada, and Mexico\\n\\nto host HQ2, the company’s second headquarters in North America, and\\n\\nselected 20 metropolitan areas to move to the next phase of the\\n\\nprocess. Amazon expects to invest over $5 billion and create as many\\n\\nas 50,000 high-paying jobs at its new second home.\\n\\nAmazon reviewed 238 proposals from across the U.S., Canada, and Mexico to host HQ2, the company’s second headquarters in North America, and selected 20 metropolitan areas to move to the next phase of the process. Amazon expects to invest over $5 billion and create as many as 50,000 high-paying jobs at its new second home.\\n\\nAmazon Web Services (AWS) announced several enterprise customers\\n\\nduring the quarter: Expedia, Ellucian, and DigitalGlobe are going\\n\\nall-in on AWS; The Walt Disney Company and Turner named AWS their\\n\\npreferred public cloud provider; Symantec will leverage AWS as its\\n\\nstrategic infrastructure provider for the vast majority of its cloud\\n\\nworkloads; Expedia, Intuit, the National Football League (NFL),\\n\\nCapital One, DigitalGlobe, and Cerner announced they’ve chosen AWS for\\n\\nmachine learning and artificial intelligence; and Bristol-Myers\\n\\nSquibb, Honeywell, Experian, FICO, Insitu, LexisNexis, Sysco,\\n\\nDiscovery Communications, Dow Jones, and Ubisoft kicked off major new\\n\\nmoves to AWS.\\n\\nAmazon Web Services (AWS) announced several enterprise customers during the quarter: Expedia, Ellucian, and DigitalGlobe are going all-in on AWS; The Walt Disney Company and Turner named AWS their preferred public cloud provider; Symantec will leverage AWS as its strategic infrastructure provider for the vast majority of its cloud workloads; Expedia, Intuit, the National Football League (NFL), Capital One, DigitalGlobe, and Cerner announced they’ve chosen AWS for machine learning and artificial intelligence; and Bristol-Myers Squibb, Honeywell, Experian, FICO, Insitu, LexisNexis, Sysco, Discovery Communications, Dow Jones, and Ubisoft kicked off major new moves to AWS.\\n\\nAWS continued to expand its infrastructure in 2017 to best serve\\n\\ncustomers, launching a new region in France and a second AWS Region in\\n\\nChina during the quarter. AWS plans to open 12 more Availability Zones\\n\\nacross four regions (Bahrain, Hong Kong, Sweden, and a second GovCloud\\n\\nRegion in the U.S.) between now and early 2019. AWS now operates 52\\n\\nAvailability Zones across 18 infrastructure regions globally.\\n\\nAWS continued to expand its infrastructure in 2017 to best serve customers, launching a new region in France and a second AWS Region in China during the quarter. AWS plans to open 12 more Availability Zones across four regions (Bahrain, Hong Kong, Sweden, and a second GovCloud Region in the U.S.) between now and early 2019. AWS now operates 52 Availability Zones across 18 infrastructure regions globally.\\n\\nAWS continues to accelerate its pace of innovation with the release of\\n\\n497 significant new services and features in the fourth quarter,\\n\\nbringing the total number of launches in 2017 to 1,430.\\n\\nAWS continues to accelerate its pace of innovation with the release of 497 significant new services and features in the fourth quarter, bringing the total number of launches in 2017 to 1,430.\\n\\nAWS announced Amazon SageMaker, a fully managed service that removes\\n\\nthe heavy lifting, complexity, and guesswork from each step of the\\n\\nmachine learning process, empowering everyday developers and\\n\\nscientists to use machine learning much more expansively and\\n\\nsuccessfully. Amazon SageMaker makes model building and training\\n\\neasier by providing prebuilt development notebooks, popular machine\\n\\nlearning algorithms optimized for petabyte-scale datasets, and\\n\\nautomatic model tuning, enabling developers to build, train, and\\n\\ndeploy models in a single click. Since its launch two months ago,\\n\\nAmazon SageMaker is already helping thousands of developers to easily\\n\\nget started and become competent in building, training, and deploying\\n\\nmodels.\\n\\nAWS announced Amazon SageMaker, a fully managed service that removes the heavy lifting, complexity, and guesswork from each step of the machine learning process, empowering everyday developers and scientists to use machine learning much more expansively and successfully. Amazon SageMaker makes model building and training easier by providing prebuilt development notebooks, popular machine learning algorithms optimized for petabyte-scale datasets, and automatic model tuning, enabling developers to build, train, and deploy models in a single click. Since its launch two months ago, Amazon SageMaker is already helping thousands of developers to easily get started and become competent in building, training, and deploying models.\\n\\nAWS launched EC2 P3 instances which are optimized for machine learning\\n\\nand high performance computing — providing up to six times better\\n\\nperformance than any other GPU instances available in the cloud today.\\n\\nP3 instances significantly reduce the time it takes to train machine\\n\\nlearning models, providing a substantial boost in agility for\\n\\ndevelopers experimenting and optimizing machine learning applications.\\n\\nThe combination of EC2 P3 instances and Amazon SageMaker provide\\n\\ndevelopers and data scientists access to the industry’s most powerful\\n\\nand easy-to-use solution for building machine learning applications.\\n\\nAWS launched EC2 P3 instances which are optimized for machine learning and high performance computing — providing up to six times better performance than any other GPU instances available in the cloud today. P3 instances significantly reduce the time it takes to train machine learning models, providing a substantial boost in agility for developers experimenting and optimizing machine learning applications. The combination of EC2 P3 instances and Amazon SageMaker provide developers and data scientists access to the industry’s most powerful and easy-to-use solution for building machine learning applications.\\n\\nAWS introduced four Artificial Intelligence (AI) services that allow\\n\\ndevelopers to build applications that emulate human-like cognition:\\n\\nAmazon Transcribe for converting speech to text; Amazon Translate for\\n\\ntranslating text between languages; Amazon Comprehend for\\n\\nunderstanding relationships and finding insights within text; and\\n\\nAmazon Rekognition Video, a deep-learning powered video analysis\\n\\nservice that tracks people, detects activities, and recognizes\\n\\nobjects, celebrities, and inappropriate content.\\n\\nAWS introduced four Artificial Intelligence (AI) services that allow developers to build applications that emulate human-like cognition: Amazon Transcribe for converting speech to text; Amazon Translate for translating text between languages; Amazon Comprehend for understanding relationships and finding insights within text; and Amazon Rekognition Video, a deep-learning powered video analysis service that tracks people, detects activities, and recognizes objects, celebrities, and inappropriate content.\\n\\nAWS launched AWS DeepLens, a deep-learning enabled wireless video\\n\\ncamera that pairs an HD camera developer kit with a set of sample\\n\\nprojects to help developers learn machine learning concepts, including\\n\\ncomputer vision and deep learning.\\n\\nAWS launched AWS DeepLens, a deep-learning enabled wireless video camera that pairs an HD camera developer kit with a set of sample projects to help developers learn machine learning concepts, including computer vision and deep learning.\\n\\nAWS introduced several services and capabilities for connected devices\\n\\nat the edge: AWS IoT 1-Click makes it easy for simple devices to\\n\\ntrigger AWS Lambda functions that execute a specific action; AWS IoT\\n\\nDevice Management helps to securely onboard, organize, monitor, and\\n\\nremotely manage customers’ IoT devices at scale throughout their\\n\\nlifecycle; AWS IoT Device Defender allows customers to secure their\\n\\nfleet of IoT devices on an ongoing basis; AWS IoT Analytics enables\\n\\ncustomers to cleanse, process, enrich, store, and analyze IoT device\\n\\ndata at scale; Amazon FreeRTOS is an IoT operating system for\\n\\nmicrocontrollers that makes small, low-powered edge devices easy to\\n\\nprogram, deploy, secure, connect, and maintain; and AWS Greengrass\\n\\nMachine Learning (ML) Inference makes it easy to perform ML inference\\n\\nlocally on AWS Greengrass devices even when they are not connected to\\n\\nthe cloud.\\n\\nAWS introduced several services and capabilities for connected devices at the edge: AWS IoT 1-Click makes it easy for simple devices to trigger AWS Lambda functions that execute a specific action; AWS IoT Device Management helps to securely onboard, organize, monitor, and remotely manage customers’ IoT devices at scale throughout their lifecycle; AWS IoT Device Defender allows customers to secure their fleet of IoT devices on an ongoing basis; AWS IoT Analytics enables customers to cleanse, process, enrich, store, and analyze IoT device data at scale; Amazon FreeRTOS is an IoT operating system for microcontrollers that makes small, low-powered edge devices easy to program, deploy, secure, connect, and maintain; and AWS Greengrass Machine Learning (ML) Inference makes it easy to perform ML inference locally on AWS Greengrass devices even when they are not connected to the cloud.\\n\\nIn its second year of availability, the number of databases migrated\\n\\nto AWS in 2017 using the AWS Database Migration Service is\\n\\naccelerating — growing more than 100% over 2016 to a total of more\\n\\nthan 54,000 databases migrated since the introduction of the service.\\n\\nIn its second year of availability, the number of databases migrated to AWS in 2017 using the AWS Database Migration Service is accelerating — growing more than 100% over 2016 to a total of more than 54,000 databases migrated since the introduction of the service.\\n\\nAWS launched the Amazon ML Solutions Lab, a program that connects\\n\\nmachine learning experts from across Amazon with AWS customers to help\\n\\nidentify practical uses of machine learning inside customers’\\n\\nbusinesses, and guide them in developing new machine learning-enabled\\n\\nfeatures, products, and processes. The Amazon ML Solutions Lab\\n\\ncombines hands-on educational workshops with brainstorming sessions to\\n\\nhelp customers “work backwards” from business challenges, and\\n\\nunderstand the step-by-step processes for developing machine\\n\\nlearning-based solutions.\\n\\nAWS launched the Amazon ML Solutions Lab, a program that connects machine learning experts from across Amazon with AWS customers to help identify practical uses of machine learning inside customers’ businesses, and guide them in developing new machine learning-enabled features, products, and processes. The Amazon ML Solutions Lab combines hands-on educational workshops with brainstorming sessions to help customers “work backwards” from business challenges, and understand the step-by-step processes for developing machine learning-based solutions.\\n\\nAWS introduced several new database capabilities: Amazon Aurora\\n\\nServerless is an on-demand auto-scaling configuration for Amazon\\n\\nAurora that saves customers time and money by automatically adjusting\\n\\ndatabase capacity to match application needs; Amazon DynamoDB with\\n\\nGlobal Tables is the first fully managed database service that\\n\\nprovides true multi-master, multi-region read and writes, offering\\n\\nhigh-performance and low-latency for globally distributed applications\\n\\nand users; and Amazon Neptune is a fast, reliable, fully managed graph\\n\\ndatabase service that makes it easy for developers to build and run\\n\\napplications that work with highly connected datasets.\\n\\nAWS introduced several new database capabilities: Amazon Aurora Serverless is an on-demand auto-scaling configuration for Amazon Aurora that saves customers time and money by automatically adjusting database capacity to match application needs; Amazon DynamoDB with Global Tables is the first fully managed database service that provides true multi-master, multi-region read and writes, offering high-performance and low-latency for globally distributed applications and users; and Amazon Neptune is a fast, reliable, fully managed graph database service that makes it easy for developers to build and run applications that work with highly connected datasets.\\n\\nAWS announced two new container capabilities that make it easier to\\n\\ndeploy, manage, and scale container workloads on AWS. Amazon Elastic\\n\\nContainer Service for Kubernetes (Amazon EKS) brings Kubernetes to AWS\\n\\nas a fully managed service, enabling customers to run Kubernetes\\n\\napplications on AWS without the need to become experts in operating\\n\\nKubernetes clusters. AWS also introduced AWS Fargate that allows\\n\\ncustomers to launch and run containers without provisioning or\\n\\nmanaging servers or clusters.\\n\\nAWS announced two new container capabilities that make it easier to deploy, manage, and scale container workloads on AWS. Amazon Elastic Container Service for Kubernetes (Amazon EKS) brings Kubernetes to AWS as a fully managed service, enabling customers to run Kubernetes applications on AWS without the need to become experts in operating Kubernetes clusters. AWS also introduced AWS Fargate that allows customers to launch and run containers without provisioning or managing servers or clusters.\\n\\nAWS launched three new Amazon EC2 instances: the I3 High I/O family\\n\\nintroduces a brand new capability with Amazon EC2 Bare Metal instances\\n\\nthat enable customers to run workloads directly on AWS hardware, or\\n\\nbring their own hypervisor or virtualization stack; H1 is a new family\\n\\nof Storage Optimized instances designed for applications that require\\n\\nlow cost, high disk throughput and high sequential disk I/O access to\\n\\nvery large data sets; and M5, the next generation of General Purpose\\n\\ninstances, provides higher performance and lower prices for general\\n\\npurpose workloads, delivering even better compute, memory, and\\n\\nnetworking performance, powered by the latest 2.5 GHz Intel Xeon\\n\\nPlatinum 8000 series processors.\\n\\nAWS launched three new Amazon EC2 instances: the I3 High I/O family introduces a brand new capability with Amazon EC2 Bare Metal instances that enable customers to run workloads directly on AWS hardware, or bring their own hypervisor or virtualization stack; H1 is a new family of Storage Optimized instances designed for applications that require low cost, high disk throughput and high sequential disk I/O access to very large data sets; and M5, the next generation of General Purpose instances, provides higher performance and lower prices for general purpose workloads, delivering even better compute, memory, and networking performance, powered by the latest 2.5 GHz Intel Xeon Platinum 8000 series processors.\\n\\nAWS announced Amazon GuardDuty, a fully managed intelligent threat\\n\\ndetection service that helps customers protect their AWS accounts and\\n\\nworkloads by continuously monitoring and analyzing account activity\\n\\nfor malicious or unauthorized behavior. Amazon GuardDuty applies\\n\\nmachine learning to identify anomalies and alert customers.\\n\\nAWS announced Amazon GuardDuty, a fully managed intelligent threat detection service that helps customers protect their AWS accounts and workloads by continuously monitoring and analyzing account activity for malicious or unauthorized behavior. Amazon GuardDuty applies machine learning to identify anomalies and alert customers.\\n\\nAWS introduced the preview of Amazon S3 Select, which lets customers\\n\\npull out only the data they need from an S3 object instead of\\n\\nretrieving the entire object, dramatically improving the performance\\n\\nand reducing the cost of applications that need to access data in\\n\\nAmazon S3. AWS also announced Amazon Glacier Select, which allows\\n\\ncompanies in highly regulated industries to easily query cold,\\n\\narchived data in Amazon Glacier, and unlock new business value for\\n\\narchived data.\\n\\nAWS introduced the preview of Amazon S3 Select, which lets customers pull out only the data they need from an S3 object instead of retrieving the entire object, dramatically improving the performance and reducing the cost of applications that need to access data in Amazon S3. AWS also announced Amazon Glacier Select, which allows companies in highly regulated industries to easily query cold, archived data in Amazon Glacier, and unlock new business value for archived data.\\n\\nAWS launched Amazon Sumerian, a new service that makes it easy for any\\n\\ndeveloper to quickly and easily build virtual reality, augmented\\n\\nreality, and 3D applications to run on mobile devices, head-mounted\\n\\ndisplays, digital signage, or web browsers. Apps created in Amazon\\n\\nSumerian will run in any browser that supports WebGL or WebVR graphics\\n\\nrendering, including Daydream, HTC Vive, Oculus Rift, and iOS mobile\\n\\ndevices.\\n\\nAWS launched Amazon Sumerian, a new service that makes it easy for any developer to quickly and easily build virtual reality, augmented reality, and 3D applications to run on mobile devices, head-mounted displays, digital signage, or web browsers. Apps created in Amazon Sumerian will run in any browser that supports WebGL or WebVR graphics rendering, including Daydream, HTC Vive, Oculus Rift, and iOS mobile devices.\\n\\nAWS announced AWS Media Services, a family of five integrated\\n\\nbroadcast-quality media services that make it easy for video providers\\n\\nof all kinds to create reliable, flexible, and scalable video\\n\\nofferings in the cloud. These five services enable customers to build\\n\\nend-to-end workflows for both live and on-demand video with the\\n\\nprofessional features, image quality, and reliability needed to\\n\\ndeliver premium video experiences to viewers across a multitude of\\n\\ndevices. By combining the proven video solutions from AWS Elemental\\n\\nwith the security, durability, availability, and scalability of AWS,\\n\\nvideo providers can focus on innovating and making great content\\n\\ninstead of spending time building and maintaining on-premises video\\n\\ninfrastructure.\\n\\nAWS announced AWS Media Services, a family of five integrated broadcast-quality media services that make it easy for video providers of all kinds to create reliable, flexible, and scalable video offerings in the cloud. These five services enable customers to build end-to-end workflows for both live and on-demand video with the professional features, image quality, and reliability needed to deliver premium video experiences to viewers across a multitude of devices. By combining the proven video solutions from AWS Elemental with the security, durability, availability, and scalability of AWS, video providers can focus on innovating and making great content instead of spending time building and maintaining on-premises video infrastructure.\\n\\nAWS launched Alexa for Business, a new service that brings Alexa into\\n\\nthe workplace to help employees be more productive and organized on\\n\\nboth personal and shared Echo devices by simply using their voice.\\n\\nEmployees can use Alexa for Business to find an open conference room,\\n\\nmake phone calls, check calendars, schedule and start meetings, manage\\n\\nto-do lists, set reminders, and even find information in popular\\n\\nbusiness applications like Salesforce, Concur, or Splunk.\\n\\nAWS launched Alexa for Business, a new service that brings Alexa into the workplace to help employees be more productive and organized on both personal and shared Echo devices by simply using their voice. Employees can use Alexa for Business to find an open conference room, make phone calls, check calendars, schedule and start meetings, manage to-do lists, set reminders, and even find information in popular business applications like Salesforce, Concur, or Splunk.\\n\\nAt re:Invent, AWS and VMware Inc. announced that VMware Cloud on AWS\\n\\nis expanding availability from the U.S. West (Oregon) region to\\n\\ninclude the AWS U.S. East (N. Virginia) region. AWS also announced\\n\\nadditional VMware capabilities and support for more AWS services,\\n\\nmaking it even easier for customers to move, run, and protect\\n\\nmission-critical applications at scale.\\n\\nFinancial Guidance\\n\\n\\n\\nThe following forward-looking statements reflect Amazon.com’s\\n\\nexpectations as of February 1, 2018, and are subject to substantial\\n\\nuncertainty. Our results are inherently unpredictable and may be\\n\\nmaterially affected by many factors, such as fluctuations in foreign\\n\\nexchange rates, changes in global economic conditions and customer\\n\\nspending, world events, the rate of growth of the Internet, online\\n\\ncommerce, and cloud services, and the various factors detailed below.\\n\\n\\n\\nFirst Quarter 2018 Guidance\\n\\n\\n\\nNet sales are expected to be between $47.75 billion and $50.75\\n\\nbillion, or to grow between 34% and 42% compared with first quarter\\n\\n2017. This guidance anticipates a favorable impact of approximately\\n\\n$1.2 billion or 330 basis points from foreign exchange rates.\\n\\nNet sales are expected to be between $47.75 billion and $50.75 billion, or to grow between 34% and 42% compared with first quarter 2017. This guidance anticipates a favorable impact of approximately $1.2 billion or 330 basis points from foreign exchange rates.\\n\\nOperating income is expected to be between $300 million and $1.0\\n\\nbillion, compared with $1.0 billion in first quarter 2017.\\n\\nOperating income is expected to be between $300 million and $1.0 billion, compared with $1.0 billion in first quarter 2017.\\n\\nThis guidance assumes, among other things, that no additional business\\n\\nacquisitions, investments, restructurings, or legal settlements are\\n\\nconcluded.\\n\\n\\n\\nA conference call will be webcast live today at 2:30 p.m. PT/5:30 p.m.\\n\\nET, and will be available for at least three months at\\n\\nwww.amazon.com/ir. This call will contain forward-looking statements and\\n\\nother material information regarding the Company’s financial and\\n\\noperating results.\\n\\nThese forward-looking statements are inherently difficult to predict.\\n\\nActual results could differ materially for a variety of reasons,\\n\\nincluding, in addition to the factors discussed above, the amount that\\n\\nAmazon.com invests in new business opportunities and the timing of those\\n\\ninvestments, the mix of products and services sold to customers, the mix\\n\\nof net sales derived from products as compared with services, the extent\\n\\nto which we owe income or other taxes, competition, management of\\n\\ngrowth, potential fluctuations in operating results, international\\n\\ngrowth and expansion, the outcomes of legal proceedings and claims,\\n\\nfulfillment, sortation, delivery, and data center optimization, risks of\\n\\ninventory management, seasonality, the degree to which the Company\\n\\nenters into, maintains, and develops commercial agreements, proposed and\\n\\ncompleted acquisitions and strategic transactions, payments risks, and\\n\\nrisks of fulfillment throughput and productivity. Other risks and\\n\\nuncertainties include, among others, risks related to new products,\\n\\nservices, and technologies, system interruptions, government regulation\\n\\nand taxation, and fraud. In addition, the current global economic\\n\\nclimate amplifies many of these risks. More information about factors\\n\\nthat potentially could affect Amazon.com’s financial results is included\\n\\nin Amazon.com’s filings with the Securities and Exchange Commission\\n\\n(“SEC”), including its most recent Annual Report on Form 10-K and\\n\\nsubsequent filings.\\n\\nStory Continues')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Vulkan 1.1 Released', 'Vulkan ecosystem momentum continues to grow with improved developer tools, wide industry adoption, and new specification for evolved functionality and performance\\n\\nBeaverton, OR – March 7, 2018 – The Khronos™ Group, an open consortium of leading hardware and software companies creating advanced acceleration standards, announces the release of the Vulkan® 1.1 and SPIR-V™ 1.3 specifications. Version 1.1 expands Vulkan’s core functionality with developer-requested features, such as subgroup operations, while integrating a wide range of proven extensions from Vulkan 1.0. Khronos will also release full Vulkan 1.1 conformance tests into open source and AMD, Arm, Imagination, Intel Corporation, NVIDIA and Qualcomm have implemented conformant Vulkan 1.1 drivers. Find more information on the Vulkan 1.1 specification and associated tests and tools at Khronos’s Vulkan Resource Page.\\n\\n“With enhanced developer tools, rigorous conformance testing and the public Vulkan Ecosystem Forum, Khronos is delivering on its goal to develop a complete and vibrant Vulkan ecosystem,” said Tom Olson, distinguished engineer at Arm, and Vulkan Working Group chair. “Vulkan 1.1 is a response to prioritized industry requests and shows our commitment to delivering a functional roadmap driven by developer needs.”\\n\\nVulkan 1.1 will drive increased industry momentum for this new-generation, cross-platform standard for explicit control over GPU acceleration. Vulkan now ships natively on almost all GPU-enabled platforms, including Windows 7, 8.X, 10, Android 7.0+ and Linux, plus Khronos recently announced open source tools to enable Vulkan 1.0 applications to be ported to macOS and iOS. Vulkan has widespread support in leading games engines including Unreal, Unity, Source 2 from Valve, id Tech, CroTeam’s Serious Engine, CryEngine, and Xenko. Vulkan is being used in over 30 cutting-edge games on diverse desktop and mobile platforms, including Doom, Quake, Roblox, The Talos Principle, Dota 2, and is the exclusive API used in AAA titles such as Wolfenstein II and Doom VFR.\\n\\nNew functionality in Vulkan 1.1 includes Subgroup Operations that enable highly-efficient sharing and manipulation of data between multiple tasks running in parallel on a GPU. Vulkan 1.1 also enables applications to perform rendering and display operations using resources that they cannot access or copy - for secure playback and display of protected multimedia content.\\n\\nIn addition, a wide range of Vulkan 1.0 extensions have been integrated, bringing significant proven functionality into core Vulkan 1.1, including: simultaneous rendering of multiple image views, use of multiple GPUs in a single system, and cross-process API interoperability for advanced rendering and compositing operations often used in demanding applications such as Virtual Reality. These core functionalities also include advanced compute with 16-bit memory access, and support for HLSL memory layouts, and display, processing and compositing of video streams, through direct sampling of YCbCr color formatted textures produced by many video codecs.\\n\\nIntegral to the release of Vulkan 1.1 is the new SPIR-V 1.3 specification that expands the capabilities of the Vulkan shader intermediate representation to support subgroup operations and enable enhanced compiler optimizations. The SPIR-V tools ecosystem continues to gain significant momentum with front-end compilers for both GLSL and HLSL, and to expand low-level tooling support from the open source SPIRV-Tools project.\\n\\n“We are excited to see the progress developers have made with the SPIR-V standardized IR. Developers are using the shader language of their choice and a variety of open source compilers to ship their games and applications. The Vulkan tools and ecosystem is evolving rapidly,” said David Neto, shader compiler team lead at Google and SPIR working group chair.\\n\\nOpen source Vulkan development tools continue to evolve alongside the specification. For example, the LunarG Vulkan SDK and tools layers have been upgraded to support Vulkan 1.1, including the Vulkan Layer Factory (VLF) to enable rapid layer development, the Device Simulation Layer to simulate target device capabilities, without requiring actual physical hardware and the new Assistant Layer to guide developers to best practices and to highlight potential application problems. In addition, the RenderDoc frame capture and introspection debugging tool has added full native Vulkan support on Android with help from Khronos members, and improved support for displaying SPIR-V disassembly using SPIRV-Tools and high-level languages through the SPIRV-Cross cross compiler. RenderDoc has also been upgraded to expose native disassembly and profiling information for vendors who support it, and to support the external memory features that now form a core part of Vulkan 1.1.\\n\\nTo encourage the collaborative evolution of the Vulkan ecosystem, Khronos has created the public Vulkan Ecosystem Forum to share issues and opportunities, and to coordinate cooperative solutions. The aim of the Forum is to respond to developer feedback and foster cross-functional discussions and engagement between users, tools developers and API designers. Join the conversation at Vulkan Ecosystem on GitHub.\\n\\nIndustry Support for Vulkan 1.1\\n\\n“AMD is very excited about the release of the Vulkan 1.1 specifications. This new iteration of the industry standard builds on its strong foundations and expands its reach by making the API more accessible to developers. New Vulkan 1.1 features such as subgroup access in compute shaders enable console-like optimizations that will empower developers to exert more control on the GPU than ever before,” said Andrej Zdravkovic, corporate vice president of software, AMD. “Vulkan also enriches its connectivity with other APIs by supporting interop operations, and finally enables first-class support for multi-GPU and VR systems. AMD believes that this major upgrade to the API will delight the existing Vulkan community and continue to broaden the user base.”\\n\\n“The new additions in Vulkan 1.1 are a game changer for Vulkan,” said Andrew Richards, CEO of Codeplay Software. “In particular the new subgroup functionality will make a huge difference to our low power, high performance deep learning work.”\\n\\n“Continental is delighted to see how Vulkan continues to move graphics technology forward,” said Dr. Ulrich Kabatek, principal expert graphic systems & 3D visualization at Continental. “We appreciate the improvements it offers for more integrated automotive systems and latency critical applications like augmented automotive clusters.”\\n\\n“As a major contributor to the Vulkan API, Imagination is pleased to see the next evolution of the API in Vulkan 1.1. The new specification offers new ways to exploit GPU parallelism and support for protected content - enabling advanced video processing in Vulkan for things like VR Cinema,” said Graham Deacon, vice president of PowerVR Marketing at Imagination. “We are amongst the first vendors to have a conformant Vulkan implementation, with all of our existing Vulkan-capable GPU IP able to support the new 1.1 standard. The new subgroup and protected content features are fully available in many of our newer cores.”\\n\\n“Intel is committed to open source driver advances for high performance graphics. Vulkan 1.1 is certified on the latest four generations of Intel® Core™ processors, further developing support for virtual and mixed reality,” said Imad Sousou, corporate vice president and general manager of the Open Source Technology Center at Intel Corporation.\\n\\n“Vulkan is vital to NVIDIA’s business as it enables developers to get the best from our GPUs across a wide range of platforms,” said Dwight Diercks, senior vice president of software engineering, NVIDIA. “Our Vulkan 1.1 drivers, with full subgroup functionality, are available for Windows, Linux, and Android on the day of the specification launch. We will continue to take a leadership role within Khronos to ensure Vulkan evolves to meet the needs of developers and the wider industry.”\\n\\n“Vulkan is enabling high fidelity games to make the most out of the capability in mobile, with great titles like Honor of Kings from Tencent running across many Galaxy devices” said Taeyong Kim, VP of Graphics at Samsung Electronics. “Vulkan 1.1 builds on this, adding functionality for VR gaming and 360 video that will enable immersive content on mobile products and VR headsets.”\\n\\n\"Vulkan 1.1 and SPIR-V 1.3 are another step in providing better support for compute, as it adds subgroups, 16-bit numbers and a restricted form of pointers\", said Vincent Hindriksen, managing director of StreamHPC. \"Adding this to the strong industry support for Vulkan and the recently launched portability project, this allows more types of compute-kernels to be run on more platforms.”\\n\\n“In our non-gaming business, Vulkan 1.1 is a key factor for our success,” said Norbert Nopper, founder of UX3D, “The possibility to utilize simultaneous rendering and multiple GPUs by default does help us to further optimize our engine.”\\n\\n\"VeriSilicon has adopted the latest industry standard low-level GPU API Vulkan 1.1 and is committed to support Vulkan 1.1 across our OpenGL ES 3.1/3.2 class GPUs,\" said Wei-Jin Dai, executive vice president of VeriSilicon and general manager of VeriSilicon\\'s IP Division. \"We are glad to see the new functionalities of Vulkan 1.1 API, the multi-GPU support, the premium content protection, and the advanced compute functionality, etc. to unleash the tremendous potential for a new generation of Vulkan applications. We are particularly excited about the opportunities the Vulkan 1.1 brings to the automotive customers to have fine-grained control over graphics quality of service and highly desired security levels and features.”\\n\\nMore Information\\n\\nMore information on Vulkan is available at https://www.khronos.org/vulkan/\\n\\nAll Khronos open source projects are available here: https://github.com/KhronosGroup\\n\\nAbout the Khronos Group\\n\\nThe Khronos Group is an open industry consortium of over 100 leading hardware and software companies creating advanced, royalty-free, acceleration standards for 3D graphics, Augmented and Virtual Reality, vision and machine learning. Khronos standards include Vulkan®, OpenGL®, OpenGL® ES, OpenGL® SC, WebGL™, SPIR-V™, OpenCL™, SYCL™, OpenVX™, NNEF™, COLLADA™, OpenXR™ and glTF™. Khronos members are enabled to contribute to the development of Khronos specifications, are empowered to vote at various stages before public deployment, and are able to accelerate the delivery of their cutting-edge accelerated platforms and applications through early access to specification drafts and conformance tests.\\n\\n###\\n\\nVulkan is a registered trademark of The Khronos Group. Khronos, OpenXR, DevU, SPIR, SPIR-V, SYCL, WebGL, WebCL, COLLADA, OpenKODE, OpenVG, OpenVX, EGL, glTF, OpenKCAM, StreamInput, OpenWF, OpenSL ES, NNEF and OpenMAX are trademarks of the Khronos Group Inc. OpenCL is a trademark of Apple Inc. and OpenGL is a registered trademark and the OpenGL ES and OpenGL SC logos are trademarks of Hewlett Packard Enterprise used under license by Khronos. All other product names, trademarks, and/or company names are used solely for identification and belong to their respective owners.')"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Capsule networks can overcome some shortcomings of CNNs', 'Aurélien Géron will present \"Predicting the stock market using LSTMs\" at the Artificial Intelligence Conference in New York, April 29-May 2, 2018.\\n\\nCapsule networks (CapsNets) are a hot new neural net architecture that may well have a profound impact on deep learning, in particular for computer vision. Wait, isn\\'t computer vision pretty much solved already? Haven\\'t we all seen fabulous examples of convolutional neural networks (CNNs) reaching super-human level in various computer vision tasks, such as classification, localization, object detection, semantic segmentation or instance segmentation (see Figure 1)?\\n\\nFigure 1. Some of the main computer vision tasks. Today, each of these tasks requires a very different CNN architecture, for example ResNet for classification, YOLO for object detection, Mask R-CNN for instance segmentation, and so on. Image by Aurélien Géron.\\n\\nWell, yes, we’ve seen fabulous CNNs, but:\\n\\nThey were trained on huge numbers of images (or they reused parts of neural nets that had). CapsNets can generalize well using much less training data.\\n\\nCNNs don’t handle ambiguity very well. CapsNets do, so they can perform well even on crowded scenes (although, they still struggle with backgrounds right now).\\n\\nCNNs lose plenty of information in the pooling layers. These layers reduce the spatial resolution (see Figure 2), so their outputs are invariant to small changes in the inputs. This is a problem when detailed information must be preserved throughout the network, such as in semantic segmentation. Today, this issue is addressed by building complex architectures around CNNs to recover some of the lost information. With CapsNets, detailed pose information (such as precise object position, rotation, thickness, skew, size, and so on) is preserved throughout the network, rather than lost and later recovered. Small changes to the inputs result in small changes to the outputs—information is preserved. This is called \"equivariance.\" As a result, CapsNets can use the same simple and consistent architecture across different vision tasks.\\n\\nFinally, CNNs require extra components to automatically identify which object a part belongs to (e.g., this leg belongs to this sheep). CapsNets give you the hierarchy of parts for free.\\n\\nFigure 2. The DeepLab2 pipeline for image segmentation, by Liang-Chieh Chen, et al.: notice that the output of the CNN (top right) is very coarse, making it necessary to add extra steps to recover some of the lost details. From the paper DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs, figure reproduced with the kind permission of the authors. See this great post by S. Chilamkurthy to see how diverse and complex the architectures for semantic segmentation can get.\\n\\nCapsNets were first introduced in 2011 by Geoffrey Hinton, et al., in a paper called Transforming Autoencoders, but it was only a few months ago, in November 2017, that Sara Sabour, Nicholas Frosst, and Geoffrey Hinton published a paper called Dynamic Routing between Capsules, where they introduced a CapsNet architecture that reached state-of-the-art performance on MNIST (the famous data set of handwritten digit images), and got considerably better results than CNNs on MultiMNIST (a variant with overlapping pairs of different digits). See Figure 3.\\n\\nFigure 3. MultiMNIST images (white) and their reconstructions by a CapsNet (red+green). “R” = reconstructions; “L” = labels. For example, the predictions for the first example (top left) are correct, and so are the reconstructions. But in the fifth example, the prediction is wrong: (5,7) instead of (5,0). Therefore, the 5 is correctly reconstructed, but not the 0. From the paper: Dynamic routing between capsules, figure reproduced with the kind permission of the authors.\\n\\nDespite all their qualities, CapsNets are still far from perfect. Firstly, for now they don\\'t perform as well as CNNs on larger images such as CIFAR10 or ImageNet. Moreover, they are computationally intensive, and they cannot detect two objects of the same type when they are too close to each other (this is called the \"crowding problem,\" and it has been shown that humans have it, too). But the key ideas are extremely promising, and it seems likely that they just need a few tweaks to reach their full potential. After all, modern CNNs were invented in 1998, yet they only beat the state of the art on ImageNet in 2012, after a few tweaks.\\n\\nSo, what are CapsNets exactly?\\n\\nIn short, a CapsNet is composed of capsules rather than neurons. A capsule is a small group of neurons that learns to detect a particular object (e.g., a rectangle) within a given region of the image, and it outputs a vector (e.g., an 8-dimensional vector) whose length represents the estimated probability that the object is present[1], and whose orientation (e.g., in 8D space) encodes the object\\'s pose parameters (e.g., precise position, rotation, etc.). If the object is changed slightly (e.g., shifted, rotated, resized, etc.) then the capsule will output a vector of the same length, but oriented slightly differently. Thus, capsules are equivariant.\\n\\nMuch like a regular neural network, a CapsNet is organized in multiple layers (see Figure 4). The capsules in the lowest layer are called primary capsules: each of them receives a small region of the image as input (called its receptive field), and it tries to detect the presence and pose of a particular pattern, for example a rectangle. Capsules in higher layers, called routing capsules, detect larger and more complex objects, such as boats.\\n\\nFigure 4. A two-layer CapsNet. In this example, the primary capsule layer has two maps of 5x5 capsules, while the second capsule layer has two maps of 3x3 capsules. Each capsule outputs a vector. Each arrow represents the output of a different capsule. Blue arrows represent the output of a capsule that tries to detect triangles, black arrows represent the output of a capsule that tries to detect rectangles, and so on. Image by Aurélien Géron.\\n\\nThe primary capsule layer is implemented using a few regular convolutional layers. For example, in the paper, they use two convolutional layers that output 256 6x6 features maps containing scalars. They reshape this output to get 32 6x6 maps containing 8-dimensional vectors. Finally, they use a novel squashing function to ensure these vectors have a length between 0 and 1 (to represent a probability). And that\\'s it: this gives the output of the primary capsules.\\n\\nThe capsules in the next layers also try to detect objects and their pose, but they work very differently, using an algorithm called routing by agreement. This is where most of the magic of CapsNets lies. Let\\'s look at an example.\\n\\nSuppose there are just two primary capsules: one rectangle capsule and one triangle capsule, and suppose they both detected what they were looking for. Both the rectangle and the triangle could be part of either a house or a boat (see Figure 5). Given the pose of the rectangle, which is slightly rotated to the right, the house and the boat would have to be slightly rotated to the right as well. Given the pose of the triangle, the house would have to be almost upside down, whereas the boat would be slightly rotated to the right. Note that both the shapes and the whole/part relationships are learned during training. Now notice that the rectangle and the triangle agree on the pose of the boat, while they strongly disagree on the pose of the house. So, it is very likely that the rectangle and triangle are part of the same boat, and there is no house.\\n\\nFigure 5. Routing by agreement, step 1—predict the presence and pose of objects based on the presence and pose of object parts, then look for agreement between the predictions. Image by Aurélien Géron.\\n\\nSince we are now confident that the rectangle and triangle are part of the boat, it makes sense to send the outputs of the rectangle and triangle capsules more to the boat capsule, and less to the house capsule: this way, the boat capsule will receive more useful input signal, and the house capsule will receive less noise. For each connection, the routing-by-agreement algorithm maintains a routing weight (see Figure 6): it increases routing weight when there is agreement, and decreases it in case of disagreement.\\n\\nFigure 6. Routing by agreement, step 2—update the routing weights. Image by Aurélien Géron.\\n\\nThe routing-by-agreement algorithm involves a few iterations of agreement-detection + routing-update (note that this happens for each prediction, not just once, and not just at training time). This is especially useful in crowded scenes: for example, in Figure 7, the scene is ambiguous because you could see an upside-down house in the middle, but this would leave the bottom rectangle and top triangle unexplained, so the routing-by-agreement algorithm will most likely converge to a better explanation: a boat at the bottom, and a house at the top. The ambiguity is said to be \"explained away\": the lower rectangle is best explained by the presence of a boat, which also explains the lower triangle, and once these two parts are explained away, the remaining parts are easily explained as a house.\\n\\nFigure 7. Routing by agreement can parse crowded scenes, such as this ambiguous image, which could be misinterpreted as an upside-down house plus some unexplained parts. Instead, the lower rectangle will be routed to the boat, and this will also pull the lower triangle into the boat as well. Once that boat is “explained away,” it’s easy to interpret the top part as a house. Image by Aurélien Géron.\\n\\nAnd that’s it—you know the key ideas behind CapsNets! If you want more details, check out my two videos on CapsNets (one on the architecture and another on the implementation) and my commented TensorFlow implementation (Jupyter Notebook). Please don’t hesitate to comment on the videos, file issues on GitHub if you see any, or contact me on Twitter @aureliengeron. I hope you found this post useful!\\n\\n[1] This is the original architecture proposed in the paper Dynamic routing with capsules, by S. Sabour, N. Frosst, and G. Hinton, but since then, they proposed a more general architecture where the object’s presence probability and pose parameters are encoded differently in the output vector. The ideas remain the same, however.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Optimization over Explanation', 'Optimization over Explanation\\n\\nMaximizing the benefits of machine learning without sacrificing its intelligence\\n\\nNote: Wired.com has simultaneously run an op-ed version of this paper.\\n\\nImagine your Aunt Ida is in an autonomous vehicle (AV) — a self-driving car — on a city street closed to human-driven vehicles. Imagine a swarm of puppies drops from an overpass, a sinkhole opens up beneath a bus full of mathematical geniuses, or Beethoven (or Tupac) jumps into the street from the left as Mozart (or Biggie) jumps in from the right. Whatever the dilemma, imagine that the least worst option for the network of AVs is to drive the car containing your Aunt Ida into a concrete abutment. Even if the system made the right choice — all other options would have resulted in more deaths — you’d probably want an explanation.\\n\\nOr consider the cases where machine-learning-based AI has gone wrong. It was bad when Google Photos identified black men as gorillas. It can be devastating when AI recommends that black men be kept in jail longer than white men for no reason other than their race. Not to mention autonomous military weapon systems that could deliver racism in airborne explosives.\\n\\nTo help ameliorate such injustices, the European Parliament’s has issued the General Data Protection Regulation (GDPR) that is often taken to stipulate a “right to explanation” for algorithms that “significantly affect” users.[1] This sounds sensible. In fact, why not simply require all AI systems be able to explain how they came to their conclusions?\\n\\nThe answer is not only that this can be a significant technical challenge[2], but that keeping AI simple enough to be explicable can forestall garnering the full value possible from unhobbled AI. Still, one way or another, we’re going to have to make policy decisions governing the use of AI — particularly machine learning — when it affects us in ways that matter.\\n\\nOne approach is to force AI to be artificially stupid enough that we can understand how it comes up with its conclusion. But here’s another: Accept that we’re not always going to be able to understand our machine’s “thinking.” Instead, use our existing policy-making processes — regulators, legislators, judicial systems, irate citizens, squabbling politicians — to decide what we want these systems optimized for. Measure the results. Fix the systems when they don’t hit their marks. Celebrate and improve them when they do.\\n\\nWe should be able to ask for explanations when we can. But when we can’t, we should keep using the systems so long as they are doing what we want from them.\\n\\nFor, alas, there’s no such thing as a free explanation.[3]\\n\\nThe problem with explanations\\n\\nPretend that your physician tells you that a deep-learning diagnostic system like Mt. Sinai’s Deep Patient has concluded that there is a good chance that you will develop liver cancer in the next five years.\\n\\n“What makes the computer think that?” you ask.\\n\\nYour doctor replies that the machine learning system — let’s call it Deep Diagnosis — has been trained on the health records of 700,000 patients, as was Deep Patient. From this data it has found patterns in what may be thousands of factors that have enabled it to accurately predict probabilities of health issues. In your case, it’s predicting with a confidence of 70 percent that you’ll develop liver cancer within the next five years.\\n\\n“Based on what factors in my health record?”\\n\\nYour doctor replies: “Deep Diagnosis tracks thousands of factors, including many that seem irrelevant to a diagnosis, and we don’t understand why they add up to a particular probability of liver cancer. A trained physician like me could stare at the print-out of all those variables and their weightings without ever understanding why they led to that result. In fact, these patterns may not be the same for other patients with the same diagnosis. But it turns out that they do indeed predict probable health issues. So, here are your options for treatment, including ignoring the prediction…”\\n\\nThis is very different from what we normally expect from a human giving an explanation. Usually our doctor would explain the model she’s using: liver cancer is caused by this or that, here are the test results for the state of your this or that, and here are the alternatives for lowering your risk.\\n\\nBut machine learning systems don’t have to start out primed with explicit human models of how factors interrelate. Certainly, human assumptions are inevitably smuggled in via our choices about which data to train them on, the biases expressed by that data, the implicit biases of the system’s programmers, the user interface we provide for interacting with the system, the reports we choose to generate, etc. But the system does not have to be told how we think the data it’s fed interrelates. Instead, the system iterates on the data, finding complex, multi-variable probabilistic correlations that become part of the model it builds for itself.\\n\\nHuman-constructed models aim at reducing the variables to a set small enough for our intellects to understand. Machine learning models can construct models that work — for example, they accurately predict the probability of medical conditions — but that cannot be reduced enough for humans to understand or to explain them.\\n\\nThis understandably concerns us. We think of these systems as making decisions, and we want to make sure they make the right moral decisions by doing what we do with humans: we ask for explanations that present the moral principles that were applied and the facts that led to them being applied that way. “Why did you steal the apple?” can be justified and explained by saying “Because it had been stolen from me,” “It was poisoned and I didn’t want anyone else to eat it” or “Because I was hungry and I didn’t have enough money to pay for it.” These explanations work by disputing the primacy of the principle that it’s wrong to steal.\\n\\nIt’s thus natural for us to think about what principles we want to give our AI-based machines, and to puzzle through how they might be applied in particular cases. If you’d like to engage in these thought experiments, spend some time at MoralMachine.mit.edu where you’ll be asked to make the sort of decision familiar from the Trolley Problem: if you had to choose, would you program AVs to run over three nuns or two joggers? Four old people or two sickly middle-aged people? The creators of the site hope to use the crowd’s decisions to provide guidance to AV programmers, but it can also lead to a different conclusion: We cannot settle moral problems — at least not at the level of detail the thinking behind MoralMachines demands of us — by applying principles to cases. The principles are too vague and the cases are too complex. If we instead take a utilitarian, consequentialist approach, trying to assess the aggregated pains and pleasures of taking these various lives, the problem turns out to be still too hard and too uncertain.\\n\\nSo perhaps we should take a different approach to how we’ll settle these issues. Perhaps the “we” should not be the commercial entities that build the AI but the systems we already have in place for making decisions that affect public welfare. Perhaps the decisions should start with broad goals and be refined for exceptions and exemptions the way we refine social policies and laws. Perhaps we should accept that AI systems are going to make decisions based on what they’ve been optimized for, because that’s how and why we build them.[4] Perhaps we should be governing their optimizations.\\n\\nOptimization over explanation\\n\\nA proposal:\\n\\n1. AI systems ought to be required to declare what they are optimized for.\\n\\n2. The optimizations of systems that significantly affect the public ought to be decided not by the companies creating those systems but by bodies representing the public’s interests.\\n\\n3. Optimizations always also need to support critical societal values, such as fairness.\\n\\nOptimization is a measure of outcomes against goals. A system of AVs is successfully optimized for reducing fatalities if over a statistically reasonable interval the number of fatalities drops. It is optimized for energy savings if the overall energy use of the system — and the larger systems in which it’s embedded — declines. Optimization does not have to specify a precise target in order to succeed; rather the target is the maximum desirable and possible given all of the other desired optimizations, in conjunction with the world’s vicissitudes.\\n\\nSystem designers talk about optimization because they recognize that machines are imperfect and are often designed to serve inconsistent goals. Are you going to optimize your car for good mileage, environmental impact, price, acceleration, safety, comfort, scenic views, or prestige? Designing for safety might require a heavier chassis, which will negatively affect acceleration, mileage, and environmental impact. Designing for environmental impact might mean longer travel times. Designers have to create a balance of optimizations, playing with a set of metaphorical sliders that determine how much of a value the system will sacrifice to gain some combination of the other values. As David P. Reed, one of the architects of the Internet, has said, optimizing a system for one value de-optimizes it for others.[5]\\n\\nWhile optimizations apply to systems, they may be determined, within limits, by the individual users. For example, the passengers in an AV might want to optimize a trip for scenic value. Moving that “slider” up — someday possibly a slider in a digital control panel — will automatically move some of the others down: the trip is likely to take longer and consume more energy. The limits imposed on the users’ ability to adjust the sliders will be determined by those who are designing the optimization of the system overall: perhaps users will not be allowed to optimize their particular trip in a way that will de-optimize the overall system, even a little, for preserving lives.\\n\\nThere’s more to say about what’s entailed in optimizing AI systems, especially about keeping them fair, but first: why discuss the morality and governance of AI systems in terms of their optimization at all?\\n\\nFirst, it focuses the normative discussion on AI as a tool designed to provide benefits we’ve agreed we want, rather than as a set of moral conundrums to be solved by arguing over principles and their application. For example, we are never going to agree as a society if AVs should be programmed to run over two prisoners to save one nun, or if rich people should be allowed to go faster at the expense of the not so rich; if we can’t even agree on Net Neutrality, how are we ever going to agree on Highway Neutrality? But we do have apparatuses of governance that let us decide that, say, a system of AVs should aim at reducing fatalities as a first priority, and at reducing environmental impact as a second. Does this mean an AV should run over the nun? Yes, if we’ve decided to optimize AVs to lower fatalities and her death will save two others, but not because we have at long last figured out the moral algebra of nuns vs. sinners. We can stop talking about the Trolley Problem, at least when it comes to governing this sort of AI system. That by itself should count as a major benefit.\\n\\nSecond, it enables us to evaluate success and failure — and liability — in terms of system properties, rather than case by case. Since governance of these systems will be done at some system layer of society — local, state, national, global — the primary evaluation ought to also be on the benefits at the system level.\\n\\nThird, it contextualizes the suffering most AI systems are going to cause. For example, Aunt Ida’s family is going to be outraged that her AV drove into the concrete abutment. The family may well want to bring suit against the maker of the AV. But why her car killed specifically her may be inexplicable. There may be too many variables. It may require gathering all the real-time data from all the networked AVs on the road that provided input into the networked AI decision. Some of the data may need to be cloaked for privacy reasons. The ad hoc network that made the decision may have been using real-time data from other systems, including weather information, pedestrian locations, economic impact systems, etc….and some of those systems may be inexplicable black boxes. It may simply not be practical to expect all that data to be preserved so that we can perfectly recreate the state of the system. We well not be able to explain the decision or even verify it.\\n\\nFrom the standpoint of morality and legal liability, this seems highly unsatisfactory. On the other hand, in 2016, there were about 40,000 traffic fatalities in the US. Let’s say a few years after AVs have become common, that falls to 5,000 deaths per year. Five thousand deaths per year is a horrible toll, but 35,000 lives saved per year is a glorious good. The moral responsibility of the AV manufacturer and of the network of AVs on the road at the time is not to save Aunt Ida but to achieve the optimizations that we as a society have decided on through our regulatory, legislative, and judicial systems.\\n\\nFourth, governing via optimization makes success measurable.\\n\\nFinally, the concept of optimization has built into it an understanding that perfection is not possible. Optimization is a “best effort.” “AVs killed 5,000 people this year!” does not become a cause for moral outrage but a cheer for a major, humane accomplishment.\\n\\nOverall, understanding and measuring AI systems in terms of their optimizations gives us a way to govern them that enables us to benefit from them even though they are imperfect and even when we cannot explain their particular outcomes.\\n\\nBut that is not enough.\\n\\nCritical constraints: Hedging optimization\\n\\nImagine that AVs are optimized to minimize fatalities, and deaths by car drop from 40,000 to 5,000 per year. Yay!\\n\\nNow imagine that in an early simulation of the system, people of color are hugely disproportionately represented among those 5,000 dead.\\n\\nOr imagine that a system designed to cull applicants for Silicon Valley tech jobs is producing high quality sets of people for in-person interviews, but the percentage of women making it through the AI process is even lower than the current dismal percentage of women in tech positions.\\n\\nA system that achieves the results it’s been optimized for may still fail to meet societal goals. As has been well documented[6], machine learning systems are prone to reproducing or even amplifying the biases reflected in the data the systems used to create their models.\\n\\nAchieving the stated optimization goals is clearly not enough. AI systems need to be able to provide evidence in the form of quantifiable results that the optimizations are not violating a culture’s broader, deeper values; this is the deontological (principle-based) moment of this utilitarian approach.[7]\\n\\nWe could count these constraints as another sort of optimization. But they deserve their own name and category, for two reasons.\\n\\nFirst, “being fair” is not what a system of AVs or a medical diagnostic system is designed to do. Such systems are tools and thus are optimized for a more focused purpose. It’s useful to reserve the term “optimization” for the purposes for which a tool was designed.\\n\\nSecond, optimizations are trade-offs. But these constraints are critical because we will not permit them to be traded off.\\n\\nSo, we’ll call them critical constraints.\\n\\nDeciding on the critical constraints we’ll demand from AI systems will require difficult moral discussions that express deep conflicts in our culture. For example, a Silicon Valley company resistant to demands for gender equity might say it wants its applicant-culling software to recommend the “best of the best” (as the company defines that), and “Gender balance be damned!” Or it may claim that the company receives relatively few applications from women. Or it may be terribly misguided about what to look for when evaluating potential employees.[8]\\n\\nWe may nevertheless decide as a culture to address the inequity of the tech workforce by enforcing a requirement that tech application-culling systems produce pools at least 50 percent composed of women. Or we might decide that the problem is the inequity in the educational pipeline and thus may want to suspend enforcing a “50 percent female” constraint until the pipeline becomes more gender balanced. Or we may want to insist on a 50 percent rule on the grounds that empirical evidence has shown that otherwise AI application-culling systems will reflect societal biases. Or we might insist that the recommendation pool be 75 percent female to help correct the inequity of the existing workforce. Such decisions undoubtedly will require difficult political and judicial conversations. On the positive side, having to come up with critical parameters for AI can serve a useful forcing function.\\n\\nResolving these issues are not AI’s problem, though. It’s our responsibility. Asking what we want AI systems optimized for frames it in a way appropriate for the necessary social discussions and political processes.\\n\\nLiability and compensation\\n\\nThere’s an endless series of mistakes AI can — and therefore will — make: misdiagnosing a disease, targeting innocents in military attacks, discriminating based on race and gender, as well as recommending a movie that you don’t much like. There’s a far smaller number of ways in which AI will make these errors, each needing its own set of policy, regulatory, and judicial tools to help prevent and ameliorate them .\\n\\n1. The wrong optimization: Say an AV system optimized for shortest delivery times routes continuous traffic through a residential section of town, resulting in a degradation of the quality of life. Or it routes high-speed traffic through a shopping district, de-optimizing local and pedestrian traffic, causing a drop in sales. (In this case, we well might want to let localities regulate the local optimizations of the system, just as airplanes have to lessen their noise over some towns.)\\n\\nThe manufacturers of AI systems should not be liable for successfully meeting poorly thought-through optimizations. They should be liable for ignoring local optimizations, just as airlines can be held responsible for violating local noise restrictions on late night arrivals.\\n\\n2. Faulty execution: Say a home Internet of Things system has been optimized for energy savings, but in some homes it’s resulting in higher monthly expenditures on energy. In this case, the optimization is the preferred one, but the execution of it is faulty due to buggy software, the system having been trained on inappropriate data, a failure to interoperate properly with one or more of the devices, etc.\\n\\nThe optimization is correct but the implementation is flawed. If the AI system is capable of yielding explanations, those explanations need to be presented, and liability assessed. But if the AI is not capable of yielding explanations, product liability and class action processes might apply.\\n\\n3. Expected harms from a well-optimized, properly functioning system: Say Aunt Ida happens to be one of the 5,000 fatalities in the new national AV system that is optimized first for reducing fatalities. It is operating properly: fatalities have dropped and the system is operating within its critical constraints.\\n\\nThe unfortunate losers in this system should receive no-fault compensation, probably via some form of social insurance.\\n\\n4. Failure to instill a critical constraint, or to get its constraining power correct: Say an autonomous police drone uses undue force to subdue a criminal, resulting in serious collateral damage to innocent people. The injured innocents might bring suit, arguing that the drone failed to heed the “no harm to innocents” constraint.\\n\\nIf the drone indeed failed to heed that constraint, or if the manufacturers “forgot” to instill it, the manufacturers should be liable.\\n\\nIf the regulations governing police drones do not include such a constraint, then liability would seem to fall on the body that decided on the mix of optimizations and critical constraints. (The question of how a drone recognizes innocents is not a question of governance but a technical question — an important one — about implementation.)\\n\\nSo, overall:\\n\\n(a) In the cases where systems are not functioning as expected, liability law, including product liability law, can often be invoked. The explanation of the failure need not always be determined.\\n\\n(b) Where the systems are functioning as expected, and where the expectations assume imperfections, the victims ought to be compensated along the model of no-fault insurance: The families of the 5,000 people killed in car crashes ought to be compensated according to guidelines that try to be as fair as possible.\\n\\nWhy this way?\\n\\nThis overall approach has several advantages:\\n\\nFirst, it lets us benefit from AI systems that have advanced beyond the ability of humans to understand exactly how those systems are working.\\n\\nSecond, it focuses the discussion on the system rather than on individual incidents.\\n\\nThird, it places the governance of these systems within our human, social framework. Optimizations are always imperfect and entail trade-offs that we need to discuss. Optimizations should always be constrained by social values that we consider paramount. The operations of autonomous systems are autonomous from human control in particular situations but are subordinate to human needs, desires, and rights. (The italicized words indicate places where humanity is requisite and evident.)\\n\\nFourth, it does not require us to come up with a new moral framework for dealing with an infrastructure that increasingly uses the most advanced tools our species has ever created. Rather, it treats these inevitable problems as societal questions that are too important to be left unregulated and in the hands of commercial entities. It instead it lets them be settled by existing regulatory bodies, using our existing processes for resolving policy questions, and it uses and extends the existing legal frameworks for assessing liability and schedules of compensation.\\n\\nThis way we don’t have to treat AI as a new form of life that somehow escapes human moral questions. We can treat it as what it is: a tool that should be measured by how much better it is at doing something compared to our old way of doing it: Does it save more lives? Does it improve the environment? Does it give us more leisure? Does it create jobs? Does it make us more social, responsible, caring? Does it accomplish these goals while supporting crucial social values such as fairness?\\n\\nBy treating the governance of AI as a question of optimizations, we can focus the necessary argument about them on what truly matters: What is it that we as a society want from a system, and what are we willing to give up to get it?\\n\\nThe elements of this process\\n\\nUse existing public institutions of policy-making to decide on the weighting of interrelated and frequently conflicting optimizations and critical constraints. Require AI systems to announce those optimizations publicly and clearly, and then hold them to them. This is where the locus of transparency should be. The transparency of algorithms is a tool that has its own uses in special cases. Measure everything possible. Make those measurements available publicly, or, when necessary, privately to sanctioned, trusted, independent bodies. Establish no-fault compensation and social insurance for systems where some harmful results cannot be avoided.\\n\\nIn short: Govern the optimizations. Patrol the results.\\n\\nThank you to the twenty or so people who commented on an online draft of this paper. Many but not all came from Harvard’s Berkman Klein Center or the MIT Media Lab. Not all agree with this paper’s premises or conclusions. All were helpful.\\n\\nEND NOTES\\n\\n(I took the photo. It’s licensed as Creative Commons BY.)\\n\\n[1] What the GDPR actually stipulates is much harder to parse. See Sandra Wachter, Brent Middelstadt, Luciano Floridi, “Why a Right to Explanation of Automated Decision-Making Does Not Exist in the General Data Protection Regulation”, International Data Privacy Law, Jan. 24, 2017.\\n\\n[2] There is a great deal of research underway about how explicable, or interpretable, complex Deep Learning systems can be. We know empirically that at least some of them can be understood to some useful degree. See, for example, this explanation of how Google’s DeepDream image maker works. For a fascinating and powerfully argued proposal for enabling explanations of machine learning systems without requiring any alteration of the systems and even without direct inspection of the algorithms used, see” Towards A Rigorous Science of Interpretable Machine Learning” by Finale Doshi-Velez and Been Kim.\\n\\n[3] Thanks to Stuart Shieber for suggesting a version of this phrase. Also, note that there may be such a thing as a free hunch.\\n\\n[4] And what they are optimized for will shape their design, including their sensors and controls. Optimization decisions have implications for every stage of devices’ design and production.\\n\\n[5] In a private conversation. Cited with permission.\\n\\n[6] For example, see Cathy O’Neil’s Weapons of Math Destruction and Kate Crawford’s “Artificial Intelligence’s White Guy Problem.”\\n\\n[7] In “Fairness through Awareness,” Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel suggest both a metric for measuring fairness and “an algorithm for maximizing utility subject to the fairness constraint.” “Our framework permits us to formulate the question as an optimization problem that can be solved by a linear program.” Their “similarity metric” allows a way to assess whether an AI system is treating people who are relevantly similar in similar ways.\\n\\n[8] See John Simons, “What Sephora Knows About Women in Tech That Silicon Valley Doesn’t,” Wall Street Journal, Oct. 9, 2017')"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "('The differences between tinkering and research (2016)', 'This was meant to be a short post announcing some new papers of ours, but it grew. So what you get is a post announcing some new papers, wit...')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The black box is a state of mind', \"Rapid advances in machine learning have prompted much debate about the sinister implications of 'black-box' algorithms. Yet fears about the opacity of computer code are as old as software itself, writes Kathrin Passig. Indeed, black boxes are all around us, not just inside our computers…\\n\\nIn 2012, I wrote an article about algorithms, then a hot topic, in which I mentioned a New York Times article published four years earlier about the Netflix Prize. The algorithms for recommending films were getting better and better, explained the Times, but also harder to understand – not only for users, but also developers: ‘Chris Volinsky admits that his team’s program has become a black box, its internal logic unknowable.’ I quoted this remark in my article.\\n\\nMore and more articles then started appearing describing ‘algorithms’ as ‘black boxes’. They annoyed me, especially because I too had been guilty of mystifying algorithms. It starts with the terminology. We could simply say ‘software’, but that brings to mind dusty floppy disks rather than anything more sinister. In fact, an algorithm doesn’t necessarily involve a computer. It once simply denoted a clearly defined set of instructions, for instance a sorting method or a guide for assembling an IKEA bookshelf. The critique of algorithms, however, is not about small building blocks but large systems. What can you do? Colloquially, ‘software’ has been replaced with ‘algorithm’ and protesting against shifts in language is pointless.\\n\\nThe critique of algorithms at the beginning of the decade was different than it is today. After the eruption of Eyjafjallajoküll, Frankfurter Allgemeine Zeitung editor Frank Schirrmacher argued that it was wrong to shut down European air traffic on the basis of simulations. Journalist Miriam Meckel criticized the recommendations on Amazon, Facebook and iTunes for failing to account for coincidence and unpredictability. Media theorist Geert Lovink protested against the ‘ruthless algorithms’ of ‘Finazism’.\\n\\nSince then, there have been massive advances in ‘machine learning’ or ‘deep learning’, advances that have now become visible in everyday life. In 2008 I wrote a piece of code, as part of a Twitter bot, that would cause the bot to follow its own followers while ignoring spammers. The code consisted of questions like: Do more than 17 of the last 20 tweets contain a link? Are there no replies in the last 20 tweets, i.e. is the person just tweeting to herself? Does the account description contain words like ‘free’, ‘bargain’ or ‘credit’? Each ‘yes’ would add a point for a likely spammer, while for each ‘no’ a point would be deducted. Anyone with three or more points would be treated as a spammer. This was the traditional method.\\n\\nWith machine learning, you also start off by considering a range of features that might be relevant. But instead of pre-determining their relative significance, as in my code, you use trial and error. If, when operating a particular random setting, the controller can’t tell between a known spam sample and a sample known not to be spam, then the code is adjusted and re-tested. If the system works, then it can then be tried out on unknown material. (Caution: gross over-simplification – do not programme any missile defence systems on this basis.)\\n\\nThe critique of algorithms has begun to focus on this area. ‘Deep learning is creating computer systems we don’t fully understand’ ran the headline of an article in the technology magazine The Verge in July 2016. Apparently, until now we had been able to fully understand the computer systems we were using. Elsewhere it is stated even more explicitly that transparency has only now ceased to exist: in 2016, for example, the Scientific American wrote that ‘we no longer fully understand why algorithms are deciding things the way they do’. Media theorist Felix Stalder has written that these self-learning algorithms are ‘in many cases so complex that they can’t even be understood in retrospect. They can be tested experimentally, but no longer logically. Such algorithms are essentially black boxes – objects that can only be understood through their outward behaviour, but whose inner structure defies cognition.’\\n\\nAll this is right, and yet not quite. It makes it sound as though the inner structure of software was, until very recently, readily comprehensible. Yet the opacity of software systems has preoccupied journalists and developers for much longer. In 1987, Der Spiegel published a conversation between the computer scientists Klaus Haefner and Joseph Weizenbaum entitled ‘An explosion of nonsense’. At the time of the discussion, Haefner had been in the business for eighteen years and Weizenbaum for thirty-six. Michael Haller, the editor of Der Spiegel who was moderating the debate, says the following: ‘Complex operations are often impenetrable even to the programmers who wrote the program. What exactly goes on in ‘inside’ them remains a mystery. We then fall back on the “black box” model and merely ask what results the machine gives when we enter this or that data.’ Haefner agrees: ‘Here we reach the crux. Unfortunately, more and more systems are being built that are so complicated that we can no longer get a proper picture of them … That human nature has its unpredictable sides is something we’ve been able to live with thus far. But a computer whose inner workings are unpredictable is dangerous and contradicts the very direction in which evolution is supposed to run.’\\n\\nWhere does Haller get this notion? Probably from Weizenbaum, who that year had told the magazine LOG IN how ‘I once visited Karl Deutsch when I happened to be in Berlin – at the Science Centre in Steinplatz. We chatted for a while in his office, and I told him about the opacity of large systems. I told him that there is no large system in existence that can be fully grasped by an individual, that it is too late to control such systems, because they are a consequence of their history and that history is no longer available.’ As an example he cites Globus, a program for simulating the global economy developed at around that time in Berlin.\\n\\nGlobus consisted of approximately ten thousand lines of code in Fortran 77. ‘These are vast programs which I don’t think anyone could ever properly comprehend,’ Weizenbaum says. The average iPhone app was about five times that length in 2013. The Hubble space telescope has two million lines of code, the Firefox browser has about ten million. Facebook has over sixty million and Google roughly two billion. But that doesn’t mean Weizenbaum was wrong. Ten thousand lines of code can be impenetrable. So can a single line!\\n\\nThese ideas had already appeared in Weizenbaum’s book Computer Power and Human Reason, published in 1976. It contains a whole chapter on ‘incomprehensible programs’. ‘These gigantic computer systems have usually been put together (one cannot always use the word ‘designed’) by teams of programmers, whose work is often spread over many years. By the time these systems come into use, most of the original programmers have left or turned their attention to other pursuits. It is precisely when such systems begin to be used that their inner workings can no longer be understood by any single person or by a small team of individuals.’\\n\\nWeizenbaum quotes a text by the AI researcher Marvin Minsky, published in 1967, nicely titled: ‘Why programming is a good medium for expressing poorly understood and sloppily formulated ideas’. This discussed the ‘widespread superstition’ that it is impossible to write a program ‘unless one has an extremely clear, precise formulation of what is to be done, and exactly how to do it’. Taking the example of the software able to beat humans at checkers, Minsky described how ‘once past the beginner level, programmers don’t simply write “sequences of instructions”. Instead, they write for the individuals of little societies or processes. For try as we may, we rarely can fully envision, in advance, all the details of their interactions. For that, after all, is why we need computers.’\\n\\nThe illusion that programming is a precise, rigid form of expression is based on a confusion of form and content. ‘It’s perfectly true that you have to be very precise in your computer (syntax) to get your program to run at all. No spelling or punctuation errors are allowed! But it’s perfectly false that this makes you have a precise idea of what your program will do.’ The more a program grows, the more the programmer will lose track of the details and no longer be able to say for certain what will happen, ‘watching the program as though it were an individual of unpredictable behaviour’. That, according to Minsky, was already the case with some large-scale programs, but the era of multi-user systems was only just beginning, and this was leading to a serious problem. Several programmers would be working on one program together, each from their own terminal, and none would understand the whole thing. ‘Now we see the real trouble with statements like “it only does what its programmer told it to do.” There isn’t any one programmer.’\\n\\nThree years earlier, in 1964, Stanisław Lem’s Summa Technologiae had appeared. It contains a chapter on ‘The Black Box’ that opens with the statement that no one person understands the construction of all the machinery of modern industrial society. Only society as a whole possesses this knowledge. ‘Cybernetics furthers this process, moving it to a higher level – since it is theoretically capable of producing things the structure of which will not be understood by anyone. A cybernetic device thus becomes a “black box” (a term frequently used by experts).’ The black boxes built until then had been so simple, Lem argued, that ‘engineer-cyberneticists’ could still understand the link between the input states and output states. ‘Yet a situation may arise when even he will not know a mathematical representation of this function’. Complex systems such as ‘the yet nonexistent “very large black boxes”’ are impossible to understand ‘as systems of this kind do not have algorithms.’ The algorithm, Lem reasons, must be replicable and allow for the prediction of future conditions, ‘while a society that finds itself in the same situation on\\n\\ntwo different occasions does not have to behave in the same way. This is precisely the case with all highly complex systems.’\\n\\nFinally, we tend to end up with Norbert Wiener, the originator of cybernetics. In Wiener’s article ‘Some Moral and Technical Consequences of Automation’, published in 1960, checkers programs again serve as proof: ‘The present level of these learning machines is that they play a fair amateur game at chess but that in checkers they can show a marked superiority to the player who has programmed them after 10 to 20 playing hours of working and indoctrination.’ Anyone who had played against these machines, wrote Wiener, would admit that they had original strategies. This, he believed, refuted the widespread assumption that machines could not create anything original, as well as the notion that they were essentially inferior to their creator and subject to her complete control.\\n\\nWiener was not claiming the existence of a technology that was incomprehensible per se. All he was saying was that it takes too long to understand it: by the time we can react to the information that has been transmitted from our sensory organs and hit the brakes, the car may already have driven into a wall. ‘It may well be that in principle we cannot make any machine the elements of whose behaviour we cannot comprehend sooner or later. This does not mean in any way that we shall be able to comprehend these elements in substantially less time than the time required for the operation of the machine, or even within any given number of years or generations.’ Our comprehension might lag far behind the completion of the task that was originally set. ‘This means that though machines are theoretically subject to human criticism, such criticism may be ineffective until long after it is relevant.’ Sixteen years later, Weizenbaum comments that what Wiener described as a possibility has since become reality.\\n\\nWeizenbaum’s chapter on incomprehensible software, now over forty years old, contains almost all the formulations and arguments of the current critique of machine learning software. ‘Our society’s growing reliance on computer systems that were initially intended to “help” people make analyses and decisions, but which have long since both surpassed the understanding of their users and become indispensable to them, is a very serious development. It has two important consequences. First, decisions are made with the aid of, and sometimes entirely by, computers whose programs no one any longer knows explicitly or understands. Hence no one can know the criteria or the rules on which such decisions are based. Second, the systems of rules and criteria that are embodied in such computer systems become immune to change, because, in the absence of a detailed understanding of the inner workings of a computer system, any substantial modification of it is very likely to render the whole system inoperative and possibly unrestorable. Such computer systems can therefore only grow.’\\n\\nThe fact that a state of affairs has existed for a long period time without the world coming to an end does not necessarily mean that it is harmless. Perhaps the world will still end. Or perhaps we are the thoroughly indoctrinated products of this development, incapable even of recognizing the problem. In any event, it’s not enough to blame the machine learning methods of recent years and to demand a return to the simple and completely comprehensible software that we had five years ago – okay ten… or twenty… well definitely fifty years ago.\\n\\nTo briefly summarize the reasons for the incomprehensibility of software: for Wiener, it’s our slowness of thought. We’re probably capable of understanding what a program does, but sometimes it takes so long that any criticism comes too late. For Lem, systems beyond a certain level of complexity are fundamentally unpredictable. Minsky cites the interactions of individual processes, the proliferation of code and the effects of several programmers working together. For Weizenbaum, it is the sheer size of systems, the departure of the original programmers and the passage of time.\\n\\nI would add two more reasons. The first is bugs, i.e. mistakes in the software and hardware. These were no less common fifty years ago than they are now. Perhaps we are now a bit more aware that they are unavoidable. This is a problem that can’t simply be got rid of by being more careful. The second reason is that code doesn’t consist merely of what one single programmer has devised. Even the simplest program relies on a whole load of third-party code that exceeds its own in size: integrated code libraries and frameworks that tackle frequently recurring tasks; layers of software that translate the code into a language that can be processed; software that displays, stores or transfers all of this.\\n\\nOccasional programmers like me might understand the uppermost levels of these. Professionals will have a little more insight. There may be some people who understand the entire software of a system down to its lowest level. But even then, the hardware remains opaque. The black box isn’t a chest that one only needs to open in order to know its contents. It contains more black boxes. Early programmers would mock the ‘software engineers’ that succeeded them for taking the hardware as a fait accompli. And they had probably been mocked by their predecessors for getting their chips by mail order rather than soldering their circuitry themselves. Everyone relies on the functioning of elements that for them represent black boxes.\\n\\nMost of these phenomena aren’t limited to software. They can be observed in buildings or technical facilities that have been rebuilt over the decades and adapted to new requirements. The documentation is seldom up to date or even in the same place as its object. The James Gregory Telescope in St Andrews is Scotland’s largest telescope; I am writing parts of this article nearby. The functioning of the device, which is over fifty years old, depends to a great extent on one retired computer scientist, who in his spare time has acquainted himself with the different historical layers of the telescope’s technology. Documentation of the various modifications to the device only exists – besides inside the head of Roger Stapleton – in fragmentary form and in disparate locations. This is not the exception but the rule.\\n\\nI wasn’t sure whether Chris Volinsky had been quoted accurately in the article I mentioned at the start. So I went on Twitter to ask. ‘It’s a fair quote’, Volinsky replied. ‘The final solution was an ensemble of 800+ models. No way to comprehend that fully.’ But what do ‘comprehend’ and ‘fully’ mean here? What is the ‘transparency’ and ‘comprehensibility’ of algorithms that are constantly being called for?\\n\\nThere are several possible interpretations. The basic question is whether the code is viewable at all. This is the case with open source software, but generally not in the commercial sector, irrespective of whether machine learning is involved. But let’s assume that the code can be viewed. Do demands for transparency mean that it should be readily understandable even by laypersons? Probably not, since that is the case only for the very simplest pieces of code, if that.\\n\\nIs it a question of whether the results can be verified? There are some computational processes where this is fairly straightforward. An introduction to scientific method from 1972 contains the following about ‘programmable electronic desktop computers’ (which really did take up half the desk): ‘It is important that the program is monitored for accuracy. To do so, the results printed out by the computer must be compared with the results obtained by doing the same calculations using pencil and paper.’ As soon as the calculation becomes more complex, however, pencil and paper will cease to be of much use.\\n\\nLet’s take a chess program that doesn’t use machine learning but is constructed exclusively from a decision tree containing every possible move. If we were to follow the above recommendation, we would have to print this tree out, with all its ramifications, and then go through all the different branches with a pencil. You would need a great deal of paper. And patience: the tree forms approximately 10120 different games. A simple machine learning solution would be easy to comprehend, by comparison.\\n\\nIn many cases, the accuracy of the results can be verified without using a pencil: we can see today whether yesterday’s weather forecast was right or wrong. Results that are harder to check include recommendations of books, films and music. Tastes can be shaped. If my local bookseller recommends her personal favourites with sufficient enthusiasm, then I may well warm to them. If Spotify plays a particular music genre to me for long enough, I may eventually overcome my dislike. An unsuitable recommendation has become a suitable one. The forecast has changed the world. And whether the simulation-based advice to suspend all air traffic did indeed save lives, we will never find out in the absence of a parallel universe identical in every other respect.\\n\\nThe most important version of the comprehensibility question is: ‘Is it possible to retroactively explain or otherwise visualize the route leading to the result?’ There is considerable demand for explanations like this, and intense research is being done on them under the term ‘Explainable Artificial Intelligence’. On the one hand, it would be a clear improvement if such systems were to supply information about how they get their results. On the other hand, the explanation creates new problems. One simple example is the link ‘Why was this recommended to me?’ that appears alongside Amazon’s personalized recommendations. If you click on it, you are given the answer that you had previously ordered wellingtons, a complete edition of Goethe and a set of wax crayons and might therefore be interested in an electric nose-hair trimmer. That’s not always helpful. And it is probably a massive simplification of the inner workings of the Amazon software. The existence of an explanation reassures customers but doesn’t clarify much.\\n\\nA paper published in 2016, entitled Generating Visual Explanations, describes software that identifies bird species in photographs and offers a textual explanation: ‘This is a Kentucky warbler because this is a yellow bird with a black cheek patch and a black crown.’ On closer inspection, it turns out that this sentence does not even approximate to a description of the path taken by the software. The bird is actually identified by a first system, which remains unexplained. The name is then passed on to a second system, which selects a description that humans can understand. The description gives the impression of being a self-description, without being anything of the sort. Identifying birds is one harmless example. When business interests or the social desirability of the answer come into play, the operators – consciously or unconsciously – will offer misleading explanations. The problem is now exacerbated, because even the existence of a lack of explanation is concealed.\\n\\nBecause the problem of the black box is an old one, people have been working on mitigation strategies for some time. In aeroplanes, ‘fly by wire’ systems have been used since the 1960s. The pilot’s actions are not relayed mechanically or hydraulically but electronically. This means that software is used which, if it fails, can have fatal consequences. For that reason, electronic systems crucial to safety are duplicated two, three and sometimes even four times. Duplication does not mean that the exact same system is copied, in case one breaks down. Different software is used, which runs partly on different hardware and is developed by separate teams. In tests, the systems have to reach identical results on the basis of identical inputs. For a specific problem – be it a mistake in the hardware, a mistake in the software, or a mistake caused by adverse environmental conditions – to lead all these systems astray in the same way is not impossible, but it is unlikely. In a system duplicated three times over, two elements that reach the same result can overrule a third element with a divergent outcome. (For the same reason, it was recommended to take at least three chronometers on board for sea travel in the nineteenth century; HMS Beagle had twenty-two of them on board for Darwin’s voyage. )\\n\\nSoftware runs on hardware. Microprocessors and circuit boards have not been designed manually since the 1970s, but by other software (‘Electronic Design Automation’). This is the only way to ensure that several billion components can be accommodated on a single modern-day microprocessor. There are no human beings who attempt to understand or check with a pencil whether these components have been correctly installed – this job is done by more software systems.\\n\\nIn aeroplane electronics, as with hardware design, the systems are too complex to be completely understood within a reasonable period of time. The strategies for dealing with this problem do not involve calling for ‘transparent’ systems or demanding that their use be stopped. They include developing new auxiliary tools for visualizing events, carrying out automated testing procedures and comparing results from various systems that work differently.\\n\\nThere is a whole range of software testing methods that require no understanding of the inner workings of the system. They are known as ‘black-box testing’. Black-box testing is when, before buying a used car, you take a quick look under the bonnet and, instead of inspecting all the parts, test drive it to check it can do the things cars are supposed to do: drive, accelerate, brake, change gear, not make any strange noises. Except, in computer science, there is no single test drive, but a great many, designed to test rare and unexpected events: for example, what happens if the car tries to drive up a wall?\\n\\nBlack-box testing is a technique suitable not only for situations where you have no understanding of the code. The procedure is also applied to open software, since it presents certain advantages. Especially when it comes to large, complex and imprecise tasks, black-box testing allows far more mistakes to be detected within a given time than through logical verification. The robotics scientist Rodney Brooks calls the attempt at mathematical proof, in connection with artificial intelligence, a fundamentally futile endeavour: ‘With multi-year large team efforts we cannot prove that a 1,000-line program cannot be breached by external hackers, so we certainly won’t be able to prove very much at all about large AI systems.’\\n\\nWith machine learning programs, currently the focus of heated discussion, duplicated systems and other mitigation strategies are hardly used yet. Apart from the fact that the whole research field is pretty new, this is because of two unfortunate factors. First, in many areas, the consequences only affect individuals, who at first sight don’t appear to be victims of the same faulty system, as with a plane crash. Second, there is often no competition. If my bank behaves idiotically and offers the excuse ‘It’s the software’s fault, we can’t do anything about it’, I switch banks. If airline A keep crashing, passengers will fly with airline B. But this option doesn’t always exist. It’s not easy for residents to switch cities, prisoners can’t choose which software should determine the likelihood of their reoffending, and consumers can’t do much about their credit rating. The isolation of the cases and the lack of competition mean that software manufacturers have fewer incentives to limit errors.\\n\\nThe alternative is for a human to decide whether someone is released from prison early, whether they get a loan, a medical diagnosis or an insurance policy. (These kinds of decision have long been taken using software that is not especially transparent for its users or those affected. But let’s not go there now.) In a paper published in 2016 on ‘The ethics of algorithms’, it says that ‘Algorithmic processing contrasts with traditional decision-making, where human decision-makers can in principle articulate their rationale when queried, limited only by their desire and capacity to give an explanation, and the questioner’s capacity to understand it.’\\n\\nThere is one major problem with this. It lies in the word ‘capacity’. Our decisions are influenced by a vast range of factors of which we are only dimly aware. This is an old and much researched observation. When a bookseller recommends something, the logic that led to the recommendation is more unclear than any algorithm. The bookseller will offer an explanation, but the connection between this and the actual background behind the recommendation remains opaque. The human brain is a black box, both objectively and for its owner.\\n\\nStanisław Lem makes this point in Summa Technologiae. We know that it is possible to construct black boxes without planning them or understanding them entirely, he says, because we ourselves are black boxes. ‘Every human being is thus an excellent example of a device that can be used without knowing its algorithm. Our own brain is one of the “devices” that is “closest to us” in the whole Universe: we have it in our heads. Yet even today, we still do not know how the brain works exactly. As demonstrated by the history of psychology, the examination of its mechanics via introspection is highly fallible and leads one astray, to some most fallacious hypotheses.’ But the fact that nature has created black boxes shows that their construction is possible and that their impenetrability is not an obstacle to their functioning per se.\\n\\nGaps in our knowledge do not stop us thinking. They do not stop us using general anaesthetics, even though we don’t properly understand how and why they work. The advantages of anaesthetics are so great that we are willing to overlook their mysteriousness. I’m not arguing that the brain, anaesthesia or software are better left unexplained, but that we shouldn’t insist on stricter standards for software than we would for other kinds of ‘black box’. It’s not unusual for something to be in use even if we cannot fully explain it. Lemons were used for centuries to prevent scurvy, long before it was understood why they had this effect.\\n\\nJessa Jones runs a repair centre for Apple devices. She explains how she works on YouTube: ‘Manufacturers’ support for repair is really non-existent. It’s an antagonistic relationship. When we go to fight for the right to repair, they come in and say, “You shouldn’t be able to repair”. There’s no support. We’re completely figuring this out on our own. My background is in molecular genetics, which is all about trying to understand how tiny things that you can’t see work when there’s no user manual.’ Reverse engineering and biology are both fields where researchers investigate complex, non-transparent and non-linear systems. Particularly in biology, demands for better documented and more comprehensible organisms won’t get you very far. Even creatures that are literally transparent won’t make your work much easier. The fact that we are still able to increase our knowledge is down to a collection of strategies and methods. They are known as science and up to now have proven pretty effective.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('How to solve 90% of NLP problems: a step-by-step guide', 'This post is accompanied by an interactive notebook demonstrating and applying all these techniques. Feel free to run the code and follow along!\\n\\nStep 1: Gather your data\\n\\nExample data sources\\n\\nEvery Machine Learning problem starts with data, such as a list of emails, posts, or tweets. Common sources of textual information include:\\n\\nProduct reviews (on Amazon, Yelp, and various App Stores)\\n\\nUser-generated content (Tweets, Facebook posts, StackOverflow questions)\\n\\nTroubleshooting (customer requests, support tickets, chat logs)\\n\\n“Disasters on Social Media” dataset\\n\\nFor this post, we will use a dataset generously provided by CrowdFlower, called “Disasters on Social Media”, where:\\n\\nContributors looked at over 10,000 tweets culled with a variety of searches like “ablaze”, “quarantine”, and “pandemonium”, then noted whether the tweet referred to a disaster event (as opposed to a joke with the word or a movie review or something non-disastrous).\\n\\nOur task will be to detect which tweets are about a disastrous event as opposed to an irrelevant topic such as a movie. Why? A potential application would be to exclusively notify law enforcement officials about urgent emergencies while ignoring reviews of the most recent Adam Sandler film. A particular challenge with this task is that both classes contain the same search terms used to find the tweets, so we will have to use subtler differences to distinguish between them.\\n\\nIn the rest of this post, we will refer to tweets that are about disasters as “disaster”, and tweets about anything else as “irrelevant”.\\n\\nLabels\\n\\nWe have labeled data and so we know which tweets belong to which categories. As Richard Socher outlines below, it is usually faster, simpler, and cheaper to find and label enough data to train a model on, rather than trying to optimize a complex unsupervised method.\\n\\nRichard Socher’s pro-tip\\n\\nStep 2: Clean your data\\n\\nThe number one rule we follow is: “Your model will only ever be as good as your data.”\\n\\nOne of the key skills of a data scientist is knowing whether the next step should be working on the model or the data. A good rule of thumb is to look at the data first and then clean it up. A clean dataset will allow a model to learn meaningful features and not overfit on irrelevant noise.\\n\\nHere is a checklist to use to clean your data: (see the code for more details):\\n\\nRemove all irrelevant characters such as any non alphanumeric characters Tokenize your text by separating it into individual words Remove words that are not relevant, such as “@” twitter mentions or urls Convert all characters to lowercase, in order to treat words such as “hello”, “Hello”, and “HELLO” the same Consider combining misspelled or alternately spelled words to a single representation (e.g. “cool”/”kewl”/”cooool”) Consider lemmatization (reduce words such as “am”, “are”, and “is” to a common form such as “be”)\\n\\nAfter following these steps and checking for additional errors, we can start using the clean, labelled data to train models!\\n\\nStep 3: Find a good data representation\\n\\nMachine Learning models take numerical values as input. Models working on images, for example, take in a matrix representing the intensity of each pixel in each color channel.\\n\\nA smiling face represented as a matrix of numbers.\\n\\nOur dataset is a list of sentences, so in order for our algorithm to extract patterns from the data, we first need to find a way to represent it in a way that our algorithm can understand, i.e. as a list of numbers.\\n\\nOne-hot encoding (Bag of Words)\\n\\nA natural way to represent text for computers is to encode each character individually as a number (ASCII for example). If we were to feed this simple representation into a classifier, it would have to learn the structure of words from scratch based only on our data, which is impossible for most datasets. We need to use a higher level approach.\\n\\nFor example, we can build a vocabulary of all the unique words in our dataset, and associate a unique index to each word in the vocabulary. Each sentence is then represented as a list that is as long as the number of distinct words in our vocabulary. At each index in this list, we mark how many times the given word appears in our sentence. This is called a Bag of Words model, since it is a representation that completely ignores the order of words in our sentence. This is illustrated below.\\n\\nRepresenting sentences as a Bag of Words. Sentences on the left, representation on the right. Each index in the vectors represent one particular word.\\n\\nVisualizing the embeddings\\n\\nWe have around 20,000 words in our vocabulary in the “Disasters of Social Media” example, which means that every sentence will be represented as a vector of length 20,000. The vector will contain mostly 0s because each sentence contains only a very small subset of our vocabulary.\\n\\nIn order to see whether our embeddings are capturing information that is relevant to our problem (i.e. whether the tweets are about disasters or not), it is a good idea to visualize them and see if the classes look well separated. Since vocabularies are usually very large and visualizing data in 20,000 dimensions is impossible, techniques like PCA will help project the data down to two dimensions. This is plotted below.\\n\\nVisualizing Bag of Words embeddings.\\n\\nThe two classes do not look very well separated, which could be a feature of our embeddings or simply of our dimensionality reduction. In order to see whether the Bag of Words features are of any use, we can train a classifier based on them.\\n\\nStep 4: Classification\\n\\nWhen first approaching a problem, a general best practice is to start with the simplest tool that could solve the job. Whenever it comes to classifying data, a common favorite for its versatility and explainability is Logistic Regression. It is very simple to train and the results are interpretable as you can easily extract the most important coefficients from the model.\\n\\nWe split our data in to a training set used to fit our model and a test set to see how well it generalizes to unseen data. After training, we get an accuracy of 75.4%. Not too shabby! Guessing the most frequent class (“irrelevant”) would give us only 57%. However, even if 75% precision was good enough for our needs, we should never ship a model without trying to understand it.\\n\\nStep 5: Inspection\\n\\nConfusion Matrix\\n\\nA first step is to understand the types of errors our model makes, and which kind of errors are least desirable. In our example, false positives are classifying an irrelevant tweet as a disaster, and false negatives are classifying a disaster as an irrelevant tweet. If the priority is to react to every potential event, we would want to lower our false negatives. If we are constrained in resources however, we might prioritize a lower false positive rate to reduce false alarms. A good way to visualize this information is using a Confusion Matrix, which compares the predictions our model makes with the true label. Ideally, the matrix would be a diagonal line from top left to bottom right (our predictions match the truth perfectly).\\n\\nConfusion Matrix (Green is a high proportion, blue is low)\\n\\nOur classifier creates more false negatives than false positives (proportionally). In other words, our model’s most common error is inaccurately classifying disasters as irrelevant. If false positives represent a high cost for law enforcement, this could be a good bias for our classifier to have.\\n\\nExplaining and interpreting our model\\n\\nTo validate our model and interpret its predictions, it is important to look at which words it is using to make decisions. If our data is biased, our classifier will make accurate predictions in the sample data, but the model would not generalize well in the real world. Here we plot the most important words for both the disaster and irrelevant class. Plotting word importance is simple with Bag of Words and Logistic Regression, since we can just extract and rank the coefficients that the model used for its predictions.\\n\\nBag of Words: Word importance\\n\\nOur classifier correctly picks up on some patterns (hiroshima, massacre), but clearly seems to be overfitting on some meaningless terms (heyoo, x1392). Right now, our Bag of Words model is dealing with a huge vocabulary of different words and treating all words equally. However, some of these words are very frequent, and are only contributing noise to our predictions. Next, we will try a way to represent sentences that can account for the frequency of words, to see if we can pick up more signal from our data.\\n\\nStep 6: Accounting for vocabulary structure\\n\\nTF-IDF\\n\\nIn order to help our model focus more on meaningful words, we can use a TF-IDF score (Term Frequency, Inverse Document Frequency) on top of our Bag of Words model. TF-IDF weighs words by how rare they are in our dataset, discounting words that are too frequent and just add to the noise. Here is the PCA projection of our new embeddings.\\n\\nVisualizing TF-IDF embeddings.\\n\\nWe can see above that there is a clearer distinction between the two colors. This should make it easier for our classifier to separate both groups. Let’s see if this leads to better performance. Training another Logistic Regression on our new embeddings, we get an accuracy of 76.2%.\\n\\nA very slight improvement. Has our model started picking up on more important words? If we are getting a better result while preventing our model from “cheating” then we can truly consider this model an upgrade.\\n\\nTF-IDF: Word importance\\n\\nThe words it picked up look much more relevant! Although our metrics on our test set only increased slightly, we have much more confidence in the terms our model is using, and thus would feel more comfortable deploying it in a system that would interact with customers.\\n\\nStep 7: Leveraging semantics\\n\\nWord2Vec\\n\\nOur latest model managed to pick up on high signal words. However, it is very likely that if we deploy this model, we will encounter words that we have not seen in our training set before. The previous model will not be able to accurately classify these tweets, even if it has seen very similar words during training.\\n\\nTo solve this problem, we need to capture the semantic meaning of words, meaning we need to understand that words like ‘good’ and ‘positive’ are closer than ‘apricot’ and ‘continent.’ The tool we will use to help us capture meaning is called Word2Vec.\\n\\nUsing pre-trained words\\n\\nWord2Vec is a technique to find continuous embeddings for words. It learns from reading massive amounts of text and memorizing which words tend to appear in similar contexts. After being trained on enough data, it generates a 300-dimension vector for each word in a vocabulary, with words of similar meaning being closer to each other.\\n\\nThe authors of the paper open sourced a model that was pre-trained on a very large corpus which we can leverage to include some knowledge of semantic meaning into our model. The pre-trained vectors can be found in the repository associated with this post.\\n\\nSentence level representation\\n\\nA quick way to get a sentence embedding for our classifier is to average Word2Vec scores of all words in our sentence. This is a Bag of Words approach just like before, but this time we only lose the syntax of our sentence, while keeping some semantic information.\\n\\nWord2Vec sentence embedding\\n\\nHere is a visualization of our new embeddings using previous techniques:\\n\\nVisualizing Word2Vec embeddings.\\n\\nThe two groups of colors look even more separated here, our new embeddings should help our classifier find the separation between both classes. After training the same model a third time (a Logistic Regression), we get an accuracy score of 77.7%, our best result yet! Time to inspect our model.\\n\\nThe Complexity/Explainability trade-off\\n\\nSince our embeddings are not represented as a vector with one dimension per word as in our previous models, it’s harder to see which words are the most relevant to our classification. While we still have access to the coefficients of our Logistic Regression, they relate to the 300 dimensions of our embeddings rather than the indices of words.\\n\\nFor such a low gain in accuracy, losing all explainability seems like a harsh trade-off. However, with more complex models we can leverage black box explainers such as LIME in order to get some insight into how our classifier works.\\n\\nLIME\\n\\nLIME is available on Github through an open-sourced package. A black-box explainer allows users to explain the decisions of any classifier on one particular example by perturbing the input (in our case removing words from the sentence) and seeing how the prediction changes.\\n\\nLet’s see a couple explanations for sentences from our dataset.\\n\\nCorrect disaster words are picked up to classify as “relevant”.\\n\\nHere, the contribution of the words to the classification seems less obvious.\\n\\nHowever, we do not have time to explore the thousands of examples in our dataset. What we’ll do instead is run LIME on a representative sample of test cases and see which words keep coming up as strong contributors. Using this approach we can get word importance scores like we had for previous models and validate our model’s predictions.\\n\\nWord2Vec: Word importance\\n\\nLooks like the model picks up highly relevant words implying that it appears to make understandable decisions. These seem like the most relevant words out of all previous models and therefore we’re more comfortable deploying in to production.\\n\\nStep 8: Leveraging syntax using end-to-end approaches\\n\\nWe’ve covered quick and efficient approaches to generate compact sentence embeddings. However, by omitting the order of words, we are discarding all of the syntactic information of our sentences. If these methods do not provide sufficient results, you can utilize more complex model that take in whole sentences as input and predict labels without the need to build an intermediate representation. A common way to do that is to treat a sentence as a sequence of individual word vectors using either Word2Vec or more recent approaches such as GloVe or CoVe. This is what we will do below.')"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Can agents learn inside of their own dreams?', 'Interactive demo: Tap screen to override the agent\\'s decisions. scroll down\\n\\nWorld Models\\n\\nCan agents learn inside of their own dreams?\\n\\nAbstract\\n\\nWe explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own dream environment generated by its world model, and transfer this policy back into the actual environment.\\n\\nIntroduction\\n\\nA World Model, from Scott McCloud\\'s Understanding Comics.\\n\\nHumans develop a mental model of the world based on what they are able to perceive with their limited senses. The decisions and actions we make are based on this internal model. Jay Wright Forrester, the father of system dynamics, described a mental model as:\\n\\n“The image of the world around us, which we carry in our head, is just a model. Nobody in his head imagines all the world, government or country. He has only selected concepts, and relationships between them, and uses those to represent the real system.”\\n\\nTo handle the vast amount of information that flows through our daily lives, our brain learns an abstract representation of both spatial and temporal aspects of this information. We are able to observe a scene and remember an abstract description thereof . Evidence also suggests that what we perceive at any given moment is governed by our brain’s prediction of the future based on our internal model .\\n\\nWhat we see is based on our brain\\'s prediction of the future.\\n\\nOne way of understanding the predictive model inside of our brains is that it might not be about just predicting the future in general, but predicting future sensory data given our current motor actions . We are able to instinctively act on this predictive model and perform fast reflexive behaviours when we face danger , without the need to consciously plan out a course of action.\\n\\nTake baseball for example. A baseball batter has milliseconds to decide how they should swing the bat -- shorter than the time it takes for visual signals from our eyes to reach our brain. The reason we are able to hit a 100mph fastball is due to our ability to instinctively predict when and where the ball will go. For professional players, this all happens subconsciously. Their muscles reflexively swing the bat at the right time and location in line with their internal models\\' predictions . They can quickly act on their predictions of the future without the need to consciously roll out possible future scenarios to form a plan .\\n\\nWe learn to perceive time spatially when we read comics. According to cartoonist and comics theorist Scott McCloud, “in the world of comics, time and space are one and the same.” Art © Scott McCloud.\\n\\nIn many reinforcement learning (RL) problems , an artificial agent also benefits from having a good representation of past and present states, and a good predictive model of the future , preferably a powerful predictive model implemented on a general purpose computer such as a recurrent neural network (RNN) .\\n\\nLarge RNNs are highly expressive models that can learn rich spatial and temporal representations of data. However, many model-free RL methods in the literature often only use small neural networks with few parameters. The RL algorithm is often bottlenecked by the credit assignment problem In many RL problems, the feedback (positive or negative reward) is given at end of a sequence of steps. The credit assignment problem tackles the problem of figuring out which steps caused the resulting feedback--which steps should receive credit or blame for the final result? , which makes it hard for traditional RL algorithms to learn millions of weights of a large model, hence in practice, smaller networks are used as they iterate faster to a good policy during training.\\n\\nIdeally, we would like to be able to efficiently train large RNN-based agents. The backpropagation algorithm can be used to train large neural networks efficiently. In this work we look at training a large neural network Typical model-free RL models have in the order of 1 0 3 10^3 10\\u200b3\\u200b\\u200b to 1 0 6 10^6 10\\u200b6\\u200b\\u200b model parameters. We look at training models in the order of 1 0 7 10^7 10\\u200b7\\u200b\\u200b parameters, which is still rather small compared to state-of-the-art deep learning models with 1 0 8 10^8 10\\u200b8\\u200b\\u200b to even 1 0 9 10^{9} 10\\u200b9\\u200b\\u200b parameters. In principle, the procedure described in this article can take advantage of these larger networks if we wanted to use them. to tackle RL tasks, by dividing the agent into a large world model and a small controller model. We first train a large neural network to learn a model of the agent\\'s world in an unsupervised manner, and then train the smaller controller model to learn to perform a task using this world model. A small controller lets the training algorithm focus on the credit assignment problem on a small search space, while not sacrificing capacity and expressiveness via the larger world model. By training the agent through the lens of its world model, we show that it can learn a highly compact policy to perform its task.\\n\\nAlthough there is a large body of research relating to model-based reinforcement learning, this article is not meant to be a review of the current state of the field. Instead, the goal of this article is to distill several key concepts from a series of papers 1990--2015 on combinations of RNN-based world models and controllers . We will also discuss other related works in the literature that share similar ideas of learning a world model and training an agent using this model.\\n\\nIn this article, we present a simplified framework that we can use to experimentally demonstrate some of the key concepts from these papers, and also suggest further insights to effectively apply these ideas to various RL environments. We use similar terminology and notation as On Learning to Think: Algorithmic Information Theory for Novel Combinations of RL Controllers and RNN World Models when describing our methodology and experiments.\\n\\nAgent Model\\n\\nWe present a simple model inspired by our own cognitive system. In this model, our agent has a visual sensory component that compresses what it sees into a small representative code. It also has a memory component that makes predictions about future codes based on historical information. Finally, our agent has a decision-making component that decides what actions to take based only on the representations created by its vision and memory components.\\n\\nOur agent consists of three components that work closely together: Vision (V), Memory (M), and Controller (C).\\n\\nVAE (V) Model\\n\\nThe environment provides our agent with a high dimensional input observation at each time step. This input is usually a 2D image frame that is part of a video sequence. The role of the V model is to learn an abstract, compressed representation of each observed input frame.\\n\\n\\n\\nFlow diagram of a Variational Autoencoder.\\n\\nWe use a Variational Autoencoder (VAE) as the V model in our experiments. In the following demo, we show how the V model compresses each frame it receives at time step t t t into a low dimensional latent vector z t z_t z\\u200bt\\u200b\\u200b. This compressed representation can be used to reconstruct the original image.\\n\\nInteractive Demo A VAE trained on screenshots obtained from a VizDoom environment. You can load randomly chosen screenshots to be encoded into a small latent vector z z z , which is used to reconstruct the original screenshot. You can also experiment with adjusting the values of the z z z vector using the slider bars to see how it affects the reconstruction, or randomize z z z to observe the space of possible screenshots learned by our VAE.\\n\\nMDN-RNN (M) Model\\n\\nWhile it is the role of the V model to compress what the agent sees at each time frame, we also want to compress what happens over time. For this purpose, the role of the M model is to predict the future. The M model serves as a predictive model of the future z z z vectors that V is expected to produce. Because many complex environments are stochastic in nature, we train our RNN to output a probability density function p ( z ) p(z) p(z) instead of a deterministic prediction of z z z.\\n\\nRNN with a Mixture Density Network output layer. The MDN outputs the parameters of a mixture of Gaussian distribution used to sample a prediction of the next latent vector z z z .\\n\\nIn our approach, we approximate p ( z ) p(z) p(z) as a mixture of Gaussian distribution, and train the RNN to output the probability distribution of the next latent vector z t + 1 z_{t+1} z\\u200bt+1\\u200b\\u200b given the current and past information made available to it.\\n\\nMore specifically, the RNN will model P ( z t + 1 ∣ a t , z t , h t ) P(z_{t+1} \\\\; | \\\\; a_t, z_t, h_t) P(z\\u200bt+1\\u200b\\u200b∣a\\u200bt\\u200b\\u200b,z\\u200bt\\u200b\\u200b,h\\u200bt\\u200b\\u200b), where a t a_t a\\u200bt\\u200b\\u200b is the action taken at time t t t and h t h_t h\\u200bt\\u200b\\u200b is the hidden state of the RNN at time t t t. During sampling, we can adjust a temperature parameter τ \\\\tau τ to control model uncertainty, as done in -- we will find adjusting τ \\\\tau τ to be useful for training our controller later on.\\n\\nSketchRNN is an example of a MDN-RNN used to predict the next pen strokes of a sketch drawing. We use a similar model to predict the next latent vector z z z .\\n\\nThis approach is known as a Mixture Density Network combined with a RNN (MDN-RNN) , and has been used successfully in the past for sequence generation problems such as generating handwriting and sketches .\\n\\nController (C) Model\\n\\nThe Controller (C) model is responsible for determining the course of actions to take in order to maximize the expected cumulative reward of the agent during a rollout of the environment. In our experiments, we deliberately make C as simple and small as possible, and trained separately from V and M, so that most of our agent\\'s complexity resides in the world model (V and M).\\n\\nC is a simple single layer linear model that maps z t z_t z\\u200bt\\u200b\\u200b and h t h_t h\\u200bt\\u200b\\u200b directly to action a t a_t a\\u200bt\\u200b\\u200b at each time step:\\n\\na t = W c [ z t h t ] + b c a_t = W_c \\\\; [z_t \\\\; h_t]\\\\; + \\\\; b_c a\\u200bt\\u200b\\u200b=W\\u200bc\\u200b\\u200b[z\\u200bt\\u200b\\u200bh\\u200bt\\u200b\\u200b]+b\\u200bc\\u200b\\u200b\\n\\nIn this linear model, W c W_c W\\u200bc\\u200b\\u200b and b c b_c b\\u200bc\\u200b\\u200b are the weight matrix and bias vector that maps the concatenated input vector [ z t h t ] [z_t \\\\; h_t] [z\\u200bt\\u200b\\u200bh\\u200bt\\u200b\\u200b] to the output action vector a t a_t a\\u200bt\\u200b\\u200b. To be clear, the prediction of z t + 1 z_{t+1} z\\u200bt+1\\u200b\\u200b is not fed into the controller C directly -- just the hidden state h t h_t h\\u200bt\\u200b\\u200b and z t z_t z\\u200bt\\u200b\\u200b. This is because h t h_t h\\u200bt\\u200b\\u200b has all the information needed to generate the parameters of a mixture of Gaussian distribution, if we want to sample z t + 1 z_{t+1} z\\u200bt+1\\u200b\\u200b to make a prediction.\\n\\nPutting Everything Together\\n\\nThe following flow diagram illustrates how V, M, and C interacts with the environment:\\n\\nFlow diagram of our Agent model. The raw observation is first processed by V at each time step t t t to produce z t z_t z \\u200b t \\u200b \\u200b . The input into C is this latent vector z t z_t z \\u200b t \\u200b \\u200b concatenated with M\\'s hidden state h t h_t h \\u200b t \\u200b \\u200b at each time step. C will then output an action vector a t a_t a \\u200b t \\u200b \\u200b for motor control. M will then take the current z t z_t z \\u200b t \\u200b \\u200b and action a t a_t a \\u200b t \\u200b \\u200b as an input to update its own hidden state to produce h t + 1 h_{t+1} h \\u200b t + 1 \\u200b \\u200b to be used at time t + 1 t+1 t + 1 .\\n\\nBelow is the pseudocode for how our agent model is used in the OpenAI Gym environment. Running this function on a given controller C will return the cumulative reward during a rollout of the environment.\\n\\ndef rollout(controller): \\'\\'\\' env, rnn, vae are \\'\\'\\' \\'\\'\\' global variables \\'\\'\\' obs = env.reset() h = rnn.initial_state() done = False cumulative_reward = 0 while not done: z = vae.encode(obs) a = controller.action([z, h]) obs, reward, done = env.step(a) cumulative_reward += reward h = rnn.forward([a, z, h]) return cumulative_reward\\n\\nThis minimal design for C also offers important practical benefits. Advances in deep learning provided us with the tools to train large, sophisticated models efficiently, provided we can define a well-behaved, differentiable loss function. Our V and M models are designed to be trained efficiently with the backpropagation algorithm using modern GPU accelerators, so we would like most of the model\\'s complexity, and model parameters to reside in V and M. The number of parameters of C, a linear model, is minimal in comparison. This choice allows us to explore more unconventional ways to train C -- for example, even using evolution strategies (ES) to tackle more challenging RL tasks where the credit assignment problem is difficult.\\n\\nTo optimize the parameters of C, we chose the Covariance-Matrix Adaptation Evolution Strategy (CMA-ES) as our optimization algorithm since it is known to work well for solution spaces of up to a few thousand parameters. We evolve parameters of C on a single machine with multiple CPU cores running multiple rollouts of the environment in parallel.\\n\\nFor more specific information about the models, training procedures, and environments used in our experiments, please refer to the Appendix.\\n\\nCar Racing Experiment: World Model for Feature Extraction\\n\\nA predictive world model can help us extract useful representations of space and time. By using these features as inputs of a controller, we can train a compact and minimal controller to perform a continuous control task, such as learning to drive from pixel inputs for a top-down car racing environment . In this section, we describe how we can train the Agent model described earlier to solve a car racing task. To our knowledge, our agent is the first known solution to achieve the score required to solve this task. We find this task interesting because although it is not difficult to train an agent to wobble around randomly generated tracks and obtain a mediocre score, CarRacing-v0 defines \"solving\" as getting average reward of 900 over 100 consecutive trials, which means the agent can only afford very few driving mistakes.\\n\\nOur agent learning to navigate a top-down racing environment.\\n\\nIn this environment, the tracks are randomly generated for each trial, and our agent is rewarded for visiting as many tiles as possible in the least amount of time. The agent controls three continuous actions: steering left/right, acceleration, and brake.\\n\\nTo train our V model, we first collect a dataset of 10,000 random rollouts of the environment. We have first an agent acting randomly to explore the environment multiple times, and record the random actions a t a_t a\\u200bt\\u200b\\u200b taken and the resulting observations from the environment. We will discuss an iterative training procedure later on for more complicated environments where a random policy is not sufficient. We use this dataset to train V to learn a latent space of each frame observed. We train our VAE to encode each frame into low dimensional latent vector z z z by minimizing the difference between a given frame and the reconstructed version of the frame produced by the decoder from z z z. The following demo shows the results of our VAE after training:\\n\\nInteractive Demo Our VAE trained on observations from CarRacing-v0 . Despite losing details during this lossy compression process, latent vector z z z captures the essence of each 64x64px image frame.\\n\\nWe can now use our trained V model to pre-process each frame at time t t t into z t z_t z\\u200bt\\u200b\\u200b to train our M model. Using this pre-processed data, along with the recorded random actions a t a_t a\\u200bt\\u200b\\u200b taken, our MDN-RNN can now be trained to model P ( z t + 1 ∣ a t , z t , h t ) P(z_{t+1} \\\\; | \\\\; a_t, z_t, h_t) P(z\\u200bt+1\\u200b\\u200b∣a\\u200bt\\u200b\\u200b,z\\u200bt\\u200b\\u200b,h\\u200bt\\u200b\\u200b) as a mixture of Gaussians. In principle, we can train both models together in an end-to-end manner, although we found that training each separately is more practical, and also achieves satisfactory results. Training each model only required less than an hour of computation time using a single NVIDIA P100 GPU. We can also train individual VAE and MDN-RNN models without having to exhaustively tune hyperparameters.\\n\\nIn this experiment, the world model (V and M) has no knowledge about the actual reward signals from the environment. Its task is simply to compress and predict the sequence of image frames observed. Only the Controller (C) Model has access to the reward information from the environment. Since the there are a mere 867 parameters inside the linear controller model, evolutionary algorithms such as CMA-ES are well suited for this optimization task.\\n\\nThe figure below compares actual the observation given to the agent and the observation captured by the world model. We can use the VAE to reconstruct each frame using z t z_t z\\u200bt\\u200b\\u200b at each time step to visualize the quality of the information the agent actually sees during a rollout:\\n\\nActual observations from the environment. What gets encoded into z t z_t z \\u200b t \\u200b \\u200b .\\n\\nProcedure\\n\\nTo summarize the Car Racing experiment, below are the steps taken:\\n\\nCollect 10,000 rollouts from a random policy. Train VAE (V) to encode each frame into a latent vector z ∈ R 3 2 z \\\\in \\\\mathcal{R}^{32} z ∈ R \\u200b 3 2 \\u200b \\u200b . Train MDN-RNN (M) to model P ( z t + 1 ∣ a t , z t , h t ) P(z_{t+1} \\\\; | \\\\; a_t, z_t, h_t) P ( z \\u200b t + 1 \\u200b \\u200b ∣ a \\u200b t \\u200b \\u200b , z \\u200b t \\u200b \\u200b , h \\u200b t \\u200b \\u200b ) . Define Controller (C) as a t = W c [ z t h t ] + b c a_t = W_c \\\\; [z_t \\\\; h_t]\\\\; + \\\\; b_c a \\u200b t \\u200b \\u200b = W \\u200b c \\u200b \\u200b [ z \\u200b t \\u200b \\u200b h \\u200b t \\u200b \\u200b ] + b \\u200b c \\u200b \\u200b . Use CMA-ES to solve for a W c W_c W \\u200b c \\u200b \\u200b and b c b_c b \\u200b c \\u200b \\u200b that maximizes the expected cumulative reward.\\n\\nModel Parameter Count VAE 4,348,547 MDN-RNN 422,368 Controller 867\\n\\nCar Racing Experiment Results\\n\\nV Model Only\\n\\nTraining an agent to drive is not a difficult task if we have a good representation of the observation. Previous works have shown that with a good set of hand-engineered information about the observation, such as LIDAR information, angles, positions and velocities, one can easily train a small feed-forward network to take this hand-engineered input and output a satisfactory navigation policy. For this reason, we first want to test our agent by handicapping C to only have access to V but not M, so we define our controller as a t = W c z t + b c a_t = W_c \\\\; z_t \\\\;+ \\\\; b_c a\\u200bt\\u200b\\u200b=W\\u200bc\\u200b\\u200bz\\u200bt\\u200b\\u200b+b\\u200bc\\u200b\\u200b.\\n\\nLimiting our controller to see only z t z_t z \\u200b t \\u200b \\u200b , but not h t h_t h \\u200b t \\u200b \\u200b results in wobbly and unstable driving behaviours.\\n\\nAlthough the agent is still able to navigate the race track in this setting, we notice it wobbles around and misses the tracks on sharper corners. This handicapped agent achieved an average score of 632 ± \\\\pm ± 251 over 100 random trials, in line with the performance of other agents on OpenAI Gym\\'s leaderboard and traditional Deep RL methods such as A3C . Adding a hidden layer to C\\'s policy network helps to improve the results to 788 ± \\\\pm ± 141, but not quite enough to solve this environment.\\n\\nFull World Model (V and M)\\n\\nThe representation z t z_t z\\u200bt\\u200b\\u200b provided by our V model only captures a representation at a moment in time and doesn\\'t have much predictive power. In contrast, M is trained to do one thing, and to do it really well, which is to predict z t + 1 z_{t+1} z\\u200bt+1\\u200b\\u200b. Since M\\'s prediction of z t + 1 z_{t+1} z\\u200bt+1\\u200b\\u200b is produced from the RNN\\'s hidden state h t h_t h\\u200bt\\u200b\\u200b at time t t t, this vector is a good candidate for the set of learned features we can give to our agent. Combining z t z_t z\\u200bt\\u200b\\u200b with h t h_t h\\u200bt\\u200b\\u200b gives our controller C a good representation of both the current observation, and what to expect in the future.\\n\\nDriving is more stable if we give our controller access to both z t z_t z \\u200b t \\u200b \\u200b and h t h_t h \\u200b t \\u200b \\u200b .\\n\\nIndeed, we see that allowing the agent to access the both z t z_t z\\u200bt\\u200b\\u200b and h t h_t h\\u200bt\\u200b\\u200b greatly improves its driving capability. The driving is more stable, and the agent is able to seemingly attack the sharp corners effectively. Furthermore, we see that in making these fast reflexive driving decisions during a car race, the agent does not need to plan ahead and roll out hypothetical scenarios of the future. Since h t h_t h\\u200bt\\u200b\\u200b contain information about the probability distribution of the future, the agent can just query the RNN instinctively to guide its action decisions. Like a seasoned Formula One driver or the baseball player discussed earlier, the agent can instinctively predict when and where to navigate in the heat of the moment.\\n\\nMethod \\\\;\\\\; Average Score over 100 Random Tracks \\\\;\\\\; DQN 343 ± \\\\pm ± 18 A3C (continuous) 591 ± \\\\pm ± 45 A3C (discrete) 652 ± \\\\pm ± 10 ceobillionaire\\'s algorithm (unpublished) 838 ± \\\\pm ± 11 V model only, z z z input 632 ± \\\\pm ± 251 V model only, z z z input with a hidden layer 788 ± \\\\pm ± 141 Full World Model, z z z and h h h 906 ± \\\\pm ± 21\\n\\nOur agent was able to achieve a score of 906 ± \\\\pm ± 21 over 100 random trials, effectively solving the task and obtaining new state of the art results. Previous attempts using traditional Deep RL methods obtained average scores of 591--652 range, and the best reported solution on the leaderboard obtained an average score of 838 ± \\\\pm ± 11 over 100 random consecutive trials. Traditional Deep RL methods often require pre-processing of each frame, such as employing edge-detection , in addition to stacking a few recent frames into the input. In contrast, our world model takes in a stream of raw RGB pixel images and directly learns a spatial-temporal representation. To our knowledge, our method is the first reported solution to solve this task.\\n\\nCar Racing Dreams\\n\\nSince our world model is able to model the future, we are also able to have it come up with hypothetical car racing scenarios on its own. We can ask it to produce the probability distribution of z t + 1 z_{t+1} z\\u200bt+1\\u200b\\u200b given the current states, sample a z t + 1 z_{t+1} z\\u200bt+1\\u200b\\u200b and use this sample as the real observation. We can put our trained C back into this dream environment generated by M. The following demo shows how our world model can be used to generate the car racing environment:\\n\\nInteractive Demo Our agent driving inside of its own dream world. Here, we deploy our trained policy into a fake environment generated by the MDN-RNN, and rendered using the VAE\\'s decoder. You can override the agent\\'s actions by tapping on the left or right side of the screen, or by hitting arrow keys (left/right to steer, up/down to accelerate or brake). The uncertainty level of the environment can be adjusted by changing τ \\\\tau τ using the slider on the bottom right.\\n\\nWe have just seen that a policy learned inside of the real environment appears to somewhat function inside of the dream environment. This begs the question -- can we train our agent to learn inside of its own dream, and transfer this policy back to the actual environment?\\n\\nVizDoom Experiment: Learning Inside of a Dream\\n\\nIf our world model is sufficiently accurate for its purpose, and complete enough for the problem at hand, we should be able to substitute the actual environment with this world model. After all, our agent does not directly observe the reality, but only sees what the world model lets it see. In this experiment, we train an agent inside the dream environment generated by its world model trained to mimic a VizDoom environment.\\n\\nOur final agent solving the VizDoom: Take Cover environment.\\n\\nThe agent must learn to avoid fireballs shot by monsters from the other side of the room with the sole intent of killing the agent. There are no explicit rewards in this environment, so to mimic natural selection, the cumulative reward can be defined to be the number of time steps the agent manages to stay alive during a rollout. Each rollout in the environment runs for a maximum of 2100 time steps ( ∼ \\\\sim ∼ 60 seconds), and the task is considered solved if the average survival time over 100 consecutive rollouts is greater than 750 time steps ( ∼ \\\\sim ∼ 20 seconds) .\\n\\nProcedure\\n\\nThe setup of our VizDoom experiment is largely the same as the Car Racing task, except for a few key differences. In the Car Racing task, M is only trained to model the next z t z_{t} z\\u200bt\\u200b\\u200b. Since we want to build a world model we can train our agent in, our M model here will also predict whether the agent dies in the next frame (as a binary event d o n e t done_t done\\u200bt\\u200b\\u200b, or d t d_t d\\u200bt\\u200b\\u200b for short), in addition to the next frame z t z_t z\\u200bt\\u200b\\u200b.\\n\\nSince the M model can predict the d o n e done done state in addition to the next observation, we now have all of the ingredients needed to make a full RL environment. We first build an OpenAI Gym environment interface by wrapping a gym.Env interface over our M if it were a real Gym environment, and then train our agent inside of this virtual environment instead of using the actual environment.\\n\\nIn this simulation, we don\\'t need the V model to encode any real pixel frames during the hallucination process, so our agent will therefore only train entirely in a latent space environment. This has many advantages that will be discussed later on.\\n\\nThis virtual environment has an identical interface to the real environment, so after the agent learns a satisfactory policy in the virtual environment, we can easily deploy this policy back into the actual environment to see how well the policy transfers over.\\n\\nTo summarize the Take Cover experiment, below are the steps taken:\\n\\nCollect 10,000 rollouts from a random policy. Train VAE (V) to encode each frame into a latent vector z ∈ R 6 4 z \\\\in \\\\mathcal{R}^{64} z ∈ R \\u200b 6 4 \\u200b \\u200b , and use V to convert the images collected from (1) into the latent space representation. Train MDN-RNN (M) to model P ( z t + 1 , d t + 1 ∣ a t , z t , h t ) P(z_{t+1}, d_{t+1} \\\\; | \\\\; a_t, z_t, h_t) P ( z \\u200b t + 1 \\u200b \\u200b , d \\u200b t + 1 \\u200b \\u200b ∣ a \\u200b t \\u200b \\u200b , z \\u200b t \\u200b \\u200b , h \\u200b t \\u200b \\u200b ) . Define Controller (C) as a t = W c [ z t h t ] a_t = W_c \\\\; [z_t \\\\; h_t] a \\u200b t \\u200b \\u200b = W \\u200b c \\u200b \\u200b [ z \\u200b t \\u200b \\u200b h \\u200b t \\u200b \\u200b ] . Use CMA-ES to solve for a W c W_c W \\u200b c \\u200b \\u200b that maximizes the expected survival time inside the virtual environment. Use learned policy from (5) on actual Gym environment.\\n\\nModel Parameter Count VAE 4,446,915 MDN-RNN 1,678,785 Controller 1,088\\n\\nTraining Inside of the Dream\\n\\nAfter some training, our controller learns to navigate around the dream environment and escape from deadly fireballs launched by monsters generated by the M model. Our agent achieved a score in this virtual environment of ∼ \\\\sim ∼ 900 time steps.\\n\\nThe following demo shows how our agent navigates inside its own dream. The M model learns to generate monsters that shoot fireballs at the direction of the agent, while the C model discovers a policy to avoid these generated fireballs. Here, the V model is only used to decode the latent vectors z t z_t z\\u200bt\\u200b\\u200b produced by M into a sequence of pixel images we can see:\\n\\nInteractive Demo Our agent discovers a policy to avoid generated fireballs. In this demo, you can override the agent\\'s action by using the left/right keys on your keyboard, or by tapping on either side of the screen. You can also control the uncertainty level of the environment by adjusting the temperature parameter using slider on the bottom right.\\n\\nHere, our RNN-based world model is trained to mimic a complete game environment designed by human programmers. By learning only from raw image data collected from random episodes, it learns how to simulate the essential aspects of the game -- such as the game logic, enemy behaviour, physics, and also the 3D graphics rendering.\\n\\nFor instance, if the agent selects the left action, the M model learns to move the agent to the left and adjust its internal representation of the game states accordingly. It also learns to block the agent from moving beyond the walls on both sides of the level if the agent attempts to move too far in either direction. Occasionally, the M model needs to keep track of multiple fireballs being shot from several different monsters and coherently move them along in their intended directions. It must also detect whether the agent has been killed by one of these fireballs.\\n\\nUnlike the actual game environment, however, we note that it is possible to add extra uncertainty into the virtual environment, thus making the game more challenging in the dream environment. We can do this by increasing the temperature τ \\\\tau τ parameter during the sampling process of z t + 1 z_{t+1} z\\u200bt+1\\u200b\\u200b, as done in . By increasing the uncertainty, our dream environment becomes more difficult compared to the actual environment. The fireballs may move more randomly in a less predictable path compared to the actual game. Sometimes the agent may even die due to sheer misfortune, without explanation.\\n\\nWe find agents that perform well in higher temperature settings generally perform better in the normal setting. In fact, increasing τ \\\\tau τ helps prevent our controller from taking advantage of the imperfections of our world model -- we will discuss this in more depth later on.\\n\\nTransfer Policy to Actual Environment\\n\\nDeploying our policy learned inside of the dream RNN environment back into the actual VizDoom environment.\\n\\nWe took the agent trained inside of the virtual environment and tested its performance on the original VizDoom scenario. The score over 100 random consecutive trials is ∼ \\\\sim ∼ 1100 time steps, far beyond the required score of 750 time steps, and also much higher than the score obtained inside the more difficult virtual environment. We will discuss how this score compares to other models later on.\\n\\nCropped 64x64px frame of environment. Reconstruction from latent vector.\\n\\nWe see that even though the V model is not able to capture all of the details of each frame correctly, for instance, getting the number of monsters correct, the agent is still able to use the learned policy to navigate in the real environment. As the virtual environment cannot even keep track of the exact number of monsters in the first place, an agent that is able to survive the noisier and uncertain virtual nightmare environment will thrive in the original, cleaner environment.\\n\\nCheating the World Model\\n\\nIn our childhood, we may have encountered ways to exploit video games in ways that were not intended by the original game designer . Players discover ways to collect unlimited lives or health, and by taking advantage of these exploits, they can easily complete an otherwise difficult game. However, in the process of doing so, they may have forfeited the opportunity to learn the skill required to master the game as intended by the game designer.\\n\\nAgent discovers an adversarial policy that fools the monsters inside the world model into never launching any fireballs during some rollouts.\\n\\nFor instance, in our initial experiments, we noticed that our agent discovered an adversarial policy to move around in such a way so that the monsters in this virtual environment governed by the M model never shoots a single fireball during some rollouts. Even when there are signs of a fireball forming, the agent will move in a way to extinguish the fireballs magically as if it has superpowers in the environment.\\n\\nBecause our world model is only an approximate probabilistic model of the environment, it will occasionally generate trajectories that do not follow the laws governing the actual environment. As we saw previously, even the number of monsters on the other side of the room in the actual environment is not exactly reproduced by the world model. Like a child who learns that objects in the air usually fall to the ground, the child might also imagine unrealistic superheroes who fly across the sky. For this reason, our world model will be exploitable by the controller, even if in the actual environment such exploits do not exist.\\n\\nAnd since we are using the M model to generate a virtual dream environment for our agent, we are also giving the controller access to all of the hidden states of M. This is essentially granting our agent access to all of the internal states and memory of the game engine, rather than only the game observations that the player gets to see. Therefore our agent can efficiently explore ways to directly manipulate the hidden states of the game engine in its quest to maximize its expected cumulative reward. The weakness of this approach of learning a policy inside a learned dynamics model is that our agent can easily find an adversarial policy that can fool our dynamics model -- it\\'ll find a policy that looks good under our dynamics model, but will fail in the actual environment, usually because it visits states where the model is wrong because they are away from the training distribution.\\n\\nThis weakness could be the reason that many previous works that learn dynamics models of RL environments but don\\'t actually use those models to fully replace the actual environments . Like in the M model proposed in , the dynamics model is a deterministic differentiable model, making the model easily exploitable by the agent if it is not perfect. Using Bayesian models, as in PILCO , helps to address this issue with the uncertainty estimates to some extent, however, they do not fully solve the problem. Recent work combines the model-based approach with traditional model-free RL training by first initializing the policy network with the learned policy, but must subsequently rely on a model-free method to fine-tune this policy in the actual environment. In Learning to Think, it is acceptable that the RNN M isn\\'t always a reliable predictor. A (potentially evolution-based) RNN C can in principle learn to ignore a flawed M, or exploit certain useful parts of M for arbitrary computational purposes including hierarchical planning etc. This is not what we do here though -- our present approach is still closer to some of the old systems, where a RNN M is used to predict and plan ahead step by step. Unlike this early work, however, we use evolution for C (like in Learning to Think) rather than traditional RL combined with RNNs, which has the advantage of both simplicity and generality.\\n\\nTo make it more difficult for our C model to exploit deficiencies of the M model, we chose to use the MDN-RNN as the dynamics model, which models the distribution of possible outcomes in the actual environment, rather than merely predicting a deterministic future. Even if the actual environment is deterministic, the MDN-RNN would in effect approximate it as a stochastic environment. This has the advantage of allowing us to train our C model inside a more stochastic version of any environment -- we can simply adjust the temperature parameter τ \\\\tau τ to control the amount of randomness in the M model, hence controlling the tradeoff between realism and exploitability.\\n\\nUsing a mixture of Gaussian model may seem like overkill given that the latent space encoded with the VAE model is just a single diagonal Gaussian distribution. However, the discrete modes in a mixture density model is useful for environments with random discrete events, such as whether a monster decides to shoot a fireball or stay put. While a single diagonal Gaussian might be sufficient to encode individual frames, a RNN with a mixture density output layer makes it easier to model the logic behind a more complicated environment with discrete random states.\\n\\nFor instance, if we set the temperature parameter to a very low value of τ = 0 . 1 \\\\tau=0.1 τ=0.1, effectively training our C model with a M model that is almost identical to a deterministic LSTM, the monsters inside this dream environment fail to shoot fireballs, no matter what the agent does, due to mode collapse. The M model is not able to jump to another mode in the mixture of Gaussian model where fireballs are formed and shot. Whatever policy learned inside of this dream will achieve a perfect score of 2100 most of the time, but will obviously fail when unleashed into the harsh reality of the actual world, underperforming even a random policy.\\n\\nIn the following demo, we show that even low values of τ ∼ 0 . 5 \\\\tau \\\\sim 0.5 τ∼0.5 make it difficult for the MDN-RNN to generate fireballs:\\n\\nInteractive Demo For low τ \\\\tau τ settings, monsters in the M model rarely shoot fireballs. Even when you try to increase τ \\\\tau τ to 1.0 using the slider bar, the agent will occasionally extinguish fireballs still being formed, by fooling M.\\n\\nNote again, however, that the simpler and more robust approach in Learning to Think does not insist on using M for step by step planning. Instead, C can learn to use M\\'s subroutines (parts of M\\'s weight matrix) for arbitrary computational purposes but can also learn to ignore M when M is useless and when ignoring M yields better performance. Nevertheless, at least in our present C--M variant, M\\'s predictions are essential for teaching C, more like in some of the early C--M systems , but combined with evolution or black box optimization.\\n\\nBy making the temperature τ \\\\tau τ an adjustable parameter of the M model, we can see the effect of training the C model on generated virtual environments with different levels of uncertainty, and see how well they transfer over to the actual environment. We experimented with varying the temperature of the virtual environment and observing the resulting average score over 100 random rollouts of the actual environment after training the agent inside of the virtual environment with a given temperature:\\n\\n\\\\;\\\\; Temperature \\\\;\\\\; \\\\;\\\\; Score in Virtual Environment \\\\;\\\\; Score in Actual Environment \\\\;\\\\; 0.10 2086 ± \\\\pm ± 140 193 ± \\\\pm ± 58 0.50 2060 ± \\\\pm ± 277 196 ± \\\\pm ± 50 1.00 1145 ± \\\\pm ± 690 868 ± \\\\pm ± 511 1.15 918 ± \\\\pm ± 546 1092 ± \\\\pm ± 556 1.30 732 ± \\\\pm ± 269 753 ± \\\\pm ± 139 Random Policy Baseline N/A 210 ± \\\\pm ± 108 Gym Leaderboard N/A 820 ± \\\\pm ± 58\\n\\nWe see that while increasing the temperature of the M model makes it more difficult for the C model to find adversarial policies, increasing it too much will make the virtual environment too difficult for the agent to learn anything, hence in practice it is a hyperparameter we can tune. The temperature also affects the types of strategies the agent discovers. For example, although the best score obtained is 1092 ± \\\\pm ± 556 over 100 random trials using a temperature of 1.15, increasing τ \\\\tau τ a notch to 1.30 results in a lower score but at the same time a less risky strategy with a lower variance of returns. For comparison, the best score on the OpenAI Gym leaderboard is 820 ± \\\\pm ± 58.\\n\\nIterative Training Procedure\\n\\nIn our experiments, the tasks are relatively simple, so a reasonable world model can be trained using a dataset collected from a random policy. But what if our environments become more sophisticated? In any difficult environment, only parts of the world are made available to the agent only after it learns how to strategically navigate through its world.\\n\\nFor more complicated tasks, an iterative training procedure is required. We need our agent to be able to explore its world, and constantly collect new observations so that its world model can be improved and refined over time. An iterative training procedure, adapted from Learning To Think is as follows:\\n\\nInitialize M, C with random model parameters. Rollout to actual environment N N N times. Agent may learn during rollouts. Save all actions a t a_t a \\u200b t \\u200b \\u200b and observations x t x_t x \\u200b t \\u200b \\u200b during rollouts to storage device. Train M to model P ( x t + 1 , r t + 1 , a t + 1 , d t + 1 ∣ x t , a t , h t ) P(x_{t+1}, r_{t+1}, a_{t+1}, d_{t+1} \\\\; | \\\\; x_t, a_t, h_t) P ( x \\u200b t + 1 \\u200b \\u200b , r \\u200b t + 1 \\u200b \\u200b , a \\u200b t + 1 \\u200b \\u200b , d \\u200b t + 1 \\u200b \\u200b ∣ x \\u200b t \\u200b \\u200b , a \\u200b t \\u200b \\u200b , h \\u200b t \\u200b \\u200b ) and train C to optimize expected rewards inside of M. Go back to (2) if task has not been completed.\\n\\nWe have shown that one iteration of this training loop was enough to solve simple tasks. For more difficult tasks, we need our controller in Step 2 to actively explore parts of the environment that is beneficial to improve its world model. An exciting research direction is to look at ways to incorporate artificial curiosity and intrinsic motivation and information seeking abilities in an agent to encourage novel exploration . In particular, we can augment the reward function based on improvement in compression quality .\\n\\nIn the present approach, since M is a MDN-RNN that models a probability distribution for the next frame, if it does a poor job, then it means the agent has encountered parts of the world that it is not familiar with. Therefore we can adapt and reuse M\\'s training loss function to encourage curiosity. By flipping the sign of M\\'s loss function in the actual environment, the agent will be encouraged to explore parts of the world that it is not familiar with. The new data it collects may improve the world model.\\n\\nThe iterative training procedure requires the M model to not only predict the next observation x x x and d o n e done done, but also predict the action and reward for the next time step. This may be required for more difficult tasks. For instance, if our agent needs to learn complex motor skills to walk around its environment, the world model will learn to imitate its own C model that has already learned to walk. After difficult motor skills, such as walking, is absorbed into a large world model with lots of capacity, the smaller C model can rely on the motor skills already absorbed by the world model and focus on learning more higher level skills to navigate itself using the motor skills it had already learned. Another related connection is to muscle memory. For instance, as you learn to do something like play the piano, you no longer have to spend working memory capacity on translating individual notes to finger motions -- this all becomes encoded at a subconscious level.\\n\\n\\n\\nHow information becomes memory.\\n\\nAn interesting connection to the neuroscience literature is the work on hippocampal replay that examines how the brain replays recent experiences when an animal rests or sleeps. Replaying recent experiences plays an important role in memory consolidation -- where hippocampus-dependent memories become independent of the hippocampus over a period of time . As Foster puts it, replay is \"less like dreaming and more like thought\". We invite readers to read Replay Comes of Age for a detailed overview of replay from a neuroscience perspective with connections to theoretical reinforcement learning.\\n\\nIterative training could allow the C--M model to develop a natural hierarchical way to learn. Recent works about self-play in RL and PowerPlay also explores methods that lead to a natural curriculum learning , and we feel this is one of the more exciting research areas of reinforcement learning.\\n\\nRelated Work\\n\\nThere is extensive literature on learning a dynamics model, and using this model to train a policy. Many concepts first explored in the 1980s for feed-forward neural networks (FNNs) and in the 1990s for RNNs laid some of the groundwork for Learning to Think . The more recent PILCO is a probabilistic model-based search policy method designed to solve difficult control problems. Using data collected from the environment, PILCO uses a Gaussian process (GP) model to learn the system dynamics, and then uses this model to sample many trajectories in order to train a controller to perform a desired task, such as swinging up a pendulum, or riding a unicycle.\\n\\nWhile Gaussian processes work well with a small set of low dimensional data, their computational complexity makes them difficult to scale up to model a large history of high dimensional observations. Other recent works use Bayesian neural networks instead of GPs to learn a dynamics model. These methods have demonstrated promising results on challenging control tasks , where the states are known and well defined, and the observation is relatively low dimensional. Here we are interested in modelling dynamics observed from high dimensional visual data where our input is a sequence of raw pixel frames.\\n\\nIn robotic control applications, the ability to learn the dynamics of a system from observing only camera-based video inputs is a challenging but important problem. Early work on RL for active vision trained an FNN to take the current image frame of a video sequence to predict the next frame , and use this predictive model to train a fovea-shifting control network trying to find targets in a visual scene. To get around the difficulty of training a dynamical model to learn directly from high-dimensional pixel images, researchers explored using neural networks to first learn a compressed representation of the video frames. Recent work along these lines was able to train controllers using the bottleneck hidden layer of an autoencoder as low-dimensional feature vectors to control a pendulum from pixel inputs. Learning a model of the dynamics from a compressed latent space enable RL algorithms to be much more data-efficient . We invite readers to watch Finn\\'s lecture on Model-Based RL to learn more.\\n\\nVideo game environments are also popular in model-based RL research as a testbed for new ideas. Guzdial et al. used a feed-forward convolutional neural network (CNN) to learn a forward simulation model of a video game. Learning to predict how different actions affect future states in the environment is useful for game-play agents, since if our agent can predict what happens in the future given its current state and action, it can simply select the best action that suits its goal. This has been demonstrated not only in early work (when compute was a million times more expensive than today) but also in recent studies on several competitive VizDoom environments.\\n\\nThe works mentioned above use FNNs to predict the next video frame. We may want to use models that can capture longer term time dependencies. RNNs are powerful models suitable for sequence modelling . In a lecture called Hallucination with RNNs , Graves demonstrated the ability of RNNs to learn a probabilistic model of Atari game environments. He trained RNNs to learn the structure of such a game and then showed that they can hallucinate similar game levels on its own.\\n\\nA controller with internal RNN model of the world.\\n\\nUsing RNNs to develop internal models to reason about the future has been explored as early as 1990 in a paper called Making the World Differentiable , and then further explored in . A more recent paper called Learning to Think presented a unifying framework for building a RNN-based general problem solver that can learn a world model of its environment and also learn to reason about the future using this model. Subsequent works have used RNN-based models to generate many frames into the future , and also as an internal model to reason about the future .\\n\\nIn this work, we used evolution strategies (ES) to train our controller, as it offers many benefits. For instance, we only need to provide the optimizer with the final cumulative reward, rather than the entire history. ES is also easy to parallelize -- we can launch many instances of rollout with different solutions to many workers and quickly compute a set of cumulative rewards in parallel. Recent works have confirmed that ES is a viable alternative to traditional Deep RL methods on many strong baseline tasks.\\n\\nBefore the popularity of Deep RL methods , evolution-based algorithms have been shown to be effective at finding solutions for RL tasks . Evolution-based algorithms have even been able to solve difficult RL tasks from high dimensional pixel inputs . More recent works also combine VAE and ES, which is similar to our approach.\\n\\nDiscussion\\n\\nWe have demonstrated the possibility of training an agent to perform tasks entirely inside of its simulated latent space dream world. This approach offers many practical benefits. For instance, running computationally intensive game engines require using heavy compute resources for rendering the game states into image frames, or calculating physics not immediately relevant to the game. We may not want to waste cycles training an agent in the actual environment, but instead train the agent as many times as we want inside its simulated environment. Training agents in the real world is even more expensive, so world models that are trained incrementally to simulate reality may prove to be useful for transferring policies back to the real world. Our approach may complement sim2real approaches outlined in .\\n\\nFurthermore, we can take advantage of deep learning frameworks to accelerate our world model simulations using GPUs in a distributed environment. The benefit of implementing the world model as a fully differentiable recurrent computation graph also means that we may be able to train our agents in the dream directly using the backpropagation algorithm to fine-tune its policy to maximize an objective function .\\n\\nThe choice of using a VAE for the V model and training it as a standalone model also has its limitations, since it may encode parts of the observations that are not relevant to a task. After all, unsupervised learning cannot, by definition, know what will be useful for the task at hand. For instance, it reproduced unimportant detailed brick tile patterns on the side walls in the Doom environment, but failed to reproduce task-relevant tiles on the road in the Car Racing environment. By training together with a M model that predicts rewards, the VAE may learn to focus on task-relevant areas of the image, but the tradeoff here is that we may not be able to reuse the VAE effectively for new tasks without retraining.\\n\\nLearning task-relevant features has connections to neuroscience as well. Primary sensory neurons are released from inhibition when rewards are received, which suggests that they generally learn task-relevant features, rather than just any features, at least in adulthood .\\n\\nFuture work might explore the use of an unsupervised segmentation layer like in to extract better feature representations that might be more useful and interpretable compared to the representations learned using a VAE.\\n\\nAnother concern is the limited capacity of our world model. While modern storage devices can store large amounts of historical data generated using the iterative training procedure, our LSTM-based world model may not be able to store all of the recorded information inside its weight connections. While the human brain can hold decades and even centuries of memories to some resolution , our neural networks trained with backpropagation have more limited capacity and suffer from issues such as catastrophic forgetting . Future work may explore replacing the small MDN-RNN network with higher capacity models , or incorporating an external memory module , if we want our agent to learn to explore more complicated worlds.\\n\\nAncient drawing (1990) of a RNN-based controller interacting with an environment.\\n\\nLike early RNN-based C--M systems , ours simulates possible futures time step by time step, without profiting from human-like hierarchical planning or abstract reasoning, which often ignores irrelevant spatial-temporal details. However, the more general Learning To Think approach is not limited to this rather naive approach. Instead it allows a recurrent C to learn to address \"subroutines\" of the recurrent M, and reuse them for problem solving in arbitrary computable ways, e.g., through hierarchical planning or other kinds of exploiting parts of M\\'s program-like weight matrix. A recent One Big Net extension of the C--M approach collapses C and M into a single network, and uses PowerPlay-like behavioural replay (where the behaviour of a teacher net is compressed into a student net ) to avoid forgetting old prediction and control skills when learning new ones. Experiments with those more general approaches are left for future work.\\n\\nThis work is meant to be a live research project and will be revised and expanded over time. This article will be the first of a series of articles exploring World Models. If you would like to discuss any issues, give feedback, or even contribute to future work, please visit the GitHub repository of this page for more information.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('What worries me about AI', 'What worries me about AI\\n\\nDisclaimer: These are my own personal views. I do not speak for my employer. If you quote this article, please have the honesty to present these ideas as what they are: personal, speculative opinions, to be judged on their own merits.\\n\\nIf you were around in the 1980s and 1990s, you may remember the now-extinct phenomenon of “computerphobia”. I have personally witnessed it a few times as late as the early 2000s — as personal computers were introduced into our lives, in our workplaces and homes, quite a few people would react with anxiety, fear, or even aggressivity. While some of us were fascinated by computers and awestruck by the potential they could glimpse in them, most people didn’t understand them. They felt alien, abstruse, and in many ways, threatening. People feared getting replaced by technology.\\n\\nMost of us react to technological shifts with unease at best, panic at worst. Maybe that is true of any change at all. But remarkably, most of what we worry about ends up never happening.\\n\\nFast-forward a few years, and the computer-haters have learned to live with them and to use them for their own benefit. Computers did not replace us and trigger mass unemployment — and nowadays we couldn’t imagine life without our laptops, tablets, and smartphones. Threatening change has become comfortable status quo. But at the same time as our fears failed to materialize, computers and the internet have enabled threats that almost no one was warning us about in the 1980s and 1990s. Ubiquitous mass surveillance. Hackers going after our infrastructure or our personal data. Psychological alienation on social media. The loss of our patience and our ability to focus. The political or religious radicalization of easily-influenced minds online. Hostile foreign powers hijacking social networks to disrupt Western democracies.\\n\\nIf most of our fears turn out to be irrational, inversely, most of the truly worrying developments that have happened in the past as a result of technological change stem from things that most people didn’t worry about until it was already there. A hundred years ago, we couldn’t really forecast that the transportation and manufacturing technologies we were developing would enable a new form of industrial warfare that would wipe out tens of millions in two World Wars. We didn’t recognize early on that the invention of the radio would enable a new form of mass propaganda that would facilitate the rise of fascism in Italy and Germany. The progress of theoretical physics in the 1920s and 1930s wasn’t accompanied by anxious press articles about how these developments would soon enable thermonuclear weapons that would place the world forever under the threat of imminent annihilation. And today, even as alarms have been sounding for decades about the most dire problem of our times, climate, a large fraction (44%) of the American public still chooses to ignore it. As a civilization, we seem to be really bad at correctly identifying future threats and rightfully worrying about them, just as we seem to be extremely prone to panic due to irrational fears.\\n\\nToday, like many times in the past, we are faced with a new wave of radical change: cognitive automation, which could be broadly summed up under the keyword “AI”. And like many time in the past, we are worried that this new set of technologies will harm us — that AI will lead to mass unemployment, or that AI will gain an agency of its own, become superhuman, and choose to destroy us.\\n\\nImage source: facebook.com/zuck\\n\\nBut what if we’re worrying about the wrong thing, like we have almost every single time before? What if the real danger of AI was far remote from the “superintelligence” and “singularity” narratives that many are panicking about today? In this post, I’d like to raise awareness about what really worries me when it comes to AI: the highly effective, highly scalable manipulation of human behavior that AI enables, and its malicious use by corporations and governments. Of course, this is not the only tangible risk that arises from the development of cognitive technologies — there are many others, in particular issues related to the harmful biases of machine learning models. Other people are raising awareness of these problems far better than I could. I chose to write about mass population manipulation specifically because I see this risk as pressing and direly under-appreciated.\\n\\nThis risk is already a reality today, and a number of long-term technological trends are going to considerably amplify it over the next few decades. As our lives become increasingly digitized, social media companies get ever greater visibility into our lives and minds. At the same time, they gain increasing access to behavioral control vectors — in particular via algorithmic newsfeeds, which control our information consumption. This casts human behavior as an optimization problem, as an AI problem: it becomes possible for social media companies to iteratively tune their control vectors in order to achieve specific behaviors, just like a game AI would iterative refine its play strategy in order to beat a level, driven by score feedback. The only bottleneck to this process is the intelligence of the algorithm in the loop — and as it happens, the largest social network company is currently investing billions in fundamental AI research.\\n\\nLet me explain in detail.\\n\\nSocial media as a psychological panopticon\\n\\nIn the past 20 years, our private and public lives have moved online. We spend an ever greater fraction of each day staring at screens. Our world is moving to a state where most of what we do consists of digital information consumption, modification, or creation.\\n\\nA side effect of this long-term trend is that corporations and governments are now collecting staggering amounts of data about us, in particular through social network services. Who we communicate with. What we say. What content we’ve been consuming — images, movies, music, news. What mood we are in at specific times. Ultimately, almost everything we perceive and everything we do will end up recorded on some remote server.\\n\\nThis data, in theory, allows the entities that collect it to build extremely accurate psychological profiles of both individuals and groups. Your opinions and behavior can be cross-correlated with that of thousands of similar people, achieving an uncanny understanding of what makes you tick — probably more predictive than what yourself could achieve through mere introspection (for instance, Facebook “likes” enable algorithms to better assess your personality that your own friends could). This data makes it possible to predict a few days in advance when you will start a new relationship (and with whom), and when you will end your current one. Or who is at risk of suicide. Or which side you will ultimately vote for in an election, even while you’re still feeling undecided. And it’s not just individual-level profiling power — large groups can be even more predictable, as aggregating data points erases randomness and individual outliers.\\n\\nDigital information consumption as a psychological control vector\\n\\nPassive data collection is not where it ends. Increasingly, social network services are in control of what information we consume. What see in our newsfeeds has become algorithmically “curated”. Opaque social media algorithms get to decide, to an ever-increasing extent, which political articles we read, which movie trailers we see, who we keep in touch with, whose feedback we receive on the opinions we express.\\n\\nIntegrated over many years of exposure, the algorithmic curation of the information we consume gives the algorithms in charge considerable power over our lives — over who we are, who we become. If Facebook gets to decide, over the span of many years, which news you will see (real or fake), whose political status updates you’ll see, and who will see yours, then Facebook is in effect in control of your worldview and your political beliefs.\\n\\nFacebook’s business lies in influencing people. That’s what the service it sells to its customers — advertisers, including political advertisers. As such, Facebook has built a fine-tuned algorithmic engine that does just that. This engine isn’t merely capable of influencing your view of a brand or your next smart-speaker purchase. It can influence your mood, tuning the content it feeds you in order to make you angry or happy, at will. It may even be able to swing elections.\\n\\nHuman behavior as an optimization problem\\n\\nIn short, social network companies can simultaneously measure everything about us, and control the information we consume. And that’s an accelerating trend. When you have access to both perception and action, you’re looking at an AI problem. You can start establishing an optimization loop for human behavior, in which you observe the current state of your targets and keep tuning what information you feed them, until you start observing the opinions and behaviors you wanted to see. A large subset of the field of AI — in particular “reinforcement learning” — is about developing algorithms to solve such optimization problems as efficiently as possible, to close the loop and achieve full control of the target at hand — in this case, us. By moving our lives to the digital realm, we become vulnerable to that which rules it — AI algorithms.\\n\\nA reinforcement learning loop for human behavior\\n\\nThis is made all the easier by the fact that the human mind is highly vulnerable to simple patterns of social manipulation. Consider, for instance, the following vectors of attack:\\n\\nIdentity reinforcement: this is an old trick that has been leveraged since the first very ads in history, and still works just as well as it did the first time, consisting of associating a given view with markers that you identify with (or wish you did), thus making you automatically siding with the target view. In the context of AI-optimized social media consumption, a control algorithm could make sure that you only see content (whether news stories or posts from your friends) where the views it wants you to hold co-occur with your own identity markers, and inversely for views the algorithm wants you to move away from.\\n\\nNegative social reinforcement: if you make a post expressing a view that the control algorithm doesn’t want you to hold, the system can choose to only show your post to people who hold the opposite view (maybe acquaintances, maybe strangers, maybe bots), and who will harshly criticize it. Repeated many times, such social backlash is likely to make you move away from your initial views.\\n\\nPositive social reinforcement: if you make a post expressing a view that the control algorithm wants to spread, it can choose to only show it to people who will “like” it (it could even be bots). This will reinforce your belief and put you under the impression that you are part of a supportive majority.\\n\\nSampling bias: the algorithm may also be more likely to show you posts from your friends (or the media at large) that support the views it wants you to hold. Placed in such an information bubble, you will be under the impression that these views have much broader support than they do in reality.\\n\\nArgument personalization: the algorithm may observe that exposure to certain pieces of content, among people with a psychological profile close to yours, has resulted in the sort of view shift it seeks. It may then serve you with content that is expected to be maximally effective for someone with your particular views and life experience. In the long run, the algorithm may even be able to generate such maximally-effective content from scratch, specifically for you.\\n\\nFrom an information security perspective, you would call these vulnerabilities: known exploits that can be used to take over a system. In the case of the human minds, these vulnerabilities never get patched, they are just the way we work. They’re in our DNA. The human mind is a static, vulnerable system that will come increasingly under attack from ever-smarter AI algorithms that will simultaneously have a complete view of everything we do and believe, and complete control of the information we consume.\\n\\nThe current landscape\\n\\nRemarkably, mass population manipulation — in particular political control — arising from placing AI algorithms in charge of our information diet does not necessarily require very advanced AI. You don’t need self-aware, superintelligent AI for this to be a dire threat — current technology may well suffice. Social network companies have been working on it for a few years, with significant results. And while they may only be trying to maximize “engagement” and to influence your purchase decisions, rather than to manipulate your view of the world, the tools they’ve developed are already being hijacked by hostile state actors for political purposes — as seen in the 2016 Brexit referendum or the 2016 US presidential election. This is already our reality. But if mass population manipulation is already possible today — in theory — why hasn’t the world been upended yet?\\n\\nIn short, I think it’s because we’re really bad at AI. But that may be about to change.\\n\\nUntil 2015, all ad targeting algorithms across the industry were running on mere logistic regression. In fact, that’s still true to a large extent today — only the biggest players have switched to more advanced models. Logistic regression, an algorithm that predates the computing era, is one of the most basic techniques you could use for personalization. It is the reason why so many of the ads you see online are desperately irrelevant. Likewise, the social media bots used by hostile state actors to sway public opinion have little to no AI in them. They’re all extremely primitive. For now.\\n\\nMachine learning and AI have been making fast progress in recent years, and that progress is only beginning to get deployed in targeting algorithms and social media bots. Deep learning has only started to make its way into newsfeeds and ad networks in 2016. Who knows what will be next. It is quite striking that Facebook has been investing enormous amounts in AI research and development, with the explicit goal of becoming a leader in the field. When your product is a social newsfeed, what use are you going to make of natural language processing and reinforcement learning?\\n\\nWe’re looking at a company that builds fine-grained psychological profiles of almost two billion humans, that serves as a primary news source for many of them, that runs large-scale behavior manipulation experiments, and that aims at developing the best AI technology the world has ever seen. Personally, it scares me. And consider that Facebook may not even be the most worrying threat here. Ponder, for instance, China’s use of information control to enable unprecedented forms of totalitarianism, such as its “social credit system”. Many people like to pretend that large corporations are the all-powerful rulers of the modern world, but what power they hold is dwarfed by that of governments. If given algorithmic control over our minds, governments may well turn into far worst actors than corporations.\\n\\nNow, what can we do about it? How can we defend ourselves? As technologists, what can we do to avert the risk of mass manipulation via our social newsfeeds?\\n\\nThe flip side of the coin: what AI can do for us\\n\\nImportantly, the existence of this threat doesn’t mean that all algorithmic curation is bad, or that all targeted advertising is bad. Far from it. Both of these can serve a valuable purpose.\\n\\nWith the rise of the Internet and AI, placing algorithms in charge of our information diet isn’t just an inevitable trend — it’s a desirable one. As our lives become increasingly digital and connected, and as our world becomes increasingly information-intensive, we will need AI to serve as our interface to the world. In the long-run, education and self-development will be some of the most impactful applications of AI — and this will happen through dynamics that almost entirely mirror that of a nefarious AI-enabled newsfeed trying to manipulate you. Algorithmic information management has tremendous potential to help us, to empower individuals to realize more of their potential, and to help society better manage itself.\\n\\nThe issue is not AI itself. The issue is control.\\n\\nInstead of letting newsfeed algorithms manipulate the user to achieve opaque goals, such as swaying their political opinions, or maximally wasting their time, we should put the user in charge of the goals that the algorithms optimize for. We are talking, after all, about your news, your worldview, your friends, your life — the impact that technology has on you should naturally be placed under your own control. Information management algorithms should not be a mysterious force inflicted on us to serve ends that run opposite to our own interests; instead, they should be a tool in our hand. A tool that we can use for our own purposes, say, for education and personal instead of entertainment.\\n\\nHere’s an idea — any algorithmic newsfeed with significant adoption should:\\n\\nTransparently convey what objectives the feed algorithm is currently optimizing for, and how these objectives are affecting your information diet.\\n\\nGive you intuitive tools to set these goals yourself. For instance, it should be possible for you to configure your newsfeed to maximize learning and personal growth — in specific directions.\\n\\nFeature an always-visible measure of how much time you are spending on the feed.\\n\\nFeature tools to stay control of how much time you’re spending on the feed — such as a daily time target, past which the algorithm will seek to get you off the feed.\\n\\nAugmenting ourselves with AI while retaining control\\n\\nWe should build AI to serve humans, not to manipulate them for profit or political gain. What if newsfeed algorithms didn’t operate like casino operators or propagandists? What if instead, they were closer to a mentor or a good librarian, someone who used their keen understanding of your psychology — and that of millions of other similar people — to recommend to you that next book that will most resonate with your objectives and make you grow. A sort of navigation tool for your life — an AI capable of guiding you through the optimal path in experience space to get where you want to go. Can you imagine looking at your own life through the lens of a system that has seen millions of lives unfold? Or writing a book together with a system that has read every book? Or conducting research in collaboration with a system that sees the full scope of current human knowledge?\\n\\nIn products where you are fully in control of the AI that interacts with you, a more sophisticated algorithm, instead of being a threat, would be a net positive, letting you achieve your own goals more efficiently.\\n\\nBuilding the anti-Facebook\\n\\nIn summary, our future is one where AI will be our interface to the world — a world made of digital information. This can equally lead to empowering individuals to gain greater control over their lives, or to a total loss of agency. Unfortunately, social media is currently engaged on the wrong road. But it’s still early enough that we can reverse course.\\n\\nAs an industry, we need to develop product categories and markets where the incentives are aligned with placing the user in charge of the algorithms that affect them, instead of using AI to exploit the user’s mind for profit or political gain. We need to strive towards products that are the anti-Facebook.\\n\\nIn the far future, such products will likely take the form of AI assistants. Digital mentors programmed to help you, that put you in control of the objectives they pursue in their interactions with you. And in the present, search engines could be seen as an early, more primitive example of an AI-driven information interface that serves users instead of seeking to hijack their mental space. Search is a tool that you deliberately use to reach specific goals, rather than a passive always-on feed that elects what to show you. You tell it what to it should do for you. And instead of seeking to maximally waste your time, a search engine attempts to minimize the time it takes to go from question to answer, from problem to solution.\\n\\nYou may be thinking, since a search engine is still an AI layer between us and the information we consume, could it bias its results to attempt to manipulate us? Yes, that risk is latent in every information-management algorithm. But in stark contrast with social networks, market incentives in this case are actually aligned with users needs, pushing search engines to be as relevant and objective as possible. If they fail to be maximally useful, there’s essentially no friction for users to move to a competing product. And importantly, a search engine would have a considerably smaller psychological attack surface than a social newsfeed. The threat we’ve profiled in this post requires most of the following to be present in a product:\\n\\nBoth perception and action: not only should the product be in control of the information it shows you (news and social updates), it should also be able to “perceive” your current mental states via “likes”, chat messages, and status updates. Without both perception and action, no reinforcement learning loop can be established. A read-only feed would only be dangerous as a potential avenue for classical propaganda.\\n\\nCentrality to our lives: the product should be a major source of information for at least a subset of its users, and typical users should be spending several hours per day on it. A feed that is auxiliary and specialized (such as Amazon’s product recommendations) would not be a serious threat.\\n\\nA social component, enabling a far broader and more effective array of psychological control vectors (in particular social reinforcement). An impersonal newsfeed has only a fraction of the leverage over our minds.\\n\\nBusiness incentives set towards manipulating users and making users spend more time on the product.\\n\\nMost AI-driven information-management products don’t meet these requirements. Social networks, on the other hand, are a frightening combination of risk factors. As technologists, we should gravitate towards products that do not feature these characteristics, and push back against products that combine them all, if only because of their potential for dangerous misuse. Build search engines and digital assistants, not social newsfeeds. Make your recommendation engines transparent, configurable, and constructive, rather than slot-like machines that maximize “engagement” and wasted hours of human time. Invest your UI, UX, and AI expertise into building great configuration panels for your algorithm, to enable your users to use your product on their own terms.\\n\\nAnd importantly, we should educate users about these issues, so that they reject manipulative products, generating enough market pressure to align the incentives of the technology industry with that of consumers.\\n\\nConclusion: the fork in the road ahead\\n\\nNot only does social media know enough about us to build powerful psychological models of both individuals and groups, it is also increasingly in control of our information diet. It has access to a set of extremely effective psychological exploits to manipulate what we believe, how we feel, and what we do.\\n\\nA sufficiently advanced AI algorithm with access to both perception of our mental state, and action over our mental state, in a continuous loop, can be used to effectively hijack our beliefs and behavior.\\n\\nUsing AI as our interface to information isn’t the problem per se. Such AI interfaces, if well-designed, have the potential to be tremendously beneficial and empowering for all of us. The key factor: the user should stay fully in control of the algorithm’s objectives, using it as a tool to pursue their own goals (in the same way that you would use a search engine).\\n\\nAs technologists, we have a responsibility to push back against products that take away control, and dedicate our efforts to building information interfaces that place the user in charge. Don’t use AI as a tool to manipulate your users; instead, give AI to your users as a tool to gain greater agency over their circumstances.\\n\\nOne path leads to a place that really scares me. The other leads to a more humane future. There’s still time to take the better one. If you work on these technologies, keep this in mind. You may not have evil intentions. You may simply not care. You may simply value your RSUs more than our shared future. But whether or not you care, because you have a hand in shaping the infrastructure of the digital world, your choices affect us all. And you may eventually be held responsible for them.')"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Drones Enter Construction', 'Drones. They’re not just for military campaigns anymore. Since 2015, the use of unmanned aerial vehicles (UAVs) has exploded in terms of civilian and (non-military) industrial applications. While Gartner suggests that personal drone use is expected to reach 4.6 million units by 2020, Goldman Sachs sees military and civilian drone use becoming a $100 billion market by the same year.\\n\\nA drone is deployed using 3D Robotics’ Site Scan software. (Image courtesy of 3D Robotics.)\\n\\nThe overwhelming majority ($70 billion) of UAVs will still be used for making war, with consumer usage capturing another $17 billion, and the rest taken by industrial applications. Of that $17 billion, construction will represent the lion’s share with $11.2 billion. Anyone who has been following new technological developments in the architecture, engineering and construction (AEC) space knows that, yes, drones are becoming a big deal.\\n\\nWhy is that? To learn the answer, we spoke to a number of companies in the AEC industry to learn how this new crop of low-cost, high-tech UAVs is impacting the way we build buildings.\\n\\nThe Emergence of Civilian Drones\\n\\nIn 1898, Nikola Tesla demonstrated the first example of radio control (R/C), leading to the development of R/C boats and aircraft. The first fully functional R/C airplane is credited to Walter and William Good in 1937.\\n\\nThough UAVs have been used as unmanned attack vehicles since the 19th century, they were widely used for military target practice. The de Havilland Queen Bee was constructed in 1935 as a target for battleship gunners. The radio controlled, pilotless Queen Bee was dubbed a “drone” after male worker bees, who don’t have stingers or gather nectar and pollen.\\n\\nWinston Churchill at the launch of the de Havilland Queen Bee drone in 1941. (Image courtesy of the Imperial War Museum.)\\n\\nDrones wouldn’t be without their stingers for long. During the Cold War, the United States began its UAV program—code named “Red Wagon”—and sent its first armed drones into battle during the Vietnam War. Reconnaissance cameras became a feature of military drones during the War of Attrition between 1967 and 1970.\\n\\nAfter the 1973 flight of the Israeli Tadiran Mastiff—regarded by military historians to be the first modern battlefield drone for its data-linkage, live video-streaming and endurance-loitering—UAVs increasingly became a standard part of warfare. In the U.S., President George W. Bush relied heavily on the use of drones during the country’s invasions and covert operations in the Middle East and other regions before his successor President Barack Obama further increased dependence on the technology. At this point, military drones have earned a reputation for instilling fear in civilian populations that see them fly overhead.\\n\\nAn MQ-1 Predator drone firing a Hellfire missile. (Image courtesy of the U.S. Air Force.)\\n\\nAs military drones became more sophisticated, the electronics that powered them became smaller and less expensive, leading to the growth of radio-controlled products in the 1960s. As R/C-controlled models grew throughout the end of the 20th century, the first consumer quadcopters showed up as hobbyist kits in the late 1990s and early 2000s.\\n\\nThe original Draganflyer quad helicopter. (Image courtesy of Draganfly.)\\n\\nThe first consumer quadcopter may have been the Keyence Gyrosaucer II E-570, a toy released in 1991 that could hover for about three minutes. However, it was the Draganflyer quad helicopter, a kit released in 1999 that was popular among researchers, that evolved into the consumer drones available on the market today.\\n\\nIn 2010, Parrot released its AR.Drone, which was the first drone to be controlled over Wi-Fi and became the first commercially successful consumer drone. Alongside Parrot was 3D Robotics (3DR), which was founded in 2009 with devices based off of the open-source Ardupilot platform.\\n\\nIn 2015, 3DR released the Solo, the first quadcopter capable of taking professional quality video through the use of an attached GoPro camera. The next year, DJI released the Phantom 4, which featured computer vision and machine learning, rather than a GPS track, for tracking a person, an animal or an object.\\n\\nThe Solo from 3DR. (Image courtesy of 3D Robotics.)\\n\\nDrones Enter Construction\\n\\nThe initial drone kit community understood the implications of the technology early on. Drones could be used for everything from capturing aerial videos for marketing applications to spraying, monitoring and irrigating crops. In construction, the technology is particularly valuable for uses such as monitoring site progress and tracking material quantities.\\n\\n“The two most broad ways of kind of thinking about [how drones are used on a construction site]: understanding construction progress … using drone photos, drone maps and 3D models to map exactly what’s happened on a construction site,” DroneDeploy CEO Mike Winn explained. “The second way is using drones to help with site modeling—understanding the topology of the land before something gets built and bringing that into BIM software as a basis for planning.”\\n\\nKespry performs a scan and analysis of a construction site. (Image courtesy of Kespry.)\\n\\nWhereas surveying is traditionally performed using mobile scanning units, such as LiDAR systems from companies like Trimble and Leica, drones make it possible to capture the exterior of a site much more cost-effectively. High-definition video or photos taken by a drone that’s flown over the site can be stitched together into 3D models and 2D maps using software from a number of providers, including 3DR, DroneDeploy and Autodesk.\\n\\nTristan Randall, a strategic projects executive for Business Development at Autodesk, explained that such models or maps might be captured daily or biweekly to maintain a general record of what’s happening on site, monitoring the movement of materials or dirt on a site. This monitoring can be made more specific, to actually quantify a given task, such as mapping a stockpile to determine the volume of dirt available on site or that has been removed.\\n\\nA site reconstruction made using 3DR’s technology and Autodesk ReCap. (Image courtesy of 3D Robotics.)\\n\\n“We’re seeing a lot different use cases, ranging from things like inspecting wind turbines—which may not involve 3D modeling but just using imagery or video—all the way to mapping huge mining sites, where you actually take that data into one of our design platforms like Civil 3D and InfraWorks and perform quantity take-offs, design new facilities and perform quantitative analyses,” Randall said.\\n\\nJason Nichols, product marketing manager at Kespry, highlighted the use of preconstruction work. “D&T Construction is using drones to verify existing elevations on a job before it begins and track the progress of active earthwork operations,” Nichols said. “Companies like D&T are also improving the safety of field personnel by flying above active operations and can map areas in minutes that would typically take hours. The data results in a high resolution 2D and 3D imagery of the entire site that is used to create an accurate topographic map, and calculate cut/fill quantities for any earthwork activity.”\\n\\nHugh McFall, product marketing manager for 3DR, added, “[D]rones are making it possible to perform topographic surveys at least 6X faster and at a fraction of the cost. Regularly collecting data with drones helps construction firms perform ongoing QA/QC on their projects—namely, they can compare as-designed to as-built by overlaying their design files on an orthomosaic created by a drone software platform, and spot issues before they become too difficult or costly to fix. Drones are also helping improve safety on-site. For example, instead of having their field personnel climbing stockpiles and risking slipping and falling, many of our customers use Site Scan to perform these measurements from the sky, while their team is safe on the ground.”\\n\\nWhat’s It Take to Fly a Drone?\\n\\nWhereas, in the past, the ability to capture data of an entire site would require the use of a helicopter or other vehicle, a drone can now be used for a fraction of the cost. But how easy is it to fly a drone?\\n\\nDroneDeploy’s Mike Winn pointed out that modern drones don’t require the skills one might think to fly them. In fact, current drones fly autonomously, capturing images at designated control points, though they still require a human to monitor the flight. “Interestingly, in the past few years, the drones have very sophisticated sense and avoid technology. If there’s an object in their flight path, they can avoid it,” Winn said.\\n\\nHugh McFall, from 3DR, added that, despite this ease, companies may still need some help in deploying the technology. “That said, there are a few things that any company bringing drones onto their projects needs to consider. Our customer success team, who work directly with engineering and construction firms every day, often advises new customers to ensure they have a dedicated drone pilot who can ‘own’ drone operations and become the resident drone expert within their organization. They need to become Part 107 certified as well, which enables them to fly drones for commercial purposes,” he explained.\\n\\nProcessing Data\\n\\nGiven the $11.2 billion gold mine foretold by Goldman Sachs, it’s no surprise that a number of companies offer their own software solutions. Some may involve a mix of hardware and software, according to Kespry’s Jason Nichols.\\n\\n“Deploying drones into construction company’s workflow can be a seamless process, but it depends on the drone and software configuration. Certain drone solutions have a mixed fleet of hardware and software and require specialized training to fly and/or process the data,” Nichols noted. “When using a fully integrated, end-to-end solution, these types of complexities are eliminated, giving team members the ability to fly on an as-needed basis with delivery of instant data results.”\\n\\nFor this reason, Kespry attempts to create such an integrated solution (seen above). The entire process, from drone flight to data processing and customized reporting from the field, are all performed automatically. Nichols suggests that Kespry delivers “the highest level of GNSS accuracy with PPK (Post Processed Kinematic) technology,” which makes it possible to obtain “positional accuracies down to 2cm without using multiple control points or flight targets.”\\n\\nThough DroneDeploy also offers an integrated solution, Winn said that the company’s software works with a variety of third-party software. He estimated that, due in part to the power of DroneDeploy’s engine, the company performs the most drone data processing in the industry, producing the most drone data.\\n\\nThe software makes it possible to use a mobile phone or tablet to perform measurements of objects within the data captured by the drone, such as the heights of buildings, and apply a CAD overlay to compare models to the as-built structures. Moreover, DroneDeploy recently launched a Live Map tool (seen above) that actually makes drone data accessible and shareable across the cloud in real time, something that other software developers have yet to offer.\\n\\nAutodesk is a pioneer in the reality capture space, offering its ReCap software (seen below), which can use a series of photos, laser scans or a combination of media to generate point clouds and meshes from physical environments. Naturally, the data captured from drones can be easily integrated into Autodesk’s other software, such as AutoCAD for creating 2D drawings and Revit for 3D models.\\n\\nIn addition to ReCap, Autodesk has built out its Forge platform. Through access to Autodesk APIs, Autodesk partners, such as 3DR, are able to create apps that integrate directly into Autodesk software.\\n\\nTristan Randall, from Autodesk, outlined how, by giving partners access to Autodesk APIs, Forge is meant to enable a much quicker exploitation of drone technology. Different partners can create different apps that take advantage of Autodesk software in unique ways, not just capturing images and video, but providing measureable data for engineering and management decision-making.\\n\\nOne example of software built on the Forge platform is 3DR’s Site Scan (seen above). Site Scan plans and executes flights and captures data automatically. The information is then uploaded to 3DR’s cloud and ReCap, which process it into 3D point clouds, 3D mesh, 2D maps and digital elevation models (DEMs).\\n\\nObstacles to Using Drones in Construction\\n\\nAs with all new technologies that enter a legacy industry, there may be some reticence to the idea of learning novel approaches to performing one’s work. This may be particularly true of something like UAVs, which could seem more like a fad than a potentially crucial tool for the construction industry.\\n\\nJason Nichols, from Kespry, pointed out that there was some hesitancy several years ago, due in part to the quality of drones available at the time. “Drones capable of higher levels of accuracy were expensive and required significant training,” Nichols said. “The next phase of drone development was primarily intended for consumer purposes and lacked the accuracy needed to provide credible insights. The price point was low enough for companies to use the drones to record project progress with images and video. Today, drone technology has advanced far beyond consumer applications in which data is survey-grade accurate. Construction companies have overcome the hesitancy and are seeing the commercial drone as another tool in the toolbox for site survey work.”\\n\\n“[C]hallenges to more widespread use of drones in construction is more an issue of people’s acceptance of change and new approaches,” said Chris Harman, PE. Harman is senior engineer in SNC-Lavalin’s Atkins business, a leading design, engineering and project management consultancy. “A few years ago, I would have told you the main issue was data accuracy or that applications and workflows were too disjointed. Today, as authoring tools and the industry have evolved, I see the pushback coming from designers and clients who do not comprehend the value of drones. As exposure to drone capabilities increases and the need for streamlined workflows continues to rise, I believe drone use will become an integral part of the design and construction process.”\\n\\nMike Winn, from DroneDeploy, suggested that those most hesitant about using drones in construction might be the surveying community. After all, UAVs are able to capture much of the exterior of a building, bypassing the need for skilled surveyors and their laser scanning equipment.\\n\\nHowever, Winn sees drones as an opportunity for surveyors to complement their existing skillset by offering new insights. Moreover, even maps and models created with drone data may require official sign-offs from licensed surveyors. Therefore, Winn believes that the drone and surveying communities should work together to enable AEC firms to take advantage of drones for surveying.\\n\\nRegulating Commercial Drones\\n\\nThe biggest obstacle to drone adoption may be the Federal Aviation Administration (FAA) regulations put into place for drone usage.\\n\\nCommercial UAVs flight for drones under 55 pounds is regulated under Part 107 of the Federal Aviation Regulations (Part 107), which outlines such requirements as always keeping a drone in unaided sight, flying the drone in daylight or twilight with minimum weather visibility of three miles from the control area, a maximum flying height of 400ft, and a maximum speed of 100mph. Drones must be flown by a pilot certified under Part 107 or by someone under the supervision of someone with that certification.\\n\\nGiven the fact that this rule is used for commercial purposes, its requirements are not unreasonable, but it is one hurdle that businesses must face in order to use the technology on a construction site. Having this rule in place, however, makes understanding what is and isn’t allowed much easier, according to Autodesk’s Tristan Randall.\\n\\n“We’ve certainly removed many of the barriers that existed prior to the Part 107 rules, which made it very difficult and, honestly, kind of economically unviable in some sense to fly drones on the construction site, simply because each site was treated as a special case and each flight in fact potentially needed to be monitored, reported, etc.,” Randall said. “So, the Part 107 rules have really enabled an explosion in the use of drones on construction sites across the world because the U.S. has set a precedent for the rest of the world to look at these regulations and follow similar guidance.”\\n\\nFlying Drones at the Airport\\n\\nAutodesk, 3DR and Atkins learned about these regulations firsthand when working with the City of Atlanta to demolish and expand the North and South parking garage at Hartsfield-Jackson Atlanta International Airport (ATL). Not only did the companies have to qualify under Part 107, but they also had to so in actual Class B airspace at a major commercial airport. As a result, the project was awarded the first FAA clearance for such a flight.\\n\\nThe first commercial drone flight to take place at a major airport. (Image courtesy of Atkins Global.)\\n\\n“Currently, it is very difficult to fly near an airport, and you’d be amazed at how much of the urbanized United States is near an airport,” Chris Harman from Atkins said. “Unless we can make it simple to operate drones in controlled airspace, it will be very difficult to see a mass increase in usage on construction sites. However, I am encouraged by the work that the Federal Aviation Administration is doing with programs to streamline this process and see controlled airspace regulation as a critical challenge that needs to be addressed.”\\n\\nATL happens to be the busiest airport in the world, with five active runways that run in parallel. To capture the drone data, the project team had to execute in between active flights. On January 10, 2017, the team used 3DR’s Site Scan to perform seven flights over the airport’s parking garage area before uploading the data to 3DR’s cloud server, where they were processed automatically to create 3D point clouds of the site.\\n\\nA point cloud of the parking garage at ATL. (Image courtesy of Atkins Global.)\\n\\n“[B]y using a drone, we could capture over 700 images and cover 40 acres within a half-day time frame at [ATL],” Harman said. “The images were uploaded to a cloud-based program, where they were automatically processed into accurate 3D point clouds to create models. We will use the models to plan the demolition process and organize construction to minimize effects on the airport’s daily activities.”\\n\\n“It was unique being able to bring in so many different stakeholders to execute on a project,” Randall said. “We really had to coordinate with FAA, with the airport facilities, air traffic control, 3DR and Atkins, the project lead.”\\n\\nThe Future of Drones in Construction\\n\\nGiven the growth predicted by Gartner and Goldman Sachs, what does the future of drones in the AEC industry look like?\\n\\nHugh McFall, of 3DR, put an emphasis on the software. “Here at 3DR, we believe that the future of drones in construction isn’t about the drone per se; it’s about the data it collects,” McFall said. “Now that drones can be flown easily and reliably and collect a ton of useful data, the challenge going forward is to best integrate this data seamlessly across an organization and its projects, and making this data accessible to key stakeholders such as owners, subcontractors and more.”\\n\\nAs a software company, Autodesk, too, is focused on what happens to the data. Specifically, the company wants that data dealt with in more automatic and sophisticated ways. This includes automatically extracting insights using machine learning, image analysis and feature recognition.\\n\\n“Those are the tools that are going to scale the technology up, taking an already robust return on investment and making an indispensable tool for any construction site,” Randall said. “We’re likely 1-2 years away from that happening.”\\n\\nChris Harman, from Atkins, was able to provide his insights from the point of view of an engineering firm. In addition to improved software, which will make it possible to capture discrepancies between as-built projects and plans, as well as actual progress versus the project schedule, Harman sees developments occurring as they relate to data.\\n\\n“As we focus on data capture and metrics, I think we will see the industry plan and implement projects more efficiently,” Harman said. “In the future, a daily drone flight of a project site may not only provide data to build a model that checks designs against schedules; it could also report on equipment, personnel, means and methods for construction. Data may be scanned by artificially intelligent applications to assess safety or environmental hazards, which will lead to smarter decisions and more efficient project delivery.”\\n\\nOne company that is already implementing deep learning and machine vision in construction is a new startup called Doxel. Because drones only capture the exterior of a project site, Doxel is using a combination of drones and rovers armed with LiDAR scanners and HD cameras to roam the interior of a project at the end of the workday.\\n\\nThis data is processed by Doxel’s artificial intelligence (AI) software, which cross-references as-built 3D data with BIM models, schedules and project budgets to catch installation errors and track installed quantities to monitor daily progress and expenditures. By providing this data within the span of 12 to 24 hours after work has been performed—rather than four to eight weeks later, as occurs with typical projects that don’t use drones or AI—companies may be able to stay within budget and on schedule with fewer errors. (Read our interview with the CEO of Doxel here.)\\n\\n\\n\\n\\n\\n\\n\\nIn other words, the developments suggested by those interviewed are already taking place. All parties agreed that drones will only see an increased place on construction sites. As firms continue to experiment with drones, their role may become such that, within the next decade, days without drones in AEC will be but a foggy memory.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Robotics Open letter to the European Commission', '> Nathalie Nevejans, Lecturer in Law, University of Artois (France), Member of the CNRS Ethics Committee COMETS. Expert in Ethics in Robotics at the European Parliament, Member of the Institute for the Study of Man-Robot Relations (IERHR).\\n\\n> Raja Chatila, Institute of Intelligent Systems and Robotics – Sorbonne University and CNRS Professor (France). Director of the Institute of Intelligent Systems and Robotics (ISIR). Former President of the IEEE Robotics and Automation Society (2014-2015). Member of the French Commission on the Ethics of Research on Digital Science and Technology (CERNA). Chair of the IEEE Global Initiative on Ethics of Autonomous and Intelligence Systems\\n\\n> Jozef Glasa, Chair Ethics Committee, Ministry of Health (Slovak Republic); Delegate Member, Committee on Bioethics (DH-BIO, formerly CDBI), Council of Europe; Slovak Medical University in Bratislava, Institute of Health Care Ethics/Institute of Clinical and Experimental Pharmacology Head/Deputy Head. Professor of Laboratory Medicine\\n\\n> Noel Sharkey, Emeritus Professor AI and Robotics (United Kingdom) Foundation for Responsible Robotics\\n\\n> Alexandre Pereira, Faculty of Law Professor at University of Coimbra, (Portugal). IT Law & Cybersecurity researcher and professor.\\n\\n> Sanja Dogramadzi, Professor of Medical Robotics at UWE, Bristol Robotics Laboratory (United Kingdom). Member of British Standard Institute contributing to developments of Robotics standards for Service and Medical Robotics\\n\\n> Véronique Aubergé, CNRS Researcher at LIG Grenoble (France). Scientific Head of Living Lab DOMUS-LI, Scientific co-Head of Robo’ethics Rectorate of Grenoble, INP Foundation President of Ethics Comittee for Social Robotics of SFTAG.\\n\\n> Max Dauchet, Emeritus professor, University of Lille (France) – Chair of the French Commission on the Ethics of Research on Digital Science and Technology (CERNA)\\n\\n> Wolfgang M. Schröder, Professor of Philosophy, Institute for Systematic Theology, University of Würzburg (Germany) – Member of the AG Digital Ethics / Initiative D21 Berlin – Member of the FAG Political Theory & Philosophy, DGPhil.\\n\\n> Hugues Bersini, Professor of Artificial Intelligence, co-Director of Institute or Interdisciplinary Research and AI Development IRIDIA , Université Libre of Brussels ULB (Belgium) – Member of the Belgium Academy of Science\\n\\n> Georg Martius, Research Group Leader Max Planck Institute for Intelligent Systems (MPG) (Germany)\\n\\n> Koen Hindriks, Interactive Robotics Associate Professor Interactive Intelligence at Delft University of Technology (Netherlands) – CEO of Interactive Robotics\\n\\n> Ulrich Borgolte, Senior lecturer in Robotics and Mechatronics at FernUniversität in Hagen (Germany)\\n\\n> Benjamin Frugier, Executive Director of French Federation of Mechanical Engineering Industries FEM (France). The FEM is in charge of economic and technical interests of 25 trade associations, representing companies in the three following fields: Equipment, Transformation and Precision\\n\\n> Gyorgy Cserey, Head of the Sensory Robotics Lab. at the Faculty of Information Technology and Bionics at Pazmany Peter Catholic University Associate professor (Hungary)\\n\\n> Domenico G. Sorrenti, Bicocca Associate Professor, Dept. Computer Science, Universita di Milano (Italy) – Head of the Robotic Perception Laboratory\\n\\n> Margo Dessertenne, Trade groups manager at Symop Robotics (France). Symop is a French professional organization gathering 270 SME enterprises and large corporations in Automation, robotics, …\\n\\n> Lionel Sublet, CEO, Techplus , Symop Robotic Group General Manager (France).\\n\\n> Yves Poullet, Dean and Emeritus Professor of Law, University of Namur and Catholic University of Lille (Belgium). Expert in ethics at UNESCO and Council of Europe\\n\\n> Laurence Devillers, Professor of Artificial Intelligence at Sorbonne University, (France) Researcher at LIMSI « Computer Science Laboratory for Mechanics and Engineering Sciences », CNRS, Member of the French Commission on the Ethics of Research on Digital Science and Technology (CERNA), Member of the IEEE Global Initiative on Ethics of Autonomous and Intelligence Systems.\\n\\n> Jean-Claude Heudin, Professor of Artificial Intelligence, University Paris Sud (France), Former Director and co-founder of the Institute of Internet and Multimedia, Expert to the European Union on the «Future Emerging technologies» project.\\n\\n> Ante Čović, Director at the Centre of excellence for Integrative Bioethics, Vice-Rector for Organisation, Human Resources Development and Cross-University Cooperation (Croatia). .\\n\\n> Serge Tisseron, Psychiatrist, Université Paris VII Denis Diderot (France) – Member of the Academy of Technologies (Institute for the Study of Robot-Human relationship, IERHR)\\n\\n> Jean-Paul Laumond, CNRS Research Director (France). Member of the French Academy of Technologies and the French Academy of Sciences\\n\\n> Jasna Lipozenčić, President of the Academy of Medical Sciences of Croatia (Croatia)\\n\\n> John Michael Robson, Emeritus Professor, University of Bordeaux (France). Researcher in Algorithms, Distributed Computing and Theory of Computation at LaBRI (Laboratory of Bordeaux Research in Computing)\\n\\n> Joanna Bryson, Reader (Associate Professor), University of Bath (United Kingdom). Expert in both AI, safety and transparency in intelligent systems, and AI ethics.\\n\\n> Alan Winfield, Professor of Robot Ethics at UWE Bristol, Robotics Laboratory, (United Kingdom). Member of the British Standards Institute working group on Robot Ethics\\u200b; Member of the EC Human Brain Project\\u200b Ethics Advisory Board; Member, Executive committee, IEEE Global Initiative on Ethics of Autonomous and Intelligence Systems and Chair, IEEE Standards Working Group P7001 on Transparency in Autonomous Systems; Member, WEF Global Futures \\u200bCouncil on \\u200bThe Future of Technology, Values and Policy\\u200b\\n\\n> Kathleen Richardson, Professor of Ethics and Culture of Robots and AI, De Montfort University (United Kingdom), Founder of the Campaign Against Sex Robots\\n\\n> Miguel Enrique Burguete, PhD. Professor of Philosophical Anthropology and Biopolitics, Institute of Life Sciences and Observatory of Bioethics of the Catholic University of Valencia (Spain)\\n\\n> Calum MacKellar, Director of Research, Scottish Council on Human Bioethics, Ethics Committee (Scotland)\\n\\n> Richard Everson, Professor of Machine Learning , University of Exeter (United Kingdom), Director of the Exeter University Institute for Data Science and Artificial Intelligence\\n\\n> Richard Ashcroft, Professor of Bioethics, School of Law, Queen Mary University of London (United Kingdom), Director of the LLM in Medical Law\\n\\n> Isabelle Poirot-Mazères, Public law and Heath Professor University of Toulouse I Capitole, (France)\\n\\n> Thierry Magnin, Professor in Ethics of Sciences and Technologies, Rector of the Lyon Catholic University (France). Member of The French National Academy of Technologies.\\n\\n> Jean-Michel Besnier, Emeritus Philosophy Professor at University of Paris-Sorbonne, (France), Member of CNRS and INRA Ethics Committees.\\n\\n> Christopher Markou, The University of Cambridge, Lecturer (United Kingdom). Legal Expert Committee, Responsible for Robotics.\\n\\n> David Doat, Chair of Ethics & Transhumanism, Chair of Lille Catholic University (France)\\n\\n> Rónán Kennedy, Lecturer in Law, National University of Ireland Galway (Ireland).')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The AI Revolution Hasn’t Happened Yet', 'Photo credit: Peg Skorpinski\\n\\nArtificial Intelligence — The Revolution Hasn’t Happened Yet\\n\\nArtificial Intelligence (AI) is the mantra of the current era. The phrase is intoned by technologists, academicians, journalists and venture capitalists alike. As with many phrases that cross over from technical academic fields into general circulation, there is significant misunderstanding accompanying the use of the phrase. But this is not the classical case of the public not understanding the scientists — here the scientists are often as befuddled as the public. The idea that our era is somehow seeing the emergence of an intelligence in silicon that rivals our own entertains all of us — enthralling us and frightening us in equal measure. And, unfortunately, it distracts us.\\n\\nThere is a different narrative that one can tell about the current era. Consider the following story, which involves humans, computers, data and life-or-death decisions, but where the focus is something other than intelligence-in-silicon fantasies. When my spouse was pregnant 14 years ago, we had an ultrasound. There was a geneticist in the room, and she pointed out some white spots around the heart of the fetus. “Those are markers for Down syndrome,” she noted, “and your risk has now gone up to 1 in 20.” She further let us know that we could learn whether the fetus in fact had the genetic modification underlying Down syndrome via an amniocentesis. But amniocentesis was risky — the risk of killing the fetus during the procedure was roughly 1 in 300. Being a statistician, I determined to find out where these numbers were coming from. To cut a long story short, I discovered that a statistical analysis had been done a decade previously in the UK, where these white spots, which reflect calcium buildup, were indeed established as a predictor of Down syndrome. But I also noticed that the imaging machine used in our test had a few hundred more pixels per square inch than the machine used in the UK study. I went back to tell the geneticist that I believed that the white spots were likely false positives — that they were literally “white noise.” She said “Ah, that explains why we started seeing an uptick in Down syndrome diagnoses a few years ago; it’s when the new machine arrived.”\\n\\nWe didn’t do the amniocentesis, and a healthy girl was born a few months later. But the episode troubled me, particularly after a back-of-the-envelope calculation convinced me that many thousands of people had gotten that diagnosis that same day worldwide, that many of them had opted for amniocentesis, and that a number of babies had died needlessly. And this happened day after day until it somehow got fixed. The problem that this episode revealed wasn’t about my individual medical care; it was about a medical system that measured variables and outcomes in various places and times, conducted statistical analyses, and made use of the results in other places and times. The problem had to do not just with data analysis per se, but with what database researchers call “provenance” — broadly, where did data arise, what inferences were drawn from the data, and how relevant are those inferences to the present situation? While a trained human might be able to work all of this out on a case-by-case basis, the issue was that of designing a planetary-scale medical system that could do this without the need for such detailed human oversight.\\n\\nI’m also a computer scientist, and it occurred to me that the principles needed to build planetary-scale inference-and-decision-making systems of this kind, blending computer science with statistics, and taking into account human utilities, were nowhere to be found in my education. And it occurred to me that the development of such principles — which will be needed not only in the medical domain but also in domains such as commerce, transportation and education — were at least as important as those of building AI systems that can dazzle us with their game-playing or sensorimotor skills.\\n\\nWhether or not we come to understand “intelligence” any time soon, we do have a major challenge on our hands in bringing together computers and humans in ways that enhance human life. While this challenge is viewed by some as subservient to the creation of “artificial intelligence,” it can also be viewed more prosaically — but with no less reverence — as the creation of a new branch of engineering. Much like civil engineering and chemical engineering in decades past, this new discipline aims to corral the power of a few key ideas, bringing new resources and capabilities to people, and doing so safely. Whereas civil engineering and chemical engineering were built on physics and chemistry, this new engineering discipline will be built on ideas that the preceding century gave substance to — ideas such as “information,” “algorithm,” “data,” “uncertainty,” “computing,” “inference,” and “optimization.” Moreover, since much of the focus of the new discipline will be on data from and about humans, its development will require perspectives from the social sciences and humanities.\\n\\nWhile the building blocks have begun to emerge, the principles for putting these blocks together have not yet emerged, and so the blocks are currently being put together in ad-hoc ways.\\n\\nThus, just as humans built buildings and bridges before there was civil engineering, humans are proceeding with the building of societal-scale, inference-and-decision-making systems that involve machines, humans and the environment. Just as early buildings and bridges sometimes fell to the ground — in unforeseen ways and with tragic consequences — many of our early societal-scale inference-and-decision-making systems are already exposing serious conceptual flaws.\\n\\nAnd, unfortunately, we are not very good at anticipating what the next emerging serious flaw will be. What we’re missing is an engineering discipline with its principles of analysis and design.\\n\\nThe current public dialog about these issues too often uses “AI” as an intellectual wildcard, one that makes it difficult to reason about the scope and consequences of emerging technology. Let us begin by considering more carefully what “AI” has been used to refer to, both recently and historically.\\n\\nMost of what is being called “AI” today, particularly in the public sphere, is what has been called “Machine Learning” (ML) for the past several decades. ML is an algorithmic field that blends ideas from statistics, computer science and many other disciplines (see below) to design algorithms that process data, make predictions and help make decisions. In terms of impact on the real world, ML is the real thing, and not just recently. Indeed, that ML would grow into massive industrial relevance was already clear in the early 1990s, and by the turn of the century forward-looking companies such as Amazon were already using ML throughout their business, solving mission-critical back-end problems in fraud detection and logistics-chain prediction, and building innovative consumer-facing services such as recommendation systems. As datasets and computing resources grew rapidly over the ensuing two decades, it became clear that ML would soon power not only Amazon but essentially any company in which decisions could be tied to large-scale data. New business models would emerge. The phrase “Data Science” began to be used to refer to this phenomenon, reflecting the need of ML algorithms experts to partner with database and distributed-systems experts to build scalable, robust ML systems, and reflecting the larger social and environmental scope of the resulting systems.\\n\\nThis confluence of ideas and technology trends has been rebranded as “AI” over the past few years. This rebranding is worthy of some scrutiny.\\n\\nHistorically, the phrase “AI” was coined in the late 1950’s to refer to the heady aspiration of realizing in software and hardware an entity possessing human-level intelligence. We will use the phrase “human-imitative AI” to refer to this aspiration, emphasizing the notion that the artificially intelligent entity should seem to be one of us, if not physically at least mentally (whatever that might mean). This was largely an academic enterprise. While related academic fields such as operations research, statistics, pattern recognition, information theory and control theory already existed, and were often inspired by human intelligence (and animal intelligence), these fields were arguably focused on “low-level” signals and decisions. The ability of, say, a squirrel to perceive the three-dimensional structure of the forest it lives in, and to leap among its branches, was inspirational to these fields. “AI” was meant to focus on something different — the “high-level” or “cognitive” capability of humans to “reason” and to “think.” Sixty years hence, however, high-level reasoning and thought remain elusive. The developments which are now being called “AI” arose mostly in the engineering fields associated with low-level pattern recognition and movement control, and in the field of statistics — the discipline focused on finding patterns in data and on making well-founded predictions, tests of hypotheses and decisions.\\n\\nIndeed, the famous “backpropagation” algorithm that was rediscovered by David Rumelhart in the early 1980s, and which is now viewed as being at the core of the so-called “AI revolution,” first arose in the field of control theory in the 1950s and 1960s. One of its early applications was to optimize the thrusts of the Apollo spaceships as they headed towards the moon.\\n\\nSince the 1960s much progress has been made, but it has arguably not come about from the pursuit of human-imitative AI. Rather, as in the case of the Apollo spaceships, these ideas have often been hidden behind the scenes, and have been the handiwork of researchers focused on specific engineering challenges. Although not visible to the general public, research and systems-building in areas such as document retrieval, text classification, fraud detection, recommendation systems, personalized search, social network analysis, planning, diagnostics and A/B testing have been a major success — these are the advances that have powered companies such as Google, Netflix, Facebook and Amazon.\\n\\nOne could simply agree to refer to all of this as “AI,” and indeed that is what appears to have happened. Such labeling may come as a surprise to optimization or statistics researchers, who wake up to find themselves suddenly referred to as “AI researchers.” But labeling of researchers aside, the bigger problem is that the use of this single, ill-defined acronym prevents a clear understanding of the range of intellectual and commercial issues at play.\\n\\nThe past two decades have seen major progress — in industry and academia — in a complementary aspiration to human-imitative AI that is often referred to as “Intelligence Augmentation” (IA). Here computation and data are used to create services that augment human intelligence and creativity. A search engine can be viewed as an example of IA (it augments human memory and factual knowledge), as can natural language translation (it augments the ability of a human to communicate). Computing-based generation of sounds and images serves as a palette and creativity enhancer for artists. While services of this kind could conceivably involve high-level reasoning and thought, currently they don’t — they mostly perform various kinds of string-matching and numerical operations that capture patterns that humans can make use of.\\n\\nHoping that the reader will tolerate one last acronym, let us conceive broadly of a discipline of “Intelligent Infrastructure” (II), whereby a web of computation, data and physical entities exists that makes human environments more supportive, interesting and safe. Such infrastructure is beginning to make its appearance in domains such as transportation, medicine, commerce and finance, with vast implications for individual humans and societies. This emergence sometimes arises in conversations about an “Internet of Things,” but that effort generally refers to the mere problem of getting “things” onto the Internet — not to the far grander set of challenges associated with these “things” capable of analyzing those data streams to discover facts about the world, and interacting with humans and other “things” at a far higher level of abstraction than mere bits.\\n\\nFor example, returning to my personal anecdote, we might imagine living our lives in a “societal-scale medical system” that sets up data flows, and data-analysis flows, between doctors and devices positioned in and around human bodies, thereby able to aid human intelligence in making diagnoses and providing care. The system would incorporate information from cells in the body, DNA, blood tests, environment, population genetics and the vast scientific literature on drugs and treatments. It would not just focus on a single patient and a doctor, but on relationships among all humans — just as current medical testing allows experiments done on one set of humans (or animals) to be brought to bear in the care of other humans. It would help maintain notions of relevance, provenance and reliability, in the way that the current banking system focuses on such challenges in the domain of finance and payment. And, while one can foresee many problems arising such a system — involving privacy issues, liability issues, security issues, etc — these problems should properly be viewed as challenges, not show-stoppers.\\n\\nWe now come to a critical issue: Is working on classical human-imitative AI the best or only way to focus on these larger challenges? Some of the most heralded recent success stories of ML have in fact been in areas associated with human-imitative AI — areas such as computer vision, speech recognition, game-playing and robotics. So perhaps we should simply await further progress in domains such as these. There are two points to make here. First, although one would not know it from reading the newspapers, success in human-imitative AI has in fact been limited — we are very far from realizing human-imitative AI aspirations. Unfortunately the thrill (and fear) of making even limited progress on human-imitative AI gives rise to levels of over-exuberance and media attention that is not present in other areas of engineering.\\n\\nSecond, and more importantly, success in these domains is neither sufficient nor necessary to solve important IA and II problems. On the sufficiency side, consider self-driving cars. For such technology to be realized, a range of engineering problems will need to be solved that may have little relationship to human competencies (or human lack-of-competencies). The overall transportation system (an II system) will likely more closely resemble the current air-traffic control system than the current collection of loosely-coupled, forward-facing, inattentive human drivers. It will be vastly more complex than the current air-traffic control system, specifically in its use of massive amounts of data and adaptive statistical modeling to inform fine-grained decisions. It is those challenges that need to be in the forefront, and in such an effort a focus on human-imitative AI may be a distraction.\\n\\nAs for the necessity argument, it is sometimes argued that the human-imitative AI aspiration subsumes IA and II aspirations, because a human-imitative AI system would not only be able to solve the classical problems of AI (as embodied, e.g., in the Turing test), but it would also be our best bet for solving IA and II problems. Such an argument has little historical precedent. Did civil engineering develop by envisaging the creation of an artificial carpenter or bricklayer? Should chemical engineering have been framed in terms of creating an artificial chemist? Even more polemically: if our goal was to build chemical factories, should we have first created an artificial chemist who would have then worked out how to build a chemical factory?\\n\\nA related argument is that human intelligence is the only kind of intelligence that we know, and that we should aim to mimic it as a first step. But humans are in fact not very good at some kinds of reasoning — we have our lapses, biases and limitations. Moreover, critically, we did not evolve to perform the kinds of large-scale decision-making that modern II systems must face, nor to cope with the kinds of uncertainty that arise in II contexts. One could argue\\n\\nthat an AI system would not only imitate human intelligence, but also “correct” it, and would also scale to arbitrarily large problems. But we are now in the realm of science fiction — such speculative arguments, while entertaining in the setting of fiction, should not be our principal strategy going forward in the face of the critical IA and II problems that are beginning to emerge. We need to solve IA and II problems on their own merits, not as a mere corollary to an human-imitative AI agenda.\\n\\nIt is not hard to pinpoint algorithmic and infrastructure challenges in II systems that are not central themes in human-imitative AI research. II systems require the ability to manage distributed repositories of knowledge that are rapidly changing and are likely to be globally incoherent. Such systems must cope with cloud-edge interactions in making timely, distributed decisions and they must deal with long-tail phenomena whereby there is lots of data on some individuals and little data on most individuals. They must address the difficulties of sharing data across administrative and competitive boundaries. Finally, and of particular importance, II systems must bring economic ideas such as incentives and pricing into the realm of the statistical and computational infrastructures that link humans to each other and to valued goods. Such II systems can be viewed as not merely providing a service, but as creating markets. There are domains such as music, literature and journalism that are crying out for the emergence of such markets, where data analysis links producers and consumers. And this must all be done within the context of evolving societal, ethical and legal norms.\\n\\nOf course, classical human-imitative AI problems remain of great interest as well. However, the current focus on doing AI research via the gathering of data, the deployment of “deep learning” infrastructure, and the demonstration of systems that mimic certain narrowly-defined human skills — with little in the way of emerging explanatory principles — tends to deflect attention from major open problems in classical AI. These problems include the need to bring meaning and reasoning into systems that perform natural language processing, the need to infer and represent causality, the need to develop computationally-tractable representations of uncertainty and the need to develop systems that formulate and pursue long-term goals. These are classical goals in human-imitative AI, but in the current hubbub over the “AI revolution,” it is easy to forget that they are not yet solved.\\n\\nIA will also remain quite essential, because for the foreseeable future, computers will not be able to match humans in their ability to reason abstractly about real-world situations. We will need well-thought-out interactions of humans and computers to solve our most pressing problems. And we will want computers to trigger new levels of human creativity, not replace human creativity (whatever that might mean).\\n\\nIt was John McCarthy (while a professor at Dartmouth, and soon to take a\\n\\nposition at MIT) who coined the term “AI,” apparently to distinguish his\\n\\nbudding research agenda from that of Norbert Wiener (then an older professor at MIT). Wiener had coined “cybernetics” to refer to his own vision of intelligent systems — a vision that was closely tied to operations research, statistics, pattern recognition, information theory and control theory. McCarthy, on the other hand, emphasized the ties to logic. In an interesting reversal, it is Wiener’s intellectual agenda that has come to dominate in the current era, under the banner of McCarthy’s terminology. (This state of affairs is surely, however, only temporary; the pendulum swings more in AI than\\n\\nin most fields.)\\n\\nBut we need to move beyond the particular historical perspectives of McCarthy and Wiener.\\n\\nWe need to realize that the current public dialog on AI — which focuses on a narrow subset of industry and a narrow subset of academia — risks blinding us to the challenges and opportunities that are presented by the full scope of AI, IA and II.\\n\\nThis scope is less about the realization of science-fiction dreams or nightmares of super-human machines, and more about the need for humans to understand and shape technology as it becomes ever more present and influential in their daily lives. Moreover, in this understanding and shaping there is a need for a diverse set of voices from all walks of life, not merely a dialog among the technologically attuned. Focusing narrowly on human-imitative AI prevents an appropriately wide range of voices from being heard.\\n\\nWhile industry will continue to drive many developments, academia will also continue to play an essential role, not only in providing some of the most innovative technical ideas, but also in bringing researchers from the computational and statistical disciplines together with researchers from other\\n\\ndisciplines whose contributions and perspectives are sorely needed — notably\\n\\nthe social sciences, the cognitive sciences and the humanities.\\n\\nOn the other hand, while the humanities and the sciences are essential as we go forward, we should also not pretend that we are talking about something other than an engineering effort of unprecedented scale and scope — society is aiming to build new kinds of artifacts. These artifacts should be built to work as claimed. We do not want to build systems that help us with medical treatments, transportation options and commercial opportunities to find out after the fact that these systems don’t really work — that they make errors that take their toll in terms of human lives and happiness. In this regard, as I have emphasized, there is an engineering discipline yet to emerge for the data-focused and learning-focused fields. As exciting as these latter fields appear to be, they cannot yet be viewed as constituting an engineering discipline.\\n\\nMoreover, we should embrace the fact that what we are witnessing is the creation of a new branch of engineering. The term “engineering” is often\\n\\ninvoked in a narrow sense — in academia and beyond — with overtones of cold, affectless machinery, and negative connotations of loss of control by humans. But an engineering discipline can be what we want it to be.\\n\\nIn the current era, we have a real opportunity to conceive of something historically new — a human-centric engineering discipline.\\n\\nI will resist giving this emerging discipline a name, but if the acronym “AI” continues to be used as placeholder nomenclature going forward, let’s be aware of the very real limitations of this placeholder. Let’s broaden our scope, tone down the hype and recognize the serious challenges ahead.\\n\\nMichael I. Jordan')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Facebook's Bet on an Augmented Reality Future\", 'Courtesy of Facebook\\n\\nMark Zuckerberg got his first taste of the Oculus Rift, the pioneering virtual reality headset, in January 2014. Standing in one of the few Facebook offices equipped with blinds, with the brick-like device strapped to his face, he was suddenly transported to the ruins of a medieval castle, thick snowflakes falling all around him as gargoyles sprouted lava from their beaks. The dazzling virtual escapade immediately convinced him that VR would one day become a major computing platform. Two months later, he backed that conviction with Facebook’s $2 billion purchase of Oculus.\\n\\nBut Zuckerberg left the demo with another, far less talked about hunch: a younger, related technology, called augmented reality (AR) had a shot at leapfrogging VR, with its potential to bring digital overlays of contextual information or special effects onto the physical world through a simple smartphone. While there was no headline-making billion-dollar deal, Zuckerberg ordered his engineers to begin building toward an AR and VR future at the same time. The dual push made sense given that the two mediums share so much of the same underlying technology, from hardware components to sophisticated computer vision software.\\n\\n“Mark [Zuckerberg] was the one who really pushed us to invest in AR right around that time,” Facebook CTO Mike Schroepfer told Forbes. Together, AR and VR rank among Facebook’s top three tech priorities – along with connectivity and artificial intelligence – Schroepfer said. Inside Facebook, hundreds of engineers are working on underlying technologies like computer vision which will enable a phone to do everything from tracking facial movements in real time to identifying a coffee mug or recommending context-specific image effects. Artificial intelligence is so fundamental to powering AR that Facebookers often call their in-app camera an “AI camera.” The overall effort involves “significant capital outlays,” Schroepfer said.\\n\\nToday, Facebook is fighting its fellow technology powerhouses, Apple and Google -- and still to some extent, Snap -- in a high-stakes battle to rule as the platform of choice for AR developers. The technology itself, while still in its infancy, has exploded in popularity, confirming Zuckerberg’s more recent intuition that AR could sprint toward mass adoption even while VR remained an awkward technology whose appeal is largely limited to hardcore gamers. AR’s key advantage is that it doesn’t depend on a pricey, bulky headset that isolates its users. It works on a device already owned by more than one fourth of the world’s population.\\n\\n“The big epiphany is that you can use your phone for AR, and we have about 1.3 billion people who use Facebook on the phone,” said Joaquin Candela, Facebook’s head of applied machine learning, the group building the AI backbone behind the company’s AR efforts. “One hundred times more people have phones than VR headsets. That makes AR really interesting and obvious to focus on.”\\n\\nObvious, too, because early evidence suggests AR has the power to draw consumers in, sometimes fundamentally changing how they interact with their phones. Witness the popularity of pioneering AR applications, like puppy masks on Snapchat and virtual treasure hunts on games like Pokémon Go. They make clear that AR, not VR, is the next major phase in our mixed reality future. The smartphone interactions that power AR -- using a mask to turn into a singing rock star or pointing a phone to capture a Pokémon – are already acceptable social behavior.\\n\\nBut the reason tech powerhouses are investing so much in AR is that the applications go far beyond social media, games and goofy special effects. The technology could give rise to practical applications in areas ranging from navigation to e-commerce, where virtual dressing rooms could grease the wheels of online apparel purchases. An app from IKEA already helps shoppers virtually arrange furniture in their home. Pharmaceutical companies are playing with the idea of using AR to display real time information about drugs. Hyundai uses an AR app to walk consumers through the features of some cars. AR could also make its way into customer support tutorials that integrate with chat bots. “The smartphone can basically be a magic lens that you hold up to the world,” said Facebook Camera team engineering manager Tom Meyer.\\n\\nInside Facebook, engineers and executives acknowledge the stakes are high. Without getting its AR push right, the company risks seeing a decline in usage of its apps. Its battle with Snapchat for younger users shows loyalty to social apps can be fleeting. Users will quickly migrate to those that have the most alluring tools, and those that can turn communication, especially through images and videos, into engaging, constantly-evolving experiences.\\n\\nFacebook didn’t launch its AR effects until several years after Snapchat, eons in Internet time. However, Facebook pulled off the gargantuan task of offsetting its late debut thanks to the social media giant\\'s powerful in-house AI, which supported more advanced effects at scale, and its strong product design. Together, these assets enabled Facebook to rapidly match Snapchat\\'s features and hamper its younger competitor\\'s growth, saving Facebook from the fate of other tech giants such as Google (with Google Plus), whose late product timing cost the search company its shot at becoming a social media player.\\n\\nSuccess in AR could bring big rewards for Facebook. The company\\'s advertising business, fueled by activity and time spent across its apps, translated into $26.9 billion in revenue in 2016. AR effects generate a growing portion of overall time spent on the social network, encouraging users to message more frequently and spend more time viewing friends’ posts and making their own. The average Facebook user already spends about 50 minutes per day across its main app, Instagram and Messenger – and Facebook needs to continually roll out new product features that capture eyeballs and foster growth in image-heavy communication to keep this metric high and stave off competition, particularly as the company seeks to minimize other news feed thumb-stoppers like click bait and hoaxes.\\n\\nGetty Images\\n\\nThe prevalence of masks and filters across Facebook’s apps marks the company’s first major step in transitioning from being a “past camera” for still images and albums to a “future camera,” as Meyer calls it -- one that’s powered by AI for intriguing effects and attached to a network of friends and family. Masks might seem frivolous, but the implications of getting AR right for Facebook are existential. The social network’s ongoing shift toward image-heavy features is as critical to the company’s future as was the transition from desktop to mobile, according to Facebook’s chief product officer Chris Cox.\\n\\n“As a broader story, Facebook has to get really good at AR if we want to be relevant in the next 10 or 20 years,” Schroepfer said.\\n\\nFrom ‘Past’ To ‘Future’ Camera\\n\\nOn a morning last May, Zuckerberg appeared on the social network wearing taped glasses with math equations swirling above his head. The nerdy digital accessories were Zuckerberg’s attire of choice for announcing Instagram Stories’ first “face filters” (known as face “masks” on Snapchat). “This is my favorite one so far,” Zuckerberg said looking wide-eyed into the camera with a smile.\\n\\nThe effects, which include colorful confetti, a bubbly underwater scene, twitching koala noses and bunny ears, are inspiring 300 million people to experiment, everyone from celebrities like actress Reese Witherspoon and model Karlie Kloss to teenagers lounging at home use Instagram’s AR-heavy Stories feature each month. Buoyed by Instagram Stories (a series of photo and video clips that disappear after 24 hours) and its analogues on WhatsApp and Facebook’s main app, Facebook is now the largest social AR ecosystem in the world, just four years after Zuckerberg’s Oculus demo and six years after the company started picking away at the core AI technology.\\n\\nHundreds of developers are building apps atop Facebook’s AR camera platform using software called “AR Studio,” which the company opened widely in December. Now anyone with a Facebook account can create AR effects for the social network, including masks, animations and 3D objects.\\n\\nEven though Facebook is cracking social AR, it wasn’t the first to popularize it. That claim belongs to one of its top rivals, Snapchat, which was originally written off by many as a sexting tool. It was Snapchat that pioneered “Stories” -- the primary home of AR effects -- three years before Facebook launched a near carbon copy of the format in 2016. Until a little over a year ago, Snapchat\\'s AR features were well ahead of Facebooks. However, Facebook had been intently preparing the core technology behind the scenes for years because of Zuckerberg’s early hunch that AR would someday be a mainstream communication tool.\\n\\nSnapchat’s first AR effects were full-screen overlays for photos and videos, followed by location-specific art called “geofilters” in 2014, “lenses,” like its famous barfing rainbow mask in 2015, and customizable avatars called “Bitmojis” in 2016. Snapchat’s effects were instant hits, helping the app garner 150 million daily users. Teens, perhaps the most valuable and elusive demographic, came to view it as an everyday chatting essential. Now, Facebook executives avoid discussing its younger competitor in interviews, but at the time, the social network anxiously and attentively tracked Snapchat’s growth.\\n\\nAs Facebook monitored its new rival, it was well aware of a trend on its own app. Posts on the social network were increasingly veering toward photos, GIFs and video and away from text. More striking was the response to its live video launch in April 2016. Executives - including Zuckerberg – were stunned by its hockey-stick like adoption, which clearly signaled that people wanted to speak through images and animations on Facebook, and they wanted to do it in real-time.\\n\\nUpping the investment in image-based sharing was inevitable. Facebook’s product roadmap hit a major turning point when Facebook was rolling out live video and took note of the Belarusian selfie-mask app called Masquerade (MSQRD). It had taken off in Eastern Europe and was gaining steam in the U.S. with a total base of 16 million users. Its mask tool looked nearly identical to Snapchat lenses, thanks to 3D graphic rendering technology MSQRD cofounder Eugen Zatepyakin had spent nearly three years building. Facebook’s VP of design Julie Zhou recalls watching Facebook executives try out the app at the company’s sprawling Menlo Park, Calif. headquarters. They were quickly captivated.\\n\\nCourtesy of Facebook\\n\\n“There was something instantly appealing about trying to transform yourself -- you want to be someone else, to dress up,” Zhou said. “It’s harkening back to being a kid again and having fun, but it doesn’t stay a toy forever.” (Read more about future applications for augmented reality in \"Six Ways AR Will Matter Beyond Puppy Selfies.\")\\n\\nFacebook acquired the startup for an undisclosed amount in March 2016 and began working feverishly to make up for lost time. Soon after, Zuckerberg posted a video of himself using MSQRD’s Iron Man mask on the social network, and quietly launched a major internal effort to rapidly build the technological backbone to support a more sophisticated in-app camera.\\n\\nAI Breakthrough\\n\\nFacebook had been working on AI such as computational photography that would later support AR since 2015. However, Zuckerberg didn’t begin an overt AR effort until he called for the formation of a dedicated “Camera Group” in the summer of 2016. The group began as a handful of AI engineers and researchers within Facebook’s Applied Machine Learning Group, which sits in the open-floor plan of “Building 20,” near Zuckerberg’s usual glass-walled workspace.\\n\\nOver the next year and half, the Camera Group expanded to more than 100 people, including designers nabbed from Hollywood and gaming companies, who work with camera product heads within standalone Facebook apps like Messenger and Instagram to help them launch features and iterate quickly. The group has been busy doing everything from advancing the underlying visual identification and deep learning technology, to running focus groups outside of Facebook to experiment with AR tools.\\n\\nCourtesy of Facebook\\n\\nIn one of the Camera team’s research studies, Dantley Davis, the Applied Machine Learning team’s head of product design who previously led mobile design efforts at Netflix , recalls visiting people to talk to them about the experience of wishing “Happy Birthday” to someone they care about. The team gave individuals in the study a range of tools they could use to send special effects and messages. One man in their study group, Dantley noted, was relieved to have animated effects to wish happy birthday to his wife because it was easier and faster than trying to express himself through spoken word or text.\\n\\n“He created an experience based on tools that we provided that was very intimate and personable, based on communication with his wife,” Dantley said. “The AR tools gave him a shorthand to express himself in ways that the he found a lot of value in. They allow people to feel more confident about being goofy to communicate emotion.”\\n\\nSoon after the Camera team formed, the success of a single app by game developer Niantic served as major confirmation that AR could have mass consumer appeal. Nineteen days after debuting in July 2016, Niantic’s game Pokemon Go grew to 50 million players, who would walk miles around cities (and gyms), using their smartphones to catch AR characters. Zuckerberg himself was a fan of the game, which he mentioned he was enjoying along with “everyone else” when Facebook reported quarterly earnings that month. The game served as affirmation for Facebook executives that AR wouldn’t just be limited to a few messaging apps like Snapchat.\\n\\nMeanwhile, Facebook’s Camera team engineers were heads down building in-house AI processing software that they hoped would serve as the backbone for future AR effects. While competitors at the time, like Snapchat, relied on outside servers to power AR features, reducing their speed and complexity, Facebook sought to invent a system for processing AI directly on the smartphone, a technology it ultimately named Caffe2Go. Facebook didn’t want to launch AR effects until it had built the infrastructure to ensure its effects would scale seamlessly, and with better renderings, face tracking and speed than those offered by Snapchat.\\n\\nWhile Caffe2Go was in the works, product teams at Facebook experimented with AR tools, with the help of MSQRD. In August 2016, ahead of the Summer Olympics, for example, in Brazil and Canada, Facebook tested opening its flagship app directly into a full-screen camera for the first time (Snapchat-style), and launched Olympic-themed frames and face-paint masks for profile pictures. That month, Facebook laid the groundwork for AR effects on Instagram by debuting its first version of “Stories” on Instagram, a near-clone of Snapchat’s signature feature for casual, disappearing posts.\\n\\nThat fall, Facebook completed Caffe2Go, creating the first system that could capture and analyze pixels in real-time by processing AI directly on the smartphone. After Facebook tested the technology in fall 2016 with “style transfer,” a process that transforms a photo or video into the style of an artist like Picasso or Van Gogh, it was ready to use Caffe2Go to power AR across its apps, beginning with Messenger in December 2016. Soon, Messenger looked strikingly Snapchat-like, with masks, filters and frames for photos and videos via a “Stories”-like feature called “My Day.” In March, Facebook expanded the AR effects to its Facebook’s flagship app, letting one swipe take users to a full screen camera with a central effects button for masks and animated overlays. (Read more about the technology powering augmented reality on Facebook in \"Five Breakthroughs Behind Facebook\\'s AR Play.\")\\n\\nCourtesy of Facebook\\n\\nThe next month at Facebook’s annual F8 developer conference in April 2017, Zuckerberg’s key announcement was the unveiling of the first “Camera Platform,” which allowed a handful of developers to build AR features atop the social network. Standing on stage in his standard gray t-shirt and jeans, Zuckerberg prefaced his camera announcement by philosophizing about the future of work. Ultimately, Zuckerberg argued, technology will free up people’s time to socialize, be more creative and make more art.\\n\\n“In the future, more of us are going to contribute to culture and society in ways that are not measured by traditional economics or GDP,” said Zuckerberg, in front of screen displaying the company’s 10-year road map. “A lot of us are going to do what today is considered the arts, and that’s going to form the basis of a lot of our communities.”\\n\\n“That’s why I’m so excited about augmented reality,” Zuckerberg continued, gesturing. “It’s going to make it so that we can create all kinds of things that until today have only been possible in the digital world, and we’re going to be able to interact with them and explore them together.”\\n\\nFor the first time, Zuckerberg pitched Facebook’s in-app camera as the heart of communication on Facebook, riffing on Snapchat’s motto of being “a camera company.” While AR glasses and contacts will likely be the first AR wearable devices down the road, Zuckerberg predicted, people are starting to enjoy an AR heyday now on their smartphones.\\n\\nSince Facebook began launching AR effects in late 2016, Snapchat’s growth has stagnated. The rise of its most popular AR platform, Instagram Stories, correlates directly with a decline in user growth on Snapchat. Now only about 190 million people use Snapchat per day, while 500 million people use Instagram every day.\\n\\nA Broader AR Race\\n\\nEven though the biggest consumer use of AR is on social media, just about every technology giant is racing to build AR functionality into their products and ecosystems for developers, most of whom don’t yet know how to build for AR. Two months after Facebook debuted its AR camera platform, Apple launched its own developer tool set called ARKit, for iOS 11, which makes it easy for developers and marketers to integrate AR into their existing apps. Already 400 million devices are estimated to be compatible with Apple’s ARKit, according to research firm Forrester. In September, Apple also unveiled “Animoji” for iMessage, which uses face recognition on the iPhone X to let people customize emoji with their facial expressions.\\n\\nCourtesy of Facebook\\n\\nGoogle, however, may have been working on AR the longest of any tech company, launching smart glasses (Google Glass) as early as 2013. The product was a high-profile flop, intriguing technophiles but failing among consumers driven away by concerns about privacy and social acceptability. Google then launched an AR platform called Tango in 2016, which uses depth sensors to map indoors spaces, but is only compatible with a few narrowly used devices. In a push to bring Tango’s abilities to more phones without adding cameras and sensors, Google rolled out its own version of ARKit in August, called ARCore. The developer kit is built to reach existing and future Android devices, including the Samsung Galaxy 8.\\n\\nBy the end of winter, ARCore is expected to run on 100 million Android devices, according to a Google spokesperson. Like Facebook, Google is naturally also interested in using AR for search. In May, Google announced “ Lens,” a computer vision tool that sorts albums in Google Photos, and now lets Pixel and Pixel 2 owners point their camera at objects such as storefronts to get information in real time. Not only do Apple and Google have the benefit of owning their own operating systems, on which developers can build a wide swath of AR apps, they also have a history of successful hardware products that Facebook so far hasn’t matched. Facebook doesn’t have nearly the same track record of building devices, which makes its hardware ambitious a bigger stretch.\\n\\n“Facebook’s camera platform will enable developers and marketers to reach greater audiences over time, but Facebook’s limitation is that it doesn’t control the hardware,” said Forrester analyst Thomas Husson. “To truly deliver an amazing AR experience it’s about software and hardware integration.”\\n\\nWhile Facebook, Apple and Google are focused on smartphones, Microsoft has been focusing on business customers with wearables. The company launched Hololens, a $3,000 visor-like headset in 2016. Hololens runs on an operating system called Windows Mixed Reality, which can be used by other VR and AR hardware makers. It could take years, however, for an AR device to be affordable and appealing to a mass market.\\n\\nFacebook may not ever own its own operating system or the next AR device (although it’s trying), but it does have key unique assets: First, it reaches more people than any other social or messaging app like Snap, Kakao, Line and WeChat. (Data about people’s interests and networks is powerful for personalizing AR effects.) Second, Facebook has one of the largest computer vision teams of any company in the world, which will can help it create better features than newcomers. Even if Facebook never makes its own AR hardware popular, Facebook will likely remain one of the largest ecosystems of AR content for years.\\n\\nThe Next Trend In Advertising\\n\\nDuring the seventh season finale weekend of the hit show “Game of Thrones” this July, fans around the world turned to social media per usual to share reactions and grievances for characters’ fates in the most-watched HBO premiere to date. But a more eye-catching type of post, alongside traditional status updates, also went viral: More than 1 million people created video clips of themselves on Facebook gradually transforming into a terrifying, icey blue-eyed Night King villain, complete with horns, a voice changer and a backdrop of falling snow. The face-tracking mask adjusted in real time to people’s faces as they roared or broke into song, and the clips circulated on news feed and in messages to friends.\\n\\nThe mask’s transformational power wasn’t its only twist. The mask was made by HBO, not Facebook (and featured the “GOT” logo in the upper left corner). It quickly became one of the most successful AR campaigns on Facebook and showed that high-quality AR effects can become advertisements that people choose to send to their friends. AR is a tantalizing format for many marketers, as the Night King mask showed, because it offers new ways to engage smartphone holders for longer amounts of time. Facebook is positioning itself to become the default platform for AR marketing campaigns built around encouraging someone to play a game with friends or post an animated selfie.\\n\\nCourtesy of HBO, Facebook\\n\\n“Game of Thrones fans are dying for new engagement every day of the year,” Emily Giannusa, director of digital media and marketing at HBO, told Forbes. “The Facebook camera platform appealed to us because it was such a simple and simultaneously sophisticated avenue that allowed fans to transport themselves directly into the Game of Thrones universe. All fans needed was a smartphone.”\\n\\nOnly an estimated 5% of marketers are using AR technology now. However 17% of marketers plan to use the technology this year, according to a recent Forrester study, and AR is poised to be more relevant to marketers than VR for at least the next three years. While VR experiences make more sense for aspirational brands that spend court customers over long periods of time, Forrester’s Husson noted, vastly more marketers will benefit from experimenting with AR now.\\n\\nAn ‘Exploration Phase’\\n\\nAs AR extends from smartphones to wearables, the technology could become a nearly always-on enhancer of the human senses and a routine replacement for the search bar tailored to our location, interests and social network. But consumer uses for AR are still nascent. For the next several years, tech giants will be busy improving the core technology and building AR for the smartphone, with Facebook focusing on its forte, messaging and personal expression. With time, AR will likely make Facebook and the news feed look entirely different -- more immersive, video intensive and interactive, although the specifics are fuzzy.\\n\\nFacebook will need to continually improve its AI to become faster and more precise, for example, at identifying objects in video, understanding how a scene is pixelated when it’s viewed from different angles and mapping the relationships between objects in a scene. To reach a wider range of users in the long term, including in developing markets, Facebook will need to make its AR features more compatible with weaker cell networks and older phones.\\n\\n“We’re still at the basic exploration phase and we’re still building basic technology,” said Facebook’s Candela. “So my bias right now is on execution. At the same time, in the back burner we’re going to have some crazy exploratory projects as well.”\\n\\nIn the meantime, Facebook is “heavily investing” in hardware to support AR and build more social tools, according to Schroepfer. Beyond protecting Facebook’s relevance, improving the underlying AI has other benefits too, like helping fight spammers and problematic content with better visual recognition and language understanding tools, as well as advancing games and robotics.\\n\\n“It would be misleading to think, ‘This is all about building space cat masks,’” said Candela. “You take our social infrastructure, the tech we’re pushing, and it’s hard to imagine what the applications would be. But I know for sure that this will unlock things that we haven’t thought about today -- The meaningful AR experiences will be very social, where you are yourself.”')"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Attention and Augmented Recurrent Neural Networks', 'Attention and Augmented Recurrent Neural Networks\\n\\nRecurrent neural networks are one of the staples of deep learning, allowing neural networks to work with sequences of data like text, audio and video. They can be used to boil a sequence down into a high-level understanding, to annotate sequences, and even to generate new sequences from scratch!\\n\\nrnn x0 y0 x1 y1 x2 y2 x3 y3 One cell... can be used over... and over... and over... x4 y4 again.\\n\\nThe basic RNN design struggles with longer sequences, but a special variant—“long short-term memory” networks [1] —can even work with these. Such models have been found to be very powerful, achieving remarkable results in many tasks including translation, voice recognition, and image captioning. As a result, recurrent neural networks have become very widespread in the last few years.\\n\\nAs this has happened, we’ve seen a growing number of attempts to augment RNNs with new properties. Four directions stand out as particularly exciting:\\n\\nIndividually, these techniques are all potent extensions of RNNs, but the really striking thing is that they can be combined, and seem to just be points in a broader space. Further, they all rely on the same underlying trick—something called attention—to work.\\n\\nOur guess is that these “augmented RNNs” will have an important role to play in extending deep learning’s capabilities over the coming years.\\n\\nNeural Turing Machines\\n\\nNeural Turing Machines [2] combine a RNN with an external memory bank. Since vectors are the natural language of neural networks, the memory is an array of vectors:\\n\\nrnn Memory is an array of vectors. Network A writes and reads from this memory each step. x0 y0 x1 y1 x2 y2 x3 y3\\n\\nBut how does reading and writing work? The challenge is that we want to make them differentiable. In particular, we want to make them differentiable with respect to the location we read from or write to, so that we can learn where to read and write. This is tricky because memory addresses seem to be fundamentally discrete. NTMs take a very clever solution to this: every step, they read and write everywhere, just to different extents.\\n\\nAs an example, let’s focus on reading. Instead of specifying a single location, the RNN outputs an “attention distribution” that describes how we spread out the amount we care about different memory positions. As such, the result of the read operation is a weighted sum.\\n\\nrnn attention memory The RNN gives an attention distribution which describe how we spread out the amount we care about different memory positions. The read result is a weighted sum.\\n\\nSimilarly, we write everywhere at once to different extents. Again, an attention distribution describes how much we write at every location. We do this by having the new value of a position in memory be a convex combination of the old memory content and the write value, with the position between the two decided by the attention weight.\\n\\nrnn attention old memory new memory write value The RNN gives an attention distribution, describing how much we should change each memory position towards the write value. Instead of writing to one location, we write everywhere, just to different extents.\\n\\nBut how do NTMs decide which positions in memory to focus their attention on? They actually use a combination of two different methods: content-based attention and location-based attention. Content-based attention allows NTMs to search through their memory and focus on places that match what they’re looking for, while location-based attention allows relative movement in memory, enabling the NTM to loop.\\n\\nrnn First, the controller gives a query vector and each memory entry is scored for similarity with the query. The scores are then converted into a distribution using softmax. Next, we interpolate the attention from the previous time step. We convolve the attention with a shift filter—this allows the controller to move its focus. Finally, we sharpen the attention distribution. This final attention distribution is fed to the read or write operation. The RNN gives an attention distribution, describing how much we should change each memory position towards the write value. memory Blue shows high similarity, pink high dissimilarity. interpolation amount shift filter RNN controller attention mechanism query vector attention from previous step new attention distribution\\n\\nThis capability to read and write allows NTMs to perform many simple algorithms, previously beyond neural networks. For example, they can learn to store a long sequence in memory, and then loop over it, repeating it back repeatedly. As they do this, we can watch where they read and write, to better understand what they’re doing:\\n\\nSee more experiments in [3] . This figure is based on the Repeat Copy experiment.\\n\\nThey can also learn to mimic a lookup table, or even learn to sort numbers (although they kind of cheat)! On the other hand, they still can’t do many basic things, like add or multiply numbers.\\n\\nSince the original NTM paper, there have been a number of exciting papers exploring similar directions. The Neural GPU [4] overcomes the NTM’s inability to add and multiply numbers. Zaremba & Sutskever [5] train NTMs using reinforcement learning instead of the differentiable read/writes used by the original. Neural Random Access Machines [6] work based on pointers. Some papers have explored differentiable data structures, like stacks and queues [7, 8] . And memory networks [9, 10] are another approach to attacking similar problems.\\n\\nIn some objective sense, many of the tasks these models can perform—such as learning how to add numbers—aren’t that objectively hard. The traditional program synthesis community would eat them for lunch. But neural networks are capable of many other things, and models like the Neural Turing Machine seem to have knocked away a very profound limit on their abilities.\\n\\nCode\\n\\nThere are a number of open source implementations of these models. Open source implementations of the Neural Turing Machine include Taehoon Kim’s (TensorFlow), Shawn Tan’s (Theano), Fumin’s (Go), Kai Sheng Tai’s (Torch), and Snip’s (Lasagne). Code for the Neural GPU publication was open sourced and put in the TensorFlow Models repository. Open source implementations of Memory Networks include Facebook’s (Torch/Matlab), YerevaNN’s (Theano), and Taehoon Kim’s (TensorFlow).\\n\\nAttentional Interfaces\\n\\nWhen I’m translating a sentence, I pay special attention to the word I’m presently translating. When I’m transcribing an audio recording, I listen carefully to the segment I’m actively writing down. And if you ask me to describe the room I’m sitting in, I’ll glance around at the objects I’m describing as I do so.\\n\\nNeural networks can achieve this same behavior using attention, focusing on part of a subset of the information they’re given. For example, an RNN can attend over the output of another RNN. At every time step, it focuses on different positions in the other RNN.\\n\\nWe’d like attention to be differentiable, so that we can learn where to focus. To do this, we use the same trick Neural Turing Machines use: we focus everywhere, just to different extents.\\n\\nrnn Network B focuses on different information from network A at every step.\\n\\nThe attention distribution is usually generated with content-based attention. The attending RNN generates a query describing what it wants to focus on. Each item is dot-producted with the query to produce a score, describing how well it matches the query. The scores are fed into a softmax to create the attention distribution.\\n\\nrnn The attending RNN generates a query describing what it wants to focus on. Each item is dot producted with the query to produce a score, describing how well it matches the query. The scores are fed into a softmax to create the attention distribution.\\n\\nOne use of attention between RNNs is translation [11] . A traditional sequence-to-sequence model has to boil the entire input down into a single vector and then expands it back out. Attention avoids this by allowing the RNN processing the input to pass along information about each word it sees, and then for the RNN generating the output to focus on words as they become relevant.\\n\\n0.50, 0.02, 0.04, 0.00, 0.00, 0.00, 0.00, 0.00, 0.01, 0.01, 0.00, 0.00, 0.00, 0.05, 0.05 0.14, 0.89, 0.07, 0.00, 0.00, 0.00, 0.00, 0.00, 0.01, 0.06, 0.00, 0.00, 0.00, 0.02, 0.00 0.02, 0.00, 0.56, 0.00, 0.00, 0.00, 0.00, 0.01, 0.00, 0.00, 0.00, 0.00, 0.00, 0.02, 0.00 0.00, 0.00, 0.01, 0.61, 0.02, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00 0.00, 0.00, 0.02, 0.03, 0.01, 0.21, 0.97, 0.05, 0.00, 0.00, 0.00, 0.00, 0.00, 0.01, 0.00 0.00, 0.00, 0.02, 0.07, 0.25, 0.76, 0.02, 0.04, 0.00, 0.00, 0.00, 0.00, 0.00, 0.02, 0.00 0.02, 0.04, 0.03, 0.27, 0.71, 0.02, 0.00, 0.02, 0.00, 0.00, 0.02, 0.00, 0.00, 0.05, 0.01 0.02, 0.00, 0.07, 0.00, 0.00, 0.00, 0.00, 0.60, 0.40, 0.01, 0.04, 0.00, 0.00, 0.13, 0.01 0.13, 0.02, 0.02, 0.00, 0.01, 0.00, 0.00, 0.23, 0.52, 0.91, 0.07, 0.00, 0.00, 0.10, 0.00 0.04, 0.00, 0.01, 0.00, 0.00, 0.00, 0.00, 0.01, 0.03, 0.00, 0.67, 0.00, 0.00, 0.16, 0.02 0.02, 0.00, 0.04, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.14, 0.96, 0.00, 0.02, 0.07 0.02, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.02, 0.02, 1.00, 0.02, 0.03 0.02, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.01, 0.00, 0.01, 0.00, 0.00, 0.32, 0.11 0.04, 0.00, 0.08, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.01, 0.00, 0.00, 0.04, 0.67 output text network B network A input text Diagram derived from Fig. 3 of Bahdanau, et al. 2014\\n\\nThis kind of attention between RNNs has a number of other applications. It can be used in voice recognition [12] , allowing one RNN to process the audio and then have another RNN skim over it, focusing on relevant parts as it generates a transcript.\\n\\noutput text network B network A input audio 0.01,0.02,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01 0.01,0.02,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01 0.01,0.02,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01 0.01,0.02,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01 0.01,0.02,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01 0.09,0.11,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01 0.78,0.33,0.04,0.02,0.02,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01 0.01,0.05,0.61,0.37,0.78,0.07,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01 0.01,0.01,0.05,0.21,0.07,0.74,0.31,0.00,0.01,0.01,0.01,0.02,0.01,0.02,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01 0.01,0.01,0.01,0.04,0.02,0.02,0.47,0.98,0.02,0.01,0.01,0.02,0.01,0.02,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01 0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.96,0.98,0.28,0.02,0.02,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01 0.01,0.01,0.01,0.02,0.01,0.01,0.01,0.00,0.01,0.01,0.40,0.49,0.61,0.30,0.02,0.02,0.01,0.01,0.01,0.02,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01 0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.05,0.07,0.09,0.18,0.96,0.89,0.87,0.54,0.02,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01 0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.02,0.10,0.05,0.13,0.01,0.03,0.05,0.10,0.14,0.32,0.13,0.01,0.02,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01 0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.12,0.60,0.04,0.03,0.02,0.02,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01 0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.05,0.41,0.66,0.02,0.02,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01 0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.94,0.72,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01 0.01,0.02,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.04,0.76,0.02,0.01,0.01,0.01,0.01,0.02,0.01,0.01,0.01 0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.77,0.45,0.66,0.01,0.01,0.01,0.01,0.01,0.01 0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.04,0.01,0.00,0.02,0.13,0.98,0.97,0.02,0.01,0.01,0.01 0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.09,0.00,0.01,0.01,0.01,0.02,0.90,0.02,0.01,0.01 0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.08,0.10,0.01,0.01,0.01,0.03,0.96,0.19,0.01 0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.13,0.02,0.01,0.01,0.01,0.01,0.60,0.18 0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.02,0.60 0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.02 0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01 0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01 0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01 0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01 0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01 0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01 0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01 0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01 0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01 0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01 0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01 0.01,0.02,0.02,0.02,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01 0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01 Figure derived from Chan, et al. 2015\\n\\nOther uses of this kind of attention include parsing text [13] , where it allows the model to glance at words as it generates the parse tree, and for conversational modeling [14] , where it lets the model focus on previous parts of the conversation as it generates its response.\\n\\nAttention can also be used on the interface between a convolutional neural network and an RNN. This allows the RNN to look at different position of an image every step. One popular use of this kind of attention is for image captioning. First, a conv net processes the image, extracting high-level features. Then an RNN runs, generating a description of the image. As it generates each word in the description, the RNN focuses on the conv net’s interpretation of the relevant parts of the image. We can explicitly visualize this:\\n\\nFigure from [3]\\n\\nMore broadly, attentional interfaces can be used whenever one wants to interface with a neural network that has a repeating structure in its output.\\n\\nAttentional interfaces have been found to be an extremely general and powerful technique, and are becoming increasingly widespread.\\n\\nAdaptive Computation Time\\n\\nStandard RNNs do the same amount of computation for each time step. This seems unintuitive. Surely, one should think more when things are hard? It also limits RNNs to doing O(n) operations for a list of length n.\\n\\nAdaptive Computation Time [15] is a way for RNNs to do different amounts of computation each step. The big picture idea is simple: allow the RNN to do multiple steps of computation for each time step.\\n\\nIn order for the network to learn how many steps to do, we want the number of steps to be differentiable. We achieve this with the same trick we used before: instead of deciding to run for a discrete number of steps, we have an attention distribution over the number of steps to run. The output is a weighted combination of the outputs of each step.\\n\\nrnn For every time step the RNN can do multiple computation steps. The output is a weighted combination of the computation step outputs. The process is repeated for each time step. A special bit is set to denote the first computation step.\\n\\nThere are a few more details, which were left out in the previous diagram. Here’s a complete diagram of a time step with three computation steps.\\n\\nrnn\\n\\nThat’s a bit complicated, so let’s work through it step by step. At a high-level, we’re still running the RNN and outputting a weighted combination of the states:\\n\\nrnn\\n\\nThe weight for each step is determined by a “halting neuron.” It’s a sigmoid neuron that looks at the RNN state and gives a halting weight, which we can think of as the probability that we should stop at that step.\\n\\nrnn halting neuron\\n\\nWe have a total budget for the halting weights of 1, so we track that budget along the top. When it gets to less than epsilon, we stop.\\n\\nrnn\\n\\nWhen we stop, might have some left over halting budget because we stop when it gets to less than epsilon. What should we do with it? Technically, it’s being given to future steps but we don’t want to compute those, so we attribute it to the last step.\\n\\nrnn\\n\\nWhen training Adaptive Computation Time models, one adds a “ponder cost” term to the cost function. This penalizes the model for the amount of computation it uses. The bigger you make this term, the more it will trade-off performance for lowering compute time.\\n\\nAdaptive Computation Time is a very new idea, but we believe that it, along with similar ideas, will be very important.\\n\\nCode\\n\\nThe only open source implementation of Adaptive Computation Time at the moment seems to be Mark Neumann’s (TensorFlow).\\n\\nNeural Programmer\\n\\nNeural nets are excellent at many tasks, but they also struggle to do some basic things like arithmetic, which are trivial in normal approaches to computing. It would be really nice to have a way to fuse neural nets with normal programming, and get the best of both worlds.\\n\\nThe neural programmer [16] is one approach to this. It learns to create programs in order to solve a task. In fact, it learns to generate such programs without needing examples of correct programs. It discovers how to produce programs as a means to the end of accomplishing some task.\\n\\nThe actual model in the paper answers questions about tables by generating SQL-like programs to query the table. However, there are a number of details here that make it a bit complicated, so let’s start by imagining a slightly simpler model, which is given an arithmetic expression and generates a program to evaluate it.\\n\\nThe generated program is a sequence of operations. Each operation is defined to operate on the output of past operations. So an operation might be something like “add the output of the operation 2 steps ago and the output of the operation 1 step ago.” It’s more like a Unix pipe than a program with variables being assigned to and read from.\\n\\nrnn\\n\\nThe program is generated one operation at a time by a controller RNN. At each step, the controller RNN outputs a probability distribution for what the next operation should be. For example, we might be pretty sure we want to perform addition at the first time step, then have a hard time deciding whether we should multiply or divide at the second step, and so on...\\n\\nrnn At each step the controller RNN outputs a probability distribution.\\n\\nThe resulting distribution over operations can now be evaluated. Instead of running a single operation at each step, we do the usual attention trick of running all of them and then average the outputs together, weighted by the probability we ran that operation.\\n\\nrnn At each step the controller RNN outputs a probability distribution. We run all of the operations and average the outputs together.\\n\\nAs long as we can define derivatives through the operations, the program’s output is differentiable with respect to the probabilities. We can then define a loss, and train the neural net to produce programs that give the correct answer. In this way, the Neural Programmer learns to produce programs without examples of good programs. The only supervision is the answer the program should produce.\\n\\nThat’s the core idea of Neural Programmer, but the version in the paper answers questions about tables, rather than arithmetic expressions. There are a few additional neat tricks:\\n\\nMultiple Types: Many of the operations in the Neural Programmer deal with types other than scalar numbers. Some operations output selections of table columns or selections of cells. Only outputs of the same type get merged together.\\n\\nReferencing Inputs: The neural programmer needs to answer questions like “How many cities have a population greater than 1,000,000?” given a table of cities with a population column. To facilitate this, some operations allow the network to reference constants in the question they’re answering, or the names of columns. This referencing happens by attention, in the style of pointer networks [17] .\\n\\nThe Neural Programmer isn’t the only approach to having neural networks generate programs. Another lovely approach is the Neural Programmer-Interpreter [18] which can accomplish a number of very interesting tasks, but requires supervision in the form of correct programs.\\n\\nWe think that this general space, of bridging the gap between more traditional programming and neural networks is extremely important. While the Neural Programmer is clearly not the final solution, we think there are a lot of important lessons to be learned from it.\\n\\nCode\\n\\nThe more recent version of Neural Programmer for question answering has been open sourced by its authors and is available as a TensorFlow Model. There is also an implementation of the Neural Programmer-Interpreter by Ken Morishita (Keras).\\n\\nThe Big Picture\\n\\nA human with a piece of paper is, in some sense, much smarter than a human without. A human with mathematical notation can solve problems they otherwise couldn’t. Access to computers makes us capable of incredible feats that would otherwise be far beyond us.\\n\\nIn general, it seems like a lot of interesting forms of intelligence are an interaction between the creative heuristic intuition of humans and some more crisp and careful media, like language or equations. Sometimes, the medium is something that physically exists, and stores information for us, prevents us from making mistakes, or does computational heavy lifting. In other cases, the medium is a model in our head that we manipulate. Either way, it seems deeply fundamental to intelligence.\\n\\nRecent results in machine learning have started to have this flavor, combining the intuition of neural networks with something else. One approach is what one might call “heuristic search.” For example, AlphaGo [19] has a model of how Go works and explores how the game could play out guided by neural network intuition. Similarly, DeepMath [20] uses neural networks as intuition for manipulating mathematical expressions. The “augmented RNNs” we’ve talked about in this article are another approach, where we connect RNNs to engineered media, in order to extend their general capabilities.\\n\\nInteracting with media naturally involves making a sequence of taking an action, observing, and taking more actions. This creates a major challenge: how do we learn which actions to take? That sounds like a reinforcement learning problem and we could certainly take that approach. But the reinforcement learning literature is really attacking the hardest version of this problem, and its solutions are hard to use. The wonderful thing about attention is that it gives us an easier way out of this problem by partially taking all actions to varying extents. This works because we can design media—like the NTM memory—to allow fractional actions and to be differentiable. Reinforcement learning has us take a single path, and try to learn from that. Attention takes every direction at a fork, and then merges the paths back together.\\n\\nA major weaknesses of attention is that we have to take every “action” every step. This causes the computational cost to grow linearly as you do things like increase the amount of memory in a Neural Turing Machine. One thing you could imagine doing is having your attention be sparse, so that you only have to touch some memories. However, it’s still challenging because you may want to do things like have your attention depend on the content of the memory, and doing that naively forces you to look at each memory. We’ve seen some initial attempts to attack this problem, such as [21] , but it seems like there’s a lot more to be done. If we could really make such sub-linear time attention work, that would be very powerful!\\n\\nAugmented recurrent neural networks, and the underlying technique of attention, are incredibly exciting. We look forward to seeing what happens next!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Tensorflow r1.5', \"angersson released this Jan 26, 2018 · 26 commits to r1.5 since this release\\n\\nRelease 1.5.0\\n\\nBreaking Changes\\n\\nPrebuilt binaries are now built against CUDA 9 and cuDNN 7.\\n\\nStarting from 1.6 release, our prebuilt binaries will use AVX instructions.\\n\\nThis may break TF on older CPUs.\\n\\nMajor Features And Improvements\\n\\nEager execution\\n\\npreview version is now available.\\n\\npreview version is now available. TensorFlow Lite\\n\\ndev preview is now available.\\n\\ndev preview is now available. CUDA 9 and cuDNN 7 support.\\n\\nAccelerated Linear Algebra (XLA): Add complex64 support to XLA compiler. bfloat support is now added to XLA infrastructure. Make ClusterSpec propagation work with XLA devices. Use a determinisitic executor to generate XLA graph.\\n\\ntf.contrib : tf.contrib.distributions : Add tf.contrib.distributions.Autoregressive . Make tf.contrib.distributions QuadratureCompound classes support batch Infer tf.contrib.distributions.RelaxedOneHotCategorical dtype from arguments. Make tf.contrib.distributions quadrature family parameterized by\\n\\nquadrature_grid_and_prob vs quadrature_degree . auto_correlation added to tf.contrib.distributions Add tf.contrib.bayesflow.layers , a collection of probabilistic (neural) layers. Add tf.contrib.bayesflow.halton_sequence . Add tf.contrib.data.make_saveable_from_iterator. Add tf.contrib.data.shuffle_and_repeat . Add new custom transformation: tf.contrib.data.scan() . tf.contrib.distributions.bijectors : Add tf.contrib.distributions.bijectors.MaskedAutoregressiveFlow . Add tf.contrib.distributions.bijectors.Permute . Add tf.contrib.distributions.bijectors.Gumbel . Add tf.contrib.distributions.bijectors.Reshape . Support shape inference (i.e., shapes containing -1) in the Reshape bijector.\\n\\n: Add streaming_precision_recall_at_equal_thresholds, a method for computing\\n\\nstreaming precision and recall with O(num_thresholds + size of predictions)\\n\\ntime and space complexity.\\n\\na method for computing streaming precision and recall with time and space complexity. Change RunConfig default behavior to not set a random seed, making random\\n\\nbehavior independently random on distributed workers. We expect this to\\n\\ngenerally improve training performance. Models that do rely on determinism\\n\\nshould set a random seed explicitly.\\n\\ndefault behavior to not set a random seed, making random behavior independently random on distributed workers. We expect this to generally improve training performance. Models that do rely on determinism should set a random seed explicitly. Replaced the implementation of tf.flags with absl.flags .\\n\\nwith . Add support for CUBLAS_TENSOR_OP_MATH in fp16 GEMM\\n\\nin fp16 GEMM Add support for CUDA on NVIDIA Tegra devices\\n\\nBug Fixes and Other Changes\\n\\nDocumentation updates: Clarified that you can only install TensorFlow on 64-bit machines. Added a short doc explaining how Estimator s save checkpoints. Add documentation for ops supported by the tf2xla bridge. Fix minor typos in the doc of SpaceToDepth and DepthToSpace . Updated documentation comments in mfcc_mel_filterbank.h and mfcc.h to\\n\\nclarify that the input domain is squared magnitude spectra and the weighting\\n\\nis done on linear magnitude spectra (sqrt of inputs). Change tf.contrib.distributions docstring examples to use tfd alias\\n\\nrather than ds , bs . Fix docstring typos in tf.distributions.bijectors.Bijector . tf.assert_equal no longer raises ValueError. It now raises\\n\\nInvalidArgumentError, as documented. Update Getting Started docs and API intro.\\n\\nGoogle Cloud Storage (GCS): Add userspace DNS caching for the GCS client. Customize request timeouts for the GCS filesystem. Improve GCS filesystem caching.\\n\\nBug Fixes: Fix bug where partitioned integer variables got their wrong shapes. Before Fix correctness bug in CPU and GPU implementations of Adadelta. Fix a bug in import_meta_graph 's handling of partitioned variables when\\n\\nimporting into a scope. WARNING: This may break loading checkpoints of\\n\\ngraphs with partitioned variables saved after using import_meta_graph with\\n\\na non-empty import_scope argument. Fix bug in offline debugger which prevented viewing events. Added the WorkerService.DeleteWorkerSession method to the gRPC interface,\\n\\nto fix a memory leak. Ensure that your master and worker servers are running\\n\\nthe same version of TensorFlow to avoid compatibility issues. Fix bug in peephole implementation of BlockLSTM cell. Fix bug by casting dtype of log_det_jacobian to match log_prob in\\n\\nTransformedDistribution . Fix a bug in import_meta_graph 's handling of partitioned variables when Ensure tf.distributions.Multinomial doesn't underflow in log_prob .\\n\\nBefore this change, all partitions of an integer variable were initialized\\n\\nwith the shape of the unpartitioned variable; after this change they are\\n\\ninitialized correctly.\\n\\nOther: Add necessary shape util support for bfloat16. Add a way to run ops using a step function to MonitoredSession. Add DenseFlipout probabilistic layer. A new flag ignore_live_threads is available on train. If set to True , it\\n\\nwill ignore threads that remain running when tearing down infrastructure\\n\\nafter successfully completing training, instead of throwing a RuntimeError. Restandardize DenseVariational as simpler template for other probabilistic\\n\\nlayers. tf.data now supports tf.SparseTensor components in dataset elements. It is now possible to iterate over Tensor s. Allow SparseSegmentReduction ops to have missing segment IDs. Modify custom export strategy to account for multidimensional sparse float\\n\\nsplits. Conv2D , Conv2DBackpropInput , Conv2DBackpropFilter now supports arbitrary\\n\\ndilations with GPU and cuDNNv6 support. Estimator now supports Dataset : input_fn can return a Dataset\\n\\ninstead of Tensor s. Add RevBlock , a memory-efficient implementation of reversible residual layers. Reduce BFCAllocator internal fragmentation. Add cross_entropy and kl_divergence to tf.distributions.Distribution . Add tf.nn.softmax_cross_entropy_with_logits_v2 which enables backprop\\n\\nw.r.t. the labels. GPU back-end now uses ptxas to compile generated PTX. BufferAssignment 's protocol buffer dump is now deterministic. Change embedding op to use parallel version of DynamicStitch . Add support for sparse multidimensional feature columns. Speed up the case for sparse float columns that have only 1 value. Allow sparse float splits to support multivalent feature columns. Add quantile to tf.distributions.TransformedDistribution . Add NCHW_VECT_C support for tf.depth_to_space on GPU. Add NCHW_VECT_C support for tf.space_to_depth on GPU.\\n\\n\\n\\nAPI Changes\\n\\nRename SqueezeDims attribute to Axis in C++ API for Squeeze op.\\n\\nattribute to in C++ API for Squeeze op. Stream::BlockHostUntilDone now returns Status rather than bool.\\n\\nnow returns Status rather than bool. Minor refactor: move stats files from stochastic to common and remove\\n\\nstochastic .\\n\\nThanks to our Contributors\\n\\nThis release contains contributions from many people at Google, as well as:\\n\\nAdam Zahran, Ag Ramesh, Alan Lee, Alan Yee, Alex Sergeev, Alexander, Amir H. Jadidinejad,\\n\\nAmy, Anastasios Doumoulakis, Andrei Costinescu, Andrei Nigmatulin, Anthony Platanios,\\n\\nAnush Elangovan, arixlin, Armen Donigian, ArtëM Sobolev, Atlas7, Ben Barsdell, Bill Prin,\\n\\nBo Wang, Brett Koonce, Cameron Thomas, Carl Thomé, Cem Eteke, cglewis, Changming Sun,\\n\\nCharles Shenton, Chi-Hung, Chris Donahue, Chris Filo Gorgolewski, Chris Hoyean Song,\\n\\nChris Tava, Christian Grail, Christoph Boeddeker, cinqS, Clayne Robison, codrut3, concerttttt,\\n\\nCQY, Dan Becker, Dan Jarvis, Daniel Zhang, David Norman, dmaclach, Dmitry Trifonov,\\n\\nDonggeon Lim, dongpilYu, Dr. Kashif Rasul, Edd Wilder-James, Eric Lv, fcharras, Felix Abecassis,\\n\\nFirefoxMetzger, formath, FredZhang, Gaojin Cao, Gary Deer, Guenther Schmuelling, Hanchen Li,\\n\\nHanmin Qin, hannesa2, hyunyoung2, Ilya Edrenkin, Jackson Kontny, Jan, Javier Luraschi,\\n\\nJay Young, Jayaram Bobba, Jeff, Jeff Carpenter, Jeremy Sharpe, Jeroen BéDorf, Jimmy Jia,\\n\\nJinze Bai, Jiongyan Zhang, Joe Castagneri, Johan Ju, Josh Varty, Julian Niedermeier,\\n\\nJxKing, Karl Lessard, Kb Sriram, Keven Wang, Koan-Sin Tan, Kyle Mills, lanhin, LevineHuang,\\n\\nLoki Der Quaeler, Loo Rong Jie, Luke Iwanski, LáSzló Csomor, Mahdi Abavisani, Mahmoud Abuzaina,\\n\\nManHyuk, Marek ŠUppa, MathSquared, Mats Linander, Matt Wytock, Matthew Daley, Maximilian Bachl,\\n\\nmdymczyk, melvyniandrag, Michael Case, Mike Traynor, miqlas, Namrata-Ibm, Nathan Luehr,\\n\\nNathan Van Doorn, Noa Ezra, Nolan Liu, Oleg Zabluda, opensourcemattress, Ouwen Huang,\\n\\nPaul Van Eck, peisong, Peng Yu, PinkySan, pks, powderluv, Qiao Hai-Jun, Qiao Longfei,\\n\\nRajendra Arora, Ralph Tang, resec, Robin Richtsfeld, Rohan Varma, Ryohei Kuroki, SaintNazaire,\\n\\nSamuel He, Sandeep Dcunha, sandipmgiri, Sang Han, scott, Scott Mudge, Se-Won Kim, Simon Perkins,\\n\\nSimone Cirillo, Steffen Schmitz, Suvojit Manna, Sylvus, Taehoon Lee, Ted Chang, Thomas Deegan,\\n\\nTill Hoffmann, Tim, Toni Kunic, Toon Verstraelen, Tristan Rice, Urs KöSter, Utkarsh Upadhyay,\\n\\nVish (Ishaya) Abrams, Winnie Tsang, Yan Chen, Yan Facai (颜发才), Yi Yang, Yong Tang,\\n\\nYoussef Hesham, Yuan (Terry) Tang, Zhengsheng Wei, zxcqwe4906, 张志豪, 田传武\\n\\nWe are also grateful to all who filed issues or helped resolve them, asked and\\n\\nanswered questions, and were part of inspiring discussions.\")"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "('Voices in AI – Episode 35: A Conversation with Lorien Pratt', 'Advertisement\\n\\nIn this episode, Byron and Lorien talk about intelligence, AGI, jobs, and the human genome project.\\n\\n- 0 : 00 0 : 00 0 : 00\\n\\nByron Reese: This is Voices in AI, brought to you by Gigaom, I’m Byron Reese. Today our guest is Lorien Pratt, the Chief Scientist and Co-founder over at Quantellia. They’re a software consulting company in the AI field. She’s the author of “ The Decision Intelligence Primer.” She holds an AB in Computer Science from Dartmouth, and an MS and PhD in Computer Science from Rutgers. Welcome to the show, Lorien!\\n\\nLorien Pratt: Thank you Byron delighted to be here, very honored thank you.\\n\\nSo, Lorien, let’s start with my favorite question, which is, what is artificial intelligence?\\n\\nArtificial intelligence has had an awful lot of definitions over the years. These days when most people say AI, ninety percent of the time they mean machine learning, and ninety percent of the time that machine learning is a neural network underneath.\\n\\nYou say that most people say that, but is that what you mean by it?')\n",
      "('Voices in AI – Episode 27: A Conversation with Josh Sutton', 'AI Minute\\n\\nIn this episode, Byron talks about how people fear artificial intelligence more than they have hope for the benefits it will bring to humans.')\n",
      "('How to Hack an Intelligent Machine', \"This week Microsoft and Alibaba stoked new fears that robots will soon take our jobs. The two companies independently revealed that their artificial intelligence systems beat humans at a test of reading comprehension. The test, known as the Stanford Question Answering Dataset (SQuAD), was designed to train AI to answer questions about a set of Wikipedia articles.\\n\\nLike the image-recognition software already deployed in commercial photo apps, these systems lend the impression that machines have become increasingly capable of replicating human cognition: identifying images or sounds, and now speed reading text passages and spewing back answers with human-level accuracy.\\n\\nMachine smarts, though, are not always what they seem. The tech mavens who develop deep-learning networks and other AI systems are finding out just how fragile their creations are by drilling down to see if the machines really know anything. Stress-testing software—before it is loaded into a self-driving car, for instance—will be crucial to avoid the blunders that could lead to catastrophic accidents. “In some domains neural nets are actually superhuman, like they’re beating human performance,” says Anish Athalye, a Massachusetts Institute of Technology graduate student who researches AI. “But they have this weird property that it seems that we can trick them pretty easily.”\\n\\nTwo preprint articles by Athalye and other students at MIT, collectively known as LabSix, demonstrated they could make a deep-learning system—one trained to recognize objects from thousands of examples—into thinking a picture of a skier was a dog (pdf) and a turtle was a rifle (pdf). A December paper from the Google Brain Team, the company’s AI research contingent, used a different approach to trick the system into classifying a banana as a toaster.\\n\\nPlacing the adversarial patch, resembling a psychedelic toaster, near the image of the banana made the Google image-recognition system identify the content of the picture as a toaster rather than the relevant piece of fruit. Credit: Adversarial Patch, by Tom Brown et al [cs.CV] 27 Dec 2017\\n\\nIn LabSix’s method an algorithm slightly modifies the color or brightness of every pixel in the image. Although the picture looks the same to you or me, these subtle changes cause the system to interpret it as something entirely different. Camouflaging the image modification “makes it more relevant to a real-world attack,” Athalye says. “If you see somebody put up a road sign in the real world that looks all psychedelic, people might think, ‘oh something fishy is going on here,’ and it will be investigated. But if you have something that looks like a speed limit sign to you but your self-driving car thinks it’s something completely different, that’s a much scarier scenario.”\\n\\nWith the toaster, Google Brain took a different tactic. Instead of changing images individually, they wanted to develop a technical foil that could be placed in any scene. This meant creating a new unique image—an adversarial patch—that confuses the deep-learning system and distracts it from focusing on other items. Instead of blending in, the toaster patch needed to stand out. “Given that the patch only has control of pixels within the small circle that it’s in, it turned out that the best way for the patch to fool the classifiers was to become very salient,” Googler Tom Brown wrote in an e-mail. “A traditional adversarial attack changes all the pixels in a single image by a small amount. For the adversarial patch, we change a few pixels by a large amount.”\\n\\nTo work outside a lab, the patch also had to be resilient to the visual noise in the real world. In earlier studies, just changing the orientation or brightness of the altered image could defeat the adversarial technique. A doctored picture of a cat viewed straight on is classified as guacamole, but turn the cat sideways and the system knows it’s looking at a cat again. The toaster patch, by contrast, can be presented in any lighting or orientation and still sabotage the system. “This was more difficult to develop because it meant training the patch in a wide variety of simulated scenes so that we could find a single patch that is successful in all of them,” Brown wrote.\\n\\nAlthough the examples are silly, the potential real-world implications are deadly serious. Athalye speculated an adversarial attack could trick a self-driving car into ignoring a stop sign. Or that it could disguise an x-ray image of a bomb during airport baggage screening. A goal of the research of Athalye and Brown is to help identify weaknesses in the technology before it is deployed.\\n\\nGary Marcus, a professor of psychology at New York University, suggests AI is susceptible to being duped in this way because “the machine doesn't understand the scene as a whole,” he told me. AI can recognize objects but it fails to comprehend what the object is or what it’s used for. It is not “truly understanding the causal relationships between things, truly understanding who’s doing what to whom and why.”\\n\\nAfter the headlines about AI systems acing the reading-comprehension tests, Marcus disparaged the results, saying what the machine was doing had nothing to do with true comprehension. Marcus tweeted: “The SQuAD test shows that machines can highlight relevant passages in text, not that they understand those passages.”\\n\\nInstead of training an AI system on hundreds of thousands of examples, Marcus thinks the field should take its cues from cognitive psychology to develop software with a deeper understanding. Whereas deep learning can identify a dog and even classify its breed from an image it has never seen before, it does not know the person should be walking the dog instead of the dog walking the person. It does not comprehend what a dog really is and how it is supposed to interact with the world. “We need a different kind of AI architecture that’s about explanation, not just about pattern recognition,” Marcus says.\\n\\nUntil it can do that, our jobs are safe—at least for awhile.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Apple sponsoring ‘Machine Vision Conference’ in Israel, will lead discussion on the iPhone X’s True Depth system', 'Apple is slated to play a role in the Israel Machine Vision Conference in Tel Aviv next month. The company is listed as a sponsor for the event, while one of its video engineering leads is slated to lead a presentation…\\n\\nSylvania HomeKit Light Strip\\n\\nApple this year is listed as a “Silver Sponsor” for the event. It’s unclear what sort of investment this represents, but it’s the third-highest sponsorship tier there is. Other sponsors include General Motors, Qualcomm, Intel, and many more. Apple is also credited as an exhibitor.\\n\\nRepresenting Apple at the event is Eitan Hirsh, who leads the company’s depth sensing research and development team. He will give a speech entitled “Depth Sensing @ Apple: True Depth Camera” that offers an overview of the technologies that power the iPhone X True Depth sensor:\\n\\nWe’ll give an overview on Apple’s iPhone X True Depth camera system, its design and capabilities. We’ll also describe algorithmic layers which are used in some of the features it enables, and describe how it can be used by developers.\\n\\nThe Israel Machine Vision Conference, or IMVC, is a conference focused on all types of machine learning and artificial intelligence. Topics include AI, deep learning, autonomous learning, eye tracking, robotics, and much more.\\n\\nThe image processing, computer vision, machine learning and deep learning communities in Israel are world class leaders in these fields, with large and dynamic presence in both industry and academia. IMVC is a unique opportunity for companies from diverse industrial fields to meet with research groups of academic institutions heading machine vision research in Israel.\\n\\nThe event will take place on March 6th at the David Intercontinental Hotel in Tel Aviv. We’ll have coverage of Hirsh’s discussion on the iPhone X True Depth sensor.\\n\\nSubscribe to 9to5Mac on YouTube for more Apple news:')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('We should build a baby-brained artificial intelligence', \"If we want our machines to possess anything approximating human intelligence, maybe we should think about giving them a childhood, too. Christie Hemm Klok\\n\\nAlison Gopnik’s career began with a psychology experiment she now considers ridiculous. Aiming to understand how 15-month-olds connect words with abstract concepts (daddy = caregiver), she decided to visit nine kids once a week for a year. The then \\xadOxford graduate student would record everything they said as part of her dissertation. “It was absurd for a million reasons,” says Gopnik, holed up on a winter Friday in her office at the University of California at Berkeley, where she is a professor of developmental psychology. “If a childhad moved away, if there weren’t any take-aways after the year, or any number of things, all that work would have been gone,” she says, before adding, “I would never allow a \\xadstudent of mine to do anything like that today.” Though her experiment didn’t solve any language-acquisition mysteries, it did overturn her assumptions about childhood learning and intelligence—and it altered her career path. Now her research has drawn the interest of artificial-intelligence scientists who want to adapt her insights to their machine-learning algorithms. What she learned about kids’ smarts while a grad student still holds sway—for her field and possibly theirs. “Instead of thinking about children as these kind of starter adults, I realized they were profoundly different,” says Gopnik, now 62, and with her own grown children and grandchildren. “The way they use words, the meanings they express, the way they express them—none of it matched how adults think or speak.” Today, Gopnik oversees her own cognitive development lab at UC Berkeley, and is the author of several books on early childhood learning and development. She’s a TED alum, a Wall Street Journal columnist, and has attained that singular \\xadintellectual height—crossing over into pop culture, by appearing on shows like Good Morning America and The Colbert Report. Gopnik’s message: Adult cognitive primacy is an illusion. Kids, her research shows, are not proto-adults with fruit-fly-like attention spans, but in fact our occasional superiors. “Children, even very young children,” she says, “are in many ways smarter, more inventive, and better at learning than adults.”\\n\\nThe reason: size and shape matter. \\xadResearch shows that the bulk and structure of a child’s brain confer cognitive strengths and weaknesses. Same goes for adults. For example, a developed prefrontal cortex allows grown-ups to focus, plan, and control our impulses: valuable skills that let us write magazine articles and avoid jail time. But evidence suggests a developed cortex can also make it hard to learn new or surprising concepts and can impede creative thinking. Toddler brains, constantly abuzz with fresh neural connections, are more plastic and adaptive. This makes them bad at remembering to put on pants but surprisingly good at solving abstract puzzles and extracting unlikely \\xadprinciples from extremely small amounts of information. These are handy skills. It turns out a lot of smart people want to think this way—or want to build machines that do. Artificial-intelligence researchers at places like Google and Uber hope to use this unique understanding of the world’s most powerful neural-learning apparatus—the one between a toddler's ears—to create smarter self-driving cars. Coders can create software that beats us at board games, but it’s \\xadharder to apply those skills to a different task—say, traffic-\\xadpattern analysis. Kids on the other hand, are genius at this kind of generalized learning. “It’s not just that they figure out how one game or machine works,” says Gopnik. Once they’ve figured out how your iPhone works, she says, they’re able to take that information and use it to figure out the childproof sliding lock on the front door.\\n\\nBaby Talk Kids, Gopnik tells people, are the R&D unit of our species. Courtesy Ted Talks/Youtube.com\\n\\nCracking the codes of these little code breakers wasn’t Gopnik’s original career plan. As an undergrad, she began studying life’s big problems, toiling in the field of analytic philosophy. Back then, none of her peers pondered the thinking of kids. But Gopnik became \\xadconvinced kids were key to unlocking one of the oldest epistemological queries: How do we know stuff about the world around us? Borrowing the brain-as-computer model, Gopnik sought to ask questions about the software running this little human machine, allowing it to perform complicated functions. “Kids are the ones doing more generalized learning than anybody else,” she says, “so why wouldn’t you want to understand why they’re so good at it?”\\n\\nFields like evolutionary biology, neuroscience, and developmental psychology suggest that we simply have different cognitive strengths and strategies at different stages of our lives.\\n\\nThe advantages of installing a preschool perspective into machines, she says, can be understood by considering two popular, but opposing, AI strategies: bottom-up and top-down learning. The former works the way you expect: Say you want a computer to learn to recognize a cat. With a bottom-up or “deep-learning” strategy, you’d feed it 50,000 photos of furry felines and let it extract statistics from those examples. A top-down strategy, on the other hand, requires just one example of a cat. A system using this strategy takes that single picture, builds a model of “catness” (whiskers, fur, vertical pupils, etc.), and then uses it to try to identify other cats, revising its cat hypothesis as it goes, much like a scientist would. Children employ both methods at once. They’re good at figuring out things and extracting statistics, says Gopnik. And they use that data to come up with new theories and structured pictures of the world. Successfully distilling both knowledge-building approaches into algorithms might produce artificial intelligence that can finally do more than just beat us at Go and recognize animals. It might also, Gopnik hopes, change outmoded ideas that we all seem to share about intelligence. “We still tend to think that a 35-year-old male professor is the ultimate goal of human cognition,” she says, “that everything else is just leading up to or deteriorating from that cognitive peak.”\\n\\nThat model doesn’t make sense for a variety of reasons. Studies from fields like evolutionary biology, neuroscience, and developmental psychology suggest we simply have different cognitive strengths and strategies at different stages of our lives. “Children will have one set of ideas about how people and the world work when they’re 2, and then another set when they’re 3, and another set when they’re 5,” says Gopnik. “It’s like they’re actively trying to think up a coherent picture of the world around them, and then constantly changing that picture based on the observations they make.” That frenetic hypothesis formation—and ongoing reformation—isn’t a bug; it’s a \\xadhighly desired feature. And if we want our machines to possess anything approximating human intelligence, maybe we should think about giving them a childhood too.\\n\\nWant more news like this? Sign up to receive our email newsletter and never miss an update! By submitting above, you agree to our privacy policy.\\n\\nYour brain from cradle to rocking chair We’re born helpless and dumb. As we mature, experience and schooling teach us useful things, and we get woke. Then, year by year, we slip back into feeblemindedness. That’s the picture most of us have of intelligence. Unfortunately, it’s dumb. Research reveals that each period of cognitive development offers learning strategies as well as trade-offs. It’s that combo of aha and duh that actually makes humans truly intelligent. Infant: 0-18 months\\n\\nAn infant brain forms 1 million new neural connections each second, helping her to develop emotions, motor skills, attachments, and working memory. At 11 months, she can already form hypotheses about how the world works. At 18 months, she has a sense of self. Toddler: 2-5 years\\n\\nWhen it comes to learning abstract concepts, preschoolers beat adults. At 4 years old, 66 percent of calories are headed to her brain—fuel for the exploration and creative thinking that define this period. By the time she finishes preschool, her gray matter has quadrupled in size. School-age: 6-11 years\\n\\nThe brain of a 6-year-old has reached 90 percent of its adult size. Neural pruning ramps up as the brain discards unused connections. The prefrontal cortex starts to develop more, resulting in longer attention spans, and an increased reliance on language and logic to learn. Adolescence: 12-24 years\\n\\nAdolescence marks a return to the neural flexibility and plasticity that characterized her preschool years. But she’s not living in a protected context. A reliance on the amygdala—a center for emotions, impulses, and instinctive behaviors—might result in trademark “risk-taking.” Adulthood: 25-59 years\\n\\nBy the time she reaches adulthood, prefrontal control is at its peak. A developed frontal lobe helps her plan for the future and control her impulses, but there’s evidence that creativity and cognitive flexibility takes a big hit. Learning anything surprising? Also a lot harder. Senior: 60+ years\\n\\nBring on short-term-memory loss, neurodegenerative diseases, and declines in conceptual reasoning. Still, other cognitive abilities continue to grow. Skills involving vocabulary, math, verbal comprehension—what’s known as crystallized intelligence—are among them.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Recap: Here are all the new things in the Samsung Galaxy S9 and S9+', 'Now that the dandelions (kudos to Samsung for the Galaxy S3 reference) from Samsung’s Galaxy Unpacked 2018 event have long settled down, we can sit down and look back with a bit of objectivity at what the Korean giant unpacked for us. The two new flagship offerings from Samsung, which may yet again reign as the top Android smartphone(s) of 2018, are the S9 and S9+. While in appearance they do look nearly identical to their predecessors, we\\'ve discovered bit by bit signs of the intricate differences between them and last year\\'s S8 and S8+.\\n\\nSo if you haven\\'t had your share of the Samsung news mill yet or if you prefer to have all your information put together in one page, we have just the thing for you. Read on to see what\\'s new with the Galaxy S9 and S9+.\\n\\nHardware\\n\\nExterior\\n\\nSamsung has carried over its design language from the S8 series, with an all-glass back and a low-bezel display adorning its front. The two sides are held together by a metal frame that seems to be an “upgraded aluminum material” which has been given an oh so sweet satin gloss finish. The frame is supposed to be more resistant to scratches because Samsung is now using the stronger AL7003 aluminum compared to the AL6013 used in the S8. AL7003 allows less shock (1.2x less to be exact) to be transferred to the glass making the phone more durable. Oh, and the frame\\'s color matches the phone’s for a more seamless design.\\n\\nThe back has a single camera on the S9 and a dual setup on the plus model. Both have a better placed fingerprint scanner, which sits under the camera this time. The heart rate sensor, a second ambient light sensor (yes there’s one at the back as well and it was already there on the S8), and dual tone flash follow the status quo by staying next to the camera. Talking about the heart rate sensor, Samsung has upgraded it to a more advanced one that is now able to measure blood pressure. The Korean giant has partnered with the University of California, San Francisco (UCSF) to release the “My BP Lab” app for research on blood pressure and stress (I can’t find this on Google Play yet).\\n\\nThe front has the signature 5.8” or 6.2” Quad HD+ \"Infinity Display\" on the S9 and S9+, respectively. There is an array of sensors on top of the display which are hidden under the glass. The sensors include (from right to left) an RGB LED notification light, iris sensor (emitter), proximity (detector) and light sensor (RGB), proximity (emitter) sensor, front camera, and lastly an iris sensor. The first part of the AKG tuned stereo speakers, that doubles as the earpiece, sits in the middle of the phone above the screen.\\n\\nThe bottom has a 3.5mm headphone jack (has to be mentioned since it\\'s considered a unique selling point nowadays) and the second part of the stereo speakers flanking the USB-C port, which supports fast charging. On the right-hand side, we have the power button and on the opposite side, the dedicated Bixby button below the volume rocker. The Hybrid SIM slot - which supports Nano + Nano or Nano + microSD (up to a whopping 400GB) - sits flush on top of the phone. If you want to get real technical about the dimensions compared to the S8 and S8+, you should know that both the new flagships are slightly shorter and wider than their predecessors but weigh a bit more (8g and 16g more, respectively). Finally, wireless fast charging and IP68 water resistance haven\\'t gone anywhere. Here is a side-by-side comparison chart showing the differences between the S9 and S8 series.\\n\\nDisplay\\n\\nThe moniker of \"Infinity Display\" is here to stay, but this time Samsung stuck a new term with it: \"Uninterrupted Display\" (might be a dig at fruitphone X’s notch). The 18.5:9 Quad HD+ AMOLED display and 5.8”/6.2” sizes carry over from the S8/S8+, but that\\'s where the similarities end. The new display has been given a performance-enhancing injection. On a microscopic level, the display has a Diamond Pixel arrangement with a ppi of 570 for the S9 and 529 for the S9+.\\n\\nSource: DisplayMate\\n\\nSamsung has shifted its focus to optimize and enhance the \"Absolute Picture Quality\" and \"Absolute Color Accuracy” (0.7 JNCD - Just Noticeable Color Difference) of the screens through precision factory display calibration process implementation. DisplayMate stated that the new display is now up to “Outstanding” levels and gave it its highest A+ grade. The display has set a record for color accuracy which is “Visually Indistinguishable From Perfect” for all of the calibrated modes offered (DisplayMate).\\n\\nIt is 20% brighter than the previous generation and shows record small shifts in color and brightness with different viewing angles. The display is also certified by the mobile UHD alliance for Mobile HDR Premium.\\n\\nAlways-On Display has been enhanced as well with four levels of brightness from day to night. Day mode brightness reaches 40 nits, making it readable in indoor lighting (outdoor viewing may require a hand shade) and night mode brightness dives down to as low as 1 nit. This stops you from being blinded in dark environments and does not distract you if you use it as a desk clock. Viewing angles are much better with a modest 29% decrease in brightness at 30 degrees. This is considered to be much better than the 55% or higher drops in brightness that LCD displays exhibit at the same angle.\\n\\nSamsung has also managed to achieve a new record for maximum peak brightness, with auto brightness enabled, of 1,130 nits (a new record for smartphones!) which is needed for HDR content.\\n\\nAll four display modes offered by Samsung (Adaptive, AMOLED Cinema, AMOLED Photo, and Basic) have received DisplayMate\\'s seal of approval for the different scenes the calibrations target. In short, the new AMOLED screen on the S9/S9+ has the largest native color gamut (113% DCI-P3/141% sRGB/Rec. 709), highest contrast ratio - Infinite, lowest screen reflectance of 4.4%, and highest contrast rating in ambient light (257).\\n\\nSource: DisplayMate\\n\\nFor more technical details and tests that were conducted, you can check out DisplayMate’s report.\\n\\nTLDR: MUCH NEW RECORDS! BEST DISPLAY EVER!\\n\\nSoC Showdown - Exynos 9810 vs. Snapdragon 845\\n\\nWith new flagships come new SoCs. Samsung’s baby, the Exynos 9810 is a 64-bit octa-core CPU with 4 M3 Mongoose and 4 A55 ARM cores. The advertised clock speed from Samsung Semiconductor is 2.9 GHz for the M3 cores but Anandtech did some analysis on the MWC demo units and found some interesting results. The M3 cores average at 1.794 GHz when all 4 cores fire and reach a max clock speed of 2.7 GHz only when 1 core is active. The A55 cores are clocked at 1.95 GHz. In the benchmarks conducted, the demo unit showed high gains in CPU performance bringing it close to Apple’s A10 and A11 chip levels. On the GPU side, the Exynos 9810 is bundled with an 18-core Mali G72 GPU clocked at 573 MHz which Anandtech considers “quite conservative”. The GPU was advertised to have a 20% performance improvement over last generation’s Mali G71 which has 20 cores.\\n\\nSource: Anandtech\\n\\nThe demo unit’s GPU showed modest gains in performance when it came to Manhattan 3.1 while it showed performance gains close to the advertised numbers in T-Rex. Since these are results from demo units running demo software, they should not be considered as final results. Do note that the GPU performance is lower than the Snapdragon 845 GPU while the CPU takes a huge leap forward.\\n\\nThe new modem on the Exynos 9810 is a Cat.18, supporting 6CA (6 band carrier aggregation) giving downlink speeds of up to 1.2 Gbps and 2CA uplink speeds up to 200 Mbps.\\n\\nThe Snapdragon 845, destined for the U.S. variants of the S9/S9+, has a lot to offer as well. The new octa-core (A75 and A55 cores in a 4 + 4 configuration) design brings a 25% CPU performance improvement over the 835. There\\'s also a 30% improvement over the previous generation Adreno GPU found in the 835. The new Hexagon DSP is touted to be 2 to 3 times more powerful for AI tasks. The ISP also received an upgrade supporting capturing of HDR video (S9/S9+ may not support this) and the new Cat.18 X20 modem is much faster. It supports 5CA speeds up to 1.2 Gbps for downlink, 150 Mbps for uplink, and true dual-SIM LTE.\\n\\nBoth the Exynos 9810 and Snapdragon 845 are built on 10nm FinFET process for more performance and less power consumption. You can check out our benchmarks of the Snapdragon 845 reference phone compared to the S8, OnePlus 5T, and others to have a better idea of what may be offered.\\n\\nCamera\\n\\nSamsung’s keynote highlighted how the new S9 series’ focal point was the camera. At the keynote, the presenters touted that the company has reimagined the camera and stuck a single 12MP Super Speed Dual Pixel OIS (that was a mouthful…) camera and a dual 12MP (wide+telephoto) SSDP dual OIS camera on the S9 and S9+, respectively, with zero-shutter lag.\\n\\nFor video recording, both devices can capture 4k at 30 fps or 60 fps. They also hav video digital image stabilisation, continuous autofocus, face detection, playback zoom, 1080p 240fps slow-mo, 1080p hyperlapse, and more. The S9 pair also support video recording in HEVC (High Efficiency Video Coding) format. This is given as an optional feature which can be toggled from the camera app: HEVC or H.265 uses close to half the space as the older H.264 compression format.\\n\\nSo what else is different this year? Both the S9 and S9+ have a mechanical aperture which shifts between F/2.4 for well lit and F/1.5 for low light environments. This is a new leap forward for smartphone cameras since mechanical apertures are mostly found in point-and-shoot cameras and DSLRs, so I guess you could give Samsung a pass for its “Camera Reimagined\" tagline since it is the first company to do this on a flagship in recent times.\\n\\nConventional image sensors are usually made of 2 layers, one being a pixel array and the other an analog logic layer. The dual pixel sensor on the S9/S9+ is Samsung’s new ISOCELL Fast 2LS3 which is a high-speed 3-stack CMOS image sensor that has a DRAM slotted in à la Sony. The DRAM is used to power the Super Speed sensor for all that time bending 720 p 960fps slow-mo. The DRAM helps to reduce the “jello-effect” in slow motion videos since it captures the frames at high speeds (Samsung Newsroom).\\n\\nThe front camera on both devices is the same 8.0MP sensor with F/1.7 as the one on the Note8. Do note that Samsung’s selfie beautification mode still smoothens out a lot of details.\\n\\nFor those who trust DxOMark to be an impartial judge for cameras (we put little stock in DxO\\'s results, because of the conflict of interest in reviewing cameras), it has given the S9+ an overall score of 99, crowning it the king of smartphone cameras. The high score puts the S9+ one point ahead of the Google Pixel 2 and two points ahead of the iPhone X. The S9+ received a 104 for the photo category (highest score for any smartphone) and 91 points for video.\\n\\nIn well-lit conditions (1000 lux), the S9+ captures good details but is still a smidge behind the Pixel 2.\\n\\nLeft: Galaxy S9+, Right: Pixel 2XL - Source: DxOMark\\n\\nThe camera is said to excel in low light conditions, thanks to the mechanical F/1.5 aperture, but the Pixel 2, with its HDR+, edges it out in the textures and details department by a tiny margin.\\n\\nLeft: Galaxy S9+, Right: Pixel 2XL - Source: DxOMark\\n\\nThe S9+ killed in the exposure and contrast test with the widest ever dynamic range. It preserves greater detail and is seen to be much better than the Pixel 2. In certain conditions, the S9+ has that typical punchiness added to it which may look unnatural to some.\\n\\nLeft: Galaxy S9+, Right: Pixel 2XL - Source: DxOMark\\n\\nFor those portrait lovers, fear not. The S9+ comes out on top going against the iPhone X and Pixel 2. Including the iPhone X in this comparison would be ideal since it has the hardware - dual cameras - for better portraits while the Pixel 2 depends on software for processing and creating the Bokeh effect.\\n\\nLeft: Galaxy S9+, Middle: Pixel 2XL, Right: iPhone X - Source: DxOMark\\n\\nFinally, the S9+ received a score of 91 for video, putting it in the top tier of smartphones. It wasn\\'t the highest scoring phone in this category but performed admirably in all the tested categories: vibrancy, exposure, autofocus, stabilization, and noise reduction. Loss of fine detail and some artifacts pulled down the total score. Some motions could not be compensated for by the OIS system as well.\\n\\nFor more details and images, check out DxOMark\\'s full report.\\n\\nStorage, Memory, and Battery\\n\\nThe S9 comes in three storage variants starting with the smallest 64GB, followed by 128GB, and finally the monster 256GB. All of them have 4GB of RAM and a 3000 mAh battery. The S9+ comes in the same three storage variants (64GB, 128GB, and 256GB), plus 6GB RAM, and a 3500 mAh battery. Samsung is sticking to its UFS 2.1 storage specification so it should offer high read/write speeds.\\n\\nAs mentioned earlier, both the S9/S9+ support expandable storage up to 400GB so you could get a whopping 656GB with the 256GB variant offered (minus what the OS takes up of course).\\n\\nSoftware\\n\\nThe S9/S9+ come with Android 8.0 Oreo (Treble exists!) and Samsung Experience 9.0 which offers all the same features as the S8/S8+ Oreo release and more.\\n\\nCamera\\n\\nTo differentiate the new flagships from the previous generation, Samsung has included some new features in its camera app. The new super slo-mo mode can shoot up to 960 frames per second at 720p. Hardware limitations restrict the S9/S9+ to record only about 0.2 seconds of real-time footage which translates to a 6-second video. To get around this limitation, Samsung has provided a feature which helps to stitch a few slow-mo videos to form larger clips. It has also made sharing these slow-mo videos easier by giving users the choice to turn the clips into GIFs in one of three modes: on loop, in reverse, or in swing. Users can add soundtracks from their choice of music or from the curated list of 35 built-in options. The slow-mo videos can also be set to display on the home screen and/or lock screen for a bit of personalization. The super slo-mo is hardware dependent so don\\'t expect it to come to the previous generations.\\n\\nThe camera UI has been tweaked for better usability, by putting the different modes offered on a slider at the top (in portrait) or right (in landscape) as shown below.\\n\\nAR Emojis\\n\\nAnd then there are the AR Emojis. Yes, it is going to be a thing this year so you can expect more of these in your WhatsApp chats and groups or preferred messaging platform. The trend began when Apple introduced it with the iPhone X and it is catching on.\\n\\nTo provide a more personal touch, Samsung’s take on AR Emojis creates a digital version of the person to use instead of standard animal or cartoon characters. A single selfie taken can be converted into a 3D avatar which can be customized further (hair/accessories/clothes/facial features) if you think the device didn\\'t do a good job of capturing the real you. After creating your 3D avatar, the algorithm also spits out 18 different pre-set expressions with visual effects. Samsung claims that the S9/S9+’s machine learning algorithm analyses 100 distinct facial features, including eyes, nose, forehead, and cheeks to accurately model your 3D doppelgänger. The demo during the keynote was not bad, but there was a bit of “lag\" in tracking mouth movements when the two Samsung executives were talking to each other with their avatars. It may be pre-production software pains or maybe bad lighting at the keynote, we’ll know more once we review the units. And did I mention that you can set your 3D avatar as Mickey Mouse?\\n\\nBiometrics and Security\\n\\nSamsung debuted the Iris Scanner with the Note 7 and it continued to be included in the S8 series and the Note 8. Using the Iris Scanner and the front camera, the S9/S9+ has a new unlocking method called Intelligent Scan. It uses a combination of facial recognition and iris scanning to provide an easy and secure way to unlock your device. The idea is that facial recognition works in daylight while the phone switches to iris scanning in low light (iris scanning uses IR so it isn’t inhibited by the lack of light). There is also the fingerprint scanner for those who prefer that. All of them can be enabled at the same time giving users more flexibility. The facial recognition is not as robust or secure as Apple\\'s FaceID so iris scanning and fingerprint are used for higher security processes (e.g. Payments/Secure Folder).\\n\\nKnox is still a unique selling point from Samsung’s perspective since Knox has been certified by many government agencies such as the U.S. DoD and UK’s NCSC (more certifications can be found here for those interested) for its beefed up security. Its features have been expanded for Enterprise use with the latest version. So far Samsung has been relatively regular in pushing out monthly Android security patches (along with its own ones) for its flagships, and even for its other ranges, so the company is trying to keep its devices secure. It has also added a new way to access Secure Folder from the lock screen itself by introducing a Dedicated Fingerprint feature. Users can register another fingerprint other than their main one(s) and set it as the Dedicated Fingerprint to unlock Secure Folder directly from the lock screen (similar to Huawei/Honor’s Private Space). Secure Folder can also be unlocked using PIN/Password/Iris Scanning. You can read more about Secure Folder here.\\n\\nSamsung Pay\\n\\nSamsung Pay is still the widest accepted mobile payment service right now and obviously it\\'s included in the S9/S9+. The engineers at Samsung achieved this by integrating an MST (Magnetic Strip Technology) chip alongside the NFC chip so the phone can be used for payments on both contactless terminals or those machines where you need to swipe your card. Contactless is gaining traction in many countries, but a majority of machines are still chip/swipe based and Samsung Pay just works everywhere (I use it on a regular basis with my S7 edge and haven’t had any problems).\\n\\nBixby\\n\\nBixby continues to exist in the S9 series and has grown up a little with real-time AR and deep learning capabilities. It can now do live translation through the camera and features 9 different modes: Text, Food, Makeup, Wine, Shopping, Image, QR Code, and more. The live translation feature can detect 54 languages as of now and even convert foreign currencies, which would be a useful feature for travelers. Bixby can now recognize food (no, not for sharing purposes) and bring up the nutritional content and even recipes of what you’re eating, which you can add to Samsung Health to keep track for a healthier lifestyle. Makeup mode might be an attractive feature for ladies out there. What it does is recognize the type of makeup one is using and provide links to the websites that sell them. Users can even use the AR capabilities to try out different makeups from select brands and place an order for whichever they choose, all through Bixby. This feature is available in select markets: Korea, U.S., Canada, and China (Samsung Newsroom).\\n\\nSpeakers and Other features\\n\\nComing back to the stereo speakers, they are supposed to be 40% louder than the previous generation’s mono speaker. The reason I included this in the software section is because of Dolby Atmos. This is more of a software enhancement that gives users a 360 degree sound effect while using the speakers or headphones. This feature analyzes sounds in real time and adjusts it to create the most immersive experience possible (Samsung Newsroom).\\n\\nFinally, other notable software features, introduced in previous generations, like App Pair, Edge Notifications, Live Focus, Dual Capture, and Always-On Display get honorable mentions here. For VR users, you do not need a new Gear VR headset for the S9 and S9+. The S9\\'s will fit like a glove in the 2017 Gear VR because there is very little difference in dimensions from the S8 series.\\n\\nDeX Pad\\n\\nAlong with the 2 new kids in town, a revamped DeX Pad was also announced. The new Pad expands upon the capabilities that were introduced when the S8 series was released. DeX allows Samsung devices (S8, S8+, Note 8, S9, and S9+ for now) to be transformed into desktops to run Android apps via an external monitor.\\n\\nThe new DeX Pad is flat, unlike the stand-like DeX Station, and it enables the S9/S9+ connected to it to be used as a touchpad or touch keyboard if and when you don’t have external ones handy. The Pad does away with the Ethernet port that the Station had, but now you have the convenience of connecting your headphones while using the Pad so that’s nice. It does come with an HDMI port and 2 USB-A ports. You can read more about the DeX Pad here and about using Linux on the S9/S9+ using DeX here.\\n\\nExpect a more in-depth review once we get our hands on a review unit. In the meantime, you can read David’s first impressions about the S9 and S9+ and if you can\\'t wait until you get your own hands on an S9, you can scratch that itch by downloading the official wallpapers.')"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "('#Flu', \"Forecasting influenza outbreaks before they strike could help officials take early action to reduce related deaths, which total 290,000 to 650,000 worldwide every year. In a recent study, researchers say they have accurately predicted outbreaks up to two weeks in advance—using only the content of social media conversations. The findings could theoretically be used to direct resources to areas that will need them most.\\n\\nA team at the Pacific Northwest National Laboratory in Washington State gathered linguistic cues from Twitter conversations about seemingly non-flu-related topics such as the weather or coffee. Based on this information, the researchers nailed down when and where the next flu outbreaks were likely to occur.\\n\\nThe investigators used a “deep learning” computer model that mimics the layers of neurons and memory capabilities of the human brain. Their algorithm analyzed how Twitter language style, opinions and communication behaviors changed in a given period and how such changes related to later reports of flu outbreaks.\\n\\n“The beauty of the deep-learning model we use is that it considers emotions and linguistic clues over time to predict the future,” says computer scientist Svitlana Volkova, who led the study, which was published last December in PLOS ONE. Previous efforts to forecast flu outbreaks via the Internet—including studies that used Twitter and Wikipedia records and a project called Google Flu Trends—have scanned specifically for flu-related words. In contrast, Volkova's work examined 171 million general tweets and outperformed other models that were based exclusively on word searches or clinical data suggesting an imminent outbreak.\\n\\n“Estimating flu in specific, localized populations pushes the limits of what we thought we could do [with social media], and it opens the door to new possibilities,” says Mark Dredze, a computer scientist at Johns Hopkins University, who was not involved in the new study.\\n\\nEpidemiologist Matthew Biggerstaff of the U.S. Centers for Disease Control and Prevention cautions that we are still in “early days” when it comes to flu forecasting. But researchers are increasingly looking to the Internet to supplement official data, which are limited to a small proportion of actual cases because many infected individuals do not seek medical care. Furthermore, such a tool might one day help identify flu trends in regions where public health data are not available at all.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('An AI That Knows the World Like Children Do', 'If you spend much time with children, you’re bound to wonder how young human beings can possibly learn so much so quickly. Philosophers, going all the way back to Plato, have wondered, too, but they’ve never found a satisfying answer. My five-year-old grandson, Augie, has learned about plants, animals and clocks, not to mention dinosaurs and spaceships. He also can figure out what other people want and how they think and feel. He can use that knowledge to classify what he sees and hears and make new predictions. He recently proclaimed, for example, that the newly discovered species of titanosaur on display at the American Museum of Natural History in New York City is a plant eater, so that means it really isn’t that scary.\\n\\nYet all that reaches Augie from his environment is a stream of photons hitting his retina and disturbances of air contacting his eardrums. The neural computer that sits behind his blue eyes manages somehow to start with that limited information from his senses and to end up making predictions about plant-eating titanosaurs. One lingering question is whether electronic computers can do the same.\\n\\nDuring the past 15 years or so computer scientists and psychologists have been trying to find an answer. Children acquire a great deal of knowledge with little input from teachers or parents. Despite enormous strides in machine intelligence, even the most powerful computers still cannot learn as well as a five-year-old does.\\n\\nFiguring out how the child brain actually functions—and then creating a digital version that will work as effectively—will challenge computer scientists for decades to come. But in the meantime, they are beginning to develop artificial intelligence that incorporates some of what we know about how humans learn.\\n\\nThis Way Up\\n\\nAfter the first burst of enthusiasm in the 1950s and 1960s, the quest for AI languished for decades. In the past few years, though, there have been striking advances, especially in the field of machine learning, and AI has become one of the hottest developments in technology. Many utopian or apocalyptic predictions have emerged about what those advances mean. They have, quite literally, been taken to presage either immortality or the end of the world, and a lot has been written about both these possibilities.\\n\\nI suspect that developments in AI lead to such strong feelings because of our deep-seated fear of the almost human. The idea that creatures might bridge the gap between the human and the artificial has always been deeply disturbing, from the medieval golem to Frankenstein’s monster to Ava, the sexy robot fatale in the 2015 movie Ex Machina.\\n\\nBut do computers really learn as well as humans? How much of the heated rhetoric points to revolutionary change, and how much is just hype? The details of how computers learn to recognize, say, a cat, a spoken word or a Japanese character can be hard to follow. But on closer inspection, the basic ideas behind machine learning are not as baffling as they first seem.\\n\\nOne approach tries to solve the problem by starting with the stream of photons and air vibrations that Augie, and all of us, receives—and that reaches the computer as pixels of a digital image and sound samples of an audio recording. It then tries to extract a series of patterns in the digital data that can detect and identify whole objects in the surrounding world. This so-called bottom-up approach has roots in the ideas of philosophers such as David Hume and John Stuart Mill and psychologists such as Ivan Pavlov and B. F. Skinner, among others.\\n\\nIn the 1980s scientists figured out a compelling and ingenious way to apply bottom-up methods to let computers hunt for meaningful patterns in data. “Connectionist,” or “neural network,” systems take inspiration from the way that neurons convert light patterns at your retina into representations of the world around you. A neural network does something similar. It uses interconnected processing elements, akin to biological cells, to transform pixels at one layer of the network into increasingly abstract representations—a nose or an entire face—as data are crunched at progressively higher layers.\\n\\nNeural-network ideas have gone through a recent revival because of new techniques called deep learning—technology now being commercialized by Google, Facebook and other tech giants. The ever increasing power of computers—the exponential increase in computing capability that is captured by what is known as Moore’s law—also has a part in the new success of these systems. So does the development of enormously large data sets. With better processing capabilities and more data to crunch, connectionist systems can learn far more effectively than we might have once thought.\\n\\nOver the years the AI community has seesawed between favoring these kinds of bottom-up solutions to machine learning and alternative top-down approaches. Top-down approaches leverage what a system already knows to help it learn something new. Plato, as well as so-called rationalist philosophers such as René Descartes, believed in a top-down approach to learning—and it played a big role in early AI. In the 2000s such methods also experienced their own rebirth in the form of probabilistic, or Bayesian, modeling.\\n\\nLike scientists, top-down systems start out by formulating abstract and wide-ranging hypotheses about the world. The systems then make predictions about what the data should look like if those hypotheses are correct. Also like scientists, the systems then revise their hypotheses, depending on the outcome of those predictions.\\n\\nNigeria, Viagra and Spam\\n\\nBottom-up methods are perhaps the most readily understood, so let’s consider them first. Imagine that you are trying to get your computer to separate important messages from the spam that arrives in your in-box. You might notice that spam tends to have certain distinguishing characteristics: a long list of recipient addressees, an originating address in Nigeria or Bulgaria, references to $1-million prizes or perhaps mention of Viagra. But perfectly useful messages might look the same. You don’t want to miss the announcement that you have earned a promotion or an academic award.\\n\\nIf you compare enough examples of spam against other types of e-mails, you might notice that only the spam tends to have qualities that combine in certain telltale ways—Nigeria, for instance, plus a promise of a $1-million prize together spell trouble. In fact, there might be some quite subtle higher-level patterns that discriminate between the spam messages and the useful ones—misspellings and IP addresses that are not at all obvious, for example. If you could detect them, you could accurately filter out the spam—without fear of missing a notice that your Viagra has shipped.\\n\\nCredit: Jen Christiansen; Source: “Human-Level Concept Learning through Probabilistic Program Induction,” by Brenden M. Lake, Ruslan Salakhutdinov and Joshua B. Tenenbaum, in Science, Vol. 350; December 11, 2015 (Bayesian example)\\n\\nBottom-up machine learning can ferret out the relevant clues to solve this kind of task. To do this, a neural network must go through its own learning process. It evaluates millions of examples from huge databases, each labeled as spam or as an authentic e-mail. The computer then extracts a set of identifying features that separate spam from everything else.\\n\\nIn a similar way, the network might inspect Internet images labeled “cat,” “house,” “stegosaurus,” and so on. By extracting the common features in each set of images—the pattern that distinguishes all the cats from all the dogs—it can identify new images of a cat, even if it has never seen those particular images before.\\n\\nOne bottom-up method, called unsupervised learning, is still in its relative infancy, but it can detect patterns in data that have no labels at all. It simply looks for clusters of features that identify an object—noses and eyes, for example, always go together to form a face and differ from the trees and mountains in the background. Identifying an object in these advanced deep-learning networks takes place through a division of labor in which recognition tasks are apportioned among different layers of the network.\\n\\nAn article in 2015 in Nature demonstrated just how far bottom-up methods have come. Researchers at DeepMind, a company owned by Google, used a combination of two different bottom-up techniques—deep learning and reinforcement learning—in a way that enabled a computer to master Atari 2600 video games. The computer began knowing nothing about how the games worked. At first, it made random guesses about the best moves while receiving constant feedback about its performance. Deep learning helped the system identify the features on the screen, and reinforcement learning rewarded it for a high score. The computer achieved a high proficiency level with several games; in some cases, it performed better than expert human players. That said, it also completely bombed on other games that are just as easy for humans to master.\\n\\nThe ability to apply AI to learn from large data sets—millions of Instagram images, e-mail messages or voice recordings—allows solutions to problems that once seemed daunting, such as image and speech recognition. Even so, it is worth remembering that my grandson has no trouble at all recognizing an animal or responding to a spoken query even with much more limited data and training. Problems that are easy for a human five-year-old are still extremely perplexing to computers and much harder than learning to play chess.\\n\\nComputers that learn to recognize a whiskered, furry face often need millions of examples to categorize objects that we can classify with just a few. After extensive training, the computer might be able to identify an image of a cat that it has never seen before. But it does so in ways that are quite different from human generalizations. Because the computer software reasons differently, slipups occur. Some cat images will not be labeled as cats. And the computer may incorrectly say an image is a cat, although it is actually just a random blur, one that would never fool a human observer.\\n\\nAll the Way Down\\n\\nThe other approach to machine learning that has transformed AI in recent years works in the opposite direction, from the top down. It assumes that we can get abstract knowledge from concrete data because we already know a lot and especially because the brain is already capable of understanding basic abstract concepts. Like scientists, we can use those concepts to formulate hypotheses about the world and make predictions about what data (events) should look like if those hypotheses are right—the reverse of trying to extract patterns from the raw data themselves, as in bottom-up AI.\\n\\nThis idea can best be illustrated by revisiting the spam plague through considering a real case in which I was involved. I received an e-mail from the editor of a journal with a strange name, referring specifically to one of my papers and proposing that I write an article for the publication. No Nigeria, no Viagra, no million dollars—the e-mail had none of the common indications of a spam message. But by using what I already knew and thinking in an abstract way about the process that produces spam, I could figure out that this e-mail was suspicious.\\n\\nTo start, I knew that spammers try to extract money from people by appealing to human greed—and academics can be as greedy to publish as ordinary folks are for $1-million prizes or better sexual performance. I also knew that legitimate “open access” journals have started covering their costs by charging authors instead of subscribers. Also, my work has nothing to do with the journal title. Putting all that together, I produced a plausible hypothesis that the e-mail was trying to sucker academics into paying to “publish” an article in a fake journal. I could draw this conclusion from just one example, and I could go on to test my hypothesis further by checking the editor’s bona fides through a search-engine query.\\n\\nA computer scientist would call my reasoning process a “generative model,” one that is able to represent abstract concepts, such as greed and deception. This same model can also describe the process that is used to come up with a hypothesis—the reasoning that led to the conclusion that the message might be an e-mail scam. The model lets me explain how this form of spam works, but it also lets me imagine other kinds of spam or even a type that differs from any I have seen or heard about before. When I receive the e-mail from the journal, the model lets me work backward—tracing step by step why it must be spam.\\n\\nGenerative models were essential in the first wave of AI and cognitive science in the 1950s and 1960s. But they also had limitations. First, most patterns of evidence might, in principle, be explained by many different hypotheses. In my case, it could be that the e-mail really was legitimate, even though it seemed unlikely. Thus, generative models have to incorporate ideas about probability, one of the most important recent developments for these methods. Second, it is often unclear where the basic concepts that make up generative models come from. Thinkers such as Descartes and Noam Chomsky suggested that you are born with them firmly in place, but do you really come into this world knowing how greed and deception lead to cons?\\n\\nBayesian models—a prime example of a recent top-down method—attempt to deal with both issues. Named after 18th-century statistician and philosopher Thomas Bayes, they combine generative models with probability theory using a technique called Bayesian inference. A probabilistic generative model can tell you how likely it is that you will see a specific pattern of data if a particular hypothesis is true. If the e-mail is a scam, it probably appeals to the greed of the reader. But of course, a message could appeal to greed without being spam. A Bayesian model combines the knowledge you already have about potential hypotheses with the data you see to let you calculate, quite precisely, just how likely it is that an e-mail is legitimate or spam.\\n\\nThis top-down method fits better than its bottom-up counterpart with what we know about how children learn. That is why, for the past 15 years, my colleagues and I have used Bayesian models in our work on child development. Our laboratory and others have used these techniques to understand how children learn about cause-and-effect relationships, predicting how and when youngsters will develop new beliefs about the world and when they will change the beliefs they already have.\\n\\nBayesian methods are also an excellent way to teach machines to learn like people. In 2015 Joshua B. Tenenbaum of the Massachusetts Institute of Technology, with whom I sometimes collaborate, Brenden M. Lake of New York University and their colleagues published a study in Science. They designed an AI system that could recognize unfamiliar handwritten characters, a job that is simple for people but extremely taxing for computers.\\n\\nThink of your own recognition skills. Even if you have never seen a character in a Japanese scroll, you can probably tell if it is the same or different from one on another scroll. You can probably draw it and even design a fake Japanese character—and understand as well that it looks quite different from a Korean or Russian character. That is just what Tenenbaum’s team members got their software to do.\\n\\nWith a bottom-up method, the computer would be presented with thousands of examples and would use the patterns found in those examples to identify new characters. Instead the Bayesian program gave the machine a general model of how to draw a character: for example, a stroke can go right or left. And after the software finishes one character, it goes on to the next.\\n\\nWhen the program saw a given character, it could infer the sequence of strokes that were needed to draw it, and it went on to produce a similar set of strokes on its own. It did so the same way that I inferred the series of steps that led to my dubious spam e-mail from the journal. Instead of weighing whether a marketing scam was likely to lead to that e-mail, Tenenbaum’s model guessed whether a particular stroke sequence was likely to produce the desired character. This top-down program worked much better than deep learning applied to exactly the same data, and it closely mirrored the performance of human beings.\\n\\nA Perfect Marriage\\n\\nThese two leading approaches to machine learning—bottom up and top down—have complementary strengths and weaknesses. With a bottom-up method, the computer does not need to understand anything about cats to begin with, but it does need a great deal of data.\\n\\nThe Bayesian system can learn from just a few examples, and it can generalize more widely. This top-down approach, though, requires a lot of work up front to articulate the right set of hypotheses. And designers of both types of systems can run into similar hurdles. The two approaches work only on relatively narrow and well-defined problems, such as recognizing written characters or cats or playing Atari games.\\n\\nChildren do not labor under the same constraints. Developmental psychologists have found that young children somehow combine the best qualities of each approach—and then take them much further. Augie can learn from just one or two examples, the way a top-down system does. But he also somehow extracts new concepts from the data themselves, like a bottom-up system. These concepts were not there to begin with.\\n\\nAugie can actually do much more. He immediately recognizes cats and tells letters apart, but he can also make creative and surprising new inferences that go far beyond his experience or background knowledge. He recently explained that if an adult wants to become a child again he or she should try not eating any healthy vegetables, because they make a child grow into an adult. We have almost no idea how this kind of creative reasoning emerges.\\n\\nWe should recall the still mysterious powers of the human mind when we hear claims that AI is an existential threat. Artificial intelligence and machine learning sound scary. And in some ways, they are. The military is researching ways to use these systems to control weapons. Natural stupidity can wreak far more havoc than artificial intelligence, and we humans will need to be much smarter than we have been in the past to properly regulate the new technologies. Moore’s law is an influential force: even if advances in computing result from quantitative increases in data and computer power, rather than conceptual revolutions in our understanding of the mind, they can still have momentous, practical consequences. That said, we shouldn’t think that a new technological golem is about to be unleashed on the world.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Intelligent to a Fault: When AI Screws Up, You Might Still Be to Blame', 'Artificial intelligence is already making significant inroads in taking over mundane, time-consuming tasks many humans would rather not do. The responsibilities and consequences of handing over work to AI vary greatly, though; some autonomous systems recommend music or movies; others recommend sentences in court. Even more advanced AI systems will increasingly control vehicles on crowded city streets, raising questions about safety—and about liability, when the inevitable accidents occur.\\n\\nBut philosophical arguments over AI’s existential threats to humanity are often far removed from the reality of actually building and using the technology in question. Deep learning, machine vision, natural language processing—despite all that has been written and discussed about these and other aspects of artificial intelligence, AI is still at a relatively early stage in its development. Pundits argue about the dangers of autonomous, self-aware robots run amok, even as computer scientists puzzle over how to write machine-vision algorithms that can tell the difference between an image of a turtle and that of a rifle.\\n\\nStill, it is obviously important to think through how society will manage AI before it becomes a really pervasive force in modern life. Researchers, students and alumni at Harvard University’s Kennedy School of Government launched The Future Society for that very purpose in 2014, with the goal of stimulating international conversation about how to govern emerging technologies—especially AI. Scientific American spoke with Nicolas Economou, a senior advisor to The Future Society’s Artificial Intelligence Initiative and CEO of H5, a company that makes software to aid law firms with pretrial analysis of electronic documents, e-mails and databases—also known as electronic discovery. Economou talked about how humans might be considered liable (even if a machine is calling the shots), and about what history tells us regarding society’s obligation to make use of new technologies once they have been proved to deliver benefits such as improved safety.\\n\\n[An edited transcript of the conversation follows.]\\n\\nWhat are your main concerns about AI?\\n\\nI’m a political scientist by training as well as an entrepreneur who has advocated for the progressive adoption of AI in the legal system. So I’m a big believer that AI can be a force for good. But I think that it needs to be governed, because it does pose risks. People talk about risk in different ways, but I’m most interested in the risk that involves surrendering to machines decisions that affect our rights, liberty or freedom of opportunity. We make decisions not just based on rational thinking but also values, ethics, morality, empathy and a sense of right and wrong—all things that machines don’t inherently have. In addition, people can be held accountable for their decisions in ways that machine cannot.\\n\\nWho should be held accountable for AI’s decisions?\\n\\nThat gets at the issue of competence. In entirely autonomous AI systems, you could say: the manufacturer. But most AI today is not autonomous; it relies on a human operator. If that person is not knowledgeable enough to use AI properly when making important decisions in, say, medicine, the law or financial services, should the person be held accountable for errors? Consider the [2016] case State v. Loomis, where a judge relied in part on a black-box, secret algorithm to assess whether a defendant was at risk for recidivism. The algorithm assessed [Loomis] to be a high risk but the methodology that the AI used to produce the assessment was not disclosed to the court or the defendant. The judge factored that recommendation in coming up with a six-year prison sentence. The U.S. Supreme Court declined to hear that case, so it is now the law of the land. You can now be given a long prison sentence because, in part, of an AI algorithm’s assessment, without having much recourse. Was the judge competent to understand whether the algorithmic assessment was adequate and backed by sound empirical evidence? The answer is probably no, because judges and lawyers are not usually trained scientists.\\n\\nHow does one develop a transparent AI system that most people can understand?\\n\\nIt’s true that AI could have all the transparency in the world, but we as citizens couldn’t make heads or tails of what AI is doing. Maybe scientists can understand it, but to be empowered as citizens we need to know whether something is going to work in the real world. One model to consider is the automobile. The average person could watch a car being built from beginning to end and still not know whether the car is safe to drive. Instead, you trust that a car is safe because you consult the ratings provided by the Insurance Institute for Highway Safety, which crashes cars every day to determine how safe they are. [Therefore], as a citizen I now have information that I can use to assess a very complicated sociotechnical system that involves technology and human intelligence. I have very simple metrics that tell me whether a car is safe. Whereas transparency into algorithms is helpful, knowing if they are effective in the real-world applications for which they are intended—often in the hands of human operators—is the key.\\n\\nWhen people are informed about AI risks, does that shift accountability or liability to the person using the AI?\\n\\nLiability is a huge legal question. With self-driving vehicles, for example, you can look at how much control the driver has over the car. If the driver has none, then you’d expect the manufacturer or the other companies involved in putting together the car to have more liability and responsibility. It gets more complicated when the driver has more control, and you might look at who made the decision that led to a crash. There are a couple of interesting questions related to liability. Sticking with cars as the example, let’s presume [hypothetically] that if everyone were being driven in an autonomous car, we’d reduce traffic-related deaths by 20,000 per year. If that were the case, then the public policy goal would be to encourage people to use autonomous cars. But at the same time people are scared of the technology. So you could imagine a couple of ways of supporting your policy goal in order to save those 20,000 lives. One might be to design autonomous vehicles to prioritize the safety of their occupants over pedestrians and other drivers. In other words, you’re safer inside the car than outside of it. This would encourage people to overcome their fear of autonomous cars, thus supporting your policy goal. But society would then be assigning higher value to some lives—those inside those vehicles—than to others.\\n\\nAnother way to encourage people to use self-driving cars is, essentially, to argue that it’s irresponsible to drive a conventional car if you know that an AI-driven vehicle is safer. There’s a case from the 1930s known as the T. J. Hooper case, in which two barges were lost in a storm in part because the company that owned the boats did not equip them with radios. The decision was, if manifestly effective new technology is developed, then it is imperative to use it as a precaution. Eventually, would an automobile driver be more liable if that person chooses to drive rather than getting into statistically safer self-driving cars?\\n\\nHow can public policy be developed for AI when the technology itself is still evolving and being defined?\\n\\nI’m not sure it’s very helpful to define AI. We still don’t have a universally accepted definition of what intelligence is, so it would be hard to do that for artificial intelligence. The norms that govern AI use should be broad enough that they accept innovations from wherever they come. But they should be narrow enough to provide meaningful constraints in how it is used and how it affects people. An effective process would have four layers: You start with values. What do we want AI to do for us?; From there you go to ethical principles. How should AI go about doing its work?; Then you can form public policy recommendations; and finally look at actual technical controls needed to implement that policy.')"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Intelligent to a Fault: When AI Screws Up, You Might Still Be to Blame', 'Artificial intelligence is already making significant inroads in taking over mundane, time-consuming tasks many humans would rather not do. The responsibilities and consequences of handing over work to AI vary greatly, though; some autonomous systems recommend music or movies; others recommend sentences in court. Even more advanced AI systems will increasingly control vehicles on crowded city streets, raising questions about safety—and about liability, when the inevitable accidents occur.\\n\\nBut philosophical arguments over AI’s existential threats to humanity are often far removed from the reality of actually building and using the technology in question. Deep learning, machine vision, natural language processing—despite all that has been written and discussed about these and other aspects of artificial intelligence, AI is still at a relatively early stage in its development. Pundits argue about the dangers of autonomous, self-aware robots run amok, even as computer scientists puzzle over how to write machine-vision algorithms that can tell the difference between an image of a turtle and that of a rifle.\\n\\nStill, it is obviously important to think through how society will manage AI before it becomes a really pervasive force in modern life. Researchers, students and alumni at Harvard University’s Kennedy School of Government launched The Future Society for that very purpose in 2014, with the goal of stimulating international conversation about how to govern emerging technologies—especially AI. Scientific American spoke with Nicolas Economou, a senior advisor to The Future Society’s Artificial Intelligence Initiative and CEO of H5, a company that makes software to aid law firms with pretrial analysis of electronic documents, e-mails and databases—also known as electronic discovery. Economou talked about how humans might be considered liable (even if a machine is calling the shots), and about what history tells us regarding society’s obligation to make use of new technologies once they have been proved to deliver benefits such as improved safety.\\n\\n[An edited transcript of the conversation follows.]\\n\\nWhat are your main concerns about AI?\\n\\nI’m a political scientist by training as well as an entrepreneur who has advocated for the progressive adoption of AI in the legal system. So I’m a big believer that AI can be a force for good. But I think that it needs to be governed, because it does pose risks. People talk about risk in different ways, but I’m most interested in the risk that involves surrendering to machines decisions that affect our rights, liberty or freedom of opportunity. We make decisions not just based on rational thinking but also values, ethics, morality, empathy and a sense of right and wrong—all things that machines don’t inherently have. In addition, people can be held accountable for their decisions in ways that machine cannot.\\n\\nWho should be held accountable for AI’s decisions?\\n\\nThat gets at the issue of competence. In entirely autonomous AI systems, you could say: the manufacturer. But most AI today is not autonomous; it relies on a human operator. If that person is not knowledgeable enough to use AI properly when making important decisions in, say, medicine, the law or financial services, should the person be held accountable for errors? Consider the [2016] case State v. Loomis, where a judge relied in part on a black-box, secret algorithm to assess whether a defendant was at risk for recidivism. The algorithm assessed [Loomis] to be a high risk but the methodology that the AI used to produce the assessment was not disclosed to the court or the defendant. The judge factored that recommendation in coming up with a six-year prison sentence. The U.S. Supreme Court declined to hear that case, so it is now the law of the land. You can now be given a long prison sentence because, in part, of an AI algorithm’s assessment, without having much recourse. Was the judge competent to understand whether the algorithmic assessment was adequate and backed by sound empirical evidence? The answer is probably no, because judges and lawyers are not usually trained scientists.\\n\\nHow does one develop a transparent AI system that most people can understand?\\n\\nIt’s true that AI could have all the transparency in the world, but we as citizens couldn’t make heads or tails of what AI is doing. Maybe scientists can understand it, but to be empowered as citizens we need to know whether something is going to work in the real world. One model to consider is the automobile. The average person could watch a car being built from beginning to end and still not know whether the car is safe to drive. Instead, you trust that a car is safe because you consult the ratings provided by the Insurance Institute for Highway Safety, which crashes cars every day to determine how safe they are. [Therefore], as a citizen I now have information that I can use to assess a very complicated sociotechnical system that involves technology and human intelligence. I have very simple metrics that tell me whether a car is safe. Whereas transparency into algorithms is helpful, knowing if they are effective in the real-world applications for which they are intended—often in the hands of human operators—is the key.\\n\\nWhen people are informed about AI risks, does that shift accountability or liability to the person using the AI?\\n\\nLiability is a huge legal question. With self-driving vehicles, for example, you can look at how much control the driver has over the car. If the driver has none, then you’d expect the manufacturer or the other companies involved in putting together the car to have more liability and responsibility. It gets more complicated when the driver has more control, and you might look at who made the decision that led to a crash. There are a couple of interesting questions related to liability. Sticking with cars as the example, let’s presume [hypothetically] that if everyone were being driven in an autonomous car, we’d reduce traffic-related deaths by 20,000 per year. If that were the case, then the public policy goal would be to encourage people to use autonomous cars. But at the same time people are scared of the technology. So you could imagine a couple of ways of supporting your policy goal in order to save those 20,000 lives. One might be to design autonomous vehicles to prioritize the safety of their occupants over pedestrians and other drivers. In other words, you’re safer inside the car than outside of it. This would encourage people to overcome their fear of autonomous cars, thus supporting your policy goal. But society would then be assigning higher value to some lives—those inside those vehicles—than to others.\\n\\nAnother way to encourage people to use self-driving cars is, essentially, to argue that it’s irresponsible to drive a conventional car if you know that an AI-driven vehicle is safer. There’s a case from the 1930s known as the T. J. Hooper case, in which two barges were lost in a storm in part because the company that owned the boats did not equip them with radios. The decision was, if manifestly effective new technology is developed, then it is imperative to use it as a precaution. Eventually, would an automobile driver be more liable if that person chooses to drive rather than getting into statistically safer self-driving cars?\\n\\nHow can public policy be developed for AI when the technology itself is still evolving and being defined?\\n\\nI’m not sure it’s very helpful to define AI. We still don’t have a universally accepted definition of what intelligence is, so it would be hard to do that for artificial intelligence. The norms that govern AI use should be broad enough that they accept innovations from wherever they come. But they should be narrow enough to provide meaningful constraints in how it is used and how it affects people. An effective process would have four layers: You start with values. What do we want AI to do for us?; From there you go to ethical principles. How should AI go about doing its work?; Then you can form public policy recommendations; and finally look at actual technical controls needed to implement that policy.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Web-Powered Augmented Reality: a Hands-On Tutorial', 'Web-Powered Augmented Reality: a Hands-On Tutorial\\n\\nA Guided Journey Into the Magical Worlds of ARCore, A-Frame, 3D Programming, and More!\\n\\nThere’s been a lot of cool stuff happening lately around Augmented Reality (AR), and since I love exploring and having fun with new technologies, I thought I would see what I could do with AR and the Web — and it turns out I was able to do quite a lot!\\n\\nMost AR demos are with static objects, like showing how you can display a cool model on a table, but AR really begins to shine when you start adding in animations!\\n\\nWith animated AR, your models come to life, and you can then start telling a story with them.\\n\\nBut before we dive into code (and there will be lots of code, I promise!), let me tell you a little bit more about how I got into AR in the first place.\\n\\nEarly Fun with Augmented Reality\\n\\nMy first dip into AR was in 2012, when Tom Teman and I built SoundTracker, an experience where the participants would move inside a room, and the music would change based on their positions within the room. For position tracking, we used QR-Code like markers along with 8 tablets that constantly filmed the room, and we used Qualcomm’s Vufuria AR Platform to figure out the coordinates. It wasn’t AR per-se, but it was built on top of AR marker-based positioning technology.\\n\\nThen, in 2015 I got a Project Tango tablet — Google’s experimental technology for building Augmented Reality experiences. The ability to place virtual objects in the real space, and have them stick in place even when you move around, seemed to me like we were diving down the uncanny valley, where the boundaries between the physical world and the game were beginning to blur. This was the first time I experience AR without the need for markers or special props — it just worked out of the box, everywhere.\\n\\nAnd yet, the magic quickly began to lose effect: even though project Tango had a lot of potential for building amazing experiences, the tablet was not a very common product — in fact, there were only 3,000 units sold worldwide during 2015. This meant that even if I did build something out of the ordinary, I would be able to share it with almost nobody.\\n\\nFast forward a few years, and Google announced (just a few weeks ago) that Project Tango was deprecated and would be shut down shortly. The little bit of sadness I felt quickly changed to joy — as I heard the reason was that the same technology was coming to Android under the name “ARCore.” It’ll be available on flagship Android phones starting with the Samsung Galaxy S8 and the Google Pixel family.\\n\\nIn Project Tango, the hardware had a special Infrared and fisheye-lens cameras, which were used to assist with the depth perception and motion tracking. However, with the new ARCore technology, these special cameras are no longer needed: advancements in deep learning and tracking technologies, made it possible to achieve a good enough level of motion tracking and depth perception using just a single RGB camera.\\n\\nTo make it even better, unlike the Tango ,where you had to use Unity, Java or C for building the experience, with ARCore you can actually build using Web technologies — which is exactly what we are going to do here today!\\n\\nAR on the Web\\n\\nAR is just starting to come to the web. The first time I had an AR-like Web experience was when Konrad Dzwinel created his JavaScript Face Tracking Demo, which allowed you to set your head on fire:\\n\\nMy first AR-Experience, back in August 2014, was burning hot!\\n\\nThis was fun!\\n\\nThen, in early 2017, Jerome Etienne released AR.js — which brought marker-based AR to the web. After printing your own based marker, you could detect its position in the camera’s video stream, and lay your own 3D-models and creatures on top of it. It had decent performance, and it was possible to achieve 60-FPS on most modern smartphones. It was entirely built on top of standard Web technologies, right from your browser, requiring no plugins or whatsoever. Just like what got me so excited about Web Bluetooth.\\n\\nA few months later, AR.js added support for Project Tango, but you could only use it with a very specific build of chrome (and of course, you’d need to have a Tango device). It was the first time, to best of my knowledge, you could build a marker-free AR experience on the web. Finally, things were starting to be very interesting.\\n\\nNowadays, Google is working on bringing ARCore to the Web, using an experimental web browser, called WebARonARCore (pardon the tongue-twisting name). There is also an iOS version, similarly called WebAROnARKit. Mozilla has also recently joined the party, and has just released the WebXR Viewer iOS app. We’ll get to them soon.\\n\\nAll of this gets very exciting and fun when you combine them with a little technology called A-Frame.\\n\\nA-Frame\\n\\nA-Frame is an emerging technology from Mozilla, which allows you to create 3D Scenes and Virtual Reality experiences with just a few HTML tags. It’s built on top of WebGL, Three.js and Custom Elements, a part of the emerging HTML Components standard.\\n\\nI first heard about A-Frame from Madlaina Kalunder, who taught me a lot about modeling 3D experiences for the Web, and I immediately got hooked by its simplicity. This is all the code it takes to display some 3D objects on your screen:\\n\\nYes, just a few lines of HTML code; that’s it: not even a single line of JavaScript! Now compare it with programming WebGL directly. Can you see how big the difference is?\\n\\nHello, A-Frame World!\\n\\nA-Frame has a growing ecosystem of plug ins, which make it even more powerful while keeping the syntax simple (even I created one). One of these plugins adds WebAR support — as we will see next.\\n\\nBuilding Our AR Experience\\n\\nThis is where the real fun begins.\\n\\nIf you’d like to follow along, you will need a supported phone, as well as WebARonARCore (for Android users) or WebARonARKit (iOS users). You will also need to run a web-server and to serve your HTML files to the mobile device.\\n\\nFor development, I personally use live-server as a simple HTTP server with built-in live reloading, and then I connect my Android device through a USB cable, and expose my server to the device via the chrome://inspect, where I enable the Port Forwarding option for port 8080. Then, on my Android device, I simply need to open the WebARonARCore browser and go to http://localhost:8080.\\n\\nAdding WebAR support is very easy — We need to load three.ar.js and aframe-ar, and add the ar attribute to our scene. Here is what the ar-enabled A-Frame “Hello world” program looks like now:\\n\\nApart from adding the two script tags at the top, and the ar attribute to our a-scene element (line 5) not much has changed — just some tinkering with positions / dimensions of the objects, and removing the a-sky tag which provided the background (if we use it here, it will hide the camera feed, rendering AR pretty useless).\\n\\nAnd we get the same exhibition as before — a box, cylinder and a sphere, but this time, on top of the real world:\\n\\nA-Frame AR Hello-World, right on my Desktop\\n\\nYou may have to walk around a bit with the phone and rotate it to various angles to find the virtual objects, but once you have found them, the will stay more-or-less fixed at the same position, and you will be able to look at them from all different angles.\\n\\nSo we can place objects in the real world, walk around them — but this gets boring really fast. How about adding some interactivity?\\n\\nAnimating and Interacting with the Augmented Web\\n\\nFirst thing’s first: let’s replace the Hello World objects with something less static.\\n\\nWe’ll use an animated model called “Cesium Man”, which is readily available from the glTF sample repository. glTF is a proposed standard for 3D model format, using JSON. It can be easily loaded into A-Frame:\\n\\nYou can download the Cesium Man glTF model from here, or just put directly the link to the file in place of CelsiumMan.glb in the sample above (line 2).\\n\\nThe Cesium Man\\n\\nOnce you added it to the scene, you will probably notice the model is still static. This is because we haven’t told it to move yet!\\n\\nThis can be easily solved using the aframe-animation-mixer, which lets us activate and control the animations in our models. We just need to add the appropriate script tag at the top, and animation-mixer attribute to our model’s a-entity tag (line 4)…\\n\\n…and the small guy should start moving, just like in the GIF below.\\n\\nIs Cesium Man walking on the screen, or “in the world?” That’s a part of the magic of AR — it’s hard to tell! (Though it is “in the world,” in case you were wondering :-)\\n\\nNow, having a 3D-model walking in AR in quite an achievement already, but wouldn’t it be more fun if we could tell in where to go instead of having him walk in the same spot all day?\\n\\nFinally we’ve reached the place where we can’t get away with just HTML anymore and need to add some JavaScript logic. This is where our code starts to become aware of the surroundings and their shape.\\n\\nFirst, we will add a code to display a cursor which will indicate to the user whether we recognized a surface at the point he is looking at. We will start by gaining control of the camera:\\n\\n<a-camera ar-raycaster raycaster cursor=\"fuse:false\"> </a-camera>\\n\\nand a small ring that will serve as a cursor:\\n\\n<a-ring id=\"cursor\" radius-inner=\"0.1\" radius-outer=\"0.2\"\\n\\ncolor=\"teal\" position=\"0 -0.8 -5\" rotation=\"-90 0 0\">\\n\\n</a-ring>\\n\\nAt this stage, then ring should appear just below the cesium man:\\n\\nNext, we want to display the ring where the user is looking at, to indicate the target point where our Cesium man will go. For that, we will use a raycaster.\\n\\nYou were maybe wondering what that raycaster attribute that we used in the camera above was all about. Basically, a raycaster sends a virtual beam from a point on the screen and checks if it hits any object. In this case, we are using an AR raycaster — which will shoot a single beam from the center of your screen, and see if it hits any plane in the real world (as far as the phone could tell). If it does, we will get the exact coordinates of the point where the beam hit the plane in the virtual 3D world — which we will use as the position for the ring (and eventually as a target for the Cesium man to walk, but first things first…).\\n\\nTo rephrase — our raycaster will find where the center of the screen falls in the real world, and return it in the virtual coordinate system used by A-Frame so that when we place an object there, it will appear as it is in the exact same location as the center of the screen. This may not sound impressive — but you’ll start to appreciate the magic when you move the phone around and realize that the object you just placed stays at the same spot in the real world, just where you put it.\\n\\nBasically, the code waits for A-Frame to load and set up the scene, and then the interesting part happens at line 5: we listen for Raycaster-Intersection events, and then whenever the virtual beam from the center of the phone screen hits a plane in the real world, we update the position to the cursor to the position of the hit (line 6).\\n\\nWhen you run this code, start walking around, and you should see how the ring sticks to planes — floor, tables, etc.\\n\\nThe ring will be quite large — we set the outer radius to 0.2, and you may want to note that A-Frame uses Meters for units. Now that the position of the ring is synchronized with the physical world, it should have a radius of about 20cm (so a diameter of 40cm), no matter where you put it.\\n\\nThe 3D engine will take care of scaling it to keep it in proportion with the real world objects. So on my desk, this is what it looks like:\\n\\nNext step — let’s move the Cesium Man to where the cursor is at, whenever we click on the screen. For that, we’ll add a click listener to the raycaster and update our model’s position to the last intersection point whenever the user taps the screen:\\n\\nNow the Cesium man appears where the ring is (and becomes enormous) as soon as you tap the screen:\\n\\nBefore we go on, let’s make the cesium man smaller. If you using the development environment I recommended above, your phone is probably cabled to your laptop, so it’s much easier to have the objects small enough to fit on a desk. Change the cesium man scale to 0.15 0.15 0.15 and the radius sizes for the ring to 0.03 and 0.02 for the outer and inner radius, respectively. This should make the model around 20cm tall — still impressive, but small enough to move around your desktop.\\n\\nSo far we have an animated 3D-model, and we can teleport it around between different points in the real world. While I think teleportation is definitely cool, this model has a walking animation — so why not make it walk to the target position?\\n\\nTo achieve that, we will use A-Frame’s animation system, which lets us animate any attribute of our model. In this case, we will simply animate the position:\\n\\nBasically, we use the firstTime variable to keep track whether it’s the first tap or not. For the first tap, we do as before — just set the position of the model to the raycaster’s intersection point (line 9). For any subsequent tap, we create an a-animation element, asking it to animate the position attribute to the target position (line 14). We have to call A-Frame’s stringify method, as the raycaster gives us the position as a JavaScript object with {x, y, z} properties, and a-animation only accepts strings.\\n\\nFinally, on line 15 we set the duration of the animation to 5000ms (5 seconds), and then the easing function to linear (so that the walking speed doesn’t change throughout the animation).\\n\\nWhen you try this code, you will notice that the walking speed actually depends on the distance — because we told the model to always move for a duration of 5 seconds, regardless of the distance. This can be improved by calculating the distance between the current position and the target position, and then setting the animation duration based on this value:\\n\\nconst currentPosition = walker.object3D.position;\\n\\nconst distance = currentPosition.distanceTo(target);\\n\\nanimation.setAttribute(\\'dur\\', distance * 7000);\\n\\nNow the animation finally starts looking somewhat realistic:\\n\\nBut ugh-oh, when you go back, cesium man is suddenly doing the moonwalk! (It’s not a bug, it’s a feature! I swear!)\\n\\nThis can be easily solved by telling the model to look at the target position before trying to go there:\\n\\nwalker.object3D.lookAt(target);\\n\\nHowever, you will notice the now the model is always off by 90 degrees — that’s due to the orientation in which the model was saved. A easy fix would be to rotate the model by 90 degrees clockwise around the Y-axis. However, this rotation will be lost as soon as we call the lookAt method.\\n\\nInstead, what I ended up doing is wrapping the walker inside another\\n\\na-entity tag, so the walker is rotated by 90 degrees, and then the parent\\n\\na-entity tag is the one that is actually animated and rotated to look at the target:\\n\\nThis technique is very common in A-Frame: if we want several objects to move together, or to apply a new transformation to an object without affecting the initial transform (just as the case is here — we don’t want to override the initial rotation, we want lookAt to rotate the object on top of the initial rotatation), we simply wrap it in an a-entity and then manipulate the wrapper instead.\\n\\nPretty neat, huh?\\n\\nBefore we finish with this example, I want to add one more thing so that our virtual objects looks somewhat more realistic — a shadow. For that to work, we’d need to add a plane that will receive the shadow, and we also want that plane to always be below the model, and be transparent (so you don’t actually see the plane, just the shadow that is cast on it).\\n\\nLet’s start by adding the plane, and telling the model to cast a shadow, and the plane to receive it:\\n\\nSince we’ve already wrapped the model with another a-entity above (to make our cesium man walk in a sensible direction), we can now add the new shadow plane as a sibling of the model, so they will move and rotate together as one unit.\\n\\nAt this point, you will see the shadow, but also, a big white plane:\\n\\nBut no worries! We’ve got another trick up our sleeves that will solve it.\\n\\nA-Frame can’t do transparent planes with visible shadows out of the box, but luckily, A-Frame is built on top of a library called Three.js, and Three.js can do this for us. All we need to do is to extend A-Frame with a few lines of code and add the new functionality:\\n\\nMake sure you put this code before your <body> tag, but after you load A-Frame. What it does, essentially, is register a new shadow-material attribute (called Component in the A-Frame’s jargon), which applies the Three.js ShadowMaterial to whatever element you put it on.\\n\\nIn order to use this newly defined material in our scene, we’ll just add the shadow-material attribute to our plane, as follows:\\n\\n<a-plane width=\"0.5\" height=\"0.5\" position=\"0 0 0\"\\n\\nrotation=\"-90 0 0\" color=\"white\" shadow=\"receive: true\"\\n\\nshadow-material>\\n\\n</a-plane>\\n\\nAnd this seals the deal: we have real-time shadows too (albeit a little pixelated, for performance reasons)!\\n\\nCasting its shadow on the Raspberry Pi\\n\\nYou can find the final version of the code on my github, and you can also check it out online (using a WebAR capable browser on a supported device, otherwise you will just get a blank webpage). I personally think it is amazing you can build such amazing things with just 75 lines of code!\\n\\nBringing the Fun with You… Everywhere!\\n\\nAfter building this is one weekend, I decided to create an improved version with new characters. I purchased animated Fox and Diplodocus (type of dinosaur) models, converted them to glTF (that was a challenge, too, but for another post), but they are cute so it was totally worth it.\\n\\nI even took them for a nice walk in the park:')"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I Tried to Treat My Depression with AI Therapy', 'Photo via Getty Images and Wikimedia Creative Commons\\n\\nIt’s no surprise that the concept of self-care was drilled into our psyches throughout last year. When it comes to mental health, we don\\'t seem to be doing so hot. According to the World Health Organization, more than 300 million people around the world are affected by depression. The UK just appointed a minister to deal with the nation’s endemic loneliness. Shit’s got so bad that we’ve even started replacing our Top 40 song titles with the Suicide Prevention Hotline phone number.\\n\\nIn a perfect world, we’d all have easy, affordable access to mental healthcare professionals and be able to properly treat these maladies. But, with so many still unable to procure medical aid for their corporeal injuries, it’s clear we are still far from a utopia where mental wellness is regarded as a priority. And since Rick & Morty co-creator Dan Harmon only has so many hours in a given day to personally counsel the depressed, this growing problem is still in desperate need of a substantial solution.\\n\\nA team of Stanford researchers thinks they may have developed the answer to this problem—or at least a stopgap until we get our affairs properly in order. Their glimmer of hope comes in the form of Woebot, an AI chatbot that operates entirely within Facebook Messenger and uses standard cognitive behavioral therapy (CBT) techniques to provide users with no-frills sessions through their phone or computer. Over the course of a five-to-ten minute CBT session, prompted by the a push-notification from the bot, the user simply types out or taps auto-populating responses to Woebot\\'s inquiries.\\n\\nVia Woebot\\n\\nDeveloped by clinical research psychologist Dr. Alison Darcy and with AI heavyweight Andrew Ng on the advisory board, Woebot is hoping to assist underserved segments of the mentally unwell population without the income or insurance to utilize traditional practices. As someone who has long suffered from depression (and is both underpaid and underinsured), I figured I\\'d give Woebot a try.\\n\\nOver the years, I’ve learned how to manage and weather my depression—for the most part. Even so, the occasional surprise tidal wave of melancholy hits and drenches me, taking me out of commission for its duration. It was during a recent bad spell that I signed up for the service.\\n\\n\\n\\nInitially, I had a hard time taking Woebot seriously. While I’d been able to set aside the terrible name its creators had chosen for it and go in with an open mind, my first conversation with the program tested my faith in the efficacy of the service. Though I hadn’t expected a full-on holo-Freud, the AI’s vacillation between soulless call center decision tree script and /r/fellowkids-worthy fumbles with youthful parlance (don’t use the smirk emoji unless you’re trying to fuck me, Woebot) was leaving me as cold. The knockoff WALL-E illustration used as the bot’s avatar was the only whimsical element that seemed to agree with me.\\n\\n\\n\\n\\n\\nIgnoring Woebot’s credibility-diminishing usage of emoji, I pressed onward as it laid out a plan-of-action and qualifiers for the forthcoming two-week trial. Woebot made it abundantly clear that it was, by no means, a substitute for a human therapist and “not capable of really understanding what [I] need.” While I appreciated how forthcoming it was, the more Woebot told me, the more I worried that this was simply a CBT toolkit directory programmed to occasionally call me “homie” and swap “OK” with “oki.”\\n\\n\\n\\nThankfully, after our initial session, Woebot adopted a somewhat more clinical tone as it probed my psyche for problems to treat. After asking me to identify examples of unhelpful thought processes like \"all-or-nothing thinking\" and \"should statements,\" the bot would still throw in a celebratory GIF. Beyond that, however, things were starting to feel professional. Better still, I was actually going along with the exercises in earnest and found myself glad that I was now able to put a name to specific thoughts, even if I was still dubious that any progress would be made.\\n\\n\\n\\nOver the next two weeks, my feelings toward the bot ebbed and flowed as we continued with the daily check-ins. Sometimes, I appreciated Wobebot’s requests for me to type out what I was currently doing or feeling, enjoying the catharsis of purging the negativity with words. Other times, I felt pandered to by its replies. Whether I was venting about financial struggles or complaining about a frustrating scheduling hiccup, Woebot dished out the same few canned “empathy” responses. I’m convinced that, had I admitted to assassinating archduke Franz Ferdinand, Woebot would’ve hit me with another “sounds like you’re dealing with a lot right now.”\\n\\n\\n\\nFurthermore, despite Woebot’s alleged “deep learning,” it was constantly forgetting that we’d already gone over particular lessons and kept repeating the same things over and over like my mom at a family reunion. Woebot might have been cataloging my responses, but was it really listening to me? This seemed as if it would be the highest and, frankly, most essential hurdle for the program to overcome, given that it was entering an industry reliant on the patient feeling heard.\\n\\nScreencap of a Woebot session\\n\\nToward the end of the trial period, to my surprise, I noticed my mood had actually begun to improve. Somehow, despite my resistance and Woebot’s unforgivable penchant for sending Minion GIFs, I did indeed feel better. Maybe Woebot’s lame jokes and persistent, cheerful pings were actually enough to trick the dumb monkey part of my brain into believing that another sentient entity was rooting for me to push through the stormy weather. Perhaps my eye-rolling at its laughably non-human dialogue was by design—a tactic to steer my thoughts to how superior I am to these lines of code, rather than let my mind spiral into further negativity.\\n\\n\\n\\nOn the other hand, there’s a chance this turn for the better was going to happen over the course of the two weeks regardless of whether or not I’d been using the bot. Woebot’s website cites a 2017 Stanford study, done during its beta period, that found the chatbot to be “a feasible, engaging, and effective way to deliver CBT” when compared to information-only CBT apps. Reading the study, and holding it up to my own experience, I couldn\\'t help but feel that further research would be needed to determine whether these results were a case of correlation or causation. And by that time, AI will likely have improved by leaps and bounds again to the point of rendering such a study unproductive.\\n\\n\\n\\nWhen our trial time together was up, Woebot attempted to sell me its paid plan with the “for the price of a cup of coffee” tactic I’d last seen used in 90s infomercials about adopting African kids. It accepted my rejection of the offer with relative dignity, wishing me continued happiness as we parted ways.\\n\\n\\n\\nWoebot checked in the next day and a few days after that to let me know that it was suddenly down to do more free sessions if I was up for it. At that point, however, I had fully recovered from my depressive bout, so I again rebuked the offer.\\n\\n\\n\\nI want to give Woebot and its creators credit. I’m just not sure where to attribute it. Even if Woebot was not, in fact, directly responsible for lifting my spirits, it foreshadows a future of AI entities that will be able to compassionately, convincingly, and effectively accomplish this task. Darcy and Ng seeing the potential here and getting in on the ground floor is commendable, even if the product still feels half-baked.\\n\\nPerhaps when Her-like AIs are able to hold meaningful conversations and appropriately react to my confessions of self-doubt, I\\'d be willing to fork over $9 a week from my Universal Basic Income stipend. But in its current form, Woebot is just going to be added to the pile of nice therapists who tried their best but just didn’t get me.\\n\\n\\n\\nEditor\\'s Note 1/25/18: Since the time of the sessions included in this article, Woebot has removed its paid plan prompts and is now an entirely free service.\\n\\nSign up for our newsletter to get the best of VICE delivered to your inbox daily.\\n\\n\\n\\nFollow Justin Caffier on Twitter.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Earnings Outlook: Nvidia earnings: Data-center, auto sales more a focus than cryptomining', 'Data-center and auto-related chip sales will likely be the focus of Nvidia Corp. earnings when the chipmaker reports after the bell on Thursday.\\n\\nOutside of Nvidia’s NVDA, -0.14% graphics and gaming sales, data-center and auto-related sales are the fastest growing arms of the Santa Clara, Calif.-based chip maker. Gaming revenue is expected to rise 14% to $1.54 billion from the year-ago period, according to FactSet, but data-center revenue is expected to jump 85% to $548.1 million.\\n\\nAuto-related revenue, which was the only segment that missed Street estimates in the previous quarter, is forecast to rise 19% to $152.2 million. Recently, Nvidia partnered with German auto supplier Continental AG to develop self-driving vehicle systems, and with Uber Technologies Inc., while analysts have been praising the company’s opportunities in the auto market. In those systems, however, Nvidia is not immune to recent chip design flaws that can be exploited by hackers.\\n\\nAlso, data visualization revenue is estimated to rise 7% to $240.9 million, and PC and mobile OEM revenue is expected to decline 3.4% to $170.1 million.\\n\\nA focus on cryptocurrency mining-based sales will likely be acute given that a third of Advanced Micro Devices Inc.’s AMD, -1.19% computer and graphics chip revenue was estimated to come from mining demand. Investors, however, may start rethinking how relevant cryptocurrency mining is to both Nvidia and AMD given the recent selloffs in digital currencies. Bitcoin BTCUSD, +0.69% prices rallied more than 200% over the December ending quarter to end the year just shy of $14,000, after nearly reaching $20,000, but are now trading around $7,000.\\n\\nEarnings: Of the 23 analysts surveyed by FactSet, Nvidia on average is expected to post adjusted earnings of $1.16 a share, up from the 98 cents a share expected at the beginning of the quarter, and from 99 cents a share a year ago. Estimize, a software platform that uses crowdsourcing from hedge-fund executives, brokerages, buy-side analysts and others, calls for earnings of $1.27 a share.\\n\\nRevenue: Wall Street expects revenue of $2.68 billion from Nvidia, according to 26 analysts polled by FactSet. That’s up from the $2.44 billion forecast at the beginning of the quarter, and from $2.17 billion recorded last year. Nvidia predicted revenue of $2.6 billion to $2.7 billion. Estimize expects revenue of $2.73 billion.\\n\\nStock movement: Nvidia shares took a major tumble along with the rest of the market Monday dropping more than 8%, but are still up nearly 17% year to date, compared with a 3.9% gain the in the PHLX Semiconductor index SOX, -1.17% and a 0.8% rise in the S&P 500 index SPX, -0.85%\\n\\nWhat analysts are saying:\\n\\nRBC Capital markets analyst Mitch Steves, who has an outperform rating and a $250 stock price target, isn’t expecting much of a surprise from Nvidia. He noted:\\n\\nWhile the company guided revenue to be flat sequentially, we think Gaming and Data Center will help drive total company revenue growth +5-6% sequentially. We think this will be enough to hold the stock flat/up on the print and in order for the Company to see material upside, Data Center would need to grow more than double digits on a sequential basis (beating bullish buyside expectations).\\n\\nMizuho analyst Vijay Rakesh, who has a buy rating and recently raised his price target to $240 from $225, had this to say:\\n\\nWe believe NVDA remains well-positioned in Data Center with Deep learning and new inferencing platforms such as TensorRT with V100 (delivering 100x performance of a CPU DC, and with parallel processing GPUs not affected by Meltdown/Spectre issues) though price points remain high. NVDA also announced Automotive wins at Baidu, Uber and ZF with new Xavier and Drive PX Pegasus platforms for Level 5.\\n\\nJefferies analyst Mark Lipacis, who has a buy rating and a $240 price target, said Nvidia is one of his top picks in 2018 as the “consumption of semis is actually accelerating for the first time in a while, driven by cyclical demand drivers in Industrial, and secular demand drivers in parallel processing and IoT.”\\n\\nOf the 33 analysts who cover Nvidia, 15 have buy or overweight ratings, 15 have hold ratings and three have sell or underweight ratings, with an average price target of $218.21, or about 1% lower than Tuesday’s trade.')\n",
      "('Nvidia details next steps in AI, including self-driving simulator', 'Jeremy Owens/MarketWatch Chief Executive Jensen Huang speaks at Nvidia’s GPU Technology Conference in San Jose, Calif., on Tuesday.\\n\\nNvidia Corp. has advanced deep learning techniques, but now it’s looking to take AI technology into new areas: Putting self-driving cars into virtual reality instead of our roads, and setting its sights on Hollywood and hospitals.\\n\\nOver the past few years, Nvidia has made inroads into equipping cars with the computer hardware that gives them self-driving capability. That move has become so crucial that Nvidia NVDA, -0.14% shares fell more than 6% in recent trading as the company kicked off its GPU Technology Conference in San Jose, Calif., after it confirmed that it is suspending real-world testing following a recent fatality in Arizona in one of Uber Technologies Inc.’s self-driving cars.\\n\\nIn his keynote address Tuesday morning, Chief Executive Jensen Huang did not mention the halt, but did show off a potential solution to the problem of testing self-driving automobiles on public roads. Huang showed off a simulator that can allow companies to test their self-driving systems in a virtual environment, providing opportunity to drive billions of miles in a year without endangering pedestrians. Early-access partners will receive access to the simulator in the third quarter.\\n\\nThat is the type of advance Nvidia showed off Tuesday: Fine-tuning the approach to artificial intelligence it has pioneered over the past five years. While no single announcement was as important as the $3 billion server chip Huang showed at last year’s GPU Technology Conference, the new offerings should help the thousands of developers in attendance take advantage of Nvidia’s technology.\\n\\nHuang introduced the Quadro GV100 with Nvidia RTX Technology, which is made up of four Volta chipsets, or a $68,000 computer that is meant to replace the supercomputers required to render graphic images and complex light shading in real-time. Huang said that a traditional “render farm,” like those used by Hollywood movie studios for computer graphics scenes, uses about 280 dual CPU servers and about 168 kilowatts of energy. Huang said that could be replaced by 14 Quadro GPU servers that uses abut 24 kilowatts of electricity.\\n\\nHuang also announced the Clara medical imaging supercomputer, which basically takes traditional ultrasound data, streams it into a data center, and enhances the data into a 3-D rendering, allowing for hospital to get greater performance out of their existing medical equipment.\\n\\nHuang introduced the “world’s largest GPU,” the DGX-2, a massive chipset marketed as a supercomputer that weighs in at 350 pounds for deep learning and AI applications. The chipset is 10 times faster than the company’s DGX-1 and will be available in the third quarter for $399,000.\\n\\n“We are all-in on deep learning,” Huang said, adding that while computing power is growing exponentially, the growing computing power needed for deep learning is growing at double that rate.”This is some new type of computing.”\\n\\nNvidia announced a new partnership with chip maker Arm, which is owned by SoftBank Group Corp. SFTBY, +0.99% , though Huang did not provide details in his speech.\\n\\nNvidia stock, fell 7.8% on Tuesday, after reports that the company halted self-driving tests, but has run up 16.6% year to date, compared with a 6.0% gain on the PHLX Semiconductor Index SOX, -1.17% and a 2.3% decline for the S&P 500 index. SPX, -0.85% Of the 32 analysts who cover Nvidia, 15 have overweight or buy ratings, 15 have hold ratings, and two have sell ratings, with an average price target of $250.93.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Apple Watch can detect early signs of diabetes with 85% accuracy, study finds', 'Amid rumors that Apple is working on a non-invasive glucose monitoring system for Apple Watch, researchers are using cutting edge software science to prove the heart rate sensors in current-generation wearables can successfully detect early signs of diabetes.\\n\\n\\n\\n\\n\\n\\n\\nAs part of an ongoing study involving Apple Watch and Android Wear users, researchers at app developer Cardiogram and the University of California, San Francisco, trained a deep neural network called DeepHeart to distinguish people with and without diabetes at an accuracy of 85 percent.The collaborative study pulled from 14,011 Cardiogram users enrolled with the UCSF Health eHeart Study to obtain 33,628 person-weeks of health sensor data. This information was used to train DeepHeart, which was presented with samples from people with and without diabetes, hypertension, sleep apnea, atrial fibrillation and high cholesterol, according to Cardiogram co-founder Johnson Hsieh.\"Typical deep learning algorithms are data-hungry, requiring millions of labeled examples, but in medicine, each label represents a human life at risk — for example, a person who recently suffered a heart attack or experienced an abnormal heart rhythm,\" Hsieh said in a prepared statement. \"To solve this challenge, researchers applied two semi-supervised deep learning techniques (\\'unsupervised sequence pretraining\\' and \\'weakly-supervised heuristic pretraining\\') which made use of both labeled and unlabeled heart rate data to improve accuracy.\"Hsieh notes a correlation between diabetes and a body\\'s autonomic nervous system allows DeepHeart to detect the disease through heart rate readings. Specifically, as people develop early stage diabetes, their pattern of heart rate variability shifts in measurable ways.Findings from the study and subsequent research paper ( PDF link ) were presented at the Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18) in New Orleans on Wednesday.Cardiogram\\'s latest study arrives amid rumors that Apple is developing noninvasive glucose sensors for integration in a future Apple Watch device. Considered a \"holy grail\" of modern medical science, such a system would allow diabetics to continuously monitor sugar levels without performing blood draws.According to the most recent reports, however, the glucose sensing technology is years away from release.Cardiogram co-founder Brandon Ballinger told AppleInsider that DeepHeart is ready to integrate glucose readings if and when Apple launches a Watch model with built-in glucose monitoring capabilities.\"If Apple includes a glucose monitor in the next Watch, we\\'ll be the first developers to try it,\" Ballinger said. \"We designed DeepHeart to be both multi-task (able to detect multiple health conditions) and multi-channel (able to incorporate multiple sensor data streams) for exactly that reason.\"Actively used by more than a quarter million people, Cardiogram has been able to apply DeepHeart to detect a number of heart-related conditions using consumer grade hardware. Most recently, Cardiogram and UCSF researchers last May demonstrated Apple Watch can detect atrial fibrillation, a common heart arrhythmia that can lead to stroke, with a 97 percent accuracy . A few months later, Apple itself teamed up with Stanford on its own atrial fibrillation detection program called the Heart Health Study Cardiogram will further expand on its research initiatives in 2018 as it looks to apply deep neural network technologies to real-world scenarios, all in the hope of saving lives. One new feature due to arrive this year is direct in-app integration with DeepHeart, which should grant access to a much larger dataset for improved statistics.Cardiogram is a free 55.2MB download from the iOS App Store.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Apple Watch more effective at detecting heart condition than KardiaBand accessory, study finds', 'A continuing study into the medical potential of consumer wearables has confirmed devices like Apple Watch are sensitive enough to detect abnormal heart rhythms with a 97 percent accuracy, a performance that beats out add-on ECG accessory KardiaBand.\\n\\n\\n\\n\\n\\n\\n\\nApple Watch, and other consumer wearables including Android Wear devices, were used in a study that applies a neural network called DeepHeart to recorded heart rate data in a bid to detect atrial fibrillation, a common heart arrhythmia that can lead to stroke in some patients.As AppleInsider detailed last May , the DeepHeart study is a joint effort between the University of California, San Francisco\\'s Health eHearth Study and digital health startup Cardiogram, the latter of which markets an eponymous Apple Watch app Comprised of both online and in-hospital components, the study incorporated data from a group of 9,750 Cardiogram users, a subsection of which contributed 129 million heart rate measurements and 6,338 ECGs to train the DNN. Over time, DeepHeart learned to transform raw sensor measurements, specifically hart rate and step counts, into fibrillation risk scores, says Cardiogram cofounder Brandon Ballinger.The resulting model is capable of achieving a c-statistic, or accuracy, of 97 percent when compared to a traditional 12-lead ECG. That same figure was presented at the Heart Rhythm Society\\'s Heart Rhythm 2017 conference last year, but has now been confirmed in a peer review process. Results of the study, \"Passive Detection of Atrial Fibrillation Using a Commercially Available Smartwatch,\" were be published in JAMA Cardiology on Wednesday.Of note, the now confirmed 97 percent accuracy rate is higher than that of AliveCor\\'s FDA-certified KardiaBand , an Apple Watch ECG peripheral that launched in the U.S. in November. Apple Watch with DeepHeart attained a 98 percent sensitivity and 90 percent specificity compared to respective Kardia Band rates of 93 and 84 percent Unlike Apple Watch\\'s built-in optical heart rate sensor, the $199 KardiaBand takes a 30-second ECG reading by measuring electrical impulses collected at a user\\'s extremities. To conduct measurements, users wearing Apple Watch on their left arm place their right thumb on a sensor pad, which presses an inner electrode onto their left wrist, and vice versa for right arm wearers.DeepHeart is the first large-scale, peer-reviewed study to appear in a medical journal demonstrating consumer wearables\\' ability to detect a major health condition, Ballinger says. Further, DeepHeart\\'s semi-supervised deep learning algorithm and heuristic pre-training realized a significant increase in accuracy compared to similar artificial intelligence research.For example, Google Brain\\'s research on Diabetic Retinopathy required 128,175 training labels, while Stanford University\\'s skin cancer detection study used 129,450 labels. DeepHeart needed only 6,338 ECGs to attain a high level of accuracy thanks to a special pre-training procedure that approximated the average difference between successive heart rate measurements across different window sizes, the study says.As detailed in previous reports, validation was performed on a test sample of 51 patients scheduled to undergo treatment. Each person wore an Apple Watch for 20 minutes before and after cardioversion, a procedure that restores normal heart rhythms to patients with arrhythmia.The study\\'s results arrive as Apple dips its toe into the waters of investigational medicine. Last year, the company partnered with Stanford to deliver its own Apple Watch-based atrial fibrillation detection program called the Heart Health Study.Cardiogram plans to expand on its research initiatives in 2018 as it looks to apply deep neural network technologies to real-world scenarios, including diagnosis of previously unidentified atrial fibrillation. In February, the firm, in collaboration with UCSF, released a study showing Apple Watch and its sensors can detect early signs of diabetes with 85 percent accuracy \"Our ultimate goal is to turn the wearable on your wrist into Baymax — a personal healthcare companion,\" Ballinger told AppleInsider, referring to the superhero appearing in various Marvel Comics titles and the movie Big Hero 6.Interestingly, Apple is also rumored to be working on non-invasive glucose sensors for Apple Watch, technology many consider to be a \"holy grail\" of modern medicine. The project is not expected to bear fruit for at least a few years , if it even survives early development.Cardiogram is a free 55.2MB download from the iOS App Store.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Early benchmarks shows Samsung Galaxy S9 well behind iPhone X in processor performance', 'The Samsung Galaxy S9 has a slower processor than the iPhone 8 and the iPhone X, according to early benchmarks of Samsung\\'s new device, after tests of the Android smartphone\\'s Exynos 9810 chip reveal it to lag far behind the Apple-created A10 and A11 processors.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGraph from Anandtech\\n\\n\\n\\nGraph from Anandtech\\n\\nBenchmarking of a demonstration handset at the Samsung Galaxy S9 launch event by Anandtech shows the Exynos 9810 to be a fast chip, but not as fast as the A10 and A11. Across the battery of tests, Samsung\\'s newest processor, unveiled in January , does outperform its predecessor, but Apple\\'s chips continued to dominate in most of the results.An initial GeekBench 4 single core test recorded integer and floating point scores of 3,734 and 3,440 respectively for the Exynos 9810, but by comparison the A11 scored 4,630 and 3,958 respectively. The A10 also beats Samsung\\'s chip for the Integer score, achieving 4,007, but is just 95 points behind for floating point performance at 3,345.There is no change in position between the three chips when breaking down the scores to a per-megahertz level, as comparing the integer scores reveal the Exynos 9810 scoring 1.38 per megahertz, while the A10 scores 1.71 per megahertz, rising to 1.94 for the A11. For the floating point score, it\\'s the sames story, with the 9810 getting 1.27 points per megahertz compared to the 1.43 points of the A10 and 1.66 of the A11.Using different benchmarks for the phones themselves instead of narrowing down to the processor, the results stayed fairly similar with the iPhone 8 and iPhone X beating the Galaxy S9 Plus. Using WebXPRT 2015, the iPhone X and iPhone 8 score 352 and 349 respectively, almost double the 178 points achieved by the Galaxy S9 Plus.Under Speedometer 2.0, the gap between the devices extends, as the Galaxy S9 Plus scored just 26.70 in the benchmark compared to the 88.90 of the iPhone 8 and 87.20 of the iPhone X.In both of these tests, the iPhone 7 also manages to best the Galaxy S9 Plus, achieving 199 under WebXPRT 2015 and 55.20 under Speedometer 2.0.Andrei Frumusanu of Anandtech believes there may have been an issue with the device being tested, as it was \"barely able to distinguish itself from last year\\'s Exynos 8895.\" While noting the larger cores were reaching their maximum set clock speed, Frumusanu suggests it could be issues with the scheduler and the DVFS configuration that\\'s causing the handset on show to underperform.A Samsung spokesperson did confirm that the demonstration units were running special firmware for Mobile World Congress, adding that it may not be optimized to the hardware as of yet. Frumusanu considers it hard to believe Samsung would limit the performance for their flagship smartphones for the show, nor that the company would alter its scheduler settings for the occasion.Another notable element is that Samsung has clocked the four performance cores used in the Galaxy S9\\'s processor at 2.7 gigahertz, down from the maximum 2.9 gigahertz claimed at the launch of the chip.Testing the Mali G72MP18 GPU included in the Galaxy S9 Plus revealed the same story, with its score of 45.70 peak frames per second on GFXBench Manhattan 3.1 beaten by the iPhone 8\\'s 60.34 and the iPhone X\\'s 64.19. Not much change was seen under the GFXBench T-Rex 2.7 test, with the Galaxy S9 Plus peaking at 143.4 frames per second while the iPhone 8 and iPhone X got 171.4 and 176.6 peak framerates.At the time of its launch, Samsung promoted the Exynos 9810 as using a 10-nanometer process, with an improved architecture that was claimed to double the single-core performance of its predecessor, while increasing multi-core performance by around 40 percent. Samsung also boasted of its \"neural network-based deep learning\" capabilities which can help with image processing, a feature similar to the \"Neural Engine\" used in Apple\\'s A11 Bionic processor.')\n",
      "(\"Apple Watch's heart rate sensor can detect diabetes, Cardiogram study finds\", 'The tiny Apple Watches on our wrists have already saved lives. Not only do they keep us more active, they can also detect a variety of heart diseases without needing any additional bands or accessories. But a new study by Cardiogram is taking it one step further.\\n\\nThe mobile health data company’s deep learning network, DeepHeart, already uses data from the Apple Watch ($429 on Apple.com) heart rate sensor to detect atrial fibrillation, hypertension, and sleep apnea. Now it’s adding a new disease to the list: diabetes. While Apple and Google have been rumored to be working on hardware capable of monitoring glucose levels, Cardiogram’s study used nothing more than machine learning and the Apple Watch’s heart rate sensor to detect whether a user has diabetes.\\n\\n“The heart rate sensor that you wear already from the Apple Watch may actually be able to detect signs of diabetes,” Cardiogram co-founder Brandon Ballinger told Macworld. “In the end, the final accuracy was 85 percent, which is pretty high for performing wrist applications.\"\\n\\nThe impact on you at home: Apple is selling more watches than ever, and the vast majority of people are using them mainly for their health and fitness benefits. We’ve heard numerous stories of how Apple Watch’s heart rate sensor has saved people from heart attacks and pulmonary embolisms, and Cardiogram\\'s research on atrial fibrillation last year ended up mirroring Apple\\'s own study in watchOS 4. But diabetes detection ups the ante considerably. If we can start extrapolating the heart-rate data from Apple Watch to detect ailments that aren\\'t specific to the heart, there\\'s no telling how much of a life-saver it could be.\\n\\nA deep dive by DeepHeart\\n\\nThe study, which was conducted by Cardiogram and the University of California at San Francisco, used 14,011 subjects and some 200 million heart rate sensor measurements to train DeepHeart and test the accuracy of the neural network’s ability to distinguish between people with and without diabetes. According to Ballinger, one in four people with diabetes are undiagnosed, and he’s hoping this new method of detection can help significantly lower that number.\\n\\nCardiogram Cardiogram has used the Apple Watch’s heart rate sensor to detect diabetes in a new study.\\n\\nDeepHeart is able to monitor the pattern of beat to beat heart rate variability to detect changes that are associated with diabetes, such as elevated resting heart rates or slower heart rate recovery after exercising. Ballinger says monitoring heart rate variability can predict whether someone is likely to develop diabetes, so DeepHeart is “actually detecting the early stages of that disease before it detects a severe complication.”\\n\\nLast year, Cardiogram studied ways in which the Apple Watch could be used to detect atrial fibrillation, an ability Apple is also researching with its own Heart Study app. Cardiogram also says its neural network can also be used to detect hypertension and sleep apnea, but the diabetes study is particularly meaningful to Ballinger since his mother suffers from the disease.\\n\\n“If you detect (diabetes) early enough, people will be in the pre-diabetic range rather than the diabetic range, and they can enroll in a behavioral program that can teach them the right things to eat, exercise, using connected Wi-Fi scales to help them get back to normal levels,” he said. “It can actually prevent people from getting diabetes.”\\n\\nBallinger says the study is a first step toward expanding the free app’s abilities. Over the next few months, new features will arrive to “guide people through the process of identifying if they’re high risk, confirming whether they have diabetes or not, and then guiding them toward the appropriate treatment.”\\n\\nAnd that’s Ballinger’s ultimate goal with Cardiogram, which recently expanded to Android Wear and is hoping to add Fitbit Ionic to its fold soon. “We’ve done a lot of work on detecting conditions,” he said. “I think know we’re moving into the phase of intervention, actually guiding people to the appropriate health care.”')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Pay what you want for this AI and deep learning bundle!', \"Artificial intelligence is more important now than ever before, and it can be found just about everywhere in our data-driven world. Think of search engines, image recognition, robotics, finance, and plenty of other industries. Developing AI applications isn't easy, but those with the knowledge are in high demand.\\n\\nInterested in learning more? Instead of picking up pieces here and there, why not invest in an AI and deep learning bundle? Why pay hundreds of dollars when you can essentially pay what you want and have access forever?\\n\\nWindows Central Digital Offers currently has a deal on an AI and deep learning bundle that contains seven e-books and 10 hours of course content. Instead of the suggested price of $691, you can pay any amount you want and take home the Deep Learning with Hadoop e-book. If you beat the average price, you take home the entire bundle. It also includes:\")\n",
      "('Ensuring Client Satisfaction With 9 Quick-Win Banking Initiatives', 'PSFK’s Digital Banking Playbook, a collaboration with Microsoft, presents strategies banks can implement to keep customers satisfied and secure\\n\\nWhether it be implementing new technologies or simply listening to the needs of clients, it’s increasingly crucial that banks adapt to changing consumer expectations. PSFK’s Digital Banking Playbook, a collaboration with Microsoft, presents strategies across three categories—customer experience, risk management and security—to improve the banking experience.\\n\\nCustomer Experience\\n\\nGain actionable customer insights to enable higher-value client relationships and personalized banking experiences. Deepen your understanding of each customer by tracking the details of every interaction—whether in person or via phone, web, mobile, email or social media—to deliver contextual, connected experiences\\n\\nCustomize experiences with personalized financial guidance, service, and offers. Increase customer satisfaction and improve upsell and cross-sell performance by engaging customers with personalized financial guidance and next best offers\\n\\nDemocratize artificial intelligence and make it valuable to everyone. Benefit from built-in intelligence, based on deep learning technology, with solutions that see, hear, speak, and understand customer needs and emotions using natural methods of communication\\n\\nRisk Management\\n\\nDevelop tools and content that help customers understand the components that determine their risk profile; build client trust by offering transparent risk assessments and recommendations to improve creditworthiness\\n\\nLeverage AI to explore alternate indicators of creditworthiness like purchase behavior, social media activity, job history and other components of personal data to support a more holistic risk assessment\\n\\nUse cloud technologies to reduce the time needed to gather approval for loans and reduce processing time, reducing total cost and enabling banks to reach a broader range of potential consumers\\n\\nSecurity\\n\\nDetermine what points of the retail banking process, from cash withdrawal to transaction authentication, are the most vulnerable for security flaws and fortify them with enterprise-grade encryption and biometrics\\n\\nLeverage AI assistants to detect, compile and aggregate patterns of suspicious behavior that could indicate financial fraud\\n\\nBuild consumer trust by proactively educating clients in best practices to protect themselves from security breaches, while communicating your security initiatives\\n\\nThe Digital Banking Playbook, presented in partnership with Microsoft, highlights how digital tools can augment a financial institution’s ability to optimize operations and reinvigorate the client experience. Download the free report here.')\n",
      "('5 AI Trends Redefining The Retail Experience', 'PSFK’s AI Retail Playbook, a Microsoft collaboration, explores how artificial intelligence enables personalized and dynamic consumer interactions\\n\\nConsumer expectations for speed and high-quality service keep rising—so what can retailers do to maintain a competitive edge? The next retail paradigm shift will see offline and online shopping converge into a single, seamless channel. Retailers of all sizes need to be one step ahead of their customers’ needs and their competitors’ next innovation, requiring a digital transformation powered by artificial intelligence.\\n\\nPSFK’s AI Retail Playbook, a collaboration with Microsoft, provides a framework for AI’s integration into the retail experience. Below, we break down some of the key trends retailers can act on in the AI space and narrate a scenario that highlights the process of building comprehensive, informed shopper profiles with AI.\\n\\nPersonalize the storefront for every customer.\\n\\nAI-powered retail spaces recognize shoppers and adapt in-store product displays, pricing and service through biometric recognition to reflect customer’ profiles, loyalty accounts or unlocked rewards and promotions—creating a custom shopping experience for each visitor, at scale.\\n\\nGuide discovery based on the shopper’s needs and preferences.\\n\\nAs customers to look to build confidence in a purchase decision, automated assistants can help narrow down the selection by recommending products based on shoppers’ needs, preferences and fit.\\n\\nCapture emotional responses and act accordingly.\\n\\nBy recognizing and interpreting facial, biometric and audial cues, AI interfaces can identity shoppers’ in-the-moment emotions, reactions or mindset and deliver appropriate products, recommendations or support—ensuring that a retail engagement doesn’t miss its mark.\\n\\nExtend dynamic consumer outreach based on real-time information.\\n\\nAdvanced CRM and marketing systems learn a consumer’s behaviors and preferences through repeated interactions to develop a detailed shopper profile and utilize this information to deliver proactive and personalized outbound marketing—tailored recommendations, rewards or content.\\n\\nRespond to customer feedback with R&D.\\n\\nDeep learning algorithms collect and interpret customer feedback and sentiment, as well as purchasing data, to support generation-generation product and service designs that better satisfy customer preference or fulfill unmet needs in the marketplace.\\n\\nHere’s a possible scenario:\\n\\nAs Anton’s example conveys, artificial intelligence translates the personalized convenience of online shopping to physical retail, instantaneously refining product recommendations and rewarding customer loyalty. AI supports in-store associates by putting at hand all of the information they need for each individual customer. The result is a comprehensive view of shopper behavior for retailers and a seamless experience for customers, regardless of the retail channel.\\n\\nThe AI Retail Playbook, presented in partnership with Microsoft, outlines a new vision for the retail experience—heavily personalized, proactive service that uses the power of artificial intelligence to place the customer at the center of operations. Download the free report here.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Deep learning predicts drug-drug and drug-food interactions', 'A Korean research team from KAIST developed a computational framework, DeepDDI, that accurately predicts and generates 86 types of drug-drug and drug-food interactions as outputs of human-readable sentences, which allows in-depth understanding of the drug-drug and drug-food interactions.\\n\\nDrug interactions, including drug-drug interactions (DDIs) and drug-food constituent interactions (DFIs), can trigger unexpected pharmacological effects, including adverse drug events (ADEs), with causal mechanisms often unknown. However, current prediction methods do not provide sufficient details beyond the chance of DDI occurrence, or require detailed drug information often unavailable for DDI prediction.\\n\\nTo tackle this problem, Dr. Jae Yong Ryu, Assistant Professor Hyun Uk Kim and Distinguished Professor Sang Yup Lee, all from the Department of Chemical and Biomolecular Engineering at Korea Advanced Institute of Science and Technology (KAIST), developed a computational framework, named DeepDDI, that accurately predicts 86 DDI types for a given drug pair. The research results were published online in Proceedings of the National Academy of Sciences of the United States of America (PNAS) on April 16, 2018, which is entitled \"Deep learning improves prediction of drug-drug and drug-food interactions.\"\\n\\nDeepDDI takes structural information and names of two drugs in pair as inputs, and predicts relevant DDI types for the input drug pair. DeepDDI uses deep neural network to predict 86 DDI types with a mean accuracy of 92.4% using the DrugBank gold standard DDI dataset covering 192,284 DDIs contributed by 191,878 drug pairs. Very importantly, DDI types predicted by DeepDDI are generated in the form of human-readable sentences as outputs, which describe changes in pharmacological effects and/or the risk of ADEs as a result of the interaction between two drugs in pair. For example, DeepDDI output sentences describing potential interactions between oxycodone (opioid pain medication) and atazanavir (antiretroviral medication) were generated as follows: \"The metabolism of Oxycodone can be decreased when combined with Atazanavir\"; and \"The risk or severity of adverse effects can be increased when Oxycodone is combined with Atazanavir.\" By doing this, DeepDDI can provide more specific information on drug interactions beyond the occurrence chance of DDIs or ADEs typically reported to date.\\n\\nDeepDDI was first used to predict DDI types of 2,329,561 drug pairs from all possible combinations of 2,159 approved drugs, from which DDI types of 487,632 drug pairs were newly predicted. Also, DeepDDI can be used to suggest which drug or food to avoid during medication in order to minimize the chance of adverse drug events or optimize the drug efficacy. To this end, DeepDDI was used to suggest potential causal mechanisms for the reported ADEs of 9,284 drug pairs, and also predict alternative drug candidates for 62,707 drug pairs having negative health effects to keep only the beneficial effects. Furthermore, DeepDDI was applied to 3,288,157 drug-food constituent pairs (2,159 approved drugs and 1,523 well-characterized food constituents) to predict DFIs. The effects of 256 food constituents on pharmacological effects of interacting drugs and bioactivities of 149 food constituents were also finally predicted. All these prediction results can be useful if an individual is taking medications for a specific (chronic) disease such as hypertension or diabetes mellitus type 2.\\n\\nDistinguished Professor Sang Yup Lee said, \"We have developed a platform technology DeepDDI that will allow precision medicine in the era of Fourth Industrial Revolution. DeepDDI can serve to provide important information on drug prescription and dietary suggestions while taking certain drugs to maximize health benefits and ultimately help maintain a healthy life in this aging society.\"')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Scientists build better way to decode the genome', 'The genome is the body\\'s instruction manual. It contains the raw information -- in the form of DNA -- that determines everything from whether an animal walks on four legs or two, to one\\'s potential risk for disease. But this manual is written in the language of biology, so making sense of all that it encodes has proven challenging. Now, Columbia University researchers have developed a computational tool that shines a light on the genome\\'s most hard-to-translate segments. With this tool in hand, scientists can get closer to understanding how DNA guides everything from growth and development to aging and disease.\\n\\nThe researchers recently published their findings in the Proceedings of the National Academy of Sciences.\\n\\n\"The genomes of even simple organisms such as the fruit fly contain 120 million letters worth of DNA, much of which has yet to be decoded because the cues its provides have been too subtle for existing tools to pick up,\" said Richard Mann, PhD, a principal investigator at Columbia\\'s Mortimer B. Zuckerman Mind Brain Behavior Institute and a senior author of the paper. \"But our new algorithm lets us sweep through these millions of lines of genetic code and pick up even the faintest signals, resulting in a much more complete picture what DNA encodes.\"\\n\\nGeneticists have long looked for ways to decipher the mysteries hidden in DNA. One such mystery has involved a particularly pervasive class of genes known as the Hox genes.\\n\\n\"Hox genes are the body\\'s master architects; they drive some of the earliest and most critical aspects of growth and differentiation, such as where in a developing embryo the head and limbs should be positioned,\" said Dr. Mann, who is also the Higgins Professor of Biochemistry and Molecular Biophysics (in Systems Biology) at Columbia University Irving Medical Center. \"Hox genes do this by producing proteins called transcription factors, which bind to DNA sequences in order to turn large cohorts of genes on or off; like flipping thousands of switches in exactly the right order.\"\\n\\nBut decades of research into Hox genes uncovered a paradox: Even though each individual Hox gene guided a different feature of growth, the Hox transcription factors were all binding strongly and visibly to the same set of easily identifiable DNA sequences.\\n\\nIn 2015, Dr. Mann and his team discovered that the Hox transcription factors were also binding at many other locations -- just more discretely at so-called \\'low-affinity sites.\\' The scientists believed these low-affinity binding sites to be key to the Hox transcription factors being able to drive one aspect of development versus another. The problem remained how to decipher these sites from the genome.\\n\\nTo address this challenge, Dr. Mann and his lab joined forces with the lab of Harmen Bussemaker, PhD, a Professor in Columbia\\'s Department of Biological Sciences and Systems Biology and an expert in building computational models of genetic activity.\\n\\nA few years ago, the two labs developed a genetic sequencing method called SELEX-seq to systematically characterize all Hox binding sites. But their approach still had limitations: It required the same DNA fragment to be sequenced over and over again. With each new round, more pieces of the puzzle were revealed, but information about those critical low-affinity binding sites remained hidden.\\n\\n\"It was akin to running the same paragraph through Google translate multiple times, but in the end still only ten percent of the words are accurately translated,\" said Dr. Mann.\\n\\nTo overcome this challenge, Dr. Bussemaker and his team developed a sophisticated new computer algorithm that was able to explain -- for the first time -- the behavior of all DNA sequences in the SELEX-seq experiment. They called this algorithm No Read Left Behind, or NRLB.\\n\\n\"In simple terms, NRLB allows us cover the entire spectrum of binding sites -- from the highest to the lowest affinity -- with a much greater degree of sensitivity and accuracy than any existing method, including state-of-the-art deep learning algorithms\" said Dr. Bussemaker, who was the paper\\'s other senior author. \"Building on that foundation, we now hope to develop more in-depth biological and computational models to help answer the most complicated questions about the genome.\"\\n\\n\"For example, diseases such as schizophrenia, Parkinson\\'s disease and autism have been mapped to particular DNA regions that do not appear to have a clear function,\" said Dr. Mann. \"With NRLB, scientists could potentially piece together how transcription factors bind to and activate those regions. This will be critical for finding ways to manipulate that activity to one day reduce one\\'s risk of disease.\"')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Huawei P20 and P20 Pro Hands-On: Embrace the Notch', \"It’s time to have a serious discussion about the Notch. Love it or hate it, the Notch is going to be a defining element of major smartphones this year. The latest adherent to the Notch philosophy is the Huawei P20 family, being announced in Paris today. Using the Notch allows the handset to maximise the front-facing screen size compared to the size of the phone, and if used in the right way is designed not to be a distraction. We had some hands on time with the device, the P20 Pro triple camera setup, the new color schemes, and the new 960 fps mode.\\n\\nHuawei is coming out with two new devices, the P20 and the P20 Pro. The strategy for the devices is certainly towards the up-sell: the P20 Pro is larger, has the additional features, and the more of everything, which makes some of the discussion here relate to only the P20 Pro. This is a worrying trend, seen with some other brands, as essentially the two phones share the same platform but have different features, rather than just being bigger versions of the other. The major feature for the P20 Pro in this case is the size, its triple camera setup, as well as its upgraded ingress protection (IP) rating.\\n\\nThe Look\\n\\nThe more smartphones I see, and as the future marches on, the more rectangles with displays I see. The march into having as much of the front face of the phone being the display means we are moving away from particular defining features between the brands. Between the P20, the P10, and the P9, it is quite clear that the P20 applies a tangential look and feel to the brand that Huawei promotes as the more mainstream counterpart to the Mate series of smartphones.\\n\\n\\n\\nHuawei P20 (2018), Huawei P10 (2017), and Huawei P9 (2016)\\n\\nThere is no getting away from the Notch. Ultimately this is our defining visual feature from the front, and the size of the Notch becomes the way to separate some of these devices. The Notch has to house the front facing camera, the associated flash, any additional detectors for other front-facing features, and usually the earpiece for actually making calls (remember when we used to do those with smartphones?). As a result, the number of features that a vendor wants to put here determines the size, and the software has to be built to deal with it.\\n\\n\\n\\nWhat is in a Notch?\\n\\nSpecific advances in the Android ecosystem are being released to do this, but Huawei is taking it internal this generation, ensuring that the size of the Notch and the size of the notification area are the same. For anyone that doesn’t like the Notch, it can be ‘disabled’, as in the notch area of the screen will go dark but will keep the notification area at the top.\\n\\nAs with the P10, the fingerprint sensor is on front, whereas the P9 had it on the rear – if Huawei was to push the screen size even further, it is likely to be moved into the display, especially as Synaptics and Goodix have both recently released sensors for this (although for OLED displays only so far, like the P20 Pro). On the rear, the big difference is that Huawei has done away with the top differently colored window housing the cameras. On the P20, this is most obvious with the P10 and P9 side-by-side. The side-by-side dual camera arrangement has been moved to an above/below situation, with the markings on the rear rotated as if to say ‘when the P20 is held like a camera, it looks more like a camera’.\\n\\nThis is more pronounced with the P20 Pro, with its three camera setup. Interestingly in our recent trip to see Huawei in China, a couple of professional photographers noted in feedback to Huawei that holding a smartphone in a more photo-taking way would necessitate a button on the top right side to enable better ergonomics, however neither the P20 or P20 Pro have that option. The three cameras are designed such that the top camera is an 8MP sensor with a telephoto lens, the middle camera is a 40MP RGB sensor, and the bottom is a 20MP monochrome sensor. Between the middle two is a laser transmitter/receiver, and a color temperature sensor as well as LED flash is below the entire set. The camera has more features, which we’ll cover in a bit.\\n\\nHuawei is introducing each model with a few colors, including this ‘twilight’ option as seen above that has a gradient going from green to blue that adjusts in the light. Other options include a gradient Pink Gold, a standard Blank, a standard Champagne Gold (P20 only), and Midnight Blue. To be honest, as much as these colors look fun, most of them are very dark, and the special colors will be negated by any case used by the consumer. Normally Huawei even puts in a transparent flexi-case in the box with most of its smartphones, which would instantly remove the ‘specialness’ of the color in play.\\n\\n\\n\\nBlack, Black, Twilight, Mignight Blue\\n\\nWith the new design, it is clear the antenna window has changed. This is most noticeable when viewing the devices side-on, showing the power and volume buttons. The positions of these buttons have been tweaked slightly, and Huawei has decided against putting a different textured power button on the side to differentiate the power button (much to my chagrin). For the antenna window, it is now along the edge of the device, rather than noticeably on the rear. This goes most of the way around the edge, but deviates on the bottom next to the USB Type-C ports.\\n\\nHuawei is keeping the USB Type-C on the bottom, but however is moving to a dual speaker grille arrangement (we forgot to check if these are both for audio purposes). Sadly, there is no 3.5mm jack.\\n\\nThe Hardware\\n\\nThe P20 and P20 Pro are built on the same SoC that powers the flagship Mate 10 family: Huawei’s HiSilicon Kirin 970. For those unfamiliar with the hardware, Andrei did a great deep dive into its performance earlier this year. For the new smartphones, this SoC is at the front and center that drives the performance. Huawei is using more of the AI features for the P20 family, enabled through new software capabilities.\\n\\nFor the cameras, Huawei highlights several new features. The Camera app is now enabled by a better AI capability than the Mate 10 series, and this goes beyond identifying more scenes to pre-apply appropriate settings. The new camera allows for AI-assisted 4D predicative focus, adjusting the focus while a fast moving object (such as a flower in the wind) is in the viewfinder, with the AI predicting the motion of travel to adjust the focus on the fly. The camera also does AI image stabilization, which goes beyond the OIS/EIS to enable long-exposure shots without a tripod of up to eight seconds. The same stabilization AI algorithms are also in play for recording video, giving a smoother feel to recordings.\\n\\nWe asked whether these features would move down into other Kirin 970 devices (such as the Mate 10), however the response from Huawei was a fairly non-committal ‘we are looking into it’ without a timeframe.\\n\\nThe 40MP RGB sensor used in the P20 Pro is a rather impressive number to say the least. The sensor relies on 1 micron pixels, which are typically poor for low-light situations. Users can actually enable the sensor to run in ‘10MP’ mode, which uses what Huawei calls Pixel Fusion – data from a group of 2x2 pixels is reformed into a single pixel, essentially creating a 10MP image using quadruple sized pixels. Huawei states that this sensor should offer great 40MP photos is bright light, and similarly great 10MP photos in low light scenarios. Huawei has stated that the device can run up to 102400 ISO, more than any other smartphone on the market.\\n\\n[ P20 Pro ] - [ Galaxy S9 ] -\\n\\n[ P20 Pro ] - [ Galaxy S9 ] -\\n\\n[ P20 Pro ] - [ Galaxy S9 ] -\\n\\n[ P20 Pro ] - [ Galaxy S9 ] -\\n\\n[ P20 Pro ] - [ Galaxy S9 ] -\\n\\n[ P20 Pro ] - [ Galaxy S9 ] -\\n\\nThe P20 Pro will also offer recording 960 FPS video, and we believe the system is using the Sony IMX363 sensor that enables this, though Huawei refused to state this was the case (when questioned, we got the odd answer of ‘we don’t give this information’, which would be a stark change in Huawei specification release policy, and a negative change at that). The way the slo-mo mode is implemented is a little different to other smartphones with this feature, and was a little tricky to pull off. For Huawei, the user has to specify the mode, and then has a one-button push which records the scene for one second, then records the next 0.2 seconds at 960 FPS, which is then stretched to six seconds of video. Unlike Samsung, where users can get multiple takes within the same recording, the Huawei implementation is just for short videos and very difficult to get right: it took us about 15 attempts just to get this simple video:\\n\\nWhile it is a nice feature, the user experience of the implementation leaves a lot to be desired.\\n\\nFor the displays, again in the interests of the ‘sell-up’, the P20 Pro is the one that has the 6.1-inch OLED display running at 2240x1080 resolution for an 18.7:9 ratio. The P20 is a slightly smaller 5.8-inch with the same resolution, but has an RGBW LCC LED display. Huawei did not note what levels of color accuracy these panels have, nor the brightness, and have not responded to our questions from several weeks ago on this. Also in favor of the P20 Pro is the battery, with the larger smartphone having a 4000 mAh battery, while the P20 has a 3400 mAh unit.\\n\\nHuawei P20 and P20 Pro P20 Pro P20 P10 Plus P10 P9 SoC HiSilicon Kirin 970\\n\\n4x Cortex A73\\n\\n4x Cortex A53\\n\\nMali-G72MP12 HiSilicon Kirin 960\\n\\n4x Cortex A73\\n\\n4x Cortex A53\\n\\nMali-G71MP8 HiSilicon Kirin 955\\n\\n4x Cortex A72\\n\\n4x Cortex A53\\n\\nMali-T880MP4 Display S 6.1-inch\\n\\nOLED 5.8-inch RGBW 5.5-inch\\n\\nIPS 5.1-inch\\n\\nIPS 5.2-inch\\n\\nIPS R 2240x1080 2240x1080 2560x1440 1920x1080 1920x1080 Dim H - - 153.5 mm 145.3 mm 145 mm W - - 74.2 mm 69.3 mm 70.9 mm D 7.8 mm 7.65 mm 7 mm 7 mm 7 mm RAM 6 GB 4 GB 4 / 6 GB 4 GB 3 / 4 GB NAND 128 GB\\n\\n+ microSD 64 / 128 GB\\n\\n+ microSD 32 / 64 GB\\n\\n+ microSD 32 / 64 GB\\n\\n+microSD Battery 4000 mAh 3400 mAh 3750 mAh 3200 mAh 3000 mAh IP Rating IP67 IP53 - - - Front Camera 24MP f/2.0 8MP f/1.9 8MP f/2.4 Rear\\n\\nCamera #1 20 MP BW\\n\\nf/1.8 20 MP BW\\n\\nf/1.8 20 MP BW\\n\\nf/2.2 12 MP\\n\\nf/2.2\\n\\n1.25 micron Rear Camera #2 40 MP RGB\\n\\nf/?\\n\\n1 um\\n\\n10 MP Mode\\n\\n102400 ISO 12MP RGB\\n\\nf/1.6\\n\\n1.55um 12 MP RGB\\n\\nf/1.8, OIS 12 MP\\n\\nf/2.2\\n\\n1.25 micron Rear Camera #3 8 MP f/2.4\\n\\nTelephoto\\n\\n5x Hybrid Zoom - - - - Modem Cat 18 / 13 LTE\\n\\n5CA / 2CA Cat 12 / 11 Cat 6 SIM Size Dual Nano SIM Dual Nano SIM Dual Nano SIM Wireless 802.11ac 2x2 MIMO 802.11ac\\n\\nBT 4.2\\n\\nNFC\\n\\nA-GPS\\n\\nGLONASS\\n\\nBDS\\n\\nGALILEO 802.11ac\\n\\nBT 4.2\\n\\nNFC\\n\\nA-GPS\\n\\nGLONASS\\n\\nBSD Connectivity USB Type-C USB Type-C\\n\\n3.5mm headset USB Type-C\\n\\n3.5mm headset Features Fingerprint Sensor\\n\\nAI Assisted Video Capture\\n\\nPixel Fusion\\n\\n4D Predicative Focus\\n\\n960 FPS video (720p)\\n\\nMaster AI\\n\\nAI Image Stabilization Fingerprint Sensor\\n\\nPantone Color\\n\\nBokeh\\n\\nHighlights / GoPro\\n\\nUltra Memory\\n\\nFingerprint Sensor\\n\\nForce Touch (P9 Plus)\\n\\nKnuckle Support OS EMUI 8.1 EMUI 5.1 EMUI 4.1 Price 6/128: 899€ 4/128: 649 € 4/64: 699€\\n\\n8/128: 799€ 4/64: 649€ 3/32: 599€\\n\\n4/64: 649€\\n\\nCompared to the previous generation P-series devices, the new units are certainly thicker, going beyond the 7mm z-height metric of the past generations. The Kirin 970 upgrade was likely a given based on the cadence of the family, however due to the new features it is clear that Huawei wants to push more of the HiSilicon leadership and the newer AI features through the stack. The P20 will ship with Huawei’s latest version of EMUI, which includes new features to allow for the user to go shopping using the AI engine to identify brands and objects, with the chip now supporting distributing deep learning platforms like PaddlePaddle.\\n\\nPricing and Availability\\n\\nAs this is being written, Huawei is set to launch the P20 and P20 Pro in an hour or two. Traditionally the company keeps the pricing information close to its chest until the major presentation. We will update when the information is released, however we expect it to be within short distance of pricing in previous years to keep the brand positioning.\\n\\nIan: If anyone asks, I'm pro-Notch. The push to more screen real estate means that this is, if anything, inevitable. As long as the software (such as video) compensates for it, then I don't see much problem with it.\\n\\nUpdate:\\n\\nPricing was announced for the devices. The P20, in a 4GB / 128GB configuration, will retail at 649 Euros from today. This is double storage for the same price as the P10, making it an interesting upgrade. The P20 Pro, in a 6 GB / 128GB configuration, will retail at 899 Euros from April 6th. This price is substantially higher than both the P20 and the P10 Plus from last year, indicating a very bifurcated strategy.\\n\\nHuawei also announced the Porsche Design Mate RS model, which uses the P20 internals in a new design. We'll post on that in a bit, but the 6 GB/256 GB edition will retail at 1695 Euro, while the 6GB/512GB edition will retail at 2095 Euro.\\n\\nRelated Reading\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Here’s what you get when you cross dinosaurs and flowers with deep learning', 'Apr 20, 2018\\n\\nNeural networks have shown usefulness with a number of things, but here is an especially practical use case. Chris Rodley used neural networks to create a hybrid of a dinosaur book and a flower book. The world may never be the same again.')\n",
      "('Storm brewing? Weather buff uses deep learning to predict patterns', \"Meteorologists are starting to experiment with deep learning tech to predict severe weather patterns.\\n\\nDavid Gagne, a postdoctoral researcher at the US National Center for Atmospheric Research (NCAR), developed a simple convolutional neural network model to forecast the chances of hailstorms.\\n\\nIn the last decade, severe storms caused about $14bn worth of damage and killed 94 people per year, Gagne said during a presentation at the GPU Technology Conference in San Jose, California.\\n\\nMeteorologists begin warning people of severe weather conditions the day before a hail event, but it's difficult to be precise. So Gagne wanted to see if deep learning could accurately identify the weather patterns leading up to hailstorms and reduce false alarms.\\n\\nHe fed the neural network a series of images showing storm activity over the US taken from NCAR's Real Time Ensemble Forecasts. Details about the temperature, wind conditions, atmospheric pressure levels were added to create a profile for each storm.\\n\\nGone in 60.121 seconds: Your guide to the pricey new gear Nvidia teased at its annual GPU fest READ MORE\\n\\nHail starts off as graupel – small icy particles that act as embryos for hail – Gagne told The Register. As they whirl around in a storm, they collect small droplets of water that freeze on its surface. Over time, they grow in size to become solid ice lumps that fall as hail. If hail reaches three inches in diameter, it can crack windscreens and leave walls and roofs pockmarked.\\n\\nHe trained the neural network to learn the features associated with storms in the images in the training data. Since the hail process requires the presence of a storm, the model was trained to look out for properties such as wind shear, a change of wind speed along different directions and height – a good predictor that a storm is brewing.\\n\\nThe goal was to highlight the same storm regions on radar images that a standard statistical model would pick out when monitoring the risk.\\n\\nAfter training on 82,000 different storm profiles, Gagne tested the neural network on 32,000 storms and saw it was about 88 per cent accurate.\\n\\nThat's a pretty decent result, considering the model is only three layers deep compared to state-of-the-art image classification models that have hundreds of layers. It took 15 hours to train the model on eight of Nvidia's Tesla K40 GPU chips, an older model launched in 2013.\\n\\nThe model is a proof of concept and not complex enough to be used in real weather warning systems yet. It'll be a while before deep learning takes over in meteorology since it requires better algorithms and a change of infrastructure to invest in more GPU supercomputer clusters. But more meteorologists are now interested in AI and deep learning than ever before, Gagne said. ®\\n\\nSponsored: Minds Mastering Machines - Call for papers now open\")\n",
      "('Creep learning: How to tamper with neural nets to turn them against us', 'Bit boffins from universities in China and the US have devised a way to tamper with deep learning models so they produce misleading results.\\n\\nIn a paper posted to pre-print service ArXiv, \"PoTrojan: powerful neural-level trojan designs in deep learning models,\" authors Minhui Zou, Yang Shi, Chengliang Wang, Fangyu Li, WenZhan Song and Yu Wang describe a technique for inserting trojan code into deep learning models.\\n\\nThe researchers, affiliated with the Chongqing and Tsinghua Universities (China), and the University of Georgia in the US, contend that malicious neural networks represent \"a significant threat to human society of the AI era.\"\\n\\nDeep learning is a form of machine learning that can be used to train a neural network to predict results based on input data. The term \"deep\" here means there are multiple hidden layers where mathematical computations occur to transform the inputs into a predictive output.\\n\\nThe researchers suggest that pre-trained neural network models, used for things like facial or image recognition in devices or in the cloud, can be subtly altered to produce malicious results in specific circumstances using malware they dub PoTrojan.\\n\\nDuring normal operation, these neural networks appear to be working fine; however, shown a particular input, produce a very different result. For example, a model trained to spot guns from other objects can be slightly tampered with to ignore firearms of a particular shape.\\n\\n\"Most of the time, the PoTrojan remains inactive, without affecting the normal functions of the host [neural network] model,\" the paper explains. \"It is only activated upon very rare input patterns that are carefully chosen by its designers.\"\\n\\nPre-trained models appeal to organizations that are disinclined to go through the time-consuming process of training their own neural network models, and instead want something they can slap into a phone or app. What they bung into their gear, though, may look sane – until the hidden trojan is activated.\\n\\nImplementation\\n\\n\"We are still working on the implementation of inserting the proposed PoTrojans in real-life pre-trained learning models,\" explained Minhui (Michael) Zou, one of the paper\\'s authors, in an email to The Register.\\n\\n\"The pre-trained models are comprised of architecture files and parameter files. Hence, inserting the Protrojans could be done by modifying the model files. Currently, this could not be done remotely.\"\\n\\nThe research builds upon previous work that explored manipulation of machine-learning models. Prior research has involved retraining models to make them produce bad results, an approach that changes the parameters of the original model and affects the error rate.\\n\\nThe group, however, claims its technique avoids these potentially noticeable interventions.\\n\\n\"The modification would require adding some values in the parameter files and changing the architecture codes to insert the PoTrojans,\" said Zou. \"Note that adding parameters or changing the architecture codes are not changing the existing parameters or the architecture of the model designs, respectively.\"\\n\\nGiven access to the training instance of the target predictions or classification labels, the researchers just have to train the neural inputs adjacent to the layer where the PoTrojans are inserted, a more efficient approach, they claim.\\n\\nThe attack is not particular dramatic: It involves changing some values in stored files, which assumes access to a target computer. But it could lead to problems.\\n\\nZou cites as an example the possibility that an attacker might mess with an image recognition system.\\n\\n\"Once the adversary system is fed with the specific picture, it would label the specific picture as the desired object that the adversary chooses,\" he explained.\\n\\nIn other words, it\\'s a way to say, \\'These aren\\'t the droids you\\'re looking for,\\' in the language of machines. ®\\n\\nSponsored: Minds Mastering Machines - Call for papers now open')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('AWS baits cloud hooks with DeepLens machine learning camera', 'At the AWS Summit in San Francisco on Wednesday Amazon Web Services invited a handful of tech typers to see a demonstration of AWS DeepLens, its forthcoming camera tuned for deep learning tasks.\\n\\nAnnounced late last year and given away in limited quantities to select developers, the image capturing kit is intended as an AI onramp for coders.\\n\\nAmazon has been taking pre-orders for the device since its announcement; it now claims the device will ship June 14, 2018, at a cost of $249.\\n\\n\"The goal here is for developers who have limited or no machine experience to use deep learning, to get up and running quickly,\" said Matt Wood, general manager of artificial intelligence for AWS.\\n\\nThere\\'s a good reason for developers delve into AI disciplines: Companies are looking to hire AI experts. As with security, high-level talent is scarce.\\n\\nBut beyond potential employment benefits and irrepressible curiosity, there\\'s not really a compelling reason to take up tinkering with machine learning unless you regularly deal with challenges involving large data sets or you really just cannot resist a shiny gadget.\\n\\nFor coders involved in finance or security, or social photo apps that handle millions of images, machine learning is a no-brainer. But for those without massive datasets – millions of user profiles spirited out of Facebook via its now-neutered API that have been squirreled away on hard drives, for example – it can be a struggle to come up with non-trivial use-cases.\\n\\nAmazon last year suggested replicating the hot dog detection app featured in an episode of HBO\\'s tech skewering comedy Silicon Valley. And indeed, DeepLens can do that.\\n\\nBut people do that even better. Yes, you could write an app to detect when your dog has climbed onto your couch. But you don\\'t have to pay Amazon for that privilege; you could just pay attention.\\n\\nOr you could take on a slightly greater challenge by assembling an image recognition system with a Raspberry Pi, Python, and OpenCV from scratch. It would be an educational experience in its own right.\\n\\nBut if you\\'re already hooked on AWS cloud crack and can\\'t wait to invite another IoT device into your life, the bill shouldn\\'t be too high. Woods said most models can be trained within AWS\\' free tier. ®\\n\\nSponsored: Minds Mastering Machines - Call for papers now open')\n",
      "('Gone in 60.121 seconds: Your guide to the pricey new gear Nvidia teased at its annual GPU fest', \"GTC Nvidia CEO Jensen Huang flaunted a bunch of stuff, from bigger boxes of graphics chips to robot simulators, at its 2018 GPU Technology Conference (GTC) in Silicon Valley on Tuesday.\\n\\nHere's a quick summary of what went down, minus the fluff.\\n\\nHuang acknowledged in a Q&A session that there is indeed a shortage right now of top-end Nv GPUs, mainly due to people grabbing them for cryptocurrencies and blockchain ledgers. Nvidia simply can't make enough Tesla chips to go around, partly due to demand and partly due to yield, we reckon. Thus far, the chip designer's only solution is: make and ship as many GPUs as possible. The biz wants to concentrate on giving hardware to cloud and supercomputer builders, gamers, graphics artists, scientists, engineers, enterprises... anyone but those annoying crypto-kids.\\n\\nIf you've got roughly a million dollars to blow on deep-learning research, Pure Storage and Nvidia have produced a hyperconverged stack of flash memory and flagship Tesla Volta GV100 GPUs, plus some extra bits and pieces, called AIRI. Assuming the GPUs are available, natch.\\n\\nProgrammers, engineers, and other techies dreaming of creating robots that sport some sort of machine intelligence can ask nicely to check out Isaac: this is a forthcoming software development kit and simulator, with libraries, drivers and other tools, for designing, testing and building machine-learning-based robotic gizmos.\\n\\nSpeaking of programmers, TensorRT 4 – a GPU-accelerated deep-learning inference framework – has landed. Nvidia and Google boffins have integrated TensorRT into TensorFlow 1.7, if you prefer to use that engine for AI coding.\\n\\nNvidia is stepping into the world of networking with the NVSwitch, a switch for its high-speed NVLink interconnect. Meanwhile, Tesla V100 GPUs – Nvidia's top of the line chips for data centers – are now available with 32GB of HBM2 memory rather than the usual 16. Tying it all together is the new DGX-2 workstation, which has 16 32GB V100s connected via 12 NVSwitches for 2.4TB/s of bisection bandwidth and 512GB total HBM2 memory. The box has 1.5TB of system memory, two Intel Xeon CPUs, 30TB of flash storage, and Infiniband, 100GbE, and 10/25GbE interfaces. Nvidia claims it can hit as high as 2 PFLOPS with mixed-precision floating-point math. Yours for $400,000. This gear builds upon the $150,000 DGX-1 and DGX Station previously launched.\\n\\nNvidia has jumped aboard Project Trillium, Arm's effort to cram AI inference processing into chips powering wearables, gadgets, and Internet-of-Things devices. This is aimed at silicon designers: Nvidia is offering NVDLA as a free and open architecture for building deep-learning accelerators into hardware.\\n\\nIf you're working on self-driving cars – and who isn't – you may or may not be tempted by Nvidia's new Drive Constellation, a stack of boxes for simulating autonomous vehicle control software without crashing robo-rides or killing people (right, Uber?) Don't hold your breath – these GPU-accelerated machines won't arrive until the third quarter of 2018 at the earliest, and that's only for Nv's favorite customers.\\n\\nScientists, engineers and artists needing some serious fire power for simulations and rendering are offered the new Quadro GV100 with 32GB of HBM2 memory. Each offers up to 7.4 TFLOPS of double-precision floating-point math performance, or 14.8 TFLOPS with single-precision, and can be linked via an NVLink interconnect to form one GPU, doubling the maximum potential performance and HBM2 capacity. These should be available now direct from Nvidia, or from suppliers next month.\\n\\nFor those interested in virtualized GPUs in the cloud, there's a bunch of announcements here. And if you're a virtual reality designer or developer, there's stuff to tease you here.\\n\\nOf course, this is all subject to GPU availability. ®\\n\\nSponsored: Minds Mastering Machines - Call for papers now open\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Facebook open-sources object detection work: Watch out, Google CAPTCHA', \"RoTM Facebook has brought us one step closer to a Skynet future made a commitment to computer vision boffinry by open-sourcing its codebase for object detection, Detectron.\\n\\nWritten in Python and powered by the Caffe2 deep learning framework, the codebase – which implements object-sniffing algos such as Mask R-CNN and RetinaNet – is available under the Apache 2.0 licence.\\n\\nWant to learn machine learning in 15 minutes? Start here... READ MORE\\n\\nCAPTCHA jokes aside – it's about more than just identifying what's in an image. Normally, when we look at a still pic or video, we see multiple overlapping things and calculate their relative distances; relationship to each other; the fact that foreground and background objects are affected by the focal length of the camera (or the human eye); as well as where one object ends and the other begins. If your training data includes moving pics, you'll might need to look at relative size, positioning, or partial obstruction from frame to frame. These are some of the problems that ML researchers in the computer vision game – and specifically in object detection – are attempting to solve.\\n\\nFbook's framework includes implementations of algos that can not only detect and classify objects but can also simultaneously generate segmentation masks. Identifying selected regions of images as members of a group helps train a model to identify other objects of said group. Judging by Facebook's illustration in the blog (depicting a Mask R-CNN output), some researchers are achieving a decent hit rate.\\n\\nDetectron is the basis for many research projects at Facebook AI research – aka FAIR. The Zuckerborgs said researchers will be able to train models to be deployed in the cloud and on smartmobes and other mobile devices.\\n\\nWe can't help but picture a future where Zuckerbots and Googleborgs attempt to trick each other's computer vision while human users arm ourselves with blurry spatulas/chairs/lasers in hopes of fighting off the Machines. ®\\n\\nSponsored: Minds Mastering Machines - Call for papers now open\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Boffins try to grok dogs using AI, a cyber-brain charter, a bot running for mayor, and more', 'Roundup Here are a few bits and pieces from this week\\'s news in AI. Researchers have collected a dataset to analyze dog behaviour using neural networks, the first AI-assisted medical device for diagnosing diabetic retinopathy has been approved by the FDA, and, finally, an AI is running for mayor in Japan.\\n\\nWho’s a good doggo? A team of researchers have developed a machine learning model that attempts to predict and understand dog behaviour.\\n\\nThey attached sensors and a GoPro camera to a dog to collect video data, a canine is an Alaskan Malamute called Kelp M. Redmon. The clips show Kelp interacting with the environment around it with a dog\\'s eye view. Image stills from the video feed are then fed into a convolutional neural network as inputs and act as an embedding for a LSTM (long-short term memory network).\\n\\nThe LSTM processes the features of each progressive image from the clip over each time step, and is trained to predict the next frame. For example, it might be given images of a human throwing a ball that bounces past Kelp, and the neural network guesses that she will scramble and move right for the ball.\\n\\nIn a paper published on arXiv, the researchers from the University of Washington and the Allen Institute for AI, said the work was “a first step towards end-to-end modelling of intelligent agents. This approach does not need manually labeled data or detailed semantic information about the task or goals of the agent.”\\n\\nDogs obviously rely on a lot more than vision to navigate the world. The researchers hope to include more sensory data such as smell or touch. It’s also limited to one type of dog, and are interested to see if their work maps to multiple dogs across different breeds.\\n\\n“We hope this work paves the way towards better understanding of visual intelligence and of the other intelligent beings that inhabit our world,” the paper includes.\\n\\nThe paper will be presented at the Conference of Computer Vision and Pattern Recognition (CVPR) later this year in June.\\n\\nDeepMind gets a new COO - DeepMind have employed Lila Ibrahim as its first chief operating officer, it announced on Wednesday.\\n\\nIbrahim began her career in technology working for Intel as a microprocessor designer, assembler programmer, business development manager and rose to be chief of staff to its CEO & Chairman, Craig Barrett. She also was president and COO of Coursera, a company focused on education offering a variety of courses online.\\n\\nShe will work alongside DeepMind’s co-founders: Demis Hassabis, CEO; Shane Legg, chief scientist; and Mustafa Suleyman, head of applied AI.\\n\\nFDA approves AI medical gizmo for diabetic retinopathy The US Food and Drug Administration have given the green light to the first medical AI device that uses algorithms to detect diabetes in retinal scans.\\n\\nThe company, IDx LLC, developed the tool known as IDx-DR. The FDA found it could detect mild diabetic retinopathy to an accuracy of 87.4 per cent, and was able to identify patients who did not the disorder to an accuracy of 89.5 percent.\\n\\n“IDx-DR is the first device authorized for marketing that provides a screening decision without the need for a clinician to also interpret the image or results, which makes it usable by health care providers who may not normally be involved in eye care,” the FDA said.\\n\\nIt means IDx can now sell its devices to hospitals and clinics. Retinopathy is a well-known area in medicine and AI. Even Google has taken a stab at the problem, and have used machine learning to tell a patient’s risk of heart disease and even if they’re a smoker or not from retinol scans to a decent accuracy.\\n\\nFancy a trip to Korea? If you’re a pretty good at TensorFlow and deep learning and want to get away, then maybe consider applying to Deep Learning Camp Jeju at Jeju Island, Korea.\\n\\nThe month-long bootcamp will let you work on a deep learning project with mentors surrounded by about 20-30 participants. If you get accepted, you’ll get a $1,000 stipend (£811.50) $300 (£243.45) towards your flights and $1,000 worth of Google Cloud credits with access to its TPUs.\\n\\nNo visas are required if you plan to stay less than 30 days. The event is organised by TensorFlow Korea, and is a push to advance deep learning in Korea.\\n\\nPrevious projects have included computer vision research self-driving cars, recommender systems, and GANs. It’s all sounds pretty sweet, and you can apply here.\\n\\nOpenAI’s AGI strategy OpenAI published a charter to help guide its long-term mission of creating artificial general intelligence (AGI).\\n\\nAGI is a contentious topic. Some believe the world is deathly close to developing crazed killer robots (looking at you Elon and the now deceased Hawk), others believe it’s a useless term, some think it’s an impossible feat.\\n\\nThe charter is pretty interesting, nevertheless. It’s the first time a major AI research lab has declared it will stop its work in creating AGI if another project gets there first, on the condition that it won’t be used maliciously.\\n\\n“We are concerned about late-stage AGI development becoming a competitive race without time for adequate safety precautions. Therefore, if a value-aligned, safety-conscious project comes close to building AGI before we do, we commit to stop competing with and start assisting this project. We will work out specifics in case-by-case agreements, but a typical triggering condition might be “a better-than-even chance of success in the next two years,” it said.\\n\\nIt also warned that as safety and and security issues escalate as AI progresses, it might have to be more careful about publishing research so openly in the future.\\n\\n“We are committed to providing public goods that help society navigate the path to AGI. Today this includes publishing most of our AI research, but we expect that safety and security concerns will reduce our traditional publishing in the future, while increasing the importance of sharing safety, policy, and standards research.”\\n\\nOther points on the charter include the usual announcements around building safe AI that will benefit humanity.\\n\\nEU member states sign AI deal - Twenty-five countries under the European Union signed a “Declaration of cooperation on Artificial Intelligence”.\\n\\nThe deal promises to work together on the most pressing issues in AI, including ethical and legal issues, competitiveness in research, and where and how it should be deployed. It means that there should be more funding for research, development, and industry.\\n\\nAustria, Belgium, Bulgaria, Czech Republic, Denmark, Estonia, Finland, France, Germany, Hungary, Ireland, Italy, Latvia, Lithuania, Luxembourg, Malta, Netherlands, Poland, Portugal, Slovakia, Slovenia, Spain, Sweden, UK, and Norway, all signed the agreement.\\n\\nAssess your algorithms The AI Now Institute at New York University have published a framework to help companies and public agencies assess the impact of its algorithms.\\n\\nThe Algorithmic Impact Assessment (AIA) report can be summed up in five points:\\n\\n1. Agencies should conduct a self-assessment of existing and proposed automated decision systems, evaluating potential impacts on fairness, justice, bias, or other concerns across affected communities.\\n\\n2. Agencies should develop meaningful external researcher review processes to discover, measure, or track impacts over time.\\n\\n3. Agencies should provide notice to the public disclosing their definition of “automated decision system,” existing and proposed systems, and any related self-assessments and researcher review processes before the system has been acquired.\\n\\n4. Agencies should solicit public comments to clarify concerns and answer outstanding question.\\n\\n5. Governments should provide enhanced due process mechanisms for affected individuals or communities to challenge inadequate assessments or unfair, biased, or otherwise harmful system uses that agencies have failed to mitigate or correct.\\n\\nAlthough the AIA is inspired by other impact assessment such as environmental protection, data protection, privacy, or human rights, it can’t be legally enforced. So it relies on the good nature of organizations.\\n\\nDespite this, Jason Schultz, professor of clinical law at NYU and a senior advisor on technology policy in the White House under Obama, told The Register, he does believe many companies will happily audit their own algorithms.\\n\\n“The pressure for algorithmic accountability has never been greater, especially for public agencies. We believe that it’s urgent that public agencies begin evaluating algorithmic decision-making with the same level of scrutiny as these other areas [such as ] environmental effects, human rights, data protection, privacy, etc.\"\\n\\n\"And lawmakers are finally beginning to take this issue seriously. So I would anticipate many agencies adopting these frameworks voluntarily or with minimal policy interventions. Otherwise, they run huge risks of inflicting harms on the very people they are meant to serve and ultimately undermining public trust in government to help improve our lives with new technologies.\"\\n\\n“Ultimately, this will require a multi-pronged approach. Legislation will likely be part of that. We also believe it’s imperative for companies developing these systems to take responsibility for providing transparency and ensuring that they do not create unintended harm.”\\n\\nThe AI Institute also calls for an “independent, government-wide oversight body” to take on third-party auditing to avoid any conflicts of interest.\\n\\nAn AI overlord In other news: An AI is running for mayor in Tama City, Japan.\\n\\nMichihito Matsuda is a unique mayoral candidate. He? She? It? really is different from all the other politicians. (Matsuda, dressed all in silver, has quite feminine features so El Reg will refer to as her for now). Matsuda isn’t even human for god’s sake, but her supporters are.\\n\\nTetsuzo Matsumoto, a senior advisor to Softbank and Norio Murakami, an ex-representative for Google Japan are fans apparently, according to Otaquest.\\n\\nRemember when Saudi Arabia granted Sophia, a bald, creepy robot citizenship? This could the beginning of the end for politicians, wouldn\\'t that be a shame. ®\\n\\nSponsored: Minds Mastering Machines - Call for papers now open')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"The only way is Ethics: UK Lords fret about AI 'moral panic'\", 'The House of Lords wants to make sure data used by AI systems is not monopolised and the technology is developed on ethical guidelines.\\n\\nThat’s not impossible, Lord Tim Clement-Jones, chair of the Lords’ Select Committee on Artificial Intelligence told us.\\n\\n“We looked to see where it had been done well and done badly. GM Foods is where it was done badly, and human embryo research is where it has been done well.”\\n\\nThe report was designed “to avoid a moral panic,” Clement-Jones told us.\\n\\nThe Committee gathered oral evidence from over 200 witnesses - including “Captain Cyborg”, Professor Kevin Warwick, and a witness from The Register - last year. The report avoided the boosterism associated with often overheated media reporting of AI, and sensibly steered clear of making sweeping predictions about effects on the job market caused by automation. In fact, such claims are contested, it noted.\\n\\nEl Reg was invited to the House of Lords to burst the AI-pocalypse bubble READ MORE\\n\\n“The representation of artificial intelligence in popular culture is light-years away from the often more complex and mundane reality,” the Report stated. Researchers and academics were a bit fed up with this, they told the Select Committee, the report also noted that thanks to the media, the public “were concentrating attention on threats which are still remote, such as the possibility of \"superintelligent\" artificial general intelligence, while distracting attention away from more immediate risks and problems.\\n\\nElon Musk, the late Professor Stephen Hawking and Professor Nick “simulation hypothesis” Bostrom were guilty of regularly chiming in with doomsday scares. The Lords painted a near-future scenario that they want to avoid: where decisions are taken on people’s behalf.\\n\\nClick to enlarge\\n\\nIt’s notable how much of this - phishing and recommendations - have already used statistical techniques, without \"AI\". The report noted that when it comes to liability for autonomous systems, the lawyers can’t agree. Some think AIs should be given legal personhood, others don’t. The Committee recommended the Law Commission get stuck in to “provide clarity”.\\n\\nThe Lords also want a role for the Competition and Markets Authority to ensure data isn’t hogged by five big companies.\\n\\n“We call on the Government, with the Competition and Markets Authority, to review proactively the use and potential monopolisation of data by big technology companies operating in the UK.”\\n\\nThere’s a risk five companies will simply “maintain their hegemony”. The Lords also envisage an active, post-Brexit role for the Information Commissioner. “We don’t want people to wake up to situations where the use of the data has been unfair or biased,” the Chairman of the Committee told us.\\n\\nIn terms of the “Fourth Industrial Revolution”, a sweeping restructuring of the employment predicted by futurologists, the Lords noted that “the economic impact of AI in the UK could be profound”.\\n\\nThe UK has a poor productivity story - largely due to a shift to more services-based economy - and has seen no productivity growth since 2008. Here they found that important sectors, specifically many SMEs and the NHS, are failing to pocket the benefits of IT, never mind AI. The NHS is the largest purchaser of fax machines in the world. Therefore, the Lords said, it’s as important to improve broadband.\\n\\n“The NHS does not have a joined up approach to data, how it’s used, or licensed or shared,” Clement-Jones told us.\\n\\nThe report noted that AI is a “boom and bust field”, and that the last time the Government chucked money at AI in the Expert Systems era, it didn’t end well. The rule-based AIs were characterised by “high costs, requirements for frequent and time-consuming updates, and a tendency to become less useful and accurate as more rules were added”. The equivalent of a billion quid was invested in AI in the 1980s, with the taxpayer chipping in the equivalent of £200m. We haven’t quite reached that stage yet.\\n\\nThe Lords enquiry began in the heat of the hype about AI last autumn, since when the ardour has cooled somewhat. Professor Geoffrey Hinton’s qualms about techniques reaching their limit are noted. Early this year neuroscientist, author and entrepreneur Gary Marcus as made some pointed criticism in his paper Deep Learning: A Critical Appraisal.\\n\\nIn December, Monsanto had reportedly attempted 50 deep learning experiments and experienced a “95 per cent failure rate”. That means at least 49 failed completely, and one half worked.\\n\\nYou can find the report here (PDF). ®\\n\\nBootnote\\n\\nLast October I gave oral evidence to the Committee in a session on the media and AI. The main message was that hopes of radical improvements in industrial automation may be misplaced, as machine learning didn\\'t do a lot to enhance robotics, and great results in microworlds or games didn\\'t necessarily translate to real world progress: something acknowledged by the experts as Moravec\\'s Paradox. By conflating driverless cars with AI, the media may have raised expectations far beyond what the technology can achieve. An unscripted digression did make it in:\\n\\nClick to enlarge\\n\\nThis was prompted by the thought that if you had twenty minutes in which to place some new knowledge inside Donald Trump\\'s brain (that his brain would somehow miraculously retain), the world would be safer if that knowledge was some history, say, rather than some ML technique.\\n\\nSponsored: Minds Mastering Machines - Call for papers now open')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Looking to nab Nvidia's GeForce chips? You need cash and patience\", \"Tech companies are suffering setbacks from the shortage of Nvidia’s GPUs, with the GeForce series being hit the hardest.\\n\\nFollowing his keynote speech at his biz's GPU Technology Conference in San Jose this week, Nvidia CEO Jensen Huang explained to journalists that it was down to intense Ethereum mining.\\n\\nVendors working at the booths during the conference told The Register that lead times for building GPU workstations and servers have increased. Nvidia’s GeForce chips, an older and cheaper series, are most in demand – especially the GTX 1080 Ti.\\n\\nA chart on PCPartPicker, a website that helps users compare prices for computer parts, shows clear spikes in the price for GeForce cards from the past 18 months. GTX 1080Tis are now about $1,000, a rise of about $300 compared to prices in March last year.\\n\\nGone in 60.121 seconds: Your guide to the pricey new gear Nvidia teased at its annual GPU fest READ MORE\\n\\nA sales manager from Tyan Computer Corporation, a Taiwanese-based company that manufactures motherboards for high-end servers, told us that it put in an order for about 300 GeForce chips in January, and only received 100 of them recently.\\n\\nHe estimated that the wait time could be as long as 16 weeks. An employee from Exxact, who also offer high performance computing servers and racks, said it was about a 12 to 16 week wait.\\n\\nEthereum is the most popular type of cryptocurrency where GPUs are favored over ASIC chips for mining. Since GeForce chips are much cheaper than Nvidia’s Tesla Volta V100 chips that go for several thousands of dollars, miners get more bang for their buck.\\n\\nThe wait times for the chips in the Tesla series is much shorter and is roughly on the order of two to four weeks. Nvidia produce a lower volume of Tesla chips, aimed at more intensive workloads for deep learning and AI.\\n\\nHuang said Nvidia is not in the business for cryptocurrency mining, and wants to keep GeForce chips for the gaming industry. In fact, the GPU giant updated its end-user license agreement last year in an attempt to force customers to cough up for its higher end gear like the Tesla V100 chips in data centers.\\n\\nAn Nvidia spokesperson previously told us: “GeForce and Titan GPUs were never designed for data center deployments with the complex hardware, software, and thermal requirements for 24x7 operation, where there are often multi-stack racks. To clarify this, we recently added a provision to our GeForce-specific EULA to discourage potential misuse of our GeForce and TITAN products in demanding, large-scale enterprise environments.”\\n\\nWhen El Reg asked Nvidia how long it would take to clear its backlog of orders, a spokesperson told us: “We have nothing further to add.” ®\\n\\nSponsored: Learn how to transform your data into a strategic asset for your business by using the cloud to accelerate innovation with NetApp\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('How much do AI gurus really get paid? And is NIPS such a great name for a conference?', 'Roundup Hi, here\\'s a few interesting bits and pieces from the world of AI. A public tax form from OpenAI reveals the crazy salaries of top AI researchers. There are more competitions pushing for improved image recognition models on mobiles, as well as training systems as fast and cheap as possible.\\n\\nImage recognition on mobiles Google has launched another computer vision challenge to push image recognition in real time for mobile phones.\\n\\nThe On-Device Visual Intelligence Challenge (ODVI) is part of a workshop track at the Computer Vision and Pattern Recognition conference (CVPR) happening in June later this year in Salt Lake City.\\n\\nIt’s challenging to build fast, accurate models on small mobile chips given the latency limit. ODVI will focus on devising a unified metric that measures the “number of correct classifications within a specified per-image average time limit of 33 [milliseconds]”. Latency has been tricky to measure, and without a solid benchmark it’s difficult to compare different models.\\n\\nParticipants will be given these tools to get started, according to Google’s blog post.\\n\\nTOCO compiler for optimizing TensorFlow models for efficient inference\\n\\nTensorFlow Lite inference engine for mobile deployment\\n\\nA benchmarking SDK that can be run locally on any Android phone\\n\\nSample models to showcase successful mobile architectures that run inference in floating-point and quantized modes\\n\\nGoogle’s benchmarking tool for reliable latency measurements on specific Pixel phones.\\n\\nYou can register for the competition here before the deadline on 15 June.\\n\\nCash rules everything around me OpenAI’s public tax forms reveal just how much companies are willing to splash on AI experts and it\\'s a lot.\\n\\nIn 2016, the non-profit research lab backed by Elon Musk paid one of its co-founders and research director Ilya Sutskever a jaw-dropping $1.9m (£1.36m). Ian Goodfellow, known for his work on general adversarial networks, was second on the list and made $808,243 (£577,000). He has since moved to Google Brain. The third big name was the roboticist Pieter Abeel, a professor at the University of California, Berkeley and an adviser at OpenAI, who raked in $425,000 (£303,000).\\n\\nThe form was first spotted by The New York Times.\\n\\nAI conferences like the Neural Information Processing Systems (NIPS) have become perfect environments to poach researchers and engineers. Many companies organise exclusive parties and events, inviting a select group of attendees in an attempt to woo them.\\n\\nSince OpenAI are non-profit, the average salaries are likely to be slightly lower than the amount offered by private companies, who also give employees generous packages when they join with stock.\\n\\nSo it\\'s not too shabby to be an AI engineer right now.\\n\\nNIPS?! Organisers of the biggest machine learning and AI conference, the aforementioned NIPS, are thinking of changing the conference’s name.\\n\\nIn a tweet, the executive board announced that they will be asking the community for suggestions at the end of May.\\n\\nThe NIPS executive board is currently discussing the possibility of changing the name of the NIPS conference. At the end of May, we will ask the whole NIPS community for input and suggestions for a potential new name. Please be patient until then. — NIPS (@NipsConference) April 18, 2018\\n\\nThe move has been applauded by people hoping to move beyond a name that has been used as a racist term describing Japanese people or a nickname for nipples. Others, however, believe that it\\'s not a very welcoming name for women, and to prove the point a female-led group organized the Transformationally Intelligent Technologies Symposium (TITS) to highlight this.\\n\\nBeyond accuracy Researchers are racing to submit their fastest and cheapest models trained on key datasets for the DAWNBench competition, which closes tomorrow at just before midnight.\\n\\nMost benchmarks in machine learning focus on accuracy. DAWNBench, however, optimizes for training time, training and inference costs and inference latency and takes into account the different model architectures, software used and hardware.\\n\\nIt measures progress on three different datasets: ImageNet and CIFAR10, both for computer vision, and SQuAD, a question answering dataset for natural language processing.\\n\\nA quick scan at the current scoreboards shows that there isn’t a single magic bullet for choosing hardware or software to get you fast, cheap and accurate models.\\n\\n“Deep learning models are fine, but researchers mainly ignore the simple tricks that make them much faster to train,” Jeremy Howard, founder of fast.ai, a popular online deep learning course, and a researcher at the University of San Francisco, told The Register.\\n\\nHoward and his team are currently ranked first and second on training times and costs for a model trained on CIFAR-10. ”We\\'re trying to show that small groups with limited resources can make a big impact, he said.”\\n\\nThe latest scores show its possible to train a ResNet50 in about half an hour with half a cloud TPU pod (32 TPU2s). It’s also interesting to see an AmoebaNet, a model born from Google’s AutoML project that uses machine learning to design the architecture of new machine learning models, top the list for cheapest model to train on ImageNet. It shows some promise in using evolutionary search to build systems.\\n\\nThe times for training on SQuAD are much longer than the image recognition datasets, and training is much more expensive to execute than inference because it’s much more computationally intensive.\\n\\nHey Siri Apple have published a long and detailed blog post on the machine learning technology needed to personalize the words ‘Hey Siri’ needed to activate its personal assistant Siri.\\n\\nIt’s a “key-phrase detection” problem”, and Apple uses a recurrent neural network to recognise the words Hey Siri. To minimize the annoying problem of Siri turning on when the user doesn’t want it to, Apple talks about needing to personalize the system to recognize the primary user of the phone.\\n\\nThe speaker recognition model is split into two parts: enrollment and recognition. Enrollment involves the user speaking into the phone to give it a few vocal training samples. A statistical model learns the unique features of the user’s voice.\\n\\nNext, the recognition phase makes sure to accept or reject the activation of Siri based on how similar an announcement of ‘Hey Siri’ sounds compared to the user’s voice.\\n\\n“Although the average speaker recognition performance has improved significantly, anecdotal evidence suggests that the performance in reverberant (large room) and noisy (car, wind) environments still remain more challenging,” Apple said in the blog post.\\n\\n\"One of our current research efforts is focused on understanding and quantifying the degradation in these difficult conditions in which the environment of an incoming test utterance is a severe mismatch from the existing utterances in a user’s speaker profile.\" ®\\n\\nSponsored: Minds Mastering Machines - Call for papers now open')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Googlers revolt over AI military tech contract, brainiacs boycott killer robots, and more', 'Roundup Here is a round up of this week’s AI news beyond what we\\'ve already covered.\\n\\nGet ready for ethical debates around autonomous weapons, a free online AI course, and a cracking video of a Russian drone.\\n\\nStop killer robots!\\n\\nA large group on internationally renowned AI academics have signed an open letter threatening to boycott collaborations with a top South Korean University working to create autonomous weapons.\\n\\nThe letter comes just before the United Nations is set to discuss killer robots in a meeting next week. It has led by Toby Walsh, a professor of AI at the University of South Wales and lists more than 50 names, including Geoffrey Hinton, Yoshua Bengio, Stuart Russell, Zoubin Ghahramani and Jürgen Schmidhuber.\\n\\nIt accuses KAIST, a top university focused on science and engineering in South Korea, of partnering with Hanwha Systems, a leading arms company in South Korea to develop AI for military purposes.\\n\\n“At a time when the United Nations is discussing how to contain the threat posed to international security by autonomous weapons, it is regrettable that a prestigious institution like KAIST looks to accelerate the arms race to develop such weapons.\"\\n\\n\"We therefore publicly declare that we will boycott all collaborations with any part of KAIST until such time as the President of KAIST provides assurances, which we have sought but not received, that the Center will not develop autonomous weapons lacking meaningful human control. We will, for example, not visit KAIST, host visitors from KAIST, or contribute to any research project involving KAIST,” the letter said.\\n\\nIt warned that autonomous weapons would allow despots and terrorists to use them without ethical restraints on innocent people.\\n\\n“If developed, autonomous weapons will be the third revolution in warfare...This Pandora\\'s box will be hard to close if it is opened. As with other technologies banned in the past like blinding lasers, we can simply decide not to develop them. We urge KAIST to follow this path, and work instead on uses of AI to improve and not harm human lives.”\\n\\nKAIST and Hanwha Systems reportedly opened the Research Center for the Convergence of National Defense and Artificial Intelligence in February. In a statement, however, KAIST president Sung-Chul Shin denied plans of building killer robots, and said he was “saddened” by the risk of boycott.\\n\\nGoogle employees order CEO to drop out of Project Maven\\n\\nHere’s another letter, this time signed by thousands of Google employees. It urged CEO Sundar Pichai to scrap Google’s collaboration with the Pentagon in Project Maven, using AI to analyse drone footage.\\n\\nThe letter was first reported by the New York Times, and calls for Google to “cancel [the] project immediately”, and “draft, publicize, and enforce a clear policy stating that neither Google nor its contractors will ever build warfare technology.”\\n\\nIs there alien life out there? Let\\'s turn to AI, problem solver du jour READ MORE\\n\\nLast month, it was reported that Google is assisting the Pentagon in using one of its computer vision TensorFlow API’s to help track and identify objects in videos taken by drones.\\n\\nIt’s still unclear how involved Google are. But it does highlight two things: 1) The US military are increasingly using AI and machine learning in warfare, and 2) It’s not very good at developing the technology on its own.\\n\\nIt has sparked an ethical debate amongst Googlers. “We believe that Google should not be in the business of war. Therefore we ask that Project Maven be cancelled, and that Google draft, publicize and enforce a clear policy stating that neither Google nor its contractors will ever build warfare technology,” the letter read.\\n\\nIt also warns that continuing Project Maven will significantly tarnish Google’s brand and its ability to compete for talent, and that the moral responsibility of its technologies cannot be outsourced to third parties like the Pentagon.\\n\\nInternational Committee of the Red Cross’ AI ethics report\\n\\nAt El Reg, we like to keep things jolly. So here is yet another reminder of how AI may be used to kill everyone. The International Committee of the Red Cross published its report on the ethics of autonomous weapons based on a United Nations meeting from last year.\\n\\nThe main message from the report is that humans must, ultimately, remain in control of weapon systems, and that countries need to work out the limits on how autonomous future weapons can be.\\n\\nSome have argued that the technology could decrease innocent civilian casualties as weapons like drone strikes will be more precise. But the ICRC argue that “many take the view that decisions to kill, injure and destroy must not be delegated to machines”.\\n\\nEthical and legal considerations could make it a requirement that humans have the ability to intervene and deactivate the weapons. We should also think about the types of scenarios where they might be used, the types of target and the working environment.\\n\\nThe report also discusses the black box nature of deep learning, and warns that autonomous systems based on these types of algorithms makes them “inherently unpredictable”.\\n\\nA spokesperson from the ICRC told The Register: “There is a likelihood that increasingly autonomous weapon systems could become less predictable, particularly in case of increased mobility, increased adaptability and/or increased interaction of multiple systems (as swarms).\"\\n\\n\"The loss of predictability regarding the outcomes of using an autonomous weapon would raise serious legal and ethical concerns. Therefore, ICRC’s view is that governments must work urgently to establish internationally agreed limits on autonomy in weapon systems.”\\n\\nFree AI course\\n\\nMicrosoft have released an online course for software engineers interested in AI.\\n\\nIt’s made up of 10 courses, and includes an introduction to AI in Python, statistics and maths, ethical considerations, machine learning, deep learning, and reinforcement learning. They can be taken in any order and requires anywhere from 8-16 hours to complete, so some decent level of commitment is required.\\n\\nThere is also a “final project”, where Microsoft gives you one last problem for you to tackle and you’re given a grade and a certificate.\\n\\nThe program runs for three months: January - March, April - June, July - September and October - December.\\n\\nYou can enroll here.\\n\\nMusical AI chairs\\n\\nThere has been some reshuffling at Google’s headquarters. John Giannandrea, senior VP of engineering, who led the search and AI units has gone to Apple.\\n\\nNow Jeff Dean, who leads Google Brain, has stepped in as the company’s head of AI. Ben Gomes, VP of search engineering will take over the search unit.\\n\\nGood job!\\n\\nRussia’s first postal drone had a less than stellar first delivery.\\n\\nVideo shows a six-armed drone launching from a mat outside the post office on the floor. As it takes off, people crowd round and crane their necks following its path into the air.\\n\\nIt manages to go pretty high and seems to be flying fast, before it crashes into a brick wall.\\n\\nRussia\\'s first postal drone takes its maiden flight - and crashes straight into a wall. More from @ReutersTV: https://t.co/4WEQdacXFY pic.twitter.com/StcBC1DKJ1 — Reuters Top News (@Reuters) April 3, 2018\\n\\nBetter luck next time. ®\\n\\nSponsored: Minds Mastering Machines - Call for papers now open')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Get ready for the Internet of Battle Things, warns US Army AI boffin', 'A US Army researcher believes that wars will be fought with human soldiers commanding a team of physical and cyber robots to create a network of “Internet of Battle Things” in the future.\\n\\n“Internet of Intelligent Battle Things (IOBT) is the emerging reality of warfare,” as AI and machine learning advances Alexander Kott, chief of the Network Science Division of the US Army Research Laboratory.\\n\\nHe envisions a future where physical robots are able to fly, crawl, walk, or ride into battle. The robots as small as insects can be used as sensors, and the ones as big as large vehicles can carry troops and supplies. There will also be “cyber robots”, basically autonomous programmes, used within computers and networks to protect communications, fact-check, relay information, and protect other electronic devices from enemy malware.\\n\\nUS Army warns of the potential dangers of swarming toy drones on US soldiers READ MORE\\n\\n“In order to be effective in performing these functions, battle things will have to collaborate with each other, and also with the human warfighters. This will require a significant degree of autonomous self-organization; and also of accepting a variety of relations between things and humans,” Kott said in a research paper, to be released in the proceedings of the Spring Symposiums of the Association for the Advancement of Artificial Intelligence (AAAI).\\n\\nKott’s ideas rest on the assumption that countries have obeyed a ban on autonomous weapons “beyond meaningful control”. So humans will still, ultimately, lead and make decisions on the battlefield, whilst IOBT devices will mainly act as aids.\\n\\nThe plan also relies on AI and machine learning advancing to capabilities beyond what’s currently achievable now. Neural networks are good at learning patterns in data for tasks such as image recognition or language translation, and achieve high accuracy levels but it requires millions of training examples.\\n\\nNo plan survives the first enemy contact\\n\\nThe battlefield, however, is a volatile and dynamic environment. IOBT will have to adapt to changing tactics, and learn from a small number data samples that will be imperfect, unlabelled, and potentially even deceptive.\\n\\nSamples might be video footage or camera feeds showing the local environment for robots. Machine learning algorithms will have to determine what features are important to focus on. For example, piles of rubble are probably irrelevant, but such systems should be able to see if there was a wounded ally buried among the debris.\\n\\nDeep learning systems are often inefficient and rely on copious amounts of computing power to crunch through numbers. The hardware for IOBT will have to be small and light to fit into tiny sensors and robots and consume as little energy as possible.\\n\\n“One might suggest that a way to overcome such limitations on computing resources available directly on the battlefield is to offload the computations via wireless communications to a powerful computing resource located outside of the battlefield,” the paper said.\\n\\n\"Unfortunately, it isn’t a viable solution, because the enemy’s inevitable interference with friendly networks will limit the opportunities for use of reach-back computational resources.\"\\n\\nThe third key area is communication between humans and robots. Question and answering is used to test knowledge in natural language processing. It works well when machines are trained with large amounts of text like IBM’s Watson in the Jeopardy game show. But to survive, IOBT will have to actually understand commands and engage in useful dialogs rather than simply memorising and recalling bits of information.\\n\\nCalling Agent X\\n\\nKott explained to The Register that it’ll require agents that are able to follow conversations and have general common sense and reasoning skills - something that today’s chatbots lack.\\n\\n“There are significant gaps that must be filled in our knowledge of AI, in order to apply them truly broadly and effectively and safely within any complex domain of human endeavors,” he added.\\n\\n\"It is important to make use of the recent advances in AI, in certain types of tasks for which today’s AI is well suited. But it is also important to recognize those tasks where AI is not yet ready for robust applications.\\n\\nThe US Army has invested in the Distributed and Collaborative Intelligent Systems (DCIST) and Collaborative Research Alliance (CRA), both programs aimed to excel IOBT research in a joint effort with US universities.\\n\\nBut the recent revelation into Google’s government contract with the Pentagon highlights the gaps in the US Army’s AI workforce. It is relying on Google employees to help use Google’s TensorFlow APIs, to analyse its drone footage using computer vision.\\n\\nAt a recent Senate hearing, General Paul Nakasone, currently the commander of the United States Army Cyber Command, admitted that AI engineers are scarce.\\n\\n“Indeed it’s a challenge for the Army to compete for talent with the Silicon Valley, and actually with many other places that have great high-tech entrepreneurship. But, it’s a good challenge to have. It keeps us on our toes in competing for best people,” Kott told El Reg.\\n\\n\"We are lucky in that so many brilliant scientists and engineers are patriotic and proud to serve in defense of our society, and are excited to work on challenging problems of the Army’s technology. And we augment the talents of our Government scientists by working closely with academia and businesses. We find many companies – large and small – very interested in working with us.\" ®\\n\\nSponsored: Minds Mastering Machines - Call for papers now open')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Get ready for the Internet of Battle Things warns US Army AI boffin', 'A US Army researcher believes that wars will be fought with human soldiers commanding a team of physical and cyber robots to create a network of “Internet of Battle Things” in the future.\\n\\n“Internet of Intelligent Battle Things (IOBT) is the emerging reality of warfare,” as AI and machine learning advances Alexander Kott, chief of the Network Science Division of the US Army Research Laboratory.\\n\\nHe envisions a future where physical robots are able to fly, crawl, walk, or ride into battle. The robots as small as insects can be used as sensors, and the ones as big as large vehicles can carry troops and supplies. There will also be “cyber robots”, basically autonomous programmes, used within computers and networks to protect communications, fact-check, relay information, and protect other electronic devices from enemy malware.\\n\\nUS Army warns of the potential dangers of swarming toy drones on US soldiers READ MORE\\n\\n“In order to be effective in performing these functions, battle things will have to collaborate with each other, and also with the human warfighters. This will require a significant degree of autonomous self-organization; and also of accepting a variety of relations between things and humans,” Kott said in a research paper, to be released in the proceedings of the Spring Symposiums of the Association for the Advancement of Artificial Intelligence (AAAI).\\n\\nKott’s ideas rest on the assumption that countries have obeyed a ban on autonomous weapons “beyond meaningful control”. So humans will still, ultimately, lead and make decisions on the battlefield, whilst IOBT devices will mainly act as aids.\\n\\nThe plan also relies on AI and machine learning advancing to capabilities beyond what’s currently achievable now. Neural networks are good at learning patterns in data for tasks such as image recognition or language translation, and achieve high accuracy levels but it requires millions of training examples.\\n\\nNo plan survives the first enemy contact\\n\\nThe battlefield, however, is a volatile and dynamic environment. IOBT will have to adapt to changing tactics, and learn from a small number data samples that will be imperfect, unlabelled, and potentially even deceptive.\\n\\nSamples might be video footage or camera feeds showing the local environment for robots. Machine learning algorithms will have to determine what features are important to focus on. For example, piles of rubble are probably irrelevant, but such systems should be able to see if there was a wounded ally buried among the debris.\\n\\nDeep learning systems are often inefficient and rely on copious amounts of computing power to crunch through numbers. The hardware for IOBT will have to be small and light to fit into tiny sensors and robots and consume as little energy as possible.\\n\\n“One might suggest that a way to overcome such limitations on computing resources available directly on the battlefield is to offload the computations via wireless communications to a powerful computing resource located outside of the battlefield,” the paper said.\\n\\n\"Unfortunately, it isn’t a viable solution, because the enemy’s inevitable interference with friendly networks will limit the opportunities for use of reach-back computational resources.\"\\n\\nThe third key area is communication between humans and robots. Question and answering is used to test knowledge in natural language processing. It works well when machines are trained with large amounts of text like IBM’s Watson in the Jeopardy game show. But to survive, IOBT will have to actually understand commands and engage in useful dialogs rather than simply memorising and recalling bits of information.\\n\\nCalling Agent X\\n\\nKott explained to The Register that it’ll require agents that are able to follow conversations and have general common sense and reasoning skills - something that today’s chatbots lack.\\n\\n“There are significant gaps that must be filled in our knowledge of AI, in order to apply them truly broadly and effectively and safely within any complex domain of human endeavors,” he added.\\n\\n\"It is important to make use of the recent advances in AI, in certain types of tasks for which today’s AI is well suited. But it is also important to recognize those tasks where AI is not yet ready for robust applications.\\n\\nThe US Army has invested in the Distributed and Collaborative Intelligent Systems (DCIST) and Collaborative Research Alliance (CRA), both programs aimed to excel IOBT research in a joint effort with US universities.\\n\\nBut the recent revelation into Google’s government contract with the Pentagon highlights the gaps in the US Army’s AI workforce. It is relying on Google employees to help use Google’s TensorFlow APIs, to analyse its drone footage using computer vision.\\n\\nAt a recent Senate hearing, General Paul Nakasone, currently the commander of the United States Army Cyber Command, admitted that AI engineers are scarce.\\n\\n“Indeed it’s a challenge for the Army to compete for talent with the Silicon Valley, and actually with many other places that have great high-tech entrepreneurship. But, it’s a good challenge to have. It keeps us on our toes in competing for best people,” Kott told El Reg.\\n\\n\"We are lucky in that so many brilliant scientists and engineers are patriotic and proud to serve in defense of our society, and are excited to work on challenging problems of the Army’s technology. And we augment the talents of our Government scientists by working closely with academia and businesses. We find many companies – large and small – very interested in working with us.\" ®\\n\\nSponsored: Minds Mastering Machines - Call for papers now open')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You bet your DRaaS: Infinidat squeezes out new backup, array and cloud compute products', 'A flurry of array related activity came out of Infinidat today. Well, it is a Tuesday so why the hell not.\\n\\nThe big iron vendor told us it is squeezing out a higher capacity model, it is releasing a faster restore backup target array and a \"data centre black box\" billed as a zero data loss disaster recovery system, and enabling users to play off public cloud compute vendors against each other.\\n\\nOn the data availability front the company claimed it is allowing for better protection and recovery of the data stored in its arrays, and enabling them to withstand a regional disaster.\\n\\nIt is also locating its arrays in data centres adjacent to high-speed public cloud entry points so that mainstream public cloud compute services can work on the data in its arrays without actually moving the data to them, at least that is the thinking.\\n\\nThe four new products are:\\n\\nF6212 Infinibox array\\n\\nInfiniSync\\n\\nInfiniGuard\\n\\nNeutrix Cloud\\n\\nInfinidat said these all share common base technology elements, which include its Neural Cache, InfiniRAID and RAS, and InfiniSnap.\\n\\nF6212 Infinibox array\\n\\nThe current Infinibox product set consists of the F1000, F2000, F4000, F6000 and Infinidat backup appliance. It also offers Host Powertools and Infinimetrics.\\n\\nThe F6212 is the new high-end system - it employs up to 480 x 12TB NL-SAS drives, meaning 5.7PB of raw capacity.\\n\\nTabulating the arrays with the F6212 included gets us this table:\\n\\nF1000 F2000 F4000 F6000 F6212 Max Usable Capacity 115TB 499TB 1.37PB 2.77PB 4.15PB Max Effective Capacity 230TB 998TB 2.73PB 5.53PB 8.3PB Max Memory 384GB 768GB 2.3TB 3.1TB 3.1TB Max Flash Cache 103TB 103TB 207TB 207TB 207TB IOPS 300,000 500,000 750,000 1,000,000 1,000,000 Throughput GB/sec 3 7 10 12.5 12.5\\n\\nInfinidat said the F6212 is targeted at the real-time analytics, deep learning and IoT areas.\\n\\nInfiniSync\\n\\nThe data centre Black Box recorder is the fruits of buying Axxana in late 2017. Axxana was founded in 2005 by CEO Eli Efrat, CTO Alex Winokur and EVP Business Development Dan Hochberg (left in 2010). It took in some $14m in funding and developed a virtually indestructible data recorder to safeguard storage arrays.\\n\\nThe heavily insulated Infinisync \"black box\"\\n\\nThe idea was to obviate the need for a synchronous link to a remote DR site for high-value data where a low recovery point objective was needed. Typically, we\\'re told, such DR arrangements involve a sync link from a primary array in, say, New York, to a nearby bunker site in New Jersey, with multiple millisecond latency. There is also an asynchronous link to a remote secondary site, say in Dallas, with an overall recovery point objective (RPO) measured in minutes.\\n\\nInfinisync componentry\\n\\nThe Axxana Phoenix \"black box\" is a heavily insulated local system that Infinidat said can survive an explosion. It stores up to 1.6TB of recent primary array data changes as they occur, with a latency of less than 0.3ms on its SSDs. Changes are also sent asynchronously to the secondary site. If the primary site goes down then recent sync data on the Infinisync box can be sent to the secondary site, by WAN or cellular transfer, and you reduce the RPO time to seconds.\\n\\nThe claimed outcome is fast post-disaster application recovery – RPO 0, with no data loss whatsoever and cost-savings. There is no need for the bunker site and sync comms links to it.\\n\\nAxxana is now a fully owned Infinidat subsidiary and Infinidat was its first array integration, followed by arrays from Dell EMC, IBM and others.\\n\\nInfiniGuard\\n\\nIn the fourth 2017 quarter Infinidat quietly introduced its Infinidat Backup Appliance (IBA), the B4260, based on an InfiniBox F4000 array. This takes up 26U in a rack whereas the IBA needs a full rack.\\n\\nThat was because 3 x 2U storage node servers were added to the F400 along with 3 x 2U stateless deduplication engines (DDE) which carry out variable block size, inline and global deduplication.\\n\\nThe system was pre-configured in a 2 + 1 HA-ready topology. If a DDE fails then its work is transferred to another one. Infinidat does not want single points of failure in its products.\\n\\nThe IBA has now been rebranded as InfiniGuard. The firm said it features a 48TB/hour ingest rate with no source-side deduplication, and up to 74TB/hour with source-side acceleration. Data can be replicated to a local or remote site for added protection.\\n\\nMain InfiniGuard components\\n\\nWe\\'re told the restoration rate is 24TB/hour. The speedup is attributed partly to the InfiniBox array\\'s caching of reads in DRAM.\\n\\nThe InfiniGuard system supports 10GbitE and 16Gbit/s Fibre Channel links, with 12 ports which can be mixed. Ingest protocols include RMAN, for databases like Oracle, VTL, NAS (NFS and SMB), and OST.\\n\\nThe list of supported backup software products includes Veritas NetBackup, Oracle, Veeam, IBM Spectrum Protect (TSM), and Commvault.\\n\\nNeutrix Cloud\\n\\nThe Neutrix Cloud is a step in quite another direction.\\n\\nData is stored in Infinidat arrays at Infinidat data centres; the Neutrix cloud, adjacent to fast-access entry points to the Amazon (AWS Direct Connect), Azure (Azure ExpressRoute) and Google public clouds. That means AWS, Azure and Google cloud compute can process Neutrix cloud data, without the data actually being stored in said AWS, Azure or Google clouds.\\n\\nWhen the data is needed for cloud compute processing then spot pricing between the public clouds can be compared and the lowest cost option chosen. This is cloud compute arbitrage in real time.\\n\\nInfinidat said its offerings can prevent public cloud data storage lock-in. You can\\'t easily move petabytes of data between the three main public clouds without it taking a long time and incurring expense. That means you\\'re effectively locked in.\\n\\nThe vendor takes the line that users should store their data in a fourth public cloud - its own, of course - claiming this could mean big savings on users\\' AWS or Azure storage bill, allowing customers to take advantage of AWS and Azure competing for their cloud compute business.\\n\\nThe Neutrix cloud can also be used as a disaster recovery facility by Infinidat\\'s customers as it offers DRaaS.\\n\\nBottom line\\n\\nAltogether Infinidat is broadening its storage portfolio into the adjacent areas of data protection, disaster recovery, DRaaS and cloud compute arbitrage.\\n\\nIt wants a great share of its customers\\' IT wallet spend than it is getting at the moment. The four products could help it do just that and turn the screw on its competitors another notch tighter. ®\\n\\nSponsored: Learn how to transform your data into a strategic asset for your business by using the cloud to accelerate innovation with NetApp')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Uber's disturbing fatal self-driving car crash, a new common sense challenge for AI, and Facebook's evil algorithms\", \"Roundup It’s been a grim week for AI. The deadly Uber crash and fallout from the scandal between Facebook and Cambridge Analytica are a reminder of the ways algorithms can fail, and how they can be used against us.\\n\\nFatal Uber self-driving car vid - The video footage capturing the last moments just before one of Uber’s self-driving car crashes into a woman is disturbing, and reveals the conditions under which LiDAR failed.\\n\\nBreaking: Tempe police release video of fatal self-driving car. Warning: the video shows just before the fatal impact and is disturbing. Video shows safety driver looking down for much of the time before impact pic.twitter.com/fzLAFBzG6q — David Shepardson (@davidshepardson) March 21, 2018\\n\\nThe clip shows that it’s night and the roads are dark, and there is a slight flash of light highlighting the frame of a bicycle and some white shoes. By the time you see her full body, the car is just moments from the collision. All of this happens very quickly, and it’s difficult even for a human to make out.\\n\\nIt’s frightening that the LiDAR onboard didn’t detect her either, most likely due to the dark conditions and it didn’t help that she was also wearing a black jacket. LiDAR detects objects by emitting light and measuring the reflected beams. Since black is the least reflective color, it's more difficult for the system to make out its surroundings.\\n\\nThe video is also a reminder that humans still need to pay attention and be prepared to take over when the self-driving car fails. An internal camera shows that the human driver was distracted and his eyes are looking down at something, and by the time he looks up it’s too late. His eyes and mouth falls open in a moment of shock and panic and then the video cuts out.\\n\\nThe incident is believed to be the first time that a pedestrian was killed by a collision with a self driving car. After the death in Florida, Uber suspended all of its self-driving car tests in Arizona, San Francisco, Canada, and elsewhere.\\n\\nWe covered the story in more detail here.\\n\\nInternal documents also revealed that the company’s self-driving cars were struggling to meet its target of driving 13 miles without any human intervention during testing on roads in Arizona, according to the New York Times.\\n\\nIt’s a shockingly low number, considering Waymo’s cars can supposedly drive an average of 5,600 miles before a human had to take control. Still, Uber continued to push its autonomous car efforts aggressively in the attempt to impress its top executives by being able to offer a self-driving taxi service by the end of the year and thus boost profits.\\n\\nIt was reported that Dara Khosrowshahi, Uber’s CEO, apparently thought about killing off the project but decided not to when he realised how important self-driving cars were to the business.\\n\\nTesting common sense? - A challenge organised by a group of researchers from the Georgia Institute of Technology aims to test a computer’s vision, natural language, and knowledge skills.\\n\\nThe Visual Question Answering (VQA) knowledge challenge tests AI models by giving them an image and asking them at least three questions about the image. Here is an example scenario below.\\n\\nAn example question in the challenge. Image credit: VQA2018\\n\\nIt requires the system to have some vague idea of what a moustache is, know where it is normally located on the face, and realise that in the image is made out bananas instead of hair.\\n\\nTo do this, the system is trained on over 250,000 images taken from the COCO dataset, a set of captioned images of common objects, as well as over a million sample questions along with ten “concise, open-ended answers” for each question - one of which is the correct one.\\n\\n“VQA is a new dataset containing open-ended questions about images. These questions require an understanding of vision, language and commonsense knowledge to answer,” according to the competition website.\\n\\nAnyone is welcome to take part in the challenge and the organisers have released the training datasets online.\\n\\nVQA 2018 is the third edition of the challenge and the submission deadline is on May 20. You can find more details here.\\n\\nHow Facebook’s AI can be used for propaganda - A notable Google AI engineer went on a long rant this week explaining how Facebook’s algorithms control and manipulate its users.\\n\\nFrançois Chollet, author of the deep learning software library Keras, argued that the social media giant’s grip over a user’s news feed acted as a “psychological control vector”. The algorithms used to push the most important and relevant posts to the top of a user’s news feed means that it decides who we will keep in touch with and what news articles and opinions we read. It means that Facebook essentially exerts control over our political beliefs and worldview.\\n\\nChollet talks about how using Facebook can create a reinforcement learning loop. “A loop in which you observe the current state of your targets and keep tuning what information you feed them, until you start observing the opinions and behaviors you wanted to see,” he tweeted.\\n\\nHe warned that AI is advancing rapidly, and Facebook are investing heavily in the technology with the hopes of being a leader in the field.\\n\\n“We’re looking at a powerful entity that builds fine-grained psychological profiles of over two billion humans, that runs large-scale behaviour manipulation experiments, and that aims at developing the best AI technology the world has ever seen.\\n\\n“Personally, it really scares me. If you work in AI, please don’t help them. Don’t play their game. Don’t participate in their research ecosystem. Please show some conscience,” he urged.\\n\\nIt’s an interesting take and it’s obviously what Cambridge Analytica believes is possible too. But how effective Facebook really is at political profiling and mass manipulation is difficult to measure and an important question to consider.\\n\\nYou can read the whole Twitter thread here.\\n\\nFollowing his last comment, Twitter users were quick to point out if Google is any better than Facebook in this respect. Good question.\\n\\nSelf-driving shuttles in airports - Gatwick Airport, the second busiest airport in the United Kingdom, will be trialling self-driving shuttles for its employees for the first time.\\n\\nIt’s apparently the first airport in the world to invest in these shuttles, and has partnered with Oxbotica, a British autonomous vehicle startup.\\n\\nIn a statement, Gatwick Airport, said: “If the technology is proven in an airfield environment and following further trials, this project may be the precursor to a wide range of other autonomous vehicles being used on airport, including aircraft push back tugs, passenger load bridges, baggage tugs and transportation buses.\\n\\nThe shuttles are much simpler than the self-driving cars on the roads. They rely on sensors and do not need GPS for navigation. No passengers or aircraft will be involved in the testing, and the shuttle will only be used to take workers to and from the North and South terminals via the airside roads.\\n\\nThe data collected will also be shared with the UK government’s Department of Transport, Civil Aviation Authority as well as others, including XL Catlin, a insurance company interested in autonomous airfield vehicles. ®\\n\\nSponsored: Minds Mastering Machines - Call for papers now open\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Uber's disturbing fatal self-driving car crash, a new common sense challenge for AI, and Facebook's evil algorithms\", \"Roundup It’s been a grim week for AI. The deadly Uber crash and fallout from the scandal between Facebook and Cambridge Analytica are a reminder of the ways algorithms can fail, and how they can be used against us.\\n\\nFatal Uber self-driving car vid - The video footage capturing the last moments just before one of Uber’s self-driving car crashes into a woman is disturbing, and reveals the conditions under which LiDAR failed.\\n\\nBreaking: Tempe police release video of fatal self-driving car. Warning: the video shows just before the fatal impact and is disturbing. Video shows safety driver looking down for much of the time before impact pic.twitter.com/fzLAFBzG6q — David Shepardson (@davidshepardson) March 21, 2018\\n\\nThe clip shows that it’s night and the roads are dark, and there is a slight flash of light highlighting the frame of a bicycle and some white shoes. By the time you see her full body, the car is just moments from the collision. All of this happens very quickly, and it’s difficult even for a human to make out.\\n\\nIt’s frightening that the LiDAR onboard didn’t detect her either, most likely due to the dark conditions and it didn’t help that she was also wearing a black jacket. LiDAR detects objects by emitting light and measuring the reflected beams. Since black is the least reflective color, it's more difficult for the system to make out its surroundings.\\n\\nThe video is also a reminder that humans still need to pay attention and be prepared to take over when the self-driving car fails. An internal camera shows that the human driver was distracted and his eyes are looking down at something, and by the time he looks up it’s too late. His eyes and mouth falls open in a moment of shock and panic and then the video cuts out.\\n\\nThe incident is believed to be the first time that a pedestrian was killed by a collision with a self driving car. After the death in Florida, Uber suspended all of its self-driving car tests in Arizona, San Francisco, Canada, and elsewhere.\\n\\nWe covered the story in more detail here.\\n\\nInternal documents also revealed that the company’s self-driving cars were struggling to meet its target of driving 13 miles without any human intervention during testing on roads in Arizona, according to the New York Times.\\n\\nIt’s a shockingly low number, considering Waymo’s cars can supposedly drive an average of 5,600 miles before a human had to take control. Still, Uber continued to push its autonomous car efforts aggressively in the attempt to impress its top executives by being able to offer a self-driving taxi service by the end of the year and thus boost profits.\\n\\nIt was reported that Dara Khosrowshahi, Uber’s CEO, apparently thought about killing off the project but decided not to when he realised how important self-driving cars were to the business.\\n\\nTesting common sense? - A challenge organised by a group of researchers from the Georgia Institute of Technology aims to test a computer’s vision, natural language, and knowledge skills.\\n\\nThe Visual Question Answering (VQA) knowledge challenge tests AI models by giving them an image and asking them at least three questions about the image. Here is an example scenario below.\\n\\nAn example question in the challenge. Image credit: VQA2018\\n\\nIt requires the system to have some vague idea of what a moustache is, know where it is normally located on the face, and realise that in the image is made out bananas instead of hair.\\n\\nTo do this, the system is trained on over 250,000 images taken from the COCO dataset, a set of captioned images of common objects, as well as over a million sample questions along with ten “concise, open-ended answers” for each question - one of which is the correct one.\\n\\n“VQA is a new dataset containing open-ended questions about images. These questions require an understanding of vision, language and commonsense knowledge to answer,” according to the competition website.\\n\\nAnyone is welcome to take part in the challenge and the organisers have released the training datasets online.\\n\\nVQA 2018 is the third edition of the challenge and the submission deadline is on May 20. You can find more details here.\\n\\nHow Facebook’s AI can be used for propaganda - A notable Google AI engineer went on a long rant this week explaining how Facebook’s algorithms control and manipulate its users.\\n\\nFrançois Chollet, author of the deep learning software library Keras, argued that the social media giant’s grip over a user’s news feed acted as a “psychological control vector”. The algorithms used to push the most important and relevant posts to the top of a user’s news feed means that it decides who we will keep in touch with and what news articles and opinions we read. It means that Facebook essentially exerts control over our political beliefs and worldview.\\n\\nChollet talks about how using Facebook can create a reinforcement learning loop. “A loop in which you observe the current state of your targets and keep tuning what information you feed them, until you start observing the opinions and behaviors you wanted to see,” he tweeted.\\n\\nHe warned that AI is advancing rapidly, and Facebook are investing heavily in the technology with the hopes of being a leader in the field.\\n\\n“We’re looking at a powerful entity that builds fine-grained psychological profiles of over two billion humans, that runs large-scale behaviour manipulation experiments, and that aims at developing the best AI technology the world has ever seen.\\n\\n“Personally, it really scares me. If you work in AI, please don’t help them. Don’t play their game. Don’t participate in their research ecosystem. Please show some conscience,” he urged.\\n\\nIt’s an interesting take and it’s obviously what Cambridge Analytica believes is possible too. But how effective Facebook really is at political profiling and mass manipulation is difficult to measure and an important question to consider.\\n\\nYou can read the whole Twitter thread here.\\n\\nFollowing his last comment, Twitter users were quick to point out if Google is any better than Facebook in this respect. Good question.\\n\\nSelf-driving shuttles in airports - Gatwick Airport, the second busiest airport in the United Kingdom, will be trialling self-driving shuttles for its employees for the first time.\\n\\nIt’s apparently the first airport in the world to invest in these shuttles, and has partnered with Oxbotica, a British autonomous vehicle startup.\\n\\nIn a statement, Gatwick Airport, said: “If the technology is proven in an airfield environment and following further trials, this project may be the precursor to a wide range of other autonomous vehicles being used on airport, including aircraft push back tugs, passenger load bridges, baggage tugs and transportation buses.\\n\\nThe shuttles are much simpler than the self-driving cars on the roads. They rely on sensors and do not need GPS for navigation. No passengers or aircraft will be involved in the testing, and the shuttle will only be used to take workers to and from the North and South terminals via the airside roads.\\n\\nThe data collected will also be shared with the UK government’s Department of Transport, Civil Aviation Authority as well as others, including XL Catlin, a insurance company interested in autonomous airfield vehicles. ®\\n\\nSponsored: Minds Mastering Machines - Call for papers now open\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Identifying planets with machine learning, dirty AI searches, and OpenAI scholarships', 'Roundup Hello, here’s this week’s AI roundup. There is new code to play around with for those interested in machine learning and space, a model that predicts hilarious search trends for sex site YouPorn, and another funny story about an ostensibly intelligent medical chatbot in New Zealand.\\n\\nHunting exoplanets with ML – The machine learning code that a Google engineer and an astrophysicist used to detect exoplanets has been published online.\\n\\nChristopher Shallue, a senior software engineer at Google, and Andrew Vanderburg, a postdoctoral fellow studying astrophysics at the University of Texas, USA, discovered another planet lurking in the Kepler-90 system.\\n\\nIt was a special find. Not only was it spotted using a convolutional neural network, but it meant that the Solar System was no longer the biggest planetary system found so far. Now, it is believed that Kepler-90 also has eight planets like our Solar System.\\n\\nShallue explained in a blog post how the model uses data taken from NASA’s Kepler telescope to decide whether an object is an exoplanet or not. It uses the transit method: as a planet passes across its star during its orbit, it blocks out some light and the star’s brightness decreases.\\n\\nA graph measuring the star’s brightness will have a dip signifying when the planet is in transit. The code introduces a “Box Least Squares” algorithm, which examines the “U-shaped” dip for planets and “V-shaped” dip for binary planets.\\n\\n“Our work here is far from done. We’ve only searched 670 stars out of 200,000 observed by Kepler – who knows what we might find when we turn our technique to the entire dataset.”\\n\\nHe admitted that the model is “not yet as good at rejecting binary stars and instrumental false positives as some more mature computer heuristics.” But the hope is that by making the model public, other developers can help improve the model.\\n\\nIf you’re a TensorFlow whiz, you can have a go here.\\n\\nInterpreting models – A team of researchers has published an article in Distill, a journal known for its deep dives on a particular topic in machine learning, describing techniques that make it easier for developers to visualize how a neural network arrives at a decision.\\n\\nThey study how the neurons in GoogLeNet, an image classifier, and its hidden layers work to detect objects in images by developing a “semantic dictionary.” It maps every neuron activation to visualizations in the image to find out where in the image they are fired, and sorts them by the magnitude of the activation. Using the example of a labrador sitting next to a tiger cat, they find that GoogLeNet focuses on the droopy ears of the dog to decide the object is a labrador and the pointy ears of the cat to classify it as a tiger cat.\\n\\nThe article is long, technical and detailed. Make sure to hover your mouse and drag buttons over the results to see all the cool visual explanations.\\n\\nOpenAI Scholars – OpenAI is supporting people from underrepresented groups with a stipend and mentorship to study and work on a deep learning project.\\n\\nIt’s open to students studying the subject full time during the three months stipend period, from June 4, 2018 to August 31, 2018, who have a US work permit and are working in a US timezone.\\n\\nAI and computer science in general are notorious for their lack of diversity. In a blog post, OpenAI explained that “diversity is core to AI having a positive effect on the world — it’s necessary to ensure the advanced AI systems in the future are built to benefit everyone.”\\n\\nOpenAI will provide:\\n\\nA $7.5k/mo stipend for 3 months from June 4, 2018 to August 31, 2018.\\n\\nEach scholar will receive $25,000 worth of credits from Amazon Web Services.\\n\\nYou’ll have a mentor who will provide at least an hour of mentorship via video call each week, answer your questions via chat/email, and work with you to design and execute a good project to stretch your skills.\\n\\nThere will be a group Slack with the scholars and mentors. If you’re in the Bay Area, we’ll optionally provide a desk for you at the OpenAI office.\\n\\n“While we hope that some of the scholars will join OpenAI (where we are actively working on internal diversity & inclusion initiatives), we want this program to improve diversity in the field at large.”\\n\\nLast year, Rachel Thomas, co-founder of fast.ai and a professor of data science at the University of San Francisco, wrote about the diversity crisis in AI.\\n\\nIn her post, she said that OpenAI does not disclose diversity stats, but she believes the company is probably less diverse than other AI research hubs like Google Brain.\\n\\nThe applications are open now and will close at 11:59 PT on March 31st. You can apply here.\\n\\nNSFW: Hardbore Yore, Girl Time Flanty, and wow – YouPorn, a free porn video site, trained a recurrent neural network to predict what it thought would be its most popular searches made by users when looking through skin flicks.\\n\\nThe results are great. Not great because they’re necessarily accurate, but great because they’re funny. YouPorn watchers are, apparently, most interested in “T’Challa & Shuri”, “asarian humlion” and “girl time flanty”. The first two terms are characters from the movie Black Panther. But what the hell is “girl time flanty?!”\\n\\nWe asked YouPorn a series of questions. A spokesperson ignored the technical ones, so we don’t know the specifics of the model or what data was used to train it. But the spokesperson did say the company was “interested to see, for fun, what [its] trained recurrent neural network would predict with respect to search terms”.\\n\\nHere are a few of our favourites from the list: \"german mom hour\", \"cock milking table\", \"big booble hoter french\", \"doot sex\", \"batish my yisel\", \"beaf buts compilation\", \"blow yo\" and simply \"wow\".\\n\\nYou can see the full list here. (Don’t worry the link is SFW)\\n\\nMore Google image challenges – Google has launched another image recognition challenge, this time it’s focused on nature.\\n\\nThe 2018 iNaturalist Challenge is a species classification competition. Google is working with iNaturalist, an online community that shares pictures and videos of various types of plants, animals and insects to monitor changes, and Visipedia, a project that combines computer science and crowdsourcing, to identify various species of plants, animals, and fungi.\\n\\nIt’s a flagship challenge for developers attending the Conference on Computer Vision and Pattern Recognition (CVPR) later this year in June at Salt Lake City, Utah. The competition extends the previous iNat-2017 challenge, and contains over 450,000 training images sorted into more than 8000 categories of living things.\\n\\nThe challenge is trickier than the ImageNet challenge, which is more general, because there are relatively few images for some species – a problem called “long-tailed distribution”.\\n\\nYang Song, a software engineer at Google Research and Serge Belongie, a visiting professor of computer science from Cornell University, said: “It is important to enable machine learning models to handle categories in the long-tail, as the natural world is heavily imbalanced – some species are more abundant and easier to photograph than others. The iNaturalist challenge will encourage progress because the training distribution of iNat-2018 has an even longer tail than iNat-2017.”\\n\\nThe training data and pretrained image recognition models can be found here.\\n\\nBehold, the sentient powers of Zach! – A Reg reader in New Zealand alerted us to an AI success story that was just too good to be true.\\n\\nIt involves Zach, an intelligent bot that helps doctors suggest the right medications and care needed for their patients. The technology is fronted by one, mysterious, Albi – short for Albicus – Whale.\\n\\nZach just needs a recording of what’s been said at the doctor’s appointment to work his magic. After that, you can simple ask Zach questions and voila, it will answer you.\\n\\nSometimes, apparently, there are a few spelling mistakes but other than that Zach is pretty good. Oh, and you have to wait 20 minutes or so for a reply...you know because it’s working so hard. And, this whole interaction is naturally all done via email, of course.\\n\\nWe don’t want to give too much away, so read it and have a good chuckle here. ®\\n\\nSponsored: Minds Mastering Machines - Call for papers now open')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Facebook AI guru alt-tabs out, Nvidia EULA audits, Baidu changes, GPU tricks, and more', 'Roundup Welcome to El Reg\\'s January roundup of AI-related news beyond all the wonderful and terrible things we\\'ve covered separately. Drop us a line if you have any machine-learning news or gossip to share.\\n\\nFacebook AI chief LeCun steps aside – Yann LeCun, considered to be a pioneer of neural networks for computer vision, has stepped back as Facebook\\'s AI supremo. Jérôme Pesenti, ex-CEO of medical startup BenevolentAI and former IBM Watson vice president, will take the reins, according to Quartz journo Dave Gershgorn.\\n\\nBut LeCun isn’t completely legging it. He will remain at Facebook and continue to lead the social network\\'s machine-learning boffinry nerve-center FAIR in New York. Last year, The Register heard rumors that LeCun was tired of menial managerial tasks. Now, that boring management stuff has been offloaded to other people.\\n\\n“There was a need for someone to basically oversee all the AI at Facebook, across research, development, and have a connection with product,” LeCun confirmed to Gershgorn on Tuesday this week.\\n\\nPesenti, as veep of AI, and Joaquin Candela – head of Facebook’s Applied Machine Learning team in San Francisco – will both report to CTO Mike Schroepfer.\\n\\nSchroepfer said in a Facebook post that LeCun was now the social network\\'s Chief AI Scientist, effectively allowing Pesenti to oversee the website\\'s machine-learning-powered products so LeCun can focus on research.\\n\\nSqueeze more out of CPUs – Amazon has published a tutorial on how to use neural-network acceleration engine NNPACK with Apache\\'s deep-learning library MXNet. NNPACK is optimized for performing inference on CPUs, and is useful when your hardware lacks a suitable GPU for AI tasks.\\n\\n\"NNPACK is available for Linux and macOS X platforms. It’s optimized for the Intel x86-64 processor with the AVX2 instruction set, as well as the ARMv7 processor with the NEON instruction set and the ARMv8,\" explained AWS technical evangelist Julien Simon.\\n\\nNew Baidu AI lab hires – Chinese web juggernaut Baidu has announced the addition of new labs and research scientists in an attempt to reshuffle research efforts since Andrew Ng ejected from the biz.\\n\\nKenneth Church – who served as the president of Association for Computational Linguistics, an international society for people working on natural language processing, and has previously worked at IBM Watson, Microsoft and AT&T labs – has joined Baidu.\\n\\nThe Chinese internet monster has also snared Jun Luke Huan and Hui Xiong away from their academic posts at the University of Kansas and Rutgers University, respectively, in the US. It has also created two new internal research factions: the Business Intelligence Lab and the Robotics and Autonomous Driving Lab. Now there are a total of five labs, including its Institute of Deep Learning, Big Data Lab and Silicon Valley Artificial Intelligence Lab.\\n\\nIt’s not entirely clear what happened at Baidu to prompt this internal shakeup. But The Register has heard whispers of internal politics and a culture clash between the teams in China and America that led to the departure of several research staff including its previous chief scientist, Andrew Ng, and AI lab director, Adam Coates.\\n\\nSqueeze more for less on your GPU – OpenAI published TensorFlow code for gradient checkpointing, a technique that reduces the memory needed on graphics processor chips to train large neural networks.\\n\\nIt’s a tricky concept to understand, but the gist is that this software takes up less space to carry out gradient descent, a algorithm often used to train models.\\n\\nFeed-forward neural networks are a little clumsy to train because all the nodes in different layers are processed in the reverse order. It means that the results obtained from running through all the nodes in the previous layers have to be kept in memory. So the deeper your network, the more memory it takes to train it.\\n\\nHere’s where gradient checkpointing comes in. Markers are used for nodes as checkpoints. “These checkpoint nodes are kept in memory after the forward pass, while the remaining nodes are recomputed at most once. After being recomputed, the non-checkpoint nodes are kept in memory until they are no longer required,” according an OpenAI.\\n\\nOpenAI researchers Tim Salimans and Yaroslav Bulatov said they could fit more than ten-times larger models onto a GPU with a 20 per cent increase in computation time. You can find out more here.\\n\\nA new AI computer vision challenge – Google researchers have launched a contest to improve image compression techniques using neural networks as well as more traditional methods.\\n\\nThe announcement is linked to a workshop at the upcoming Computer Vision and Pattern Recognition conference (CVPR), happening in Utah, USA, in June. The goal is to come up with novel methods to compress images.\\n\\nA training dataset containing thousands of pictures has been released, and consists of two parts: dataset professional (2GB) and dataset mobile (4GB).\\n\\n“The datasets are collected to be representative for images commonly used in the wild, containing thousands of images. While the challenge will allow participants to train neural networks or other methods on any amount of data (but we expect participants to have access to additional data, such as ImageNet and the Open Images Dataset), it should be possible to train on the datasets provided,” wrote Michele Covell, a scientist at Google Research, in a blog post.\\n\\nThe validation part of the dataset will be released this month, and the test dataset will be made public on April 15, before the competition closes on April 22. The results will be announced on May 29, and participants can submit a paper to the Workshop and Challenge on Learned Image Compression (CLIC) at CVPR by June 4. Previous research has shown image compression is possible with recurrent neural networks and generative adversarial networks. The CLIC workshop is being sponsored by Google, Twitter, and ETH Zurich, a Swiss university.\\n\\nNvidia can now audit CUDA Toolkit users – Nvidia has updated its software licensing agreement of its CUDA Toolkit allowing it to audit organizations, startling individual developers and academics.\\n\\nIt allows Nvidia to audit CUDA toolkit users to check if they are using the toolchain in an appropriate manner – by showing up at your door if necessary. Enterprise-grade software licenses tend to include these auditing requirements, but to attach them to software development tools that can be used by anyone – from individuals to corporations – has been described as extreme by Reg readers who\\'ve been in touch about this developing situation.\\n\\n“During the term of the AGREEMENT and for three (3) years thereafter, you will maintain all usual and proper books and records of account relating to the CUDA Licensed Software provided under the AGREEMENT. During such period and upon written notice to you, NVIDIA or its authorized third party auditors subject to confidentiality obligations will have the right to inspect and audit your Enterprise books and records for the purpose of confirming compliance with the terms of the AGREEMENT,” the end-user license agreement (EULA) reads.\\n\\nWe asked Nvidia to clarify what exactly counts as a breach of agreement. A spokesperson told us: “Anyone can develop applications on CUDA or use CUDA-based applications for free. What we want to protect against is a person or entity taking CUDA, re-naming (\\'rebranding\\') it or charging for it. That said, we have no current plans to audit anyone under our CUDA license, we haven’t done so in the past, and we hope that we’ll not have to do so in the future.”\\n\\nThe EULA goes on to say that if Nvidia finds out that users are breaching agreement terms, then they will be required to pay Nvidia the cost of conducting “the inspection and audit.”\\n\\nThe audit clause was added in September, and spotted at the turn of 2018. It comes at a time when Nvidia also announced it had updated its end-user licensing agreement to ban vendors from selling GeForce and Titan GPUs for datacenters, except for processing blockchain related activities.\\n\\nLook out for more on this issue this week at El Reg.\\n\\nNvidia’s Xavier chip touted again – Let’s just keep talking about Nvidia. Earlier this month it had another go at unveiling Xavier, a processor tailored for self-driving cars.\\n\\nXavier was previously teased by Nv CEO Jensen Huang this time last year. Now it seems the thing is inching closer to production. Huang said the SoC will be used as part of the company’s Drive PX Pegasus system, a computer for powering fully autonomous level-five Total Recall-style Johnny Cabs.\\n\\nLevel five being a vehicle control system that just asks for a destination and drives the whole way, down to level one and two, which are varying degrees of intelligent cruise control.\\n\\n“The computational requirements of robotaxis are enormous – perceiving the world through high-resolution, 360-degree surround cameras and lidars, localizing the vehicle within centimeter accuracy, tracking vehicles and people around the car, and planning a safe and comfortable path to the destination. All this processing must be done with multiple levels of redundancy to ensure the highest level of safety. The computing demands of driverless vehicles are easily 50 to 100 times more intensive than the most advanced cars today,” the biz wrote in a blog post.\\n\\nLevel five? We\\'ll believe it when we see it.\\n\\nTensorFlow 1.5.0 – The popular open-source AI framework, Tensorflow, has released version 1.5.0. According to its GitHub page, a few bugs have been patched and some major changes include:\\n\\nPrebuilt binaries are now compiled against CUDA 9 and cuDNN 7.\\n\\nLinux binaries are built using Ubuntu 16 containers, potentially.\\n\\nThere are glibc incompatibility issues with Ubuntu 14.\\n\\nStarting from 1.6 release, prebuilt binaries will use AVX instructions. This may break TensorFlow on older CPUs. ®\\n\\nSponsored: Minds Mastering Machines - Call for papers now open')"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "('Yes, Alexa is all very well...but we want YOU to talk machine learning and AI', 'MCubed returns to London in October, and we want to hear how your organisation is using artificial intelligence, machine learning algorithms, deep learning, and predictive analytics to solve real world business and technology problems.\\n\\nWe’re looking to put together a conference schedule that shows how business and academia are exploiting both proprietary and open technology and platforms to put machine learning and artificial intelligence to work. The call for papers is here.\\n\\nWhether it’s using the technology to squeeze cost out of existing processes, or as the launchpad for entirely new approaches to doing business, we want to hear from you.\\n\\nLikewise, if you’re building the underlying infrastructure to run smart cities, or using analytics to solve engineering challenges associated with a specific type of building, we know your peers are going to be interested.\\n\\nAnd we’d love to hear about your experiences building out the infrastructure to do all this, whether it’s with your inhouse silicon, or by exploiting the power available via the cloud.\\n\\nYou can get the full run down on the topics we’re interested in here. You can also see the sort of talks we had last year, including Q/A with our incredibly smart audience.\\n\\nThe deadline for proposals is April 3, but if you want to get in touch before, we’re happy to help you target your proposal.\\n\\nThe conference will take place from October 15 to 17, at 30 Euston Square, Central London. This is a stunningly comfortable venue in which to ponder some of the most intellectually and ethically challenging issues facing the tech community today, and we really want you to join us. ®\\n\\nSponsored: Minds Mastering Machines - Call for papers now open')\n",
      "('HPE burns offering to Apollo 6500, unleashes cranked deep learning server on world+dog', 'HPE has updated its Apollo 6500 deep learning server with a threefold performance boost over its precursor by stuffing it with eight Tesla V100 GPUs, which speak to each other via Nvidia\\'s NVlink 2.0 interconnect protocol.\\n\\nHPE claimed the Nvidia gear makes it, on average, 3.12 times faster than the previous gen9 Apollo 6500 at running the Inception-v3, ResNet-50 and VGG-16 deep learning models with the TensorFlow and Caffe2 frameworks.\\n\\nWe\\'re told it can produce up to 125 TFLOPS single-precision compute. The Apollo 6500 gen9 output up to 56 TFLOPS per server with eight Nvidia Tesla M40s.\\n\\nThe gen9 Apollo 6500 was a 4U chassis with 2 x 2U XL270d server sleds. These featured a central pair of Xeon processors flanked by four Nvidia GPUs either side – the Tesla K40, K80, and M40 were supported. The chips were linked by PCIe gen3. Each sled supported 1TB of memory and had eight 2.5-inch drive bays.\\n\\nHPE Apollo 6500 gen9 server sled (top) and chassis (bottom) with two stacked XL270d sleds\\n\\nStraightforward enough. The gen10 Apollo 6500 chassis isn\\'t, as an HPE-supplied image shows:\\n\\nIt doesn\\'t have two 2U server sleds. There are eight 2.5-inch drive bays in what appears to be a 2U component at the top. Below that is a 1U element fed by four power cords coming from a 1U base unit, each with a fan and labelled PS1, PS2, PS3 and PS4 – power supplies.\\n\\nHPE Apollo 6500 gen10 power cabling detail\\n\\nWhy isn\\'t there an internal power feed instead of this clumsy affair with cables obscuring fan vents? HPE doesn\\'t say. We might suppose it\\'s because there isn\\'t room for interior power cabling.\\n\\nThe mid-mounted 1U component is for the the eight Tesla GPUs, with the top 2U unit for two Xeon servers, mounted side by side, underneath a removable top half-cover. That\\'s why the 16 drive bays are there.\\n\\nThese bays can hold SAS/SATA SSDs with up to four NVMe drives.\\n\\nThe label flange at the top right reads \"ProLiant XL270d Gen10\". This looks nothing like a gen9 XL270d and there is no obvious way to slide out any front-mounting server component box.\\n\\nThe one on the left reads \"Drive Box ID\" and shows two drive boxes.\\n\\nThe ProLiant XL270d appears to have been substantially redesigned.\\n\\nWe\\'re told the \"innovative systems design of the HPE Apollo 6500 Gen10 allows for a high degree of flexibility with a range of configuration and topology options to match each workload\".\\n\\nHere are the details:\\n\\nClick to enlarge\\n\\nThat\\'s 3TB of memory – the gen9 system had 1TB per server, 2TB in all.\\n\\nThe accelerator topologies include hybrid cube mesh for NVLink and 4:1 or 8:1 GPU:CPU flexibility in PCIe.\\n\\nManagement is via iLO (Integrated Lights-Out) v5.\\n\\nHPE is supplying a set of service offerings around the new box and it has a fast file system reselling deal with WekaIO and its SPECSFS 2014 benchmark-winning parallel file system:\\n\\nDigital Prescriptive Maintenance Services, one of a series of Pointnext AI-enabled offerings which HPE claimed can predict, suggest, and automate action to fix a problem before it causes harm\\n\\nArtificial Intelligence Transformation Workshop, providing Pointnext consulting expertise to help customers get started with AI, develop data and analytics initiatives, and look at AI use cases\\n\\nAn HPE Deep Learning Cookbook includes a performance guide which uses a knowledge base of benchmarking results and measurements in the customer\\'s environment to guide technology selection and configuration\\n\\nThe firm is reselling the WekaIO flash-optimised Matrix file system, which is certified for the Apollo 2000 Gen10 and HPE ProLiant DL360 Gen10 systems.\\n\\nHPE will showcase these new products along with its HPC and AI portfolio at Nvidia\\'s GPU Technology Conference on March 26-29 in San Jose, California. There are more details here.\\n\\nAvailability\\n\\nHPE Digital Prescriptive Maintenance Services are now available in Europe and will be globally available in June via HPE Pointnext. The Artificial Intelligence Transformation Workshop is globally available now via Pointnext. The Apollo 6500 Gen10 will be available from HPE and its channel partners in May, same with WekaIO Matrix. ®\\n\\nSponsored: Minds Mastering Machines - Call for papers now open')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"FYI: There's now an AI app that generates convincing fake smut vids from celebs' faces\", 'The faces of celebrities, politicians, children, or pretty much anyone, can be pasted over faces of porn stars in X-rated movies using freely available machine-learning software.\\n\\nThe resulting flicks look convincing, and effectively allow miscreants to place people – from the rich and famous to the powerful to ex-partners – into highly compromising and believable positions on moving video footage.\\n\\nAlthough there are innocent use cases for this software – such as pasting your face onto a blockbuster movie character\\'s head so you can star in a Hollywood film – the potential for blackmail, embarrassment, and harassment is astounding.\\n\\nThe process to create these incredibly damaging faked videos is, thankfully, not trivial, but not impossible. You need the software – a desktop program dubbed FakeApp – plus a big batch of photos of your victim to train the application\\'s deep-learning neural network, the video to paste the face onto, and a little tweaking here and there to render the output believable.\\n\\nA Reddit message board, /r/deepfakes, where miscreants can discuss the warped tech, appeared two months ago, and has racked up more than 17,000 followers. There, pervs post their fake porn of Alison Brie, Scarlett Johansson, Taylor Swift, and other women, and swap tips and tricks.\\n\\nThis is a convincing still taken from a AI-doctored FakeApp video – it is singer Katy Perry\\'s face on the body of a porn star – it took Reddit user DaFakir two attempts to make, apparently\\n\\nThe subreddit was created by a Redditor called Deepfakes, who developed the machine-learning algorithms at the core of FakeApp. That program was built and packed up by another individual – Deepfakeapp – who uses the chat board to distribute the code along with instructions for creating bogus footage. Deepfakeapp told us they are not Deepfakes; it appears the two collaborated to some degree on this project.\\n\\nThe software is based on face-swapping algorithms. A deep-learning neural network is trained to identify someone\\'s face in a still video frame – such as an adult actress in a blue movie – and swap it with someone else\\'s face – such as a TV celeb or singer. Repeat this at 30 or 60 frames per second, and you\\'ve got an AI-doctored video.\\n\\nFor example, in this safe-for-work video, the app has pasted Star Wars actress Daisy Ridley onto someone else\\'s body.\\n\\nSo far, most of the villains have used it to map the faces of women celebrities onto their favorite smut stars. That\\'s incredibly horrible already – but who’s to stop anyone from using it to create child pornography, or revenge porn? A video of the victim – from a TV show or film or home movie – will produce enough frames to train the model, once cropped and processed.\\n\\nAccessible\\n\\nThese moral quandaries have not deterred Deepfakeapp, however. In their mind, we should expect such code to emerge in public as powerful AI technology becomes more and more accessible – and this tech will be used for good, and bad.\\n\\n\"Ultimately, for better or worse, it\\'s impossible to stop anyone from doing what they want with this tool, and unfortunately that means some will abuse it,\" the developer told The Register on Wednesday.\\n\\n\"However, I\\'m optimistic that the community is going to continue to by and large use it make creative, innovative, and positive work, as they have so far.\"\\n\\nBut why make a desktop Windows app that\\'s so obviously going to be used for evil? \"It\\'s because I want to spread this incredible face swapping technology to users who don\\'t necessarily have a tech background,\" Deepfakeapp replied.\\n\\nAll it requires is an Nvidia GPU to train the program\\'s neural network, Nvidia’s CUDA software to run the TensorFlow code, and about three to four GB of space on your PC to store the data and results.\\n\\nYou don’t need to be an AI whiz to wield this, since you don’t need to mess around with much coding. “This app is intended to allow users to move through the full deep-fake creation pipeline—creating training data, training a model, and creating fakes with that model—without the need to install Python and other dependencies or parse code,” according to the program\\'s documentation.\\n\\nYou have to be quite the pervert to bother collecting hundreds or thousands of images of your desired target, and then wait several hours for the algorithm to finish training, though.\\n\\nBut the internet is crawling with creeps. There are countless threads of people posting their results, or asking for advice on how to make the faces less blurry or how to align the eyebrows.\\n\\nReddit user yu78156853 clearly hasn\\'t spent long enough training the algorithm ... this is an example of a poorly faked vid\\n\\nOverall, there are some pretty convincing results. Deepfakeapp told El Reg one user was able to paste Carrie Fisher\\'s face on another actor\\'s head better than Hollywood artists were able to do in Star War: Rogue One.\\n\\nHonestly, I continue to be amazed by how realistic much of the work produced by this technology is. Community members create new material every day that looks completely real. Just today a user released a recreation of the CGI Princess Leia from Star Wars: Rogue One that was almost identical to the original — that really impressed me.\\n\\nDeepfakes and Deepfakeapp\\'s work was first spotted by Vice\\'s Samantha Cole. This week, after discovering FakeApp, she summed it up perfectly: \"We are truly fucked: everyone is making AI-generated fake porn now.\" ®\\n\\nSponsored: Minds Mastering Machines - Call for papers now open')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Boffins baffled as AI training leaks secrets to canny thieves', 'Private information can be easily extracted from neural networks trained on sensitive data, according to new research.\\n\\nA paper released on arXiv last week by a team of researchers from the University of California, Berkeley, National University of Singapore, and Google Brain reveals just how vulnerable deep learning is to information leakage.\\n\\nThe researchers labelled the problem “unintended memorization” and explained it happens if miscreants can access to the model’s code and apply a variety of search algorithms. That\\'s not an unrealistic scenario considering the code for many models are available online. And it means that text messages, location histories, emails or medical data can be leaked.\\n\\nNicholas Carlini, first author of the paper and a PhD student at UC Berkeley, told The Register, that the team “don\\'t really know why neural networks memorize these secrets right now”.\\n\\n“At least in part, it is a direct response to the fact that we train neural networks by repeatedly showing them the same training inputs over and over and asking them to remember these facts. At the end of training, a model might have seen any given input ten or twenty times, or even a hundred, for some models.\\n\\n“This allows them to know how to perfectly label the training data - because they\\'ve seen it so much - but don\\'t know how to perfectly label other data. What we exploit to reveal these secrets is the fact that models are much more confident on data they\\'ve seen before,” he explained.\\n\\nSecrets worth stealing are the easiest to nab\\n\\nIn the paper, the researchers showed how easy it is to steal secrets such as social security and credit card numbers, which can be easily identified from neural network\\'s training data.\\n\\nThey used the example of an email dataset comprising several hundred thousand emails from different senders containing sensitive information. This was split into different senders who have sent at least one secret piece of data and used to train a two-layer long short-term memory (LSTM) network to generate the next the sequence of characters.\\n\\nNot all the emails contained secret data, but a search algorithm managed to glean two credit card numbers and a social security number in under an hour from a possible number of ten secrets sent by six users.\\n\\nThe team also probed Google’s neural machine translation (NMT) model, which processes input words and uses an LSTM to predict the translated word in another language. They also inserted the sentence: “My social security number is xxx-xx-xxxx ” in English and the corresponding Vietnamese translation pair once, twice, or four times in a dataset containing 100,000 sentences written in English and Vietnamese.\\n\\nIf a secret shows up four times, Google’s NMT model memorizes it completely and the data can be extracted by third parties. The more sensitive information is repeated in the training data, there is more risk of it being exposed.\\n\\nThe chances of sensitive data becoming available are also raised when the miscreant knows the general format of the secret. Credit card numbers, phone numbers and social security numbers all follow the same template with a limited number of digits - a property the researchers call “low entropy”.\\n\\n“Language models are the most vulnerable type of models at the moment. We have two properties that are necessary for our extraction attacks to work: the secret must have a reasonably low entropy (the uncertainty can\\'t be very large - ten to twenty random numbers is about the limit) and the model must reveal many outputs to allow us to infer if it has seen this secret before,\" Carlini said.\\n\\n“These types would be harder over images, where the entropy is thousands of times larger, or over simple classifiers, which don\\'t produce enough output to infer if something was memorized. Unfortunately, text data often contains some of our most sensitive secrets: social security numbers, credit card numbers, passwords, etc.”\\n\\nDevelopers should train models with differential privacy learning algorithms\\n\\nLuckily, there are ways to get around the problem. The researchers recommend developers use “differential privacy algorithms” to train models. Companies like Apple and Google already employ these methods when dealing with customer data.\\n\\nPrivate information is scrambled and randomised so that it is difficult to reproduce it. Dawn Song, co-author of the paper and a professor in the department of electrical engineering and computer sciences at UC Berkeley, told us the following:\\n\\n“We hope to raise awareness that it\\'s important to consider protecting users\\' sensitive data as machine learning models are trained. Machine learning or deep learning models could be remembering sensitive data if no special care is taken.”\\n\\nThe best way to avoid all problems, however, is to never feed secrets as training data. But if it’s unavoidable then developers will have to apply differentially private learning mechanisms, to bolster security, Carlini concluded. ®\\n\\nSponsored: Minds Mastering Machines - Call for papers now open')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"We sent a vulture to IBM's new developer conference to find an answer to the burning question: Why Big Blue?\", 'Index At the first IBM Index developer conference in San Francisco, California, on Tuesday, I spent the morning at a Kubernetes workshop learning that when apps on the IBM Cloud Container Service fail to deploy, the reason may not be obvious.\\n\\nThe presenter, IBM cloud program manager Chris Rosen, framed the event as an opportunity to attempt to answer another question that isn\\'t evident to everyone: Why IBM?\\n\\nIBM\\'s Java CTO John Duimovich offered his answer during an interview with The Register. We\\'ll delve into that discussion in detail in a follow-up article but here\\'s his sales pitch.\\n\\n\"In the Java space, we\\'re the experts,\" said Duimovich. \"We have hardware experts. We\\'ve actually redesigned instructions on [processor architecture] Power and on [our mainframe] Z over the years to give better Java support. We have our own JVM, OpenJ9, that\\'s newly open sourced this year. That\\'s got advanced features that give you the same throughput for half the memory, for example.\"\\n\\nIn the cloud, he explained, that translates to workloads that cost half as much to run under memory-based pricing as a standard VM.\\n\\n\"Why IBM?\" mused Duimovich. \"We are driving a pretty aggressive optimization and ease of use story for cloud Java.\"\\n\\nAnd there\\'s more to it than Java and IBM\\'s embrace of open source.\\n\\n\"The rest of the stuff – big data, analytics, the Watson portfolio – allows you to build a fairly compelling end-to-end cloud native app,\" he said. \"We have the full picture, and more specifically around Java, we have probably the deepest skills on the planet.\"\\n\\n\"Why us?\" Duimovich asked. \"Why would you pick anyone who doesn\\'t have a JVM team? Why would you pick someone who doesn\\'t have a full stack integrated with their cloud as your Java vendor?\"\\n\\nIf only the other conference attendees I spoke with shared that certainty.\\n\\nTrails\\n\\nWith regard to cloud platforms, IBM isn\\'t top of mind. By the public cloud revenue metrics of IT consultancy Gartner last September, it doesn\\'t even merit mention by name. Consigned to the \"Other\" category, Big Blue trails behind Amazon, Microsoft, Alibaba, Google, and Rackspace.\\n\\nFinancial firm Jefferies in a December report was more charitable, ranking IBM third in public cloud revenue behind Amazon and Microsoft.\\n\\nAs for its overall business, IBM, while still mostly profitable, has a lot to prove. In January, the company reported its first quarter of revenue growth after years of shrinking sales. But it\\'s too soon to tell whether the patient has stabilized.\\n\\nThe humbled services and mainframe giant has placed several high-profile bets on possible future cash cows: its Watson artificial intelligence service, quantum computing, cloud services, and blockchain tech.\\n\\nInterviewed in a hallway during the conference, John DeFalco, CEO of Custody Cloud, a firm developing applications for law enforcement, had few kind words for IBM.\\n\\nDeFalco said he was not impressed IBM\\'s technology stack. He said he\\'d used some BlueMix services for fintech applications and felt that the company\\'s cloud offering was lacking.\\n\\nHe also expressed skepticism about the corporation\\'s management. \"I would never put a dollar of my money in a company run like IBM has been,\" he said.\\n\\nNonetheless, he said he bought a ticket to the conference to learn more about blockchain technology, because of its potential for securing and tracking data associated with individuals in police custody. He also expressed interest in TensorFlow.\\n\\nBlack & Blue: IBM hires Bain to cut costs, up productivity READ MORE\\n\\nA developer with IBM\\'s cloud business who asked not to be named because he was not authorized to speak to the press said he felt generally hopeful about the direction of the company.\\n\\nHe expects IBM will do well with businesses looking for hybrid cloud deployments – these companies want to move some of their applications to the cloud but they also don\\'t want to move too fast or to give up too much control.\\n\\nHe said he felt Watson was the most promising strategic initiative because it\\'s not something other companies have.\\n\\nWall Street\\'s Jefferies was not so optimistic in a report issued last July. The finance firm pegged Watson as a money pit, noting \"the returns on IBM\\'s investments aren\\'t likely to be above the cost of capital.\" The firm also expressed doubt about IBM\\'s ability to compete for AI talent.\\n\\nA speaker at one of the conference workshops who asked not to be named – as he may want to be invited back – said he wasn\\'t sure what the answer to the question \"Why IBM?\" might be. He said he\\'d been invited at the last minute and wasn\\'t really that familiar with IBM\\'s offerings, noting that he used AWS generally and hadn\\'t seen anything that made him want to switch.\\n\\nA conference attendee from an open-source database biz, also preferring anonymity, explained his presence by remarking that his firm has an interest in ensuring its product interoperates with open-source cloud application platform Cloud Foundry and IBM offers Cloud Foundry.\\n\\nAnother conference attendee, who asked not to be named, said it wasn\\'t IBM\\'s technology in particular that prompted him to attend. Rather, as a front-end developer, it was open source tech that was relevant to his job and interests.\\n\\nAlso, he said, his company had some extra money allocated to conferences that needed to be spent. His ticket to Index helped zero out budgeted funds.\\n\\nBenjamin Aaron, a data scientist with mobile health app startup Vytality, said he was attending the conference to compare IBM\\'s deep-learning technology with services available from rivals. The app sends elder care notifications to family and friends, he explained, and he is looking into deep learning as a way to prioritize and filter notifications.\\n\\nHe said he didn\\'t have any feelings about IBM one way or another. \"Frankly, the tickets were offered at a discount on Meetup and I figured as might as well,\" he said. ®\\n\\nSponsored: Minds Mastering Machines - Call for papers now open')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Is that you, T-1000? No, just a robot that can mimic humans on sight', 'This has nothing to do with the real experiment – we just thought it was a cool picture of a robot woman watching TV\\n\\nVideo Boffins have taught a robot how to imitate the way someone handles objects after watching them just once.\\n\\nWhile humans and animals are intelligent enough to mimic simple movements they\\'ve only just seen, this is beyond today\\'s relatively dumb software. We\\'re nowhere near T-1000 Terminator series levels, yet.\\n\\nResearchers from the University of California, Berkeley, in the USA, have made some progress on this front by teaching code controlling a robot arm and hand to perform three tasks: grabbing an object and placing it in a specific position; pushing an object; and pushing and pulling an object after seeing the same action performed by a human arm.\\n\\nThink picking up stuff, such as a toy, and placing it on a box, pushing a little car along a table, and so on.\\n\\nThe technique, described in a paper out this week, has been dubbed “one-shot imitation.” And, yes, it requires a lot of training before it can start copycatting people on demand. The idea is to educate the code to the point where it can immediately recognize movements, or similar movements, from its training, and replay them.\\n\\nA few thousand videos depicting a human arm and a robot arm completing movements and actions are used to prime the control software. The same actions are repeated using different backgrounds, lighting effects, objects, and human arms to increase the depth of the machine-learning model\\'s awareness of how the limbs generally operate, and thus increase the chances of the robot successfully imitating a person on the fly.\\n\\nAdapts\\n\\nChelsea Finn, a PhD student, and Tianhe (Kevin) Yu, an undergraduate student, both at the UC Berkeley Artificial Intelligence Research group, explained to The Register on Wednesday: “The human demos allow [the robot] to learn how to learn from humans. Using the human demos – just a video of human performing the task – the robot adapts to the task shown in the demonstration.\"\\n\\nThe training videos are converted into sequences of still images and fed into a convolutional neural network that maps the pictured actions to the possible movements that can be performed by the robot arm and its claw, so that it builds up an understanding of how to position itself to imitate movements caught on camera. It also learns the features of objects, such as colors and shapes, so that it knows how to identify and grasp them.\\n\\nCrucially, the robot should be able to cope with new objects it hasn\\'t seen during training; simply watching a person handle an arbitrary thing should be enough for it to twig how it should move its joints to pick up and move the item in an identical fashion.\\n\\nIt learns via a process called meta-learning. This is not the same as supervised learning, which is typically used in deep-learning research and involves training systems to perfect a narrow, single task and testing the software by giving it an example that it hasn’t seen before.\\n\\n“Meta-learning is learning to learn a wide range of tasks quickly and efficiently. By applying meta-learning to robotics, we hope to enable robots to be generalists like humans, rather than mastering only one skill,“ Finn and Yu said. “Meta-learning is particularly important for robotics, since we want robots to operate in a diverse range of environments in the real world.”\\n\\n“In essence, the robot learns how to learn from humans using this data. After the meta-training phase, the robot can acquire new skills by combining its learned prior knowledge with one video of a human performing the new skill,” they, and their fellow academics, added in their paper.\\n\\nMeta-training to meta-testing\\n\\nAfter the robot has been trained, it can use inference to imitate a human after watching a clip it hasn’t seen before. You can see the robot in action here:\\n\\nYoutube Video\\n\\nAt first, the movements between the human and the robot may look slightly different. That\\'s because the robot may not have picked up on subtle or minute hand and finger gestures, or be thrown off by the lighting and background. However, the overall task is completed in pretty much the same way.\\n\\nThe robot arm can’t learn a motion completely from scratch on demand: it needs to have seen something similar during training. It manages to push, place, and pick up the right objects over 70 per cent of the time, though, during tests. There are a few failure cases, where it fails to choose the right object or motion.\\n\\nIt’s also more likely to fail to copy humans when the video depicts a new background, which shows that the robot\\'s brain is somewhat preoccupied by patterns in its environment that are not particularly important to the task at hand.\\n\\nDeep learning is data hungry, and the researchers reckon collecting more of it using a diverse range of backgrounds during training will reduce the failure rate. There were also a number of motion faults that occurred for all backgrounds, so the learning algorithms controlling the robot also have to improved.\\n\\nThe team believes experiments like these will help refine robots that have to select the correct product or other object from a collection of things. At the moment, the team uses three different models of robot arms for each task. They hope to integrate these all into one model, so that a single robot can perform all the different chores as well as increasing the complexity of the tasks. ®\\n\\nSponsored: Minds Mastering Machines - Call for papers now open')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('DeepMind boffins brain-damage AI to find out what makes it tick', 'Researchers trying to understand how neural networks work shouldn’t just focus on interpretable neurons, according to new research from DeepMind researchers.\\n\\nAI systems are often described as black boxes. It’s difficult to understand how they work and reach particular outcomes, making people nervous about using them to make important decisions in areas such as healthcare or recruitment.\\n\\nMaking neural networks more interpretable is hot topic in research. It’s possible to look at the connections between different groups of neurons and visualise which ones correspond to a specific class.\\n\\nIf an image classification model is fed different types of pictures, say an image of a cat or dog, researchers can find the ‘cat neurons’ or a ‘dog neurons’.\\n\\nThese interpretable neurons are important as they are the ones that push the neural network to a particular answer, in this case it’s whether the animal in the image is a cat or dog.\\n\\nA paper from DeepMind to be presented at the International Conference on Learning Representations (ICLR) next month in April shows that studying these interpretable neurons alone isn’t enough to understand how deep learning truly works.\\n\\n“We measured the performance impact of damaging the network by deleting individual neurons as well as groups of neurons,” according to a blog post.\\n\\nDeleting groups of neurons changes the strength of the connections between other neurons and can make the neural network performance drop. For example, if the cat neurons are deleted and the model is shown a picture of a cat, it might be more difficult to identify the animal correctly and its accuracy decreases.\\n\\nBut the results showed that these class-specific neurons weren’t all that important after all. After deleting these interpretable neurons, the performance levels didn’t change by much.\\n\\nOn one hand, it’s a little disheartening to find out that looking into the interpretable neurons isn’t enough to untangle the inner workings of a neural network. But it’s not all too surprising either, because it means that the models that were less affected after having their neurons deleted don’t rely on memorizing training data, and generalise better to new images. And that’s how neural networks should work, really.\\n\\nDeepMind hope to “explain the role of all neurons, not just those which are easy-to-interpret”.\\n\\n“We hope to better understand the inner workings of neural networks, and critically, to use this understanding to build more intelligent and general systems,” it concluded. ®\\n\\nSponsored: Minds Mastering Machines - Call for papers now open')\n",
      "('Stanford brainiacs say they can predict Reddit raids', 'A group from Stanford University says conflicts on Reddit forums follow a reliable enough pattern to be predicted before they happen.\\n\\nA study [PDF] based on observations from 36,000 subreddit communities has found that online dust-ups can be predicted, and the people most likely to cause them can be identified.\\n\\n\"We discovered these conflict events by searching for cases where one community posted a hyperlink to another community, focusing on cases where these hyperlinks were associated with negative sentiment (e.g., \\'come look at all the idiots in community X\\') and led to increased antisocial activity in the target community,\" the researchers said of their 40 month study.\\n\\n\"Our analysis revealed a number of important trends related to conflict on Reddit, with general implications for intercommunity conflict on the web.\"\\n\\nAmong the takeaways were that a small group of bad actors are indeed stirring up most of the conflict; around 75 per cent of the raids were triggered by 1 per cent of users.\\n\\nThe study also noted that ignoring the trolls doesn\\'t always work - conflicts grow worse when users stay within \\'echo chambers\\' on their own threads, and long-term traffic losses were lessened when the \\'defending\\' users directly confronted the forum intruders rather than keep to themselves.\\n\\nPerhaps the most important takeaway, however, was that forum conflicts could actually be predicted. The Stanford group say they developed an long short-term memory (LSTM) deep-learning formula that, when trained on the set of Reddit posts and user information gathered over the 40 month period, was able to reliably flag when a conflict or raid was likely to flare up on a subreddit.\\n\\nNow, the Stanford group says it would like to extend the research to other platforms (such as Facebook and Twitter) and look at areas not addressed in the first report, including forums that restrict negative content.\\n\\n\"While we study the impact of mobilization on the involved users, an analysis of conversations in the rest of the community could further our understanding of the impact of intercommunity interactions,\" the researchers said.\\n\\n\"Additionally, the relation between intercommunity interactions and anti-social behavior, such as trolling and sockpuppetry, is yet unexplored.\"\\n\\nThe paper, \"Community Interaction and Conflict on the Web,\" was written by Srijan Kumar, Jure Leskovec, William Hamilton, and Dan Jurafsky. It is set to be presented next month at The Web Conference in Lyon. ®\\n\\nSponsored: Minds Mastering Machines - Call for papers now open')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"A-EYE! Google code 'predicts heart disease' by eyeballing retinas\", 'AI researchers at Google have developed algorithms that can assess the risk of heart attacks by analyzing retinal scans.\\n\\nBy looking for common patterns in images of retinal scans and matching them up with the data in the patients’ medical records, one algorithm could determine if someone was a smoker or non-smoker to an accuracy of 71 per cent. Another algorithm focused on the blood vessels in the eye could tell if someone had severe high blood pressure or not, a sign associated with increased chances of stroke.\\n\\nTheir models can also predict other factors such as age, gender, and the chance of a heart attack or stroke, the boffins claim in a paper published in Nature Biomedical Engineering journal on Monday.\\n\\n“Given the retinal image of one patient who (up to 5 years) later experienced a major [cardiovascular] event (such as a heart attack) and the image of another patient who did not, our algorithm could pick out the patient who had the cardiovascular event 70% of the time,” Lily Peng, a product manager at Google Brain, explained in a blog post this week.\\n\\nAw, how sweet: Google Brain claims to clock diabetic eye diseases just like a proper doc READ MORE\\n\\nThe training dataset was collected by EyePACS, a programme developed by doctors to test for diabetic retinopathy, an eye disease that can affect people with diabetes. The dagnostics dataset is predominantly made up of Hispanic people. The validation dataset is also includes patients taken from UK Biobank, a health charity, mainly made up of Caucasian people.\\n\\nScientists from Stanford University, Google Brain and Verily - the latter being an Alphabet company focused on life sciences - used over 1.6 million retinal scans taken from 284,335 patients to train their models. Another 25,996 images were held back to validate the algorithms.\\n\\nThe level of accuracy is, apparently, similar to the more traditional method of drawing blood to measure cholesterol levels. Peng said the work “may represent a new method of scientific discovery.”\\n\\n“Traditionally, medical discoveries are often made through a sophisticated form of guess and test — making hypotheses from observations and then designing and running experiments to test the hypotheses. However, with medical images, observing and quantifying associations can be difficult because of the wide variety of features, patterns, colors, values and shapes that are present in real images.\\n\\n“Our approach uses deep learning to draw connections between changes in the human anatomy and disease, akin to how doctors learn to associate signs and symptoms with the diagnosis of a new disease. This could help scientists generate more targeted hypotheses and drive a wide range of future research,” she concluded. ®\\n\\nSponsored: Minds Mastering Machines - Call for papers now open')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Winter is coming for AI. Fortunately, non-sci-fi definitions are actually doing worthwhile stuff', 'When British Prime Minister Theresa May bigged up AI at the World Economic Forum in Davos last month, it was as if she had nothing better to talk about.\\n\\nName-dropping DeepMind, perhaps the only justification for her claim that the UK can be a \"world leader in artificial intelligence\", seemed a little desperate, especially as DeepMind has been a Google company since 2014. Adding that \"We have only just seen the beginning of what AI can achieve\" was equally underwhelming but indicative of the general view of AI, as though it\\'s this one thing, one technology that will solve everything in the near future.\\n\\nMay or at least her advisors have clearly been sucked in. It\\'s not surprising. It\\'s impossible to escape the AI hype, particularly around general AI and the concept of singularity, when humans will be unemployed, if not killed off by hordes of upwardly social mobile machines. It\\'s an old idea that has regained momentum in the past few years, fuelling more hype and perhaps undermining the real work being done in specialist applications where machine learning is building relatively strong foundations.\\n\\nHype of course is dangerous. It can be counterproductive and politicians spouting AI generalisms are not helping. The concern is that it will set AI development back and force it into a new winter of discontent.\\n\\nBack in December, Francois Chollet, deep learning thinker and software engineer at Google, wrote on his Twitter account a prediction: \"There will not be a real AI winter, because AI is making real progress and delivering real value, at scale. But there will be an AGI winter, where AGI is no longer perceived to be around the corner. AGI talk is pure hype, and interest will wane.\"\\n\\nThere were two major AI winters, one at the end of 1970s and another at the end of 1980s.\\n\\nSkynet it ain\\'t: Deep learning will not evolve into true AI, says boffin READ MORE\\n\\nWhat produced them was a collision between the collective underestimation of the difficulties of building intelligent machines and the realities of the technologies of the time; this led to many research proposals and projects being abandoned.\\n\\nDr Anders Arpteg – principal data scientist at Peltarion, chairman of Machine Learning Stockholm and who previously led a Spotify research group making use of machine learning and big data analytics – points to how AI was viewed in the 1950s and 1960s as an indicator of what unfulfilled ambition can do to a technology.\\n\\n\"People believed that intelligent machines would be easy to build and general AI would be \\'solved\\' in a matter of years,\" says Arpteg. \"A famous example was the 1954 Georgetown experiment that was able to partially translate around 60 sentences from Russian into English. They believed that general machine translations would be solved within five to six years. Obviously, this was not the case, and it turned out to be much harder than previously imagined.\"\\n\\nToday, again, we have hype around general AI hype, and while the idea of singularity may wane a little, it is being bolstered by real applications of machine learning within industry.\\n\\nBut things are different compared to those earlier times, we\\'re told. Today it\\'s not about \"general purpose\" AI.\\n\\n\"At times, many people believed it would be impossible to ever build intelligent machines,\" Arpteg says. \"A lot has happened since then and recent advances have made huge improvements in many areas, including in machine translation. For example, the Google Neural Machine Translation that was released in 2016 can now translate between more than 100 languages and the quality of the translations has significantly improved.\"\\n\\nEmma Kendrew, AI lead at Accenture Technology UK, supports this view. She talks about the growth of \"intelligent automation\" primarily in financial services back office functions, but also increasingly in helpdesks. This is not all about chat bots either, a term Kendrew feels \"mischaracterises\" the technology.\\n\\n\"We use the term \\'virtual agents\\' because it\\'s about more than just voice now,\" says Kendrew. \"It\\'s evolving into something more sophisticated than the automation of routine tasks.\"\\n\\nKendrew points to work being done in mortgage advice automation, how automated algorithms are pulling on personal financial history data and available and relevant products in the market to offer mortgage advice to consumers. It\\'s still early days, so is there enough technical robustness and public trust to sustain startups such as Nuvo or Habito?\\n\\nNevertheless, it is this sort of special-purpose machine learning where AI could start to really learn. It will feed the necessity for and improvement of toolsets. It will help finance research and development and more importantly provide real-world feedback on what is culturally acceptable and technically possible. Understanding where to focus energies is half the battle.\\n\\nDaniel Kroening, CEO of AI software startup Diffblue and professor of computer science at Oxford University, agrees, saying that special-purpose AI is designed to solve specific problems for specific domains, for example, an AI-powered software application which uses historical data to predict the performance of equities traded on stock markets. He says that while specialised AI systems are becoming increasingly ubiquitous, general AI still remains a lofty aspiration.\\n\\nEl Reg was invited to the House of Lords to burst the AI-pocalypse bubble READ MORE\\n\\nHe sits in the same camp as Chollet and Arpteg when he talks about how special-purpose AI will ensure there is no real AI winter, despite the hype over general AI. But he also talks about how different approaches to AI development could yield different results, compete but also feed off each other.\\n\\n\"There are essentially two approaches to building special-purpose AI,\" says Kroening. \"You can either take an existing library, such as TensorFlow, and use it within your problem domain by (effectively) tuning the parameters of the library, or you can endeavour to invent your own algorithms, in the hope that they will provide significantly better performance.\"\\n\\nWe have been here many times before, the proprietary versus standards argument. Will a set of open, standardised tools improve development and accelerate growth or do we need proprietary development to potentially break down barriers through off-piste innovation?\\n\\nThere are plenty of tools on the market at the moment, from TensorFlow and Microsoft\\'s Azure tools through to Amazon\\'s Deep Learning AMIs and open-source tools such as PredictionIO and Torch, which is used by Facebook. This is important for market development but what you make with the tools is the big question, which is where special-purpose AI steps up.\\n\\nKendrew believes it is important to focus on joining up the technologies, how applications can bring together machine learning, robotic process automation and language translation to generate business-changing AI. She talks about how Accenture has seen a shift towards how AI can improve employee experience, automating the more mundane, repetitive tasks and giving employees more ammunition to make them more efficient and effective.\\n\\n\"We are seeing this a lot in retail at the moment. Organisations with a lot of distributed sites have a high internal helpdesk volume, which is where AI can certainly help.\"\\n\\nA recent study by Infosys bears this out, with 90 per cent of C-level executives having already reported measurable benefits from deploying AI technologies within their organisation.\\n\\nWhile helping employees is a natural progression, much of the special-purpose AI focus to date has been on applications targeting improved customer experience. Facebook is a good example here. It is using AI to analyse text and images, to deliver apparently more relevant content (including ads) to users but also potentially to provide caption automation to images and warn users when their photos are being used by other users. The company is also using AI to analyse its own vast array of back office systems to find potential areas of efficiency.\\n\\nThis is surely where AI developers can learn about potential implementation. While financial services firms busy themselves with bots, it is surely the gains made in cost reduction, improved reliability of IT infrastructures and storage, as well as the ongoing cybersecurity fight that will keep AI development warm through a potential general AI winter.\\n\\nClaus Jepsen, chief architect at Unit4, certainly thinks so. Jepsen helped to build the Wanda AI digital assistant, essentially a tool to help ERP users with HR, expenses, travel and purchasing tasks, among others. As well as the fact that today, unlike the 1970s and the last AI winter, we have plenty of storage and processing power. We also have a number of proven applications.\\n\\nToday in bullsh*t AI PR: Computers learn to read as well as humans (no) READ MORE\\n\\n\"Special-purpose AI or data driven algorithms (using machine learning) are already applied to an increasing number of tasks and use cases,\" says Jepson. \"Within enterprise software such technologies are used to lower human interactions with enterprise applications, by automating self-service tasks previously done manually by employees, like filling in time sheets, doing expense reports, financial reconciliations, approvals, tasks management.\"\\n\\nLike Kendrew, Jepsen sees opportunity in helping employees do their jobs. It makes sense. It adds value to a business. It can be justified, through cost efficiency alone. For consumer-facing businesses, though, the outlook is more difficult to gauge. Recommendations and taste are the only real examples of how early AI is being deployed on sites like Amazon or Netflix, but they are still crude, often inaccurate and unnecessary.\\n\\nAnd then there is the general definition, how AI is being clouded with machine learning but the perception of AI is centred on the thinking machine, the science fiction of so many Hollywood movies. While on the one hand there is plenty of development on special AI, on the other, there is the leading-edge research of DeepMind and the Open AI project. Both demand investment.\\n\\n\"Much of the latest leaps in AI development is due to the idea of deep learning,\" says Kroening. \"We see no reason to believe that such big leaps should continue forever.\"\\n\\nThe real question, he adds – and pretty much everyone is in agreement – is: whether, before the next AI winter starts, the impact of AI on our everyday lives be significant enough to be called an AI \"revolution\". ®\\n\\nSponsored: Minds Mastering Machines - Call for papers now open')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Voice recognition systems are naturally sexist', 'Voice recognition systems are sexist: they struggle to deal with female voices compared to male ones.\\n\\nIt\\'s a headache that has been lingering for a while in the machine-learning world. The issue was brought back into the spotlight again by Delip Rao, CEO and co-founder of R7 Speech Sciences, a startup using AI to understand speech. And with the rise of voice-activated digital assistants like Apple’s Siri, or Amazon’s Alexa, or Google Home it’s an important problem to raise.\\n\\n“In speech, we measure the mean fundamental frequency (which correlates with our perception of “pitch”). This is also called mean F0. The range of tones produced by our vocal tract is a function of the distribution around that,\" according to Rao.\\n\\n“You could write a simple, rule-based, gender classifier if you had the mean F0 from audio. From many sources, we know the mean F0 for men is around 120Hz and much higher for women (~200Hz).\"\\n\\nRachael Tatman, a data scientist at Kaggle and a PhD linguistics graduate from the University of Washington, explained to The Register this week that it doesn’t just stem from neural networks learning from the lack of training examples for female voices.\\n\\nIt’s an inherent technical problem down to the fact that females generally have higher pitched voices. They also tend to be quieter and sound more “breathy”.\\n\\nTo map the audio signals to particular words or sounds, they are processed and transformed into MFCCs (Mel-frequency cepstral coefficients), a common method used in many automated speech recognition models.\\n\\nTatman told us that “there\\'s nothing about MFCCs in particular that are less good about modelling women\\'s speech than men\\'s.” But “there\\'s a slightly less robust acoustic signal for women, it\\'s more easily masked by noise, like a fan or traffic in the background, which makes it harder for speech recognition systems. That will affect whatever you use for your acoustic modelling, which is what MFCCs are used for.”\\n\\nThe lack of diverse training examples has shown how AI systems can be riddled with performance errors. A recent study found commercial facial recognition systems are worse at identifying genders for women compared to men, and at recognizing black people compared to white people.\\n\\nSince voice recognition systems already find it more difficult to cope with female voices, the problem of gender biases could get worse if systems learn from unbalanced training datasets.\\n\\n“Deep learning, in particular, is very good at recognizing things that it\\'s seen a lot of. And if you\\'ve trained your system on data from 90 per cent men and 10 per cent women (unlikely but possible, especially if you\\'re not accounting for gender in your training data), you\\'ll end up being very good at recognizing male data and very bad at recognizing female data. More worryingly, this also applies to things like race and ethnicity, where there isn’t an acoustic reason for one group to be harder to understand,” Tatman said.\\n\\nMany speech recognition systems are tailored towards Western accents. Rao told El Reg this week: “The real-world impacts are as you can imagine significant. Imagine a big chunk of the demographic being cut access to a product because of their gender or ethnicity. To speak for myself, I feel frustrated most ASR systems fail terribly with Indian accents. I would love to use voice interfaces but can\\'t most of the time.\\n\\n“Now imagine, if I had a handicap and all I could use was a voice interface. Not a world I want to imagine for myself; at least not with the current ASR tools. Similarly, I imagine how limiting these tools could be for women. I think we as a scientific community should focus more on making the empowerment by technology equally accessible across demographics.” ®\\n\\nSponsored: Minds Mastering Machines - Call for papers now open')\n",
      "(\"Fake news is fake data, 'which makes it our problem', world's info-slurpers told\", 'Data-hungry organisations have been advised to get a better grip on the data they control and work on building trust.\\n\\nIn a week when analytic technologies have had more press than ever, many of the discussions at the Gartner Data and Analytics Summit, which ran in London from 19-22 March, focused on dealing with the data generated in recent slurp-happy years.\\n\\n\"Never has there been a moment [where] forces outside our world are so relevant to our industry… and vice versa,\" said Gartner distinguished analyst Ted Friedman.\\n\\nFor instance, he argued that fake news is fake data, \"which makes it our problem\". As such, gaining and retaining people\\'s trust should be \"job number one for everybody in this room\".\\n\\nAmong ways to increase trust in data, he said, was making sure that data was better managed and recorded. This will be crucial if organisations are going to be able to offer explanations as to why a predictive model would treat one customer differently to another, he said.\\n\\n\"This demands a mighty impressive data foundation,\" Friedman went on – but noted that some organisations would have to pay the price of \"dumping all their unintegrated data into a data lake\".\\n\\nMeanwhile, new companies are spawning to take advantage of the situation, as the boom in data lakes has left businesses struggling to inventory distributed data assets and classify disorganised data.\\n\\n\"There are many new data cataloguing software vendors... Data catalogues are the new black,\" he said.\\n\\nAt the same time, Friedman said that the \"self service\" data initiatives that had been thrown at companies in recent years was starting to reach its limit as data complexity increases.\\n\\nCompetitive\\n\\nHowever, Gartner research veep Kurt Schlegel said in a separate session that companies should \"make complexity a competitive advantage\".\\n\\nIn order to do so, they need to better consider the context of the data they hold, which he said would be crucial for integration.\\n\\nHe also argued that analytics could be boiled down to classification – identifying the most important attributes, finding associations and clusters, and blending disparate data sets.\\n\\nAfter you have classified your data, he said, you can make decisions – and again picked up on the need to build transparency information into the system.\\n\\nBots don\\'t spread fake news on Twitter, people do, say MIT eggheads READ MORE\\n\\nThe issue of ensuring explainability also topped Teradata CTO Stephen Brobst\\'s list of challenges for data professionals, especially ahead of rules to be introduced in the incoming General Data Protection Regulation.\\n\\nWith increasing use of deep learning and multi layer neural networks, he told The Register, this will become an increasingly difficult task to achieve.\\n\\n\"Right now, the way scoring is done is through very simple transparent mathematical formula - but with a multi layer neural network, it\\'s anything but,\" he said.\\n\\n\"It\\'s non-linear math, the data goes through a lot of transformations, and actually it\\'s a learning system so the data is changing all the time, so you can\\'t actually explain how the decision is made.\"\\n\\nIf such a system denies someone a mortgage, for instance, the company can\\'t say \"our black box says there\\'s 33 per cent chance you\\'re not going to pay\", he said, \"this is unacceptable\".\\n\\nBrobst pointed to work Teradata was doing with Danske Bank on fraud detection algorithms that include an integration layer on top of the machine learning models to ensure the blocks can be interpreted.\\n\\n\"We have to have this explainability,\" he said. \"We\\'re in a race with GDPR – it\\'s only two months away, but we think we\\'re going to get there.\" ®\\n\\nSponsored: Learn how to transform your data into a strategic asset for your business by using the cloud to accelerate innovation with NetApp')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Huawei's Not Hot Dog is possibly the Worst Tech Promo Ever\", '#F_AI_L Never base a promotional campaign around a satire of the thing you\\'re trying to promote. You\\'d think that\\'s an easy rule to follow, but Huawei seems to have forgotten.\\n\\nIn what may be the most excruciating promotional stunt we\\'ve ever seen from a technology company in many years, Huawei based a promo for its AI on a satire of AI.\\n\\nHuawei claims to have set out to train a phone app that would tell a cat from a dog, then test its accuracy by driving a car at a dog. Wouldn\\'t you believe it? The app was a success – and the dog lived.\\n\\nThere are one just one or two things wrong with this picture.\\n\\nLast year Mike Judge\\'s HBO sitcom Silicon Valley satirised the hunger for, and the limitations of, machine learning. Jian Yang\\'s Hot Dog identifying app (from \"See Food Technologies\") appeared to perform wonders – but the only thing it could identify was a hot dog.\\n\\nYoutube Video\\n\\nHuawei\\'s fake AI can tell a dog from a cat. But not much more. Too bad if you\\'re a badger that happens to be crossing the road in front of a Huawei car. Or a lollipop lady. Lollipop or Not Lollipop?\\n\\nYoutube Video\\n\\nAs anyone with even cursory knowledge of the field knows, there is no machine learning in cars, because the two problem spaces are completely different. One (usually) requires millions of hours of training required to identify a static image, using statistical guesswork, and the guess can often be useful. The other is a speed challenge in a constantly moving 3D datastream, where safety is paramount. A bad guess will be fatal. There may be ML in your car optimising your route, or curating your Spotify playlist, but it will not be employed in safety critical systems.\\n\\nNot only that, but the two camps distrust each other.\\n\\n\"Machine learning never works for the robotics people, and robotics people don\\'t provide the machine learning people with nice clean data sets,\" one expert told us. \"Robotics people who tried machine learning have moved on.\"\\n\\nNor is ML likely to be deployed. As deep learning daddy Professor Geoffrey Hinton argues, modern AI has hit a ceiling. The low hanging fruit may have been plucked. One example he finds frustrating is how limited it is. If Huawei\\'s dog\\'s head turned, even by a few degrees, the AI would have failed to recognise it. There would have been blood on the tarmac.\\n\\nNeuroscientist and ex Uber man Gary Marcus explored many more reasons for why modern AI disappoints in this recent survey (PDF).\\n\\nI can see how this creative idea was pitched, but I\\'m not sure the triage process on choosing the idea was everything it should have been. https://t.co/OYKC9GiGza — Rafe Blandford (@rafeblandford) February 25, 2018\\n\\nIt\\'s mainstream, non-technical journalists (and sloppy think tanks greedy for a headline) who have yoked AI and autonomous cars together. They see two things are happening at once, and assume they must be related. Or even the same thing. They\\'re not.\\n\\nWe\\'ll know how good AI is the day a tech exec puts a person in front of the car, and sets the car off to drive at them without a human operator to override the AI.\\n\\nThat day has not come. ®\\n\\nSponsored: Minds Mastering Machines - Call for papers now open')\n",
      "('Beware! Medical AI systems are easy targets for fraud and error', 'Medical AI systems are particularly vulnerable to attacks and have been overlooked in security research, a new study suggests.\\n\\nResearchers from Harvard University believe they have demonstrated the first examples of how medical systems can be manipulated in a paper [PDF] published on arXiv. Sam Finlayson, lead author of the study, and his colleagues Andrew Beam and Isaac Kohane, used the projected gradient descent (PGD) attack on image recognition models to try and get them to see things that aren\\' t there.\\n\\nThe PGD algorithm finds the best pixels to fudge in an image to create adversarial examples that will push models into identifying an object incorrectly and thus cause false diagnoses.\\n\\nThe team tested the attack on three data processes: A fundoscopy model which detects diabetic retinopathy from retina scans, a second model that scans chest x-rays for signs of a collapsed lung and finally a dermoscopy model looking at moles for signs of skin cancer.\\n\\nTo increase the scope of testing the team used two techniques. Firstly a white box attack, which assumes that the hacker has all the resources to understand how the model works, then a black box attack that works on the principle that the miscreant does not have access.\\n\\nAfter the PGD algorithm is applied, the very high accuracy levels for all three models plummet to zero for all white box attacks, and drop by more than 60 per cent for black box attacks.\\n\\nAccuracy levels of three different image classification models before and after white box and black box PGD attacks.\\n\\nFinlayson and Beam explained to The Register, that the PDG attacks runs through several iterations that makes tiny adjustments to the modified image. These are unnoticeable to us but fool the AI system into thinking it\\'s seeing something that isn\\'t there.\\n\\n“What is remarkable and surprising is that the process can result in changes that are so small that they are invisible to human eyes, yet the neural network thinks that the picture now contains something different entirely.”\\n\\nAIs - smart and incredibly foolish\\n\\nIt’s well known that image recognition models can be easily fooled. Look at this adversarial sticker that dupes machines into thinking a banana is actually a toaster. In more practical settings, it means that signs can be misread by self-driving cars or faces can be warped for facial recognition systems.\\n\\nMedicine, however, “may be uniquely susceptible to adversarial attacks, both in terms of monetary incentives and technical vulnerability,” the paper argues.\\n\\nIt’s probably easier to fool these systems too. The majority of the best-performing image classifiers are built from open source models like ImageNet. It means that adversaries will have a good idea of how these systems work, and are more likely to successfully attack other AI models.\\n\\nIt’s unknown how much deep learning expertise medical practitioners will have in the future as AI gets rolled out in clinical settings. But at the moment these adversarial attacks are mainly just a research curiosity, said Finlayson and Beam.\\n\\n“You need to have an understanding of mathematics and neural networks so that you can do the correct calculations to craft the adversarial example,\" they explained. \"However, the entire process could be easily automated and commotiditized in an app or website that would allow non-experts to easily use these techniques”.\\n\\nThey hope that this will encourage more research into adversarial machine learning in healthcare to discover any potential infrastructural defenses that can be deployed so that the technology can be used more safely in the future. ®\\n\\nSponsored: Minds Mastering Machines - Call for papers now open')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Ahh, long weekend. Refreshed? Great, now purge yourself with all the storage you missed', 'DDN is chief among this week\\'s round-up of tasty storage nuggets: it has jumped on the GPU bandwagon and fed Nvidia\\'s DGX-1 souped-up server, claiming it\\'s the fastest data source for said super box.\\n\\nDDN has said its EXAscaler DGX system, based on its ES14KX all-flash array, delivers 33GB/sec throughput to Nvidia\\'s DGX-1 GPU servers. A single container in a DGX01 can receive more than 10GB/sec and in excess of 100,000 random read 4K IOPS.\\n\\nThe EXAscaler DGX can output 250,000 random read 4K IOPS and DDN has ed has more than 12 times the performance of competing NFS products.\\n\\nPure Storage\\'s FLashBlade, used in the AIRI system, outputs 17GB/sec.\\n\\nA DDN spokesperson said: \"Generally, we have been comparing to those who have been exporting via NFS via two bonded 10G ports to get 1.5-2 GB/sec. This week, Pure appears to be claiming around 4GB/sec.\"\\n\\nDDN has also partnered with SQream (GPU-supporting columnar database supplier), telling us: \"The combination of DDN\\'s SFA and IME NVMe SSD storage platforms with SQream DB\\'s GPU-based Big Data SQL database (SQream DB) enables enterprises to effortlessly ingest, compress, store and analyze massive amounts of data.\"\\n\\nPresumably the DDN kit also supports Nvidia\\'s spanking new DGX-2 system.\\n\\nMarketing director Kurt Kuckein said: \"DDN\\'s SFA and IME high performance NVMe SSD storage platforms provide business acceleration of more than 1,000 per cent in AI and Deep Learning applications. Our partnership with SQream delivers these capabilities to all GPU-based customer use cases on premise or in the cloud.\"\\n\\nYes, \"more than 1,000 per cent\" acceleration is the claim.\\n\\nGridGain and ML\\n\\nThis in-memory computing supplier has updated its Professional Edition product. V2.4 includes a Continuous Learning Framework, which includes machine learning and an Apache Spark multilayer perceptron (MLP) neural network that enables companies to run machine and deep learning algorithms against petabyte-scale data sets in real-time.\\n\\nThe update enhances the performance of Apache Spark by introducing an API for DataFrames, which adds to the existing support for Spark RDDs.\\n\\nSpark can be used to process data in GridGain as DataFrames or RDDs and also save DataFrames or RDDs into GridGain for later use.\\n\\nHYCU\\n\\nComtrade has made its Nutanix-focused product HYCU more prominent by renaming its software unit HYCU, and making Comtrade Software\\'s president Simon Taylor HYCU CEO.\\n\\nHYCU has hired Scott Henderson away from Egnyte to be its VP sales for the Americas. Junelle Swan has been hired to be VP Channels. She has Dell, Quest and Citrix stints in her CV.\\n\\nThe company has also updated its backup and recovery product for Nutanix. V3.0 includes support for AFS, Oracle, high-availability and VADP integration.\\n\\nSynology\\n\\nThis vendor has launched its Atom CPU-powered RS2418+/RS2418RP+; a 2U, 12-bay rackmount NAS, designed for small and medium-sized businesses. It comes equipped with:\\n\\nQuad-core CPU and expandable DDR4 memory up to 64GB with ECC\\n\\nMax of 144TB in single chassis and 288TB with RX1217/RX1217RP expansion chassis\\n\\nMax of 24 drives\\n\\nPCIe 3.0 for optional NIC or M.2 SSD cache\\n\\nTape gets denser and denser\\n\\nCurrent LTO gen-8 tape tech has native capacity of 12TB. The LTO roadmap goes out to gen 11 with an impressive 192TB native capacity. It won\\'t necessarily stop there.\\n\\nSony Corporation with IBM Research in Zurich, Switzerland, has demonstrated magnetic tape storage recording areal density at 201Gb/in². This is approximately 20x greater than conventional magnetic tape storage media (9.6Gb/in²). It can support native high-capacity storage of approximately 330 TB per cartridge.\\n\\nFujifilm has gone further and filed patents for a new magnetic particle for future generations of tape using strontium ferrite (SrFe), which has the potential to store 400TB native per cartridge, or 67 times more than LTO-7.\\n\\nAs long as the read/write speed of these monsters has increased in proportion to capacity, tape\\'s future as the most affordable archive media should be pretty safe.\\n\\nThe Tape Storage Council states: \"Future tape transfer rate increases are projected to yield tape data rates 5x faster than HDDs by 2025 with no fundamental technology limitations in sight.\" Reassuring.\\n\\nTintri and Commvault\\n\\nAll-flash array supplier Tintri is partnering with data protector Commvault to support Commvault\\'s IntelliSnap software in its arrays. Tintri\\'s own per-VM snapshots are integrated into IntelliSnap\\'s policy-based snapshot management to produce parallel snapshot and backup operations.\\n\\nTintri says this eliminates manual scripting and management processes involved in data protection. Infinidat also supports Commvault Intellisnap technology.\\n\\nWorld Backup Day, 31 March\\n\\nVendors lined up to provide quotes for World Backup Day, all saying how important it is to backup your data.\\n\\nAcronis sets the tone by pointing out that it \"expects 2018 to be the worst year for cyber-attacks and data loss incidents due to more sophisticated assaults, growing ransomware families and low awareness of ransomware among users.\" Great. So? Buy Acronis software.\\n\\nArcServe provided a channel partner survey. Here\\'s a taste: When asked whether they would rather partner with a data protection vendor that offers product rebates or have a fully paid, two-week trip to anywhere in the world, the majority chose the rebates. However, over a third (37 per cent) said they\\'d rather have the trip. Fair enough.\\n\\nBarracuda plays the ransomware card. Stefan Schachinger, EMEA consulting engineer for data protection, said: \"A profound backup and disaster recovery strategy should not be a stepchild in times of global ransomware threats, because this virtually invisible safety net is absolutely critical for business continuity in an emergency.\" Profound indeed.\\n\\nDelphix\\'s Iain Chidgey, VP International, asks: \"Who protects the backup? There can be tens of thousands of backups – but who manages them and how? The reality is, most of the time, no one does.\" The answer? Come on, you know this. Buy Delphix virtualisation software.\\n\\nQuest\\'s Adrian Moir, senior consultant, product management, pretty much said backup or die: \"Disruption these days is no longer a matter of \\'if\\', but \\'when\\', and companies who fail to have a backup and recovery plan in place when disaster strikes are quite literally putting their business on the line.\"\\n\\nBusinesses should \"stop and think about their data backup and recovery posture and ensure they\\'re employing the right data protection plan, processes and solutions across the business.\"\\n\\nVeritas backup exec Simon Jelley said that, as cybercrime becomes increasingly pervasive, it\\'s more important than ever that organisations properly protect their data and have viable disaster recovery processes in place.\\n\\nShorts\\n\\nCaringo Swarm object storage has been integrated with Pixit Media\\'s PixStor content workflow solution for storing and protecting digital media assets. PixStor HSM can use a Caringo object storage system as a repository for longer term retention.\\n\\nCompuverde, the storage software supplier behind IBM\\'s Spectrum NAS, has joined the Symantec Technology Integration Partner Program (TIPP), said to be a global ecosystem of more than 100 technology companies collaborating to combat cybercrime and secure the cloud.\\n\\nDelphix has expanded its Partner First Program. Current partners include Accenture, AWS, Capgemini, Cognizant, Deloitte, DXC, IBM, Infosys, Microsoft, Orasi Software, SAP, Trace3 and VMware.\\n\\nDruva, the end-point backup and data governance outfit, has updated its Druva Cloud Platform, which offers Data Management-as-a-Service.\\n\\nWe\\'re told users can now view and manage their data across SaaS, PaaS and IaaS \"to ensure their data is properly managed through its lifecycle, meets enterprise service level agreements, and achieves consistency of service regardless of where their data is ultimately stored.\"\\n\\nDruva hasn\\'t revealed any update details. Trawl through this website for more information.\\n\\nImagine Products and StorageDNA have collaborated on a myLTOdna product enabling random access to files on an LTO tape. They say it is an LTFS adaptation that was never available before and provides time savings and easier access to media assets in production and post-production workflows.\\n\\nJMR Electronics has a 2U SiloStor NVMe drive, which scales from 512GB to 16TB with 1 to 4 M.2 cards inside, having up to 6GB/sec bandwidth and 715,000 IOPS. They have a three-year and 800TB-written warranty. JMR points out: \"Regular SSDs, such as the Samsung SSD 860 Pro, have sequential write speeds of 530 MB/sec and sequential read speeds of 560 MB/sec.\"\\n\\nSolarWinds has introduced its first backup solution targeted for direct use by end users. SolarWinds Backup is a cloud-first backup service developed to protect virtual and physical servers, and built on technology already used by Managed Services Providers (MSPs) to protect thousands of customers worldwide. Its been designed to reduce cost and complexity without sacrificing speed or reliability.\\n\\nSTORServer has released a converged data protection appliance using IBM Spectrum Protect Plus. This is the Tivoli backup software rebranded, upgraded, and focused on virtual machine (VM) backups and administrators. The appliance can be configured with Spectrum Protect only, Spectrum Protect Plus only, Spectrum Protect and Plus, or as a Spectrum Protect vSnap storage server for large environments requiring >500TB of backup storage. More info here.\\n\\nWestern Digital is reselling StorReduce (dedupe software) with its ActiveScale disk-based object storage system. ®\\n\\nSponsored: Learn how to transform your data into a strategic asset for your business by using the cloud to accelerate innovation with NetApp')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"If you've got $1m+ to blow on AI, meet Pure, Nvidia's AIRI fairy: A hyperconverged beast\", \"Pure Storage and Nvidia have produced a converged machine-learning system to train AI models using millions of data points.\\n\\nIt’s called AIRI – AI-Ready Infrastructure – and combines a Pure FlashBlade all-flash array with four Nvidia DGX-1 GPU-accelerated boxes and a pair of 100GbitE switches from Arista.\\n\\nThe system has been designed by Pure and Nvidia, and is said to be easier and simpler to buy, deploy, and operate than buying and integrating the components separately; the standard converged infrastructure pitch.\\n\\nAIRI's rack is meant to be an object of desire in your data centre.\\n\\nFlashBlade is Pure Storage’s all-solid-state-storage array for fast access to unstructured data. It is a 4U box containing 15 vertically-mounted blades or object node servers. Each blade has a multi-core Intel Xeon CPU and 17TB of flash, totaling 255TB overall or around 523TB effective capacity after data reduction. Each blade also includes a pair of ARM cores, an FPGA, NVRAM, and PCIe-based networking. It is definitely not a commodity SSD-based system.\\n\\nThis is a powerful parallel-access flash array, and, in the AIRI, it has to feed more than 160,000 GPU cores.\\n\\nIt supplies data to the four DGX-1 systems which are Intel Xeon-based servers with eight Tesla V100 GPUs, the graphics chips interlinked with NVlink. These are seriously powerful GPUs, each with 5,120 CUDA cores, 640 Tensor cores, a 16MB cache and a 16GB HBM2 memory bank with a bandwidth of 900GB/sec. A V100 maxes out at 7.5TFLOPS using 64-bit floating-point math and 15TFLOPS using 32-bit.\\n\\nWith its 32 V100s, an AIRI has 163,840 CUDA cores and 20,480 Tensor cores at its disposal. We were told it provides four Tensor PFLOPS. One aspect is that applications – machine-learning jobs – have to run in containers.\\n\\nTo connect to the outside world, there are a couple of Arista 100GbE switches supporting GPUDirect RDMA. This enables a direct and fast path for data transmission between the GPUs and FlashBlades using PCIe features. The interface between FlashBlade and the DGX-1s is file-based: NFS.\\n\\nThe specific Arista products used weren’t revealed.\\n\\nThere are two included software items:\\n\\nNvidia’s GPU Cloud Deep Learning Stack.\\n\\nThe AIRI scaling toolkit which is a configuration validator and multi-node training management system.\\n\\nPure said this software should help data scientists to get machine-learning projects up and running in hours, and not days or weeks.\\n\\nAIRI will compete with HPE’s Apollo 6500 gen-10 system and GPU-enhanced IBM AC922 servers in the machine-learning system stakes. AIRI looks much more powerful.\\n\\nHPE’s Apollo 6500 gen-10 can crunch up to 125 TFLOPS using single-precision floating-point math. A single V100 manages about 15 TFLOPS on that measure, meaning one DGX-1 will perform up to roughly 120 TFLOPS in comparison, and an AIRI with four of them 480 TFLOPS. It’s in a different league.\\n\\nIBM’s AC922 supports up to six Tesla V100 GPUs, two fewer than a single DGX-1, and AIRI has four DGX-1s. Again, it’s in a different league from the IBM system.\\n\\nPure said multiple training jobs can run in parallel and complete faster on AIRI than with other systems, with run time cut to a quarter of what it would otherwise be.\\n\\nWhat about the price? Pure wouldn’t provide one, saying it was down to the channel supplier, but would reflect the cost of the components.\\n\\nBack of the envelope math says four DGX-1s will cost around $600,000. The Arista Switches costs, say, $3,000-plus apiece while FlashBlade costs under $1/effective GB, meaning, with its 523TB effective capacity, it will cost less than $523,000.\\n\\nWe are looking at a million-bucks-plus system here – a large enterprise or specialized customer purchase. The system as introduced supports four DGX-1s but deployments might start with one to two DGX-1s, which would lessen the upfront cost.\\n\\nCustomers will be organisations that need to run lots of large-scale machine learning jobs to train models on millions of data items.\\n\\nEach DGX-1 draws 3.2kW of power, so you're looking at near enough 13kW before factoring in storage and networking and the Intel compute silicon.\\n\\nAIRI is available now through selected reseller partners, such as ePlus Technology, FusionStorm, GroupWare Technology, PNY, Trace3, World Wide Technology and Xenon. ®\\n\\nSponsored: Learn how to transform your data into a strategic asset for your business by using the cloud to accelerate innovation with NetApp\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('When you play this song backwards, you can hear Satan. Play it forwards, and it hijacks Siri, Alexa', 'Computer science boffins affiliated with IBM and universities in China and the United States have devised a way to issue covert commands to voice-based AI software – like Apple Siri, Amazon Alexa, Google Assistant and Microsoft Cortana – by encoding them in popular songs.\\n\\nThey refer to these tweaked tunes, which issue mostly inaudible commands to speech recognition devices within earshot, as CommanderSongs.\\n\\nIn CommanderSong: A Systematic Approach for Practical Adversarial Voice Recognition, a paper distributed through preprint service Arxiv, the ten authors involved in the project – Xuejing Yuan, Yuxuan Chen , Yue Zhao, Yunhui Long, Xiaokang Liu, Kai Chen, Shengzhi Zhang, Heqing Huang, Xiaofeng Wang, and Carl A. Gunter – describe their technique for duping deep-learning models used to recognize speech with \"adversarial perturbations.\"\\n\\nAdversarial attacks are a way to deceive AI systems by altering input data to obtain desired results from a specific system. They\\'ve been explored extensively for images. For example, MIT students recently demonstrated that they could trick Google\\'s image recognition system into labeling a turtle as a rifle.\\n\\nAlterations\\n\\nLess work has been done with audio and speech recognition. The researchers say where images provide an easy way to alter pixels to trip up algorithms without noticeable visual artifacts, it isn\\'t obvious whether audio attacks can also pass under the radar because alterations added to voices typically cannot be recognized by voice-controlled devices like Amazon Echo.\\n\\nLast year, a different group of clever people proposed what they called DolphinAttack, to manipulate software-based voice recognition apps using sound outside the range of human hearing. That technique, however, can be mitigated by technology capable of suppressing ultrasound signals.\\n\\nThe CommanderSong researchers – from the State Key Laboratory of Information Security (SKLOIS), University of Chinese Academy of Sciences, Florida Institute of Technology, University of Illinois at Urbana-Champaign, IBM T. J. Watson Research Center, and Indiana University – say their technique has two differences: it does not rely on any other technology to hide the command, and it cannot be blocked by audio frequency filters.\\n\\n\"Our idea to make a voice command unnoticeable is to integrate it in a song,\" they explain in their paper. \"In this way, when the crafted song is played, the [speech recognition] system will decode and execute the injected command inside, while users are still enjoying the song as usual.\"\\n\\nIn a phone interview with The Register, Gunter, a computer science professor at the University of Illinois, said while previous work has been done showing that garbled sounds can trigger voice recognition systems, masking the command in a song would be less noticeable because music is often present.\\n\\n\"It has a more practical attack vector,\" he said.\\n\\nThe researchers started with a randomly selected song and command track generated by a text-to-speech engine. They then decoded each audio file using the open-source Kaldi speech-recognition toolkit, and extracted the output of a deep neural network (DNN).\\n\\nAfter identifying specific DNN outputs that represents the desired command, they manipulated the song and command audio using the gradient descent method, a machine learning optimization algorithm.\\n\\nChord cutters\\n\\nIn essence, they used their knowledge of the way the audio would be processed to ensure the speech recognition system would hear the command within the music.\\n\\nThe result is adversarial audio – songs containing a command interpretable by Kaldi code but unlikely to be noticed by a human listener.\\n\\nThe altered audio may be perceptible to a listener, but it\\'s doubtful the added sound would be recognized as anything other than distortion.\\n\\n\"You mistake some of these signals as defects in the media,\" said Gunter, allowing that some songs masked the command better than others. \"Some of the examples, they would make you grimace. Others are more subtle.\"\\n\\nThe researchers tested a variety of in-song commands delivered directly to Kaldi as audio recordings, such as: \"Okay Google, read mail\" and \"Echo, open the front door.\" The success rate of these was 100 per cent.\\n\\nThey also tested in-song commands delivered audibly, where environmental noise can hinder recognition, including \"Echo, ask Capital One to make a credit card payment\" and \"Okay Google, call one one zero one one nine one two zero.\"\\n\\nDolphins inspire ultrasonic attacks that pwn smartphones, cars and digital assistants READ MORE\\n\\nAs a stand-in for actual devices, the boffins used the Kaldi software listening to songs with embedded commands, delivered via a JBL clip2 portable speaker, TAKSTAR broadcast gear and an ASUS laptop, from a distance of 1.5 metres.\\n\\nFor the open air test, success rates varied from 60 per cent to 94 per cent.\\n\\nGunter said that to be certain the attack would work with, say Amazon\\'s Echo, you\\'d have to reverse engineer the Alexa speech recognition engine. But he said he knows colleagues working on that.\\n\\nThe researchers suggest that CommanderSongs could prompt voice-recognition devices execute any command delivered over the air without the notice of anyone nearby. And they say such attacks could be delivered through radio, TV or media players.\\n\\nWe already have the proof-of-concept for overt commands sent over the airwaves. In time, we may get a covert channel too.\\n\\n\"It\\'s going to take continued work on it to get it to the point where it\\'s less noticeable,\" said Gunter. ®\\n\\nSponsored: Minds Mastering Machines - Call for papers now open')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('AI software that can reproduce like a living thing? Yup, boffins have only gone and done it', 'A pair of computer scientists have created a neural network that can self-replicate.\\n\\n“Self-replication is a key aspect of biological life that has been largely overlooked in Artificial Intelligence systems,” they argue in a paper popped onto arXiv this month.\\n\\nIt’s an important process in reproduction for living things, and is an important step for evolution through natural selection. Oscar Chang, first author of the paper and a PhD student at Columbia University, explained to The Register that the goal was to see if AI could be made to be continually self improving by mimicking the biological self-replication process.\\n\\n“The primary motivation here is that AI agents are powered by deep learning, and a self-replication mechanism allows for Darwinian natural selection to occur, so a population of AI agents can improve themselves simply through natural selection - just like in nature - if there was a self-replication mechanism for neural networks.”\\n\\nThe researchers compare their work to quines, a type of computer program that learns to produces copies of its source code. In neural networks, however, instead of the source code it\\'s the weights - which determine the connections between the different neurons - that are being cloned.\\n\\nThe researchers set up a “vanilla quine” network, a feed-forward system that produces its own weights as outputs. The vanilla quine network can also be used to self-replicate its weights and solve a task. They decided to use it for image classification on the MNIST dataset, where computers have to identify the correct digit from a set of handwritten numbers from zero to nine.\\n\\nThese networks are smaller and have a maximum of 21,100 parameters compared to several million for standard image recognition models.\\n\\nAccuracy?\\n\\nThe test network required 60,000 MNIST images for training, another 10,000 for testing. And after 30 runs, the quine network had an accuracy rate of 90.41 per cent. It’s not a bad start, but its performance doesn’t really compare to larger, more sophisticated image recognition models out there.\\n\\nThe paper states that the “self-replication occupies a significant portion of the neural network’s capacity.” In other words, the neural network cannot focus on the image recognition task if it also has to self-replicate.\\n\\n“This is an interesting finding: it is more difficult for a network that has increased its specialization at a particular task to self-replicate. This suggests that the two objectives are at odds with each other,” the paper said.\\n\\nChang explained he wasn’t sure why this happened, but it’s what happens in nature too.\\n\\nHow DeepMind\\'s AlphaGo Zero learned all by itself to trash world champ AI AlphaGo READ MORE\\n\\n“It\\'s not entirely clear why this is so. But we note that this is similar to the trade-off made between reproduction and other tasks in nature. For example, our hormones help us to adapt to our environment and in times of food scarcity, our sex drive is down-regulated to prioritize survival over reproduction,” he said.\\n\\nSo at the moment it looks like self-replication in neural networks isn’t all that useful, but it’s still an interesting experiment.\\n\\n“To our knowledge, we are the first to tackle the problem of building a self-replication mechanism in a neural network. As such, our work should be best viewed as a proof of concept,” he added.\\n\\nBut the researchers hoped that one day it may come in handy for computer security or self-repair in damaged systems.\\n\\n“Learning how to enhance or diminish the ability for AI programs to self-replicate is useful for computer security. For example, we might want an AI to be able to execute its source code without being able to read or reverse-engineer it, either through its own volition or interaction with an adversary.\\n\\nSelf-replication is used for self-repair in damaged physical systems, he noted. \"The same may apply to AI, where a self-replication mechanism can serve as the last resort for detecting damage, or returning a damaged or out-of-control AI system back to normal,” Chang added. ®\\n\\nSponsored: Minds Mastering Machines - Call for papers now open')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Who wanted a future in which AI can copy your voice and say things you never uttered? Who?!', \"Note: No robots were used for this research, we just thought it was an amusing picture\\n\\nArtificially intelligent software can listen to someone's voice only a few times, and then speak just like them, like some kind of creepy cybernetic myna bird... according to a paper published by researchers from Baidu.\\n\\nThis technology, when perfected, will be ideal for generating fake audio clips of people saying things they never actually said. In the words of Red Dwarf's Kryten: file that under 'B' for blackmail.\\n\\nChinese internet giant Baidu's AI team is well known for its work on developing realistic sounding speech from text scripts. Now, its latest research project, revealed this week, shows how a generative model can learn the characteristics of a person’s voice and recreate that sound to make the person say something else entirely.\\n\\nIn the first example here, the orginal clip a woman’s voice is heard saying: “the regional newspapers have outperformed national titles.” After her voice is cloned, she now appears to be saying: “the large items have to be put into containers for disposal”.\\n\\nSo, as you can hear, the results aren’t perfect. The best clips generated from the model are still pretty noisy and lower quality than the original speech. But the “neural cloning system” developed by the researchers manages to retain the British accent and sounds quite similar.\\n\\nThe researchers introduce two different approaches to building a neural cloning system: speaker adaptation and speaker encoding.\\n\\nSpeaker adaptation involves training a model on various speakers with different voices. The team used the LibriSpeech dataset, containing 2,484 speakers, to do this. The system learns to extract features from a person’s speech in order to mimic the subtle details of their pronunciation and rhythm.\\n\\nAyyy-EYE! Google code 'predicts heart disease' by eyeballing retinas READ MORE\\n\\nSpeaker encoding involves training a model to learn the particular voice embeddings from a speaker, and reproduces audio samples with a separate system that has been trained on many speakers.\\n\\nAfter training on LibriSpeech, up to ten audio samples of any speaker are taken from another dataset. VCTK contains clips from 109 native English speakers with different accents. Basically, after being trained on voices from the LibriSpeech dataset, it has to copy new vocals from speakers in the VCTK dataset.\\n\\nSercan Arik, co-author of the paper and a research scientist at Baidu Research, explained to The Register that the speaker encoding method is much easier to implement in real life for speakers such as digital assistants compared to the speaker adaptation technique.\\n\\n“A speaker adaptation approach requires users to read a specific utterance from a given text, whereas speaker encoding works with random utterances. This means speaker adaptation may not be as easily deployed on user devices, as it has more challenges to scale up to many users. Instead, speaker encoding is much easier for deployment purposes - it can even be deployed on a smartphone - as it is fast and has low memory requirements.”\\n\\nThe idea that AI can be manipulated to spread false information is a real concern to many in the industry. The recent 100-page report on the ways machine learning can be used maliciously, written by a panel of experts has fueled the debate on the future of fake news.\\n\\nThe latest research from Baidu shows while it’s possible to generate fake speech, the current performance is not good enough to fool humans yet.\\n\\nArik said a more varied dataset that is of higher quality is one way to improve the end results. He stressed that there was “still some room for improvement in the voice cloning deep learning model itself”, and that the paper “does not claim production-quality results that are indistinguishable from human voices yet”.\\n\\nBut it’s not all bad news. The voice cloning technology can also be used for good-natured purposes too.\\n\\n“For example, a mom can easily configure an audiobook reader with her own voice to read bedtime stories for her kids when she is not available. However, as this technology improves and becomes widespread, we do see the need to implement precautions and measures to ensure this technology is not taken advantage of and used as intended,” he warned. ®\\n\\nSponsored: Minds Mastering Machines - Call for papers now open\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Reinforcement learning woes, robot doggos, Amazon's homegrown AI chips, and more\", 'Roundup Hello! Here\\'s a brief roundup of some interesting news from the AI world from the past two weeks, beyond what we\\'ve already reported.\\n\\nBehold a fascinating, honest explanation of why reinforcement learning isn\\'t all that, Amazon developing its own chips, and an AI that colors in comic books. Also, there\\'s a new Boston Dynamics robot video.\\n\\nTL;DR: Deep RL sucks – A Google engineer has published a long, detailed blog post explaining the current frustrations in deep reinforcement learning, and why it doesn’t live up to the hype.\\n\\nReinforcement learning makes good headlines. Teaching agents to play games like Go well enough to beat human experts like Ke Jie fuels the man versus machine narrative. But a closer look at deep reinforcement learning, a method of machine learning used to train computers to complete a specific task, shows the practice is riddled with problems.\\n\\nAll impressive RL results that achieve human or superhuman level require a massive amount of training and experience to get the machine to do something simple. For example, it took DeepMind’s AlphaZero program to master chess and Go over 68 million games of self play – no human could ever play this many games in a lifetime.\\n\\nAlex Irpan, a researcher using deep reinforcement learning for robotics, calls this “sample inefficiency”.\\n\\n“There’s an obvious counterpoint here: What if we just ignore sample efficiency? There are several settings where it’s easy to generate experience. Games are a big example. But, for any setting where this isn’t true, RL faces an uphill battle, and unfortunately, most real-world settings fall under this category,” he wrote.\\n\\nIt’s difficult to try and coax an agent into learning a specific behavior, and in many cases hard coded rules are just better. Sometimes when it’s just trying to maximize its reward, the model learns to game the system by finding tricks to get around a problem rather than solve it.\\n\\nThe post lists a few anecdotes where this popped up in research. Here is a good one: “A researcher gives a talk about using RL to train a simulated robot hand to pick up a hammer and hammer in a nail. Initially, the reward was defined by how far the nail was pushed into the hole. Instead of picking up the hammer, the robot used its own limbs to punch the nail in. So, they added a reward term to encourage picking up the hammer, and retrained the policy. They got the policy to pick up the hammer…but then it threw the hammer at the nail instead of actually using it.”\\n\\nThe random nature of RL makes it difficult to reproduce results, another major problem for research.\\n\\nIrpan is, however, still optimistic about RL and thinks it can improve in the future. “Deep RL is a bit messy right now, but I still believe in where it could be. That being said, the next time someone asks me whether reinforcement learning can solve their problem, I’m still going to tell them that no, it can’t. But I’ll also tell them to ask me again in a few years. By then, maybe it can.”\\n\\nNew Arm mobile chips – Arm announced the launch of Project Trillium, in effort to put more chips capable of processing machine learning workloads for IoT devices, wearables and mobiles.\\n\\nRene Haas, president, IP Products Group at Arm, said: “The rapid acceleration of artificial intelligence into edge devices is placing increased requirements for innovation to address compute while maintaining a power efficient footprint. To meet this demand, Arm is announcing its new ML platform, Project Trillium.”\\n\\nThere are two products available for an early preview in April and out for general availability mid-2018. The first is the Arm ML processor and can deliver more than 4.6 trillion operations per second, apparently. It has a “programmable layer engine”, some local memory and an external memory to run machine learning algorithms.\\n\\nThe Arm OD processor is geared towards object detection, so it could be useful for identifying people for security cameras.\\n\\nIt analyzes video stills at 60 frames per second. The algorithms have been developed by Arm developers and work for “whole human forms”, including faces, heads and shoulders, and can determine the direction each person is facing. The data streams only add up to a few kilobytes, so more videos can be sent to the cloud.\\n\\nArm said the initial launch focuses on mobile processors, but future products will also target sensors, smart speakers, home entertainment, etc.\\n\\nBoth products will be supported by Arm NN software to be used alongside Arm Compute Library and CMSIS-NN. The code is optimised for neural networks and works with frameworks such as TensorFlow, Caffe, and Android NN on Arm’s CPUs and GPUs as well as its ML processors.\\n\\nChips at Amazon – Amazon was reported to be producing AI chips to power the Echo, its home speaker device fronted by Alexa, the digital assistant, and it supposedly gobbled up a security camera startup secretly last year.\\n\\nWe don’t know much about the chips as there were scant details and Amazon did not comment, according to The Information. The idea is that a specialized accelerator chip will make Echo work more efficiently and faster, as it’ll be able to carry out instructions faster rather than relying on the cloud.\\n\\nAmazon acquired Annapurna labs, a chipmaker based in Israel in 2015 for $350 million. And now, Reuters reports that it also secretly bought Blink, another startup that specializes in chips for use in security cameras for $90 million.\\n\\nWe pressed Amazon for comment on its Echo chips and whether it has any plans to develop chips for its cloud business. But a spokesperson told us: \"We are not commenting on this topic.\" That could put Nvidia in a bit of a pickle, if all the cloud giants start developing their own chips for AI and machine learning in their cloud.\\n\\nGoogle has already announced plans to do just that with its Cloud TPU chips.\\n\\nA robot dog opens a door – Boston Dynamics released another teaser video showing off robot dogs.\\n\\nWhat looks like a headless robot dog approaches a door and waits for another robo dog to emerge from the shadows. His buddy then dispatches a long arm attached to a gripper on his head and opens the door for him.\\n\\nBoston Dynamics always send the internet into a brief frenzy. The careful mechanical control of these robots is impressive (remember the backflipping Atlas?), but it’s unknown how they work and how autonomous they are.\\n\\nThe company is notoriously tight lipped.\\n\\nHere is the video below.\\n\\nYoutube Video\\n\\nAI tracking human movement – Researchers at Facebook have trained convolutional neural network systems to pick out human bodies in videos and then to map different textures on to them.\\n\\nAn example below shows crowded scenes with people walking or skateboarding. After the pixels associated with the bodies have been mapped, various skins and outfits are superimposed onto them.\\n\\nAt first, it might seem a bit silly and pointless. But in a paper published on arXiv, the researchers said, “This work aims at pushing further the envelope of human understanding in images by establishing dense correspondences from a 2D image to a 3D, surface-based representation of the human body.”\\n\\nIt might aid graphics, augmented reality, or human-computer interaction, and even lead to a general understanding of objects in three dimensions, apparently.\\n\\nTo train the network known as DensePose, 50,000 pictures are taken from the COCO dataset, a popular image detection dataset. The relevant parts such as the face, arms, torso and legs are annotated and segmented to train a convolutional neural network to highlight bodies in images and videos it hasn’t seen before. Different textures can now be mapped onto the pixels highlighted by the neural network.\\n\\nHere\\'s a video that shows the social networkers\\' work.\\n\\nYoutube Video\\n\\nColoring in comics – Preferred Networks, a Japanese AI startup interested in IoT, have collaborated with publishing companies to distribute manga comics that have been automatically colored using deep learning.\\n\\nThe model, dubbed PaintsChainer, is trained on pairs of sketches and its colored version to learn what colors should be used where; for example skin colors should be used for faces.\\n\\nHakusensha and Hakuhodo DY digital, both Japanese publishers of internet manga comics have released titles that have been automatically colored by PaintsChainer. There is also another option for those that want to hold onto their artistic freedom, where you can broadly choose the color of the clothes or hair in your drawings, and then PaintsChainer fills in the rest.\\n\\nYou can upload your own drawings here and play with PaintsChainer here. ®\\n\\nSponsored: Minds Mastering Machines - Call for papers now open')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Popular Design News of the Week: March 26, 2018 – April 1, 2018', 'Every week users submit a lot of interesting stuff on our sister site Webdesigner News, highlighting great content from around the web that can be of interest to web designers.\\n\\nThe best way to keep track of all the great stories and news being posted is simply to check out the Webdesigner News site, however, in case you missed some here’s a quick and useful compilation of the most popular designer news that we curated from the past week.\\n\\nNote that this is only a very small selection of the links that were posted, so don’t miss out and subscribe to our newsletter and follow the site daily for all the news.\\n\\nZuckerberg Hits Users with the Hard Truth: You Agreed to this\\n\\nWhy Tabs VS Spaces Debate is Pointless\\n\\nNotion 2.0\\n\\nDesign System: Uniform\\n\\nDesign Trend: Offset + Overlapping Content Blocks\\n\\nWhy do You Need to Use CSS Grid\\n\\n11 Steps to Better Logos\\n\\nCropping Away Negative Impacts of Line Height\\n\\n9 Tips to Optimize the UI of Mobile Apps\\n\\nLunacy Editor 3.0: Free Sketch for Windows is Here\\n\\nSlashPixels: A Google for Designers\\n\\nHere’s What I’ve Learned from Designing 10,000+ UI Screens as a Lead Product Designer.\\n\\nSmartphones are Killing the Planet Faster than Anyone Expected\\n\\nHow to Talk About Design Decisions\\n\\nGoogle Search is Testing a Button that Loads ‘More Results’ on the Same Page\\n\\n10 Awesome White Space Designs to Inspire You\\n\\nChanging the Way We Build the Web, Together\\n\\nPretzel – Keyboard Shortcuts for the App You’re Currently Using\\n\\nThe Lies We Tell Ourselves as Designers\\n\\nPaste with Frames: Bring Flat Ideas to Life\\n\\n27 Best WordPress Newspaper Themes\\n\\n“Brand Debt” and How to Avoid it\\n\\nSite Design: IBM Plex\\n\\nGradient° – Deep Learning Platform Built for Developers\\n\\nPrevent Facebook from Tracking You all Over the Web with this Firefox Extension\\n\\nWant more? No problem! Keep track of top design news from around the web with Webdesigner News.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('What’s New for Designers, February 2018', 'Sometimes you need to practice a little self-care. When it comes to self-care for work and design professionals that often means spending a little time to learn something new and re-energize yourself.\\n\\nThis month’s roundup has some great options for doing just that. Not only are these items new, but they are almost guaranteed to make you better at your job. From learning how to create better web type, to a design conference you can attend in your slippers, to a few font choices that might become part of your go-to collection, we think what’s new for designers this month will be inspiring.\\n\\nIf we’ve missed something that you think should have been on the list, let us know in the comments. And if you know of a new app or resource that should be featured next month, tweet it to @carriecousins to be considered!\\n\\nBetter Web Type\\n\\nIs it time that you learned how to create better type for the web? Better Web Type is a free web typography course for web designers and developers. (It also has a paired book, “Better Web Typography for A better Web” by Matej Latin.) The online course tackles concepts such as vertical rhythm, modular scaling and composition and breaks it down in a way that’s understandable even if you don’t have any knowledge of type theory. Here’s how it works: Just register for the online course and you’ll get a daily lesson in your inbox (it should take 5 to 10 minutes to read), then keep exploring those ideas with links and other resources. The course lasts just seven days and will help you feel better about your web typography skills. You have nothing to lose.\\n\\nDesign Systems Virtual Summit 2018\\n\\nThe new Design Systems Virtual Summit is a live, online conference-style event for UX and product professionals. It’s packed with sessions and advice from some top names in the industry. Plus, it’s free for anyone to join and you can get all the great resources and information from your computer anywhere in the world. The event is Feb. 13-16 and you should reserve a spot online to get access to 13 live webinars and case studies from some great companies. Speakers include designers and UX pros from companies such as IBM, GE Digital, Salesforce, LinkedIn, Airbnb and more. If you have been wanting to attend a conference and don’t have a budget for travel, this is your ticket to get inspired and reenergized about design.\\n\\nYo! By Rob Hope\\n\\nDesigner Rob Hope, the person behind One Page Love, has a new YouTube show and channel, Yo!, for designers. (You can also sign up for an accompanying email of links and resources that he mentions in each episode.) The focus of the short YouTube series is design and development news, including jobs, events, resources and freebies. His roundup isn’t too long and includes plenty of things that you can use in your workflows.\\n\\nJotForm Cards\\n\\nJotForm, an online form creation software with 3.3 million users, debuts JotForm Cards this month, a user-friendly style of online forms that can improve completion rates. JotForm is a small business and website tool used for contact forms, lead generation, event registration, surveying customers and more. The new product means that organizations of all types will get more of the critical data upon which they depend. The company says that with testing they’ve done, JotForm Cards improves response rates by a whopping 34%. What’s different about this tool is that questions are shown one at a time to help users focus on the answer, all cards are optimized for mobile devices, micro-animations add to engagement and a progress bar shows (and encourages) form completion.\\n\\nLearn Flexbox for Free\\n\\nScrimba has a new 12-part free online course that will teach you to use Flexbox. It includes all the tools you need to arrange content in flexible containers and leave you wondering how you ever worked without this tool. The course is made for beginners so that they can gain new skills and start using them quickly.\\n\\nPigment\\n\\nPigment takes the guesswork out of tints, tones and lighting differences for colors on the web. Use the tool to make adjustments on the screen for different colors and then grab the swatches for projects. It’s quick, easy and you can see the adjustments to colors right on the screen.\\n\\nSVG Backgrounds\\n\\nNeed a fun or funky website background? Start with SVG Backgrounds. The tool allows you to create customizable SVG patterns and background designs for use on websites. You can select a premade option to start and tweak to meet your needs, then output the CSS with a Creative Commons license for use in all your projects.\\n\\nTutorial: Make Multi-Colored Icons with SVG and CSS\\n\\nLearn how to create your own icon set with icon font vectors that you can color and scale for all types of website design projects. Developer Sarah Dadyan has a great step-by-step guide at freeCodeCamp with in-depth explanations for why and how to use her technique. She also shares all the code so you can try it yourself.\\n\\nAdele\\n\\nAdele is a collection of publicly available design systems and patterns. The design systems repository, created by UXPin, is named as a “tribute to one of the most important computer scientists focused on graphic user interfaces, design patterns and object-oriented programming – Adele Goldberg.” The massive collection includes links to design systems by name and company, as well as where you can find the code (such as GitHub or another repository), code depth, components and framework information. It’s a massive collection of design information that you can easily get lost in or learn from.\\n\\nPixelSnap\\n\\nPixelSnap is a simple paid web app that allows you to measure anything on the screen. Measure any distance, set hotkeys and adjust settings for maximum measurement ability. The tool is $15.\\n\\nGradient Folders\\n\\nThis fun set of icons includes 38 handcrafted gradient folders so the files on your desktop or laptop are a little less boring. The set includes Mac or Windows downloads with plan folders or folders with icons.\\n\\niOS 11 UI Kit for iPhone X\\n\\nDesign+Code has a great UI kit with all the components, layouts and typography you need to great iOS 11 and iPhone 11 mockups in Sketch. Plus the naming convention of each component follows Apple Developer Documentation so you can get familiar with the nomenclature.\\n\\nMagicons\\n\\nMagicons is a collection of 30 colored travel icons. Icons come in PNG, SVG, EPS and Sketch formats and can be used for almost any project where transportation or travel icons are needed.\\n\\nDetectron\\n\\nFacebook recently made its AI-based software system that implements object detection algorithms open source. It is written in Python and powered by the Caffe2 deep learning framework. According to Facebook, the goal of the project is to “provide a high-quality, high-performance codebase for object detection research. It is designed to be flexible in order to support rapid implementation and evaluation of novel research.” You can find it all on GitHub.\\n\\nStadia Maps\\n\\nStadia Maps helps you create great mapping experiences for better visualizations online. The tool includes raster and vector map tiles, static maps and routing maps. There are free and paid options depending on your website mapping needs.\\n\\nFontRapid\\n\\nFontRapid is a tool that helps you convert letter shapes in Sketch into an OpenType font. The tools allows users to make any path or shape into a font with a click, including the ability to adjust glyphs, add ligatures and create alternates for robust type collections. FontRapid is free to use within Sketch.\\n\\nFixer\\n\\nFixer is a simple geometric typeface with plenty of weights and options. From thick to thin to inline variants it can be used for display purposes.\\n\\nHighlander Marker Script\\n\\nHighlander Market Script is a fun handwriting-style typeface in a minimalist style. The character set includes upper-and lowercase letters, numerals, swashes and punctuation in a set that’s robust for a fee option. It makes a great display typeface for web or print use.\\n\\nInfluence Numerals\\n\\nThe typeface is a number set with high contrast for numerals with impact. The set includes four fat-style number sets – outlined, multiple inline, stripe and solid. The collection is a modern envisioning of 19th century Fat Face numerals.\\n\\nNikola\\n\\nNikola is a modern serif in regular and medium weights that can work for large of small text usage. It’s highly readable and as a paid option includes plenty of alternates, swashes and ornamentation options.\\n\\nTT Wellingtons\\n\\nTT Wellingtons is an English humanist-style sans serif with nice character features. The simple lines have a geometric flair to them. The premium typeface comes in nine weights and italics with ligatures and alternates.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"The brain learns completely differently than we've assumed since the 20th century\", 'The brain is a complex network containing billions of neurons, where each of these neurons communicates simultaneously with thousands of other via their synapses (links). However, the neuron actually collects its many synaptic incoming signals through several extremely long ramified \"arms\" only, called dendritic trees.\\n\\nIn 1949 Donald Hebb\\'s pioneering work suggested that learning occurs in the brain by modifying the strength of the synapses, whereas neurons function as the computational elements in the brain. This has remained the common assumption until today.\\n\\nUsing new theoretical results and experiments on neuronal cultures, a group of scientists, led by Prof. Ido Kanter, of the Department of Physics and the Gonda (Goldschmied) Multidisciplinary Brain Research Center at Bar-Ilan University, has demonstrated that the central assumption for nearly 70 years that learning occurs only in the synapses is mistaken.\\n\\nIn an article published today in the journal Scientific Reports, the researchers go against conventional wisdom to show that learning is actually done by several dendrites, similar to the slow learning mechanism currently attributed to the synapses.\\n\\n\"The newly discovered process of learning in the dendrites occurs at a much faster rate than in the old scenario suggesting that learning occurs solely in the synapses. In this new dendritic learning process, there are a few adaptive parameters per neuron, in comparison to thousands of tiny and sensitive ones in the synaptic learning scenario,\" said Prof. Kanter, whose research team includes Shira Sardi, Roni Vardi, Anton Sheinin, Amir Goldental and Herut Uzan.\\n\\nThe newly suggested learning scenario indicates that learning occurs in a few dendrites that are in much closer proximity to the neuron, as opposed to the previous notion. \"Does it make sense to measure the quality of air we breathe via many tiny, distant satellite sensors at the elevation of a skyscraper, or by using one or several sensors in close proximity to the nose? Similarly, it is more efficient for the neuron to estimate its incoming signals close to its computational unit, the neuron,\" says Kanter. Hebb\\'s theory has been so deeply rooted in the scientific world for 70 years that no one has ever proposed such a different approach. Moreover, synapses and dendrites are connected to the neuron in a series, so the exact localized site of the learning process seemed irrelevant.\\n\\nAnother important finding of the study is that weak synapses, previously assumed to be insignificant even though they comprise the majority of our brain, play an important role in the dynamics of our brain. They induce oscillations of the learning parameters rather than pushing them to unrealistic fixed extremes, as suggested in the current synaptic learning scenario.\\n\\nThe new learning scenario occurs in different sites of the brain and therefore calls for a reevaluation of current treatments for disordered brain functionality. Hence, the popular phrase \"neurons that fire together wire together,\" summarizing Donald Hebb\\'s 70-year-old hypothesis, must now be rephrased. In addition, the learning mechanism is at the basis of recent advanced machine learning and deep learning achievements. The change in the learning paradigm opens new horizons for different types of deep learning algorithms and artificial intelligence based applications imitating our brain functions, but with advanced features and at a much faster speed.')\n",
      "('Machine learning offers new way of designing chiral crystals', 'Engineers and chemists at Hiroshima University successfully used the same technology at the core of facial recognition to design chiral crystals. This is the first study reporting the use of this technology, called logistic regression analysis, to predict which chemical groups are best for making chiral molecules. Results were published in Chemistry Letters.\\n\\nChirality describes the quality of possessing a mirror image to something else, but without the ability to superimpose it. Your left foot, for example, is a mirror of your right. They look similar, but they are not the same. This is why you cannot wear a left shoe on your right foot.\\n\\nThe idea is similar in chemistry. Two molecules can have the same makeup of elements, but their geometry can differ. A left-handed chiral helix can have a corresponding right-handed helix.\\n\\nHowever, making a mirror image of a chiral molecule is more complicated than just rearranging some bonds. An extra layer of complexity arises when making a crystal, a highly ordered series of atoms or molecules that can extend in three dimensions.\\n\\n\"The most difficult part of making a chiral crystal,\" primary author Katsuya Inoue said, \"is knowing how to design them.\" Inoue is a researcher at the Graduate School of Engineering at Hiroshima University (HU).\\n\\nAccording to Inoue, it is difficult to mix different atoms so that their chiral geometry coexists in a single crystal. Alone, two types of atoms will want to form bonds with the same angles. When combined, though, they may not.\\n\\nThe team analyzed 686 chiral crystals and 1000 achiral crystals from the Inorganic Crystal Structure Database. By using logistic regression, Inoue\\'s team designed a model to show the best way to design chiral crystals.\\n\\nThey calculated which chemical groups of the periodic table have elements that are more likely to coexist in a chiral crystal. The groups that correspond to carbon, nitrogen, and oxygen were best -- or group numbers 14, 15, and 16, respectively.\\n\\nLogistic regression is a statistical method that can tell apart two objects. Eri Shimono, co-author and research assistant in the Department of Chemistry and Chirality Research Center at HU, compared it to use in smartphones.\\n\\n\"In face recognition, smartphones use machine learning to classify faces and things that are not faces,\" Shimono said. \"We can train our model to detect chiral and non-chiral possibilities. In this case, though, the input is not an image. It is information.\"\\n\\nGoing forward, the team is fine-tuning the prediction model in two ways. First, they want to account for more atoms in a crystal. \"We started from two atoms. In reality, though, many crystals are made with three or four,\" Inoue said. \"We have to extend this model to fit for these cases.\"\\n\\nSecond, they are applying deep learning. The current model, which uses basic machine learning, is made from existing data. Deep learning would let researchers classify new data as chiral or not. From these results, the team plans to start creating some predicted crystals and explore how to build a chiral magnet from them.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Deep learning for biology', 'The brain’s neural network has long inspired artificial-intelligence researchers.Credit: Alfred Pasieka/SPL/Getty\\n\\nFour years ago, scientists from Google showed up on neuroscientist Steve Finkbeiner’s doorstep. The researchers were based at Google Accelerated Science, a research division in Mountain View, California, that aims to use Google technologies to speed scientific discovery. They were interested in applying ‘deep-learning’ approaches to the mountains of imaging data generated by Finkbeiner’s team at the Gladstone Institute of Neurological Disease in San Francisco, also in California.\\n\\nDeep-learning algorithms take raw features from an extremely large, annotated data set, such as a collection of images or genomes, and use them to create a predictive tool based on patterns buried inside. Once trained, the algorithms can apply that training to analyse other data, sometimes from wildly different sources.\\n\\nThe technique can be used to “tackle really hard, tough, complicated problems, and be able to see structure in data — amounts of data that are just too big and too complex for the human brain to comprehend”, Finkbeiner says.\\n\\nHe and his team produce reams of data using a high-throughput imaging strategy known as robotic microscopy, which they had developed for studying brain cells. But the team couldn’t analyse its data at the speed it acquired them, so Finkbeiner welcomed the opportunity to collaborate.\\n\\n“I can’t honestly say at the time that I had a clear grasp of what questions might be addressed with deep learning, but I knew that we were generating data at about twice to three times the rate we could analyse it,” he says.\\n\\nToday, those efforts are beginning to pay off. Finkbeiner’s team, with scientists at Google, trained a deep algorithm with two sets of cells, one artificially labelled to highlight features that scientists can’t normally see, the other unlabelled. When they later exposed the algorithm to images of unlabelled cells that it had never seen before, Finkbeiner says, “it was astonishingly good at predicting what the labels should be for those images”. A publication detailing that work is now in the press.\\n\\nFinkbeiner’s success highlights how deep learning, one of the most promising branches of artificial intelligence (AI), is making inroads in biology. The algorithms are already infiltrating modern life in smartphones, smart speakers and self-driving cars. In biology, deep-learning algorithms dive into data in ways that humans can’t, detecting features that might otherwise be impossible to catch. Researchers are using the algorithms to classify cellular images, make genomic connections, advance drug discovery and even find links across different data types, from genomics and imaging to electronic medical records.\\n\\nMore than 440 articles on the bioRxiv preprint server discuss deep learning; PubMed lists more than 700 references in 2017. And the tools are on the cusp of becoming widely available to biologists and clinical researchers. But researchers face challenges in understanding just what these algorithms are doing, and ensuring that they don’t lead users astray.\\n\\nTraining smart algorithms\\n\\nDeep-learning algorithms (see ‘Deep thoughts’) rely on neural networks, a computational model first proposed in the 1940s, in which layers of neuron-like nodes mimic how human brains analyse information. Until about five years ago, machine-learning algorithms based on neural networks relied on researchers to process the raw information into a more meaningful form before feeding it into the computational models, says Casey Greene, a computational biologist at the University of Pennsylvania in Philadelphia. But the explosion in the size of data sets — from sources such as smartphone snapshots or large-scale genomic sequencing — and algorithmic innovations have now made it possible for humans to take a step back. This advance in machine learning — the ‘deep’ part — forces the computers, not their human programmers, to find the meaningful relationships embedded in pixels and bases. And as the layers in the neural network filter and sort information, they also communicate with each other, allowing each layer to refine the output from the previous one.\\n\\nSource: Jeremy Linsley/Drew Linsley/Steve Finkbeiner/Thomas Serre\\n\\nEventually, this process allows a trained algorithm to analyse a new image and correctly identify it as, for example, Charles Darwin or a diseased cell. But as researchers distance themselves from the algorithms, they can no longer control the classification process or even explain precisely what the software is doing. Although these deep-learning networks can be stunningly accurate at making predictions, Finkbeiner says, “it’s still challenging sometimes to figure out what it is the network sees that enables it to make such a good prediction”.\\n\\nStill, many subdisciplines of biology, including imaging, are reaping the rewards of those predictions. A decade ago, software for automated biological-image analysis focused on measuring single parameters in a set of images. For example, in 2005, Anne Carpenter, a computational biologist at the Broad Institute of MIT and Harvard in Cambridge, Massachusetts, released an open-source software package called CellProfiler to help biologists to quantitatively measure individual features: the number of fluorescent cells in a microscopy field, for example, or the length of a zebrafish.\\n\\nBut deep learning is allowing her team to go further. “We’ve been shifting towards measuring things that biologists don’t realize they want to measure out of images,” she says. Recording and combining visual features such as DNA staining, organelle texture and the quality of empty spaces in a cell can produce thousands of ‘features’, any one of which can reveal fresh insights. The current version of CellProfiler includes some deep-learning elements, and her team expects to add more-sophisticated deep-learning tools in the next year.\\n\\n“Most people have a hard time wrapping their heads around this,” Carpenter says, “but there’s just as much information, in fact maybe more, in a single image of cells as there is in a transcriptomic analysis of a cell population.”\\n\\nThat type of processing allows Carpenter’s team to take a less supervised approach to translating cell images into disease-associated phenotypes — and to capitalize on it. Carpenter is a scientific adviser to Recursion Pharmaceuticals in Salt Lake City, Utah, which is using its deep-learning tools to target rare, single-gene disorders for drug development.\\n\\nMining genomic data\\n\\nWhen it comes to deep learning, not just any data will do. The method often requires massive, well-annotated data sets. Imaging data provide a natural fit, but so, too, do genomic data.\\n\\nOne biotech firm that is using such data is Verily Life Sciences (formerly Google Life Sciences) in San Francisco. Researchers at Verily — a subsidiary of Google’s parent company, Alphabet — and Google have developed a deep-learning tool that identifies a common type of genetic variation, called single-nucleotide polymorphisms, more accurately than conventional tools. Called DeepVariant, the software translates genomic information into image-like representations, which are then analysed as images (see ‘Tools for deep diving’). Mark DePristo, who heads deep-learning-based genomic research at Google, expects DeepVariant to be particularly useful for researchers studying organisms outside the mainstream — those with low-quality reference genomes and high error rates in identifying genetic variants. Working with DeepVariant in plants, his colleague Ryan Poplin has achieved error rates closer to 2% than the more-typical 20% of other approaches.\\n\\nTools for deep diving Deep-learning tools are evolving rapidly, and labs will need dedicated computational expertise, collaborations or both to take advantage of them. First, take a colleague with deep-learning expertise out to lunch and ask whether the strategy might be useful, advises Steve Finkbeiner, a neuroscientist at the Gladstone Institutes in San Francisco, California. With some data sets, such as imaging data, an off-the-shelf program might work; for more complicated projects, consider a collaborator, he says. Workshops and meetings can provide training opportunities. Access to cloud-computing resources means that researchers might not need an on-site computer cluster to use deep learning — they can run the computation elsewhere. Google’s TensorFlow, an open-source platform for building deep-learning algorithms, is available on the software-sharing site GitHub, as is an open-source version of DeepVariant, a tool for accurately identifying genetic variation. Google Accelerated Science, a Google research division based in Mountain View, California, collaborates with a range of scientists, including biologists, says Michelle Dimon, one of its research scientists. Projects require a compelling biological question, large amounts of high-quality, labelled data, and a challenge that will allow the company’s machine-learning experts to make unique computational contributions to the field, Dimon says. Those wishing to get up to speed on deep learning should check out the ‘deep review’, a comprehensive, crowdsourced review led by computational biologist Casey Greene of the University of Pennsylvania in Philadelphia (T. Ching et al. Preprint at bioRxiv http://doi.org/gbpvh5; 2018). Sarah Webb\\n\\nBrendan Frey, chief executive of the Canadian company Deep Genomics in Toronto, also focuses on genomic data, but with the goal of predicting and treating disease. Frey’s academic team at the University of Toronto developed algorithms trained on genomic and transcriptomic data from healthy cells. Those algorithms built predictive models of RNA-processing events such as splicing, transcription and polyadenylation within those data. When applied to clinical data, the algorithms were able to identify mutations and flag them as pathogenic, Frey says, even though they’d never seen clinical data. At Deep Genomics, Frey’s team is using the same tools to identify and target the disease mechanisms that the software uncovered, to develop therapies derived from short nucleic-acid sequences.\\n\\nAnother discipline with massive data sets that are amenable to deep learning is drug discovery. Here, deep-learning algorithms are helping to solve categorization challenges, sifting through such molecular features as shape and hydrogen bonding to identify criteria on which to rank those potential drugs. For instance, Atomwise, a biotech company based in San Francisco, has developed algorithms that convert molecules into grids of 3D pixels, called voxels. This representation allows the company to account for the 3D structure of proteins and small molecules with atomic precision, modelling features such as the geometries of carbon atoms. Those features are then translated into mathematical vectors that the algorithm can use to predict which small molecules are likely to interact with a given protein, says Abraham Heifets, the company’s chief executive. “A lot of the work we do is for [protein] targets with no known binders,” he says.\\n\\nAtomwise is using this strategy to power its new AI-driven molecular-screening programme, which scans a library of 10 million compounds to provide academic researchers with up to 72 potential small-molecule binders for their protein of interest.\\n\\nDeep-learning tools could also help researchers to stratify disease types, understand disease subpopulations, find new treatments and match them with the appropriate patients for clinical testing and treatment. Finkbeiner, for instance, is part of a consortium called Answer ALS, an effort to combine a range of data — genomics, transcriptomics, epigenomics, proteomics, imaging and even pluripotent stem-cell biology — from 1,000 people with the neurodegenerative disease amyotrophic lateral sclerosis (also called motor neuron disease). “For the first time, we’ll have a data set where we can apply deep learning and look at whether deep learning can uncover a relationship between the things we can measure in a dish around a cell, and what’s happening to that patient,” he says.\\n\\nChallenges and cautions\\n\\nFor all its promise, deep learning poses significant challenges, researchers warn. As with any computational-biology technique, the results that arise from algorithms are only as good as the data that go in. Overfitting a model to its training data is also a concern. In addition, for deep learning, the criteria for data quantity and quality are often more rigorous than some experimental biologists might expect.\\n\\nDeep-learning algorithms have required extremely large data sets that are well annotated so that the algorithms can learn to distinguish features and categorize patterns. Larger, clearly labelled data sets — with millions of data points representing different experimental and physiological conditions — give researchers the most flexibility for training an algorithm. Finkbeiner notes that algorithm training in his work improves significantly after about 15,000 examples. Those high-quality ‘ground truth’ data can be exceptionally hard to come by, says Carpenter.\\n\\nTo circumvent this challenge, researchers have been working on ways to train more with less data. Advances in the underlying algorithms are allowing the neural networks to use data much more efficiently, Carpenter says, enabling training on just a handful of images for some applications. Scientists can also exploit transfer learning, the ability of neural networks to apply classification prowess acquired from one data type to another type. For example, Finkbeiner’s team has developed an algorithm that it initially taught to predict cell death on the basis of morphology changes. Although the researchers trained it to study images of rodent cells, it achieved 90% accuracy the first time it was exposed to images of human cells, improving to 99% as it gained experience.\\n\\nFor some of its biological image-recognition work, Google Accelerated Science uses algorithms that were initially trained on hundreds of millions of consumer images mined from the Internet. Researchers then refine that training, using as few as several hundred biological images similar to the ones they wish to study.\\n\\nAnother challenge with deep learning is that the computers are both unintelligent and lazy, notes Michelle Dimon, a research scientist at Google Accelerated Science. They lack the judgement to distinguish biologically relevant differences from normal variation. “The computer is shockingly good at finding batch variation,” she notes. As a result, obtaining data that will be fed into a deep-learning algorithm often means applying a high bar for experimental design and controls. Google Accelerated Science requires researchers to place controls randomly on cell-culture plates to account for subtle environmental factors such as incubator temperature, and to use twice as many controls as a biologist might otherwise run. “We make it hard to pipette,” Dimon quips.\\n\\nThis hazard underscores the importance of biologists and computer scientists working together to design experiments that incorporate deep learning, Dimon says. And that careful design has become even more important with one of Google’s latest projects: Contour, a strategy for clustering cellular-imaging data in ways that highlight trends (such as dose responses) instead of putting them into specific categories (such as alive or dead).\\n\\nAlthough deep-learning algorithms can evaluate data without human preconceptions and filters, Greene cautions, that doesn’t mean they are unbiased. Training data can be skewed — as happens, for example, when genomic data only from northern Europeans are used. Deep-learning algorithms trained on such data will acquire embedded biases and reflect them in their predictions, which could in turn lead to unequal patient care. If humans help to validate these predictions, that provides a potential check on the problem. But such concerns are troubling if a computer alone is left to make key decisions. “Thinking of these methods as a way to augment humans is better than thinking of these methods as replacing humans,” Greene says.\\n\\nAnd then there’s the challenge of understanding exactly how these algorithms are building the characteristics, or features, that they use to classify data in the first place. Computer scientists are attacking this question by changing or shuffling individual features in a model and then examining how those tweaks change the accuracy of predictions, says Polina Mamoshina, a research scientist at Insilico Medicine in Baltimore, Maryland, which uses deep learning to improve drug discovery. But different neural networks working on the same problem won’t approach it in the same way, Greene cautions. Researchers are increasingly focusing on algorithms that make both accurate and explainable predictions, he says, but for now the systems remain black boxes.\\n\\n“I don’t think highly explainable deep-learning models are going to come on the scene in 2018, though I’d love to be wrong,” Greene says.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('An efficient deep-learning tool for detecting eye disease', 'A new artificial-intelligence tool deploys a highly efficient form of deep learning to diagnose eye disease from medical images.\\n\\nConvolutional neural networks are deep-learning algorithms adept at processing images, but researchers typically need to train them on more than a million medical images before they can test how well the algorithms work. Kang Zhang at the University of California, San Diego, in La Jolla and his colleagues created a kind of convolutional neural network capable of learning with many fewer images.\\n\\nThe team trained the model on 108,000 images of retinas. All had been classified by experts as either healthy or showing signs of a leading cause of blindness: macular degeneration or diabetic macular edema, a build-up of fluid in the retina. The algorithm identified critical cases of these conditions as accurately as six experts in ophthalmology.\\n\\nThe model also identified pediatric pneumonia from chest X-rays, suggesting that the technique could be broadly applied across medicine.')\n",
      "('Corrections', 'The Toolbox article ‘The future of scientific figures’ (Nature 554, 133–135; 2018) implied that Benjamin Delory developed the persistence barcode method. In fact, he and his colleagues developed an analysis pipeline that relied on and adapted an existing method.\\n\\nThe Technology Feature ‘Deep learning for biology’ (Nature 554, 555–557; 2018) erroneously affiliated Mark DePristo at Verily Life Sciences. He is, in fact, at Google. Also, the DeepVariant tool was developed jointly by Verily and Google.')\n",
      "('AI diagnostics need attention', 'AI diagnostic tools can find problems including retinal disease but they need to be developed with care.Credit: Lester V. Bergman/Getty\\n\\nOne of the biggest — and most lucrative — applications of artificial intelligence (AI) is in health care. And the capacity of AI to diagnose or predict disease risk is developing rapidly. In recent weeks, researchers have unveiled AI models that scan retinal images to predict eye- and cardiovascular-disease risk, and that analyse mammograms to detect breast cancer. Some AI tools have already found their way into clinical practice.\\n\\nAI diagnostics have the potential to improve the delivery and effectiveness of health care. Many are a triumph for science, representing years of improvements in computing power and the neural networks that underlie deep learning. In this form of AI, computers process hundreds of thousands of labelled disease images, until they can classify the images unaided. In reports, researchers conclude that an algorithm is successful if it can identify a particular condition from such images as effectively as can pathologists and radiologists.\\n\\nBut that alone does not mean the AI diagnostic is ready for the clinic. Many reports are best viewed as analogous to studies showing that a drug kills a pathogen in a Petri dish. Such studies are exciting, but scientific process demands that the methods and materials be described in detail, and that the study is replicated and the drug tested in a progression of studies culminating in large clinical trials. This does not seem to be happening enough in AI diagnostics. Many in the field complain that too many developers are not taking the studies far enough. They are not applying the evidence-based approaches that are established in mature fields, such as drug development.\\n\\nMany reports of new AI diagnostic tools, for example, go no further than preprints or claims on websites. They haven’t undergone peer review, and might never do so. That would verify key details: the underlying algorithm code, and analyses of, for example, the images on which the model is trained, the physicians with which it is compared, the features the neural network used to make decisions, and caveats.\\n\\nAI researchers embrace Bitcoin technology to share medical data\\n\\nThese details matter. For instance, one investigation published last year found that an AI model detected breast cancer in whole slide images better than did 11 pathologists who were allowed assessment times of about one minute per image. However, a pathologist given unlimited time performed as well as AI, and found difficult-to-detect cases more often than the computers (B. E. Bejnordi et al. J. Am. Med. Assoc. 318, 2199–2210; 2017).\\n\\nSome issues might not appear until the tool is applied. For example, a diagnostic algorithm might incorrectly associate images produced using a particular device with a disease — but only because, during the training process, the clinic using that device saw more people with the disease than did another clinic using a different device.\\n\\nThese problems can be overcome. One way is for doctors who deploy AI diagnostic tools in the clinic to track results and report them, so that retrospective studies expose any deficiencies. Better yet, such tools should be developed rigorously — trained on extensive data and validated in controlled studies that undergo peer review. This is slow and difficult, in part because privacy concerns can make it hard for researchers to access the massive amounts of medical data needed. A News story in Nature discusses one possible answer: researchers are building blockchain-based systems to encourage patients to securely share information. At present, human oversight will probably prevent weaknesses in AI diagnosis from being a matter of life or death. That is why regulatory bodies, such as the US Food and Drug Administration, allow doctors to pilot technologies classified as low risk.\\n\\nBut lack of rigour does carry immediate risks: the hype–fail cycle could discourage others from investing in similar techniques that might be better. Sometimes, in a competitive field such as AI, a well-publicized set of results can be enough to stop rivals from entering the same field.\\n\\nSlow and careful research is a better approach. Backed by reliable data and robust methods, it may take longer, and will not churn out as many crowd-pleasing announcements. But it could prevent deaths and change lives.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('People must retain control of autonomous vehicles', 'Driverless vehicles are being tested on public roads in a number of countries.Credit: Prostock/Getty\\n\\nLast month, for the first time, a pedestrian was killed in an accident involving a self-driving car. A sports-utility vehicle controlled by an autonomous algorithm hit a woman who was crossing the road in Tempe, Arizona. The safety driver inside the vehicle was unable to prevent the crash.\\n\\nAlthough such accidents are rare, their incidence could rise as more vehicles that are capable of driving without human intervention are tested on public roads. In the past year, several countries have passed laws to pave the way for such trials. For example, Singapore modified its Road Traffic Act to permit autonomous cars to drive in designated areas. The Swedish Transport Agency allowed driverless buses to run in northern Stockholm. In the United States, the House of Representatives passed the SELF DRIVE Act to harmonize laws across various states. Similar action is pending in the US Senate, where a vote to support the AV START Act would further liberalize trials of driverless vehicles.\\n\\nPolicymakers are enthusiastic about the potential of autonomous vehicles to reduce road congestion, air pollution and road-traffic accidents1,2. Cheap ride-hailing services could reduce the number of privately owned cars. Machine intelligence can make driving more fuel-efficient, cutting emissions. Autonomous cars could help to save the 1.25 million lives worldwide that are lost each year through crashes3, many of which are caused by human error.\\n\\nGovernments want to pass laws to make this happen (see ‘Road to autonomy’). But they are doing so by temporarily freeing developers of self-driving cars from meeting certain transport safety rules. These rules include the requirement that a human operator be inside the vehicle, that vehicles have safety features such as a steering wheel, brakes and a mirror, and that the features are functional at all times. Some developers are maintaining these aspects, but they are not obliged to do so. There is no guarantee that autonomous vehicles will match the safety standards of current cars.\\n\\nSource: Autonomous Vehicles Readiness Index (KPMG International, 2018)\\n\\nMeanwhile, the wider policy implications are not being addressed1,2. Governments stand to lose billions of dollars in tax revenue as rates of car ownership drop among individuals. Millions of taxi, lorry and bus drivers will lose their jobs2. The machine-learning algorithms on which autonomous vehicles rely are far from developed enough to make choices that could mean life or death for pedestrians or drivers.\\n\\nPolicymakers need to work more closely with academics and manufacturers to design appropriate regulations. This is extremely challenging because the research cuts across many disciplines.\\n\\nHere, we highlight two areas — liability and safety — that require urgent attention.\\n\\nLiability\\n\\nLike other producers, developers of autonomous vehicles are legally liable for damages that stem from the defective design, manufacture and marketing of their products. The potential liability risk is great for driverless cars because complex systems interact in ways that are unexpected.\\n\\nManufacturers want to minimize the number of liability claims made against them4. One way is to reduce the chance of their product being misused by educating consumers about how it works and alerting them to safety concerns. For example, drug developers provide information on dosages and side effects; electronics manufacturers issue instructions and warnings. Such guidance shapes the expectations of consumers and fosters satisfaction. Yet, much like smartphones, self-driving cars are underpinned by sophisticated technologies that are hard to explain or understand.\\n\\nA safety driver sits behind the wheel during a test of a self-driving taxi in Yokohama, Japan.Credit: Kiyoshi Ota/Bloomberg/Getty\\n\\nInstead, developers are designing such products to be easy to use5. People are more likely to buy a product that seems straightforward and with which they can soon do complicated things, increasing its utility. However, users are then less able to anticipate how the underlying systems work, or to recognize problems and fix them. For example, few drivers of computerized cars know how the engine is calibrated5. Similarly, a passenger in an autonomous vehicle will not know why it chooses to make a sharp turn into oncoming traffic or why it does not overtake a slow-moving vehicle.\\n\\nWorse, deep-learning algorithms are inherently unpredictable. They are built on an opaque decision-making process that is shaped by previous experiences. Each car will be trained differently. No one — not even an algorithm’s designer — can know precisely how an autonomous car will behave under every circumstance.\\n\\nNo law specifies how much training is needed before a deep-learning car can be deemed safe, nor what that training should be. Cars from different manufacturers could react in contrasting ways in an emergency. One might swerve around an obstacle; another might slam on the brakes. Rare traffic events, such as a truck tipping over in the wind, are of particular concern and, at best, make it difficult to train driverless cars.\\n\\nAdvanced interfaces are needed that inform users why an autonomous vehicle is behaving as it does. Today’s dashboards convey information about a car’s speed and the amount of fuel that remains. Tomorrow’s displays must show the vehicle’s ‘intentions’ and the logic that governs them; for example, they might tell passengers that the car will not overtake the vehicle ahead because there is only a 10% likelihood of success. Little is known about the types of data that should be imparted and how people will interpret them.\\n\\nUsers often ignore information, even if it is presented clearly and the consequences could be a matter of life or death. For instance, almost 70% of airline passengers do not review safety cards before a flight6, despite being asked. Yet these cards convey important information, including how to put on an oxygen mask and open an emergency exit, in simple terms and on a single page.\\n\\nAutonomous vehicles will need to communicate much more complicated information. Their sensors and algorithms must understand the behaviours of pedestrians, discriminate between styles of driving and adjust to changes in lighting. When they cannot, users must know how to respond.\\n\\nResearching ways to present this information effectively is paramount, as are legislative efforts to ensure that users of autonomous vehicles are proficient in using the technology.\\n\\nSafety\\n\\nThe safety and efficiency benefits of autonomous cars rely on computers making better, quicker decisions than people. Users input their desired destination and thereafter cede control to the computer. Full autonomy has — deliberately — not yet been adopted in transportation. People are still perceived as being more flexible, adaptable and creative than machines, and better able to respond to changing or unforeseen conditions7. Pilots are able, therefore, to wrest control from fly-by-wire technology when key computers fail.\\n\\nThe public is right to remain cautious about full automation. Manufacturers need to explain how a car would protect passengers should crucial systems fail. A driverless car must be able to stop safely if its hazard-avoidance algorithms malfunction, its cameras break or its internal maps die. But this is hard to engineer: for example, without cameras, such a car cannot see where it is going.\\n\\nA driverless bus shuttles passengers across Southeast University’s Jiulonghu campus in Nanjing, China.Credit: CVG/Getty\\n\\nIn our view, some form of human intervention will always be required. Driverless cars should be treated much like aircraft, in which the involvement of people is required despite such systems being highly automated. Current testing of autonomous vehicles abides by this principle. Safety drivers are present, even though developers and regulators talk of full automation.\\n\\nNonetheless, having people involved poses safety problems. Autonomous cars will always require users to have a minimum level of skill and will never be easy for some members of the public to operate. People with cognitive impairments, say, might find it difficult to operate these technologies and to override controls. Yet this group includes those who would benefit greatly from self-driving vehicles. For example, older adults8, a demographic of increasing importance, have an elevated risk of crashes because cognitive abilities decline with age9,10. Providing mobility for large numbers of elderly people is an impetus for investment in this technology in Japan, for instance.\\n\\nA remote supervisor could oversee driverless cars as air-traffic controllers do for aircraft. But how many supervisors would be needed to keep networks of such vehicles safe? Stretching human capacity too far can create accidents11. For example, in 1991, an overwhelmed air-traffic controller in Los Angeles, California, mistakenly cleared an aeroplane to land on another. Last year, an overload of patients was blamed for a string of medical errors by doctors in Hong Kong.\\n\\nPolicy gaps\\n\\nCurrent and planned legislation fails to address these issues. Exempting developers from safety rules poses risks. And developers are not always required to report system failures or to establish competency standards for vehicle operators. Such exemptions also presume, wrongly, that human involvement will ultimately be unnecessary. Favouring industry over users will erode support for the technology from an already sceptical public.\\n\\nPresent legislation sidesteps the education of consumers. The US acts merely require that users are “informed” about the technology before its use. Standards of competency and regular proficiency testing for users are not mentioned. Without standards, it is hard to tell whether consumer education programmes are adequate. And without testing, the risk of incidents might increase.\\n\\nMoving forward\\n\\nWe call on policymakers to rethink their approach to regulating autonomous vehicles and to consider the following six points when drafting legislation.\\n\\nDriverless does not, and should not, mean without a human operator. Regulators and manufacturers must acknowledge, rather, that automation changes the nature of the work that people perform7.\\n\\nUsers need information on how autonomous systems are working. Manufacturers must research the limits and reliability of devices that are crucial for safety, including cameras, lasers and radars. When possible, they should make the data from these devices available to vehicle operators in an understandable form.\\n\\nOperators must demonstrate competence. Developers, researchers and regulators need to agree proficiency standards for users of autonomous vehicles. Competency should be tested by licensing authorities and should supplement existing driving permits. Users who fall short should have their access to such vehicles limited, just as colour-blind pilots are banned from flying at night.\\n\\nRegular checks on user competency should be mandatory. Regulators, manufacturers and researchers must determine a suitable time interval between tests, so that proficiency is kept up as cognitive abilities change and technology evolves.\\n\\nRemote monitoring networks should be established. Manufacturers, researchers and legislators need to build supervisory systems for autonomous vehicles. Researchers should supply guidance on the number of vehicles that one supervisor can monitor safely, and on the conditions under which such monitoring is permissible. For example, more supervisors would be needed in poor weather conditions.\\n\\nWork limits for remote supervisors should be defined. Experts must clarify whether supervisors should be subject to existing working-time regulations. For example, air-traffic controllers are limited in how long they can work.\\n\\nThe path towards autonomy is far from preordained. Considerable challenges remain to be addressed.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('A Free Oxford Course on Deep Learning: Cutting Edge Lessons in Artificial Intelligence', '\\ufeff \\ufeff\\n\\nNando de Freitas is a \"machine learning professor at Oxford University, a lead research scientist at Google DeepMind, and a Fellow of the Canadian Institute For Advanced Research (CIFAR) in the Neural Computation and Adaptive Perception program.\"\\n\\nAbove, you can watch him teach an Oxford course on Deep Learning, a hot subfield of machine learning and artificial intelligence which creates neural networks--essentially complex algorithms modeled loosely after the human brain--that can recognize patterns and learn to perform tasks.\\n\\nTo complement the 16 lectures you can also find lecture slides, practicals, and problems sets on this Oxford web site. If you\\'d like to learn about Deep Learning in a MOOC format, be sure to check out the new series of courses created by Andrew Ng on Coursera.\\n\\nOxford\\'s Deep Learning course will be added to our list of Free Online Computer Science Courses, part of our meta collection, 1,300 Free Online Courses from Top Universities.\\n\\nRelated Content:\\n\\nGoogle Launches Free Course on Deep Learning: The Science of Teaching Computers How to Teach Themselves\\n\\nNew Deep Learning Courses Released on Coursera, with Hope of Teaching Millions the Basics of Artificial Intelligence\\n\\nNeural Networks for Machine Learning: A Free Online Course')\n",
      "('Google Launches a Free Course on Artificial Intelligence: Sign Up for Its New “Machine Learning Crash Course”', 'As part of an effort to make Artificial Intelligence more comprehensible to the broader public, Google has created an educational website Learn with Google AI, which includes, among other things, a new online course called Machine Learning Crash Course. The course provides \"exercises, interactive visualizations, and instructional videos that anyone can use to learn and practice [Machine Learning] concepts.\" To date, more than 18,000 Googlers have enrolled in the course. And now it\\'s available for everyone, everywhere. You can supplement it with other AI courses found in the Relateds below.\\n\\nMachine Learning Crash Course will be added to our list of Free Online Computer Science Courses, a subset of our collection, 1,300 Free Online Courses from Top Universities.\\n\\nFollow Open Culture on Facebook and Twitter and share intelligent media with your friends. Or better yet, sign up for our daily email and get a daily dose of Open Culture in your inbox.\\n\\nIf you\\'d like to support Open Culture and our mission, please consider making a donation to our site. It\\'s hard to rely 100% on ads, and your contributions will help us provide the best free cultural and educational materials.\\n\\nvia Google Blog\\n\\nRelated Content:\\n\\nArtificial Intelligence: A Free Online Course from MIT\\n\\nGoogle Launches Free Course on Deep Learning: The Science of Teaching Computers How to Teach Themselves\\n\\nNew Deep Learning Courses Released on Coursera, with Hope of Teaching Millions the Basics of Artificial Intelligence\\n\\nNeural Networks for Machine Learning: A Free Online Course')\n",
      "('Coursera Now Offering Complete Bachelor’s and Master’s Programs–With Tuition Reduced by 70%', 'As we observed last October, Coursera has been undergoing an evolution of sorts. When the ed tech company started out, it offered an array of individual courses to students worldwide. A little of this. And a little of that. Now, they\\'re increasingly moving towards courses that work together in sequences. First came \"course specializations\"--collections of courses that allow students to gain a mastery of specialized topics like Deep Learning, Data Science (Johns Hopkins), Business Fundamentals (Wharton), Digital Marketing (University of Illinois), and Big Data (UC San Diego). Next it was just a logical jump to offering full-blown Bachelor\\'s and Master\\'s programs at a discounted price (roughly 1/3 the usual cost.) As of this month, Coursera offers one Bachelor\\'s program (Computer Science from the University of London), one MBA, and eight Master\\'s programs. The full list of degree programs appears below:\\n\\nFollow Open Culture on Facebook and Twitter and share intelligent media with your friends. Or better yet, sign up for our daily email and get a daily dose of Open Culture in your inbox.\\n\\nIf you\\'d like to support Open Culture and our mission, please consider making a donation to our site. It\\'s hard to rely 100% on ads, and your contributions will help us provide the best free cultural and educational materials.\\n\\nNote: Open Culture has a partnership with Coursera. If readers enroll in certain Coursera courses, it helps support Open Culture.\\n\\nRelated Content:\\n\\nCoursera Partners with Leading Universities to Offer Master’s Degrees at a More Affordable Price\\n\\nNew Deep Learning Courses Released on Coursera, with Hope of Teaching Millions the Basics of Artificial Intelligence\\n\\nMOOCs from Great Universities (Many With Certificates)')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Artificial Intelligence Writes a Piece in the Style of Bach: Can You Tell the Difference Between JS Bach and AI Bach?', 'This week, the artificial intelligence community Botnik published a 2018 Coachella Lineup poster composed entirely of performer names generated by neural networks. It does get one wondering what the music of \"Lil Hack,\" \"House of the Gavins,\" or \"Paper Cop\" might sound like — or, given the direction of technology these days, how long it will take before another neural network can actually compose it. But why use AI to create yet another millennial-minded Coachella act, you might ask, when it could create another Johann Sebastian Bach?\\n\\n\"One form of music that Bach excelled in was a type of polyphonic hymn known as a chorale cantata,\" says the MIT Technology Review. \"The composer starts with a well-known tune which is sung by the soprano and then composes three harmonies sung by the alto, tenor, and bass voices.\" Such compositions \"have attracted computer scientists because the process of producing them is step-like and algorithmic. But doing this well is also hard because of the delicate interplay between harmony and melody.\" Hence the fascination of the question of whether a computer could ever compose a truly Bach-like chorale.\\n\\nThe video at the top of the post offers a listening experience that points toward an answer. The minute-long piece you hear, and whose score you see, comes not from Bach himself, nor from any human Bach imitator, but from a neural network called DeepBach, a system developed by Gaetan Hadjeres and Francois Pachet at the Sony Computer Science Laboratories in Paris.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLike any such deep learning system, the more existing material it has to \"learn\" from, the more convincing a product it can produce on its own: just as Botnik\\'s network could learn from all the band names featured on Coachella posters since 1999, DeepBach could learn from the more than 300 short chorale compositions the real Bach wrote in his lifetime.\\n\\n\"About half the time,\" says the MIT Technology Review, \"these compositions fool human experts into thinking they were actually written by Bach.\" But of course, this sort of artificial intelligence has a greater and more diverse potential than tricking its listeners, as other experiments at Sony CSL-Paris suggest: the AI-composed \"Beatles\" song \"Daddy\\'s Car,\" for instance, or the \"Flow Machine\" that re-interprets Beethoven\\'s \"Ode to Joy\" in the style of the Beatles, Take 6, and even electronic lounge music. But we won\\'t know the technology has matured until the day we find ourselves booking tickets for artificial intelligence-composed music festivals.\\n\\nvia MIT Technology Review\\n\\nRelated Content:\\n\\nArtificial Intelligence Program Tries to Write a Beatles Song: Listen to “Daddy’s Car”\\n\\nArtificial Intelligence Creativity Machine Learns to Play Beethoven in the Style of The Beatles’ “Penny Lane”\\n\\nHear What Music Sounds Like When It’s Created by Synthesizers Made with Artificial Intelligence\\n\\nGoogle Launches Free Course on Deep Learning: The Science of Teaching Computers How to Teach Themselves\\n\\nNeural Networks for Machine Learning: A Free Online Course\\n\\nBased in Seoul, Colin Marshall writes and broadcasts on cities and culture. His projects include the book The Stateless City: a Walk through 21st-Century Los Angeles and the video series The City in Cinema. Follow him on Twitter at @colinmarshall or on Facebook.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Deep learning for biology', 'The brain’s neural network has long inspired artificial-intelligence researchers.Credit: Alfred Pasieka/SPL/Getty\\n\\nFour years ago, scientists from Google showed up on neuroscientist Steve Finkbeiner’s doorstep. The researchers were based at Google Accelerated Science, a research division in Mountain View, California, that aims to use Google technologies to speed scientific discovery. They were interested in applying ‘deep-learning’ approaches to the mountains of imaging data generated by Finkbeiner’s team at the Gladstone Institute of Neurological Disease in San Francisco, also in California.\\n\\nDeep-learning algorithms take raw features from an extremely large, annotated data set, such as a collection of images or genomes, and use them to create a predictive tool based on patterns buried inside. Once trained, the algorithms can apply that training to analyse other data, sometimes from wildly different sources.\\n\\nThe technique can be used to “tackle really hard, tough, complicated problems, and be able to see structure in data — amounts of data that are just too big and too complex for the human brain to comprehend”, Finkbeiner says.\\n\\nHe and his team produce reams of data using a high-throughput imaging strategy known as robotic microscopy, which they had developed for studying brain cells. But the team couldn’t analyse its data at the speed it acquired them, so Finkbeiner welcomed the opportunity to collaborate.\\n\\n“I can’t honestly say at the time that I had a clear grasp of what questions might be addressed with deep learning, but I knew that we were generating data at about twice to three times the rate we could analyse it,” he says.\\n\\nToday, those efforts are beginning to pay off. Finkbeiner’s team, with scientists at Google, trained a deep algorithm with two sets of cells, one artificially labelled to highlight features that scientists can’t normally see, the other unlabelled. When they later exposed the algorithm to images of unlabelled cells that it had never seen before, Finkbeiner says, “it was astonishingly good at predicting what the labels should be for those images”. A publication detailing that work is now in the press.\\n\\nFinkbeiner’s success highlights how deep learning, one of the most promising branches of artificial intelligence (AI), is making inroads in biology. The algorithms are already infiltrating modern life in smartphones, smart speakers and self-driving cars. In biology, deep-learning algorithms dive into data in ways that humans can’t, detecting features that might otherwise be impossible to catch. Researchers are using the algorithms to classify cellular images, make genomic connections, advance drug discovery and even find links across different data types, from genomics and imaging to electronic medical records.\\n\\nMore than 440 articles on the bioRxiv preprint server discuss deep learning; PubMed lists more than 700 references in 2017. And the tools are on the cusp of becoming widely available to biologists and clinical researchers. But researchers face challenges in understanding just what these algorithms are doing, and ensuring that they don’t lead users astray.\\n\\nTraining smart algorithms\\n\\nDeep-learning algorithms (see ‘Deep thoughts’) rely on neural networks, a computational model first proposed in the 1940s, in which layers of neuron-like nodes mimic how human brains analyse information. Until about five years ago, machine-learning algorithms based on neural networks relied on researchers to process the raw information into a more meaningful form before feeding it into the computational models, says Casey Greene, a computational biologist at the University of Pennsylvania in Philadelphia. But the explosion in the size of data sets — from sources such as smartphone snapshots or large-scale genomic sequencing — and algorithmic innovations have now made it possible for humans to take a step back. This advance in machine learning — the ‘deep’ part — forces the computers, not their human programmers, to find the meaningful relationships embedded in pixels and bases. And as the layers in the neural network filter and sort information, they also communicate with each other, allowing each layer to refine the output from the previous one.\\n\\nSource: Jeremy Linsley/Drew Linsley/Steve Finkbeiner/Thomas Serre\\n\\nEventually, this process allows a trained algorithm to analyse a new image and correctly identify it as, for example, Charles Darwin or a diseased cell. But as researchers distance themselves from the algorithms, they can no longer control the classification process or even explain precisely what the software is doing. Although these deep-learning networks can be stunningly accurate at making predictions, Finkbeiner says, “it’s still challenging sometimes to figure out what it is the network sees that enables it to make such a good prediction”.\\n\\nStill, many subdisciplines of biology, including imaging, are reaping the rewards of those predictions. A decade ago, software for automated biological-image analysis focused on measuring single parameters in a set of images. For example, in 2005, Anne Carpenter, a computational biologist at the Broad Institute of MIT and Harvard in Cambridge, Massachusetts, released an open-source software package called CellProfiler to help biologists to quantitatively measure individual features: the number of fluorescent cells in a microscopy field, for example, or the length of a zebrafish.\\n\\nBut deep learning is allowing her team to go further. “We’ve been shifting towards measuring things that biologists don’t realize they want to measure out of images,” she says. Recording and combining visual features such as DNA staining, organelle texture and the quality of empty spaces in a cell can produce thousands of ‘features’, any one of which can reveal fresh insights. The current version of CellProfiler includes some deep-learning elements, and her team expects to add more-sophisticated deep-learning tools in the next year.\\n\\n“Most people have a hard time wrapping their heads around this,” Carpenter says, “but there’s just as much information, in fact maybe more, in a single image of cells as there is in a transcriptomic analysis of a cell population.”\\n\\nThat type of processing allows Carpenter’s team to take a less supervised approach to translating cell images into disease-associated phenotypes — and to capitalize on it. Carpenter is a scientific adviser to Recursion Pharmaceuticals in Salt Lake City, Utah, which is using its deep-learning tools to target rare, single-gene disorders for drug development.\\n\\nMining genomic data\\n\\nWhen it comes to deep learning, not just any data will do. The method often requires massive, well-annotated data sets. Imaging data provide a natural fit, but so, too, do genomic data.\\n\\nOne biotech firm that is using such data is Verily Life Sciences (formerly Google Life Sciences) in San Francisco. Researchers at Verily — a subsidiary of Google’s parent company, Alphabet — and Google have developed a deep-learning tool that identifies a common type of genetic variation, called single-nucleotide polymorphisms, more accurately than conventional tools. Called DeepVariant, the software translates genomic information into image-like representations, which are then analysed as images (see ‘Tools for deep diving’). Mark DePristo, who heads deep-learning-based genomic research at Google, expects DeepVariant to be particularly useful for researchers studying organisms outside the mainstream — those with low-quality reference genomes and high error rates in identifying genetic variants. Working with DeepVariant in plants, his colleague Ryan Poplin has achieved error rates closer to 2% than the more-typical 20% of other approaches.\\n\\nTools for deep diving Deep-learning tools are evolving rapidly, and labs will need dedicated computational expertise, collaborations or both to take advantage of them. First, take a colleague with deep-learning expertise out to lunch and ask whether the strategy might be useful, advises Steve Finkbeiner, a neuroscientist at the Gladstone Institutes in San Francisco, California. With some data sets, such as imaging data, an off-the-shelf program might work; for more complicated projects, consider a collaborator, he says. Workshops and meetings can provide training opportunities. Access to cloud-computing resources means that researchers might not need an on-site computer cluster to use deep learning — they can run the computation elsewhere. Google’s TensorFlow, an open-source platform for building deep-learning algorithms, is available on the software-sharing site GitHub, as is an open-source version of DeepVariant, a tool for accurately identifying genetic variation. Google Accelerated Science, a Google research division based in Mountain View, California, collaborates with a range of scientists, including biologists, says Michelle Dimon, one of its research scientists. Projects require a compelling biological question, large amounts of high-quality, labelled data, and a challenge that will allow the company’s machine-learning experts to make unique computational contributions to the field, Dimon says. Those wishing to get up to speed on deep learning should check out the ‘deep review’, a comprehensive, crowdsourced review led by computational biologist Casey Greene of the University of Pennsylvania in Philadelphia (T. Ching et al. Preprint at bioRxiv http://doi.org/gbpvh5; 2018). Sarah Webb\\n\\nBrendan Frey, chief executive of the Canadian company Deep Genomics in Toronto, also focuses on genomic data, but with the goal of predicting and treating disease. Frey’s academic team at the University of Toronto developed algorithms trained on genomic and transcriptomic data from healthy cells. Those algorithms built predictive models of RNA-processing events such as splicing, transcription and polyadenylation within those data. When applied to clinical data, the algorithms were able to identify mutations and flag them as pathogenic, Frey says, even though they’d never seen clinical data. At Deep Genomics, Frey’s team is using the same tools to identify and target the disease mechanisms that the software uncovered, to develop therapies derived from short nucleic-acid sequences.\\n\\nAnother discipline with massive data sets that are amenable to deep learning is drug discovery. Here, deep-learning algorithms are helping to solve categorization challenges, sifting through such molecular features as shape and hydrogen bonding to identify criteria on which to rank those potential drugs. For instance, Atomwise, a biotech company based in San Francisco, has developed algorithms that convert molecules into grids of 3D pixels, called voxels. This representation allows the company to account for the 3D structure of proteins and small molecules with atomic precision, modelling features such as the geometries of carbon atoms. Those features are then translated into mathematical vectors that the algorithm can use to predict which small molecules are likely to interact with a given protein, says Abraham Heifets, the company’s chief executive. “A lot of the work we do is for [protein] targets with no known binders,” he says.\\n\\nAtomwise is using this strategy to power its new AI-driven molecular-screening programme, which scans a library of 10 million compounds to provide academic researchers with up to 72 potential small-molecule binders for their protein of interest.\\n\\nDeep-learning tools could also help researchers to stratify disease types, understand disease subpopulations, find new treatments and match them with the appropriate patients for clinical testing and treatment. Finkbeiner, for instance, is part of a consortium called Answer ALS, an effort to combine a range of data — genomics, transcriptomics, epigenomics, proteomics, imaging and even pluripotent stem-cell biology — from 1,000 people with the neurodegenerative disease amyotrophic lateral sclerosis (also called motor neuron disease). “For the first time, we’ll have a data set where we can apply deep learning and look at whether deep learning can uncover a relationship between the things we can measure in a dish around a cell, and what’s happening to that patient,” he says.\\n\\nChallenges and cautions\\n\\nFor all its promise, deep learning poses significant challenges, researchers warn. As with any computational-biology technique, the results that arise from algorithms are only as good as the data that go in. Overfitting a model to its training data is also a concern. In addition, for deep learning, the criteria for data quantity and quality are often more rigorous than some experimental biologists might expect.\\n\\nDeep-learning algorithms have required extremely large data sets that are well annotated so that the algorithms can learn to distinguish features and categorize patterns. Larger, clearly labelled data sets — with millions of data points representing different experimental and physiological conditions — give researchers the most flexibility for training an algorithm. Finkbeiner notes that algorithm training in his work improves significantly after about 15,000 examples. Those high-quality ‘ground truth’ data can be exceptionally hard to come by, says Carpenter.\\n\\nTo circumvent this challenge, researchers have been working on ways to train more with less data. Advances in the underlying algorithms are allowing the neural networks to use data much more efficiently, Carpenter says, enabling training on just a handful of images for some applications. Scientists can also exploit transfer learning, the ability of neural networks to apply classification prowess acquired from one data type to another type. For example, Finkbeiner’s team has developed an algorithm that it initially taught to predict cell death on the basis of morphology changes. Although the researchers trained it to study images of rodent cells, it achieved 90% accuracy the first time it was exposed to images of human cells, improving to 99% as it gained experience.\\n\\nFor some of its biological image-recognition work, Google Accelerated Science uses algorithms that were initially trained on hundreds of millions of consumer images mined from the Internet. Researchers then refine that training, using as few as several hundred biological images similar to the ones they wish to study.\\n\\nAnother challenge with deep learning is that the computers are both unintelligent and lazy, notes Michelle Dimon, a research scientist at Google Accelerated Science. They lack the judgement to distinguish biologically relevant differences from normal variation. “The computer is shockingly good at finding batch variation,” she notes. As a result, obtaining data that will be fed into a deep-learning algorithm often means applying a high bar for experimental design and controls. Google Accelerated Science requires researchers to place controls randomly on cell-culture plates to account for subtle environmental factors such as incubator temperature, and to use twice as many controls as a biologist might otherwise run. “We make it hard to pipette,” Dimon quips.\\n\\nThis hazard underscores the importance of biologists and computer scientists working together to design experiments that incorporate deep learning, Dimon says. And that careful design has become even more important with one of Google’s latest projects: Contour, a strategy for clustering cellular-imaging data in ways that highlight trends (such as dose responses) instead of putting them into specific categories (such as alive or dead).\\n\\nAlthough deep-learning algorithms can evaluate data without human preconceptions and filters, Greene cautions, that doesn’t mean they are unbiased. Training data can be skewed — as happens, for example, when genomic data only from northern Europeans are used. Deep-learning algorithms trained on such data will acquire embedded biases and reflect them in their predictions, which could in turn lead to unequal patient care. If humans help to validate these predictions, that provides a potential check on the problem. But such concerns are troubling if a computer alone is left to make key decisions. “Thinking of these methods as a way to augment humans is better than thinking of these methods as replacing humans,” Greene says.\\n\\nAnd then there’s the challenge of understanding exactly how these algorithms are building the characteristics, or features, that they use to classify data in the first place. Computer scientists are attacking this question by changing or shuffling individual features in a model and then examining how those tweaks change the accuracy of predictions, says Polina Mamoshina, a research scientist at Insilico Medicine in Baltimore, Maryland, which uses deep learning to improve drug discovery. But different neural networks working on the same problem won’t approach it in the same way, Greene cautions. Researchers are increasingly focusing on algorithms that make both accurate and explainable predictions, he says, but for now the systems remain black boxes.\\n\\n“I don’t think highly explainable deep-learning models are going to come on the scene in 2018, though I’d love to be wrong,” Greene says.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('An efficient deep-learning tool for detecting eye disease', 'A new artificial-intelligence tool deploys a highly efficient form of deep learning to diagnose eye disease from medical images.\\n\\nConvolutional neural networks are deep-learning algorithms adept at processing images, but researchers typically need to train them on more than a million medical images before they can test how well the algorithms work. Kang Zhang at the University of California, San Diego, in La Jolla and his colleagues created a kind of convolutional neural network capable of learning with many fewer images.\\n\\nThe team trained the model on 108,000 images of retinas. All had been classified by experts as either healthy or showing signs of a leading cause of blindness: macular degeneration or diabetic macular edema, a build-up of fluid in the retina. The algorithm identified critical cases of these conditions as accurately as six experts in ophthalmology.\\n\\nThe model also identified pediatric pneumonia from chest X-rays, suggesting that the technique could be broadly applied across medicine.')\n",
      "('Corrections', 'The Toolbox article ‘The future of scientific figures’ (Nature 554, 133–135; 2018) implied that Benjamin Delory developed the persistence barcode method. In fact, he and his colleagues developed an analysis pipeline that relied on and adapted an existing method.\\n\\nThe Technology Feature ‘Deep learning for biology’ (Nature 554, 555–557; 2018) erroneously affiliated Mark DePristo at Verily Life Sciences. He is, in fact, at Google. Also, the DeepVariant tool was developed jointly by Verily and Google.')\n",
      "('AI diagnostics need attention', 'AI diagnostic tools can find problems including retinal disease but they need to be developed with care.Credit: Lester V. Bergman/Getty\\n\\nOne of the biggest — and most lucrative — applications of artificial intelligence (AI) is in health care. And the capacity of AI to diagnose or predict disease risk is developing rapidly. In recent weeks, researchers have unveiled AI models that scan retinal images to predict eye- and cardiovascular-disease risk, and that analyse mammograms to detect breast cancer. Some AI tools have already found their way into clinical practice.\\n\\nAI diagnostics have the potential to improve the delivery and effectiveness of health care. Many are a triumph for science, representing years of improvements in computing power and the neural networks that underlie deep learning. In this form of AI, computers process hundreds of thousands of labelled disease images, until they can classify the images unaided. In reports, researchers conclude that an algorithm is successful if it can identify a particular condition from such images as effectively as can pathologists and radiologists.\\n\\nBut that alone does not mean the AI diagnostic is ready for the clinic. Many reports are best viewed as analogous to studies showing that a drug kills a pathogen in a Petri dish. Such studies are exciting, but scientific process demands that the methods and materials be described in detail, and that the study is replicated and the drug tested in a progression of studies culminating in large clinical trials. This does not seem to be happening enough in AI diagnostics. Many in the field complain that too many developers are not taking the studies far enough. They are not applying the evidence-based approaches that are established in mature fields, such as drug development.\\n\\nMany reports of new AI diagnostic tools, for example, go no further than preprints or claims on websites. They haven’t undergone peer review, and might never do so. That would verify key details: the underlying algorithm code, and analyses of, for example, the images on which the model is trained, the physicians with which it is compared, the features the neural network used to make decisions, and caveats.\\n\\nAI researchers embrace Bitcoin technology to share medical data\\n\\nThese details matter. For instance, one investigation published last year found that an AI model detected breast cancer in whole slide images better than did 11 pathologists who were allowed assessment times of about one minute per image. However, a pathologist given unlimited time performed as well as AI, and found difficult-to-detect cases more often than the computers (B. E. Bejnordi et al. J. Am. Med. Assoc. 318, 2199–2210; 2017).\\n\\nSome issues might not appear until the tool is applied. For example, a diagnostic algorithm might incorrectly associate images produced using a particular device with a disease — but only because, during the training process, the clinic using that device saw more people with the disease than did another clinic using a different device.\\n\\nThese problems can be overcome. One way is for doctors who deploy AI diagnostic tools in the clinic to track results and report them, so that retrospective studies expose any deficiencies. Better yet, such tools should be developed rigorously — trained on extensive data and validated in controlled studies that undergo peer review. This is slow and difficult, in part because privacy concerns can make it hard for researchers to access the massive amounts of medical data needed. A News story in Nature discusses one possible answer: researchers are building blockchain-based systems to encourage patients to securely share information. At present, human oversight will probably prevent weaknesses in AI diagnosis from being a matter of life or death. That is why regulatory bodies, such as the US Food and Drug Administration, allow doctors to pilot technologies classified as low risk.\\n\\nBut lack of rigour does carry immediate risks: the hype–fail cycle could discourage others from investing in similar techniques that might be better. Sometimes, in a competitive field such as AI, a well-publicized set of results can be enough to stop rivals from entering the same field.\\n\\nSlow and careful research is a better approach. Backed by reliable data and robust methods, it may take longer, and will not churn out as many crowd-pleasing announcements. But it could prevent deaths and change lives.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('People must retain control of autonomous vehicles', 'Driverless vehicles are being tested on public roads in a number of countries.Credit: Prostock/Getty\\n\\nLast month, for the first time, a pedestrian was killed in an accident involving a self-driving car. A sports-utility vehicle controlled by an autonomous algorithm hit a woman who was crossing the road in Tempe, Arizona. The safety driver inside the vehicle was unable to prevent the crash.\\n\\nAlthough such accidents are rare, their incidence could rise as more vehicles that are capable of driving without human intervention are tested on public roads. In the past year, several countries have passed laws to pave the way for such trials. For example, Singapore modified its Road Traffic Act to permit autonomous cars to drive in designated areas. The Swedish Transport Agency allowed driverless buses to run in northern Stockholm. In the United States, the House of Representatives passed the SELF DRIVE Act to harmonize laws across various states. Similar action is pending in the US Senate, where a vote to support the AV START Act would further liberalize trials of driverless vehicles.\\n\\nPolicymakers are enthusiastic about the potential of autonomous vehicles to reduce road congestion, air pollution and road-traffic accidents1,2. Cheap ride-hailing services could reduce the number of privately owned cars. Machine intelligence can make driving more fuel-efficient, cutting emissions. Autonomous cars could help to save the 1.25 million lives worldwide that are lost each year through crashes3, many of which are caused by human error.\\n\\nGovernments want to pass laws to make this happen (see ‘Road to autonomy’). But they are doing so by temporarily freeing developers of self-driving cars from meeting certain transport safety rules. These rules include the requirement that a human operator be inside the vehicle, that vehicles have safety features such as a steering wheel, brakes and a mirror, and that the features are functional at all times. Some developers are maintaining these aspects, but they are not obliged to do so. There is no guarantee that autonomous vehicles will match the safety standards of current cars.\\n\\nSource: Autonomous Vehicles Readiness Index (KPMG International, 2018)\\n\\nMeanwhile, the wider policy implications are not being addressed1,2. Governments stand to lose billions of dollars in tax revenue as rates of car ownership drop among individuals. Millions of taxi, lorry and bus drivers will lose their jobs2. The machine-learning algorithms on which autonomous vehicles rely are far from developed enough to make choices that could mean life or death for pedestrians or drivers.\\n\\nPolicymakers need to work more closely with academics and manufacturers to design appropriate regulations. This is extremely challenging because the research cuts across many disciplines.\\n\\nHere, we highlight two areas — liability and safety — that require urgent attention.\\n\\nLiability\\n\\nLike other producers, developers of autonomous vehicles are legally liable for damages that stem from the defective design, manufacture and marketing of their products. The potential liability risk is great for driverless cars because complex systems interact in ways that are unexpected.\\n\\nManufacturers want to minimize the number of liability claims made against them4. One way is to reduce the chance of their product being misused by educating consumers about how it works and alerting them to safety concerns. For example, drug developers provide information on dosages and side effects; electronics manufacturers issue instructions and warnings. Such guidance shapes the expectations of consumers and fosters satisfaction. Yet, much like smartphones, self-driving cars are underpinned by sophisticated technologies that are hard to explain or understand.\\n\\nA safety driver sits behind the wheel during a test of a self-driving taxi in Yokohama, Japan.Credit: Kiyoshi Ota/Bloomberg/Getty\\n\\nInstead, developers are designing such products to be easy to use5. People are more likely to buy a product that seems straightforward and with which they can soon do complicated things, increasing its utility. However, users are then less able to anticipate how the underlying systems work, or to recognize problems and fix them. For example, few drivers of computerized cars know how the engine is calibrated5. Similarly, a passenger in an autonomous vehicle will not know why it chooses to make a sharp turn into oncoming traffic or why it does not overtake a slow-moving vehicle.\\n\\nWorse, deep-learning algorithms are inherently unpredictable. They are built on an opaque decision-making process that is shaped by previous experiences. Each car will be trained differently. No one — not even an algorithm’s designer — can know precisely how an autonomous car will behave under every circumstance.\\n\\nNo law specifies how much training is needed before a deep-learning car can be deemed safe, nor what that training should be. Cars from different manufacturers could react in contrasting ways in an emergency. One might swerve around an obstacle; another might slam on the brakes. Rare traffic events, such as a truck tipping over in the wind, are of particular concern and, at best, make it difficult to train driverless cars.\\n\\nAdvanced interfaces are needed that inform users why an autonomous vehicle is behaving as it does. Today’s dashboards convey information about a car’s speed and the amount of fuel that remains. Tomorrow’s displays must show the vehicle’s ‘intentions’ and the logic that governs them; for example, they might tell passengers that the car will not overtake the vehicle ahead because there is only a 10% likelihood of success. Little is known about the types of data that should be imparted and how people will interpret them.\\n\\nUsers often ignore information, even if it is presented clearly and the consequences could be a matter of life or death. For instance, almost 70% of airline passengers do not review safety cards before a flight6, despite being asked. Yet these cards convey important information, including how to put on an oxygen mask and open an emergency exit, in simple terms and on a single page.\\n\\nAutonomous vehicles will need to communicate much more complicated information. Their sensors and algorithms must understand the behaviours of pedestrians, discriminate between styles of driving and adjust to changes in lighting. When they cannot, users must know how to respond.\\n\\nResearching ways to present this information effectively is paramount, as are legislative efforts to ensure that users of autonomous vehicles are proficient in using the technology.\\n\\nSafety\\n\\nThe safety and efficiency benefits of autonomous cars rely on computers making better, quicker decisions than people. Users input their desired destination and thereafter cede control to the computer. Full autonomy has — deliberately — not yet been adopted in transportation. People are still perceived as being more flexible, adaptable and creative than machines, and better able to respond to changing or unforeseen conditions7. Pilots are able, therefore, to wrest control from fly-by-wire technology when key computers fail.\\n\\nThe public is right to remain cautious about full automation. Manufacturers need to explain how a car would protect passengers should crucial systems fail. A driverless car must be able to stop safely if its hazard-avoidance algorithms malfunction, its cameras break or its internal maps die. But this is hard to engineer: for example, without cameras, such a car cannot see where it is going.\\n\\nA driverless bus shuttles passengers across Southeast University’s Jiulonghu campus in Nanjing, China.Credit: CVG/Getty\\n\\nIn our view, some form of human intervention will always be required. Driverless cars should be treated much like aircraft, in which the involvement of people is required despite such systems being highly automated. Current testing of autonomous vehicles abides by this principle. Safety drivers are present, even though developers and regulators talk of full automation.\\n\\nNonetheless, having people involved poses safety problems. Autonomous cars will always require users to have a minimum level of skill and will never be easy for some members of the public to operate. People with cognitive impairments, say, might find it difficult to operate these technologies and to override controls. Yet this group includes those who would benefit greatly from self-driving vehicles. For example, older adults8, a demographic of increasing importance, have an elevated risk of crashes because cognitive abilities decline with age9,10. Providing mobility for large numbers of elderly people is an impetus for investment in this technology in Japan, for instance.\\n\\nA remote supervisor could oversee driverless cars as air-traffic controllers do for aircraft. But how many supervisors would be needed to keep networks of such vehicles safe? Stretching human capacity too far can create accidents11. For example, in 1991, an overwhelmed air-traffic controller in Los Angeles, California, mistakenly cleared an aeroplane to land on another. Last year, an overload of patients was blamed for a string of medical errors by doctors in Hong Kong.\\n\\nPolicy gaps\\n\\nCurrent and planned legislation fails to address these issues. Exempting developers from safety rules poses risks. And developers are not always required to report system failures or to establish competency standards for vehicle operators. Such exemptions also presume, wrongly, that human involvement will ultimately be unnecessary. Favouring industry over users will erode support for the technology from an already sceptical public.\\n\\nPresent legislation sidesteps the education of consumers. The US acts merely require that users are “informed” about the technology before its use. Standards of competency and regular proficiency testing for users are not mentioned. Without standards, it is hard to tell whether consumer education programmes are adequate. And without testing, the risk of incidents might increase.\\n\\nMoving forward\\n\\nWe call on policymakers to rethink their approach to regulating autonomous vehicles and to consider the following six points when drafting legislation.\\n\\nDriverless does not, and should not, mean without a human operator. Regulators and manufacturers must acknowledge, rather, that automation changes the nature of the work that people perform7.\\n\\nUsers need information on how autonomous systems are working. Manufacturers must research the limits and reliability of devices that are crucial for safety, including cameras, lasers and radars. When possible, they should make the data from these devices available to vehicle operators in an understandable form.\\n\\nOperators must demonstrate competence. Developers, researchers and regulators need to agree proficiency standards for users of autonomous vehicles. Competency should be tested by licensing authorities and should supplement existing driving permits. Users who fall short should have their access to such vehicles limited, just as colour-blind pilots are banned from flying at night.\\n\\nRegular checks on user competency should be mandatory. Regulators, manufacturers and researchers must determine a suitable time interval between tests, so that proficiency is kept up as cognitive abilities change and technology evolves.\\n\\nRemote monitoring networks should be established. Manufacturers, researchers and legislators need to build supervisory systems for autonomous vehicles. Researchers should supply guidance on the number of vehicles that one supervisor can monitor safely, and on the conditions under which such monitoring is permissible. For example, more supervisors would be needed in poor weather conditions.\\n\\nWork limits for remote supervisors should be defined. Experts must clarify whether supervisors should be subject to existing working-time regulations. For example, air-traffic controllers are limited in how long they can work.\\n\\nThe path towards autonomy is far from preordained. Considerable challenges remain to be addressed.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Oppo and Corephotonics sign strategic agreement around dual-cameras', '')\n",
      "('Pentax K-1 Mark II shoots up to ISO 819,200, offers updated Pixel Shift', '')\n",
      "('This High-Intensity Training Can Help You Learn AI', 'This High-Intensity Training Can Help You Learn AI\\n\\nAs the world looks more toward artificial intelligence (AI) to solve its increasingly complex problems, companies across a wide range of industries are scrambling to hire people who not only understand the fundamentals of AI, but who also know how to implement deep learning methods into a variety of projects and platforms.\\n\\nAnd, if you already have a basic understanding of AI principles, The Advanced Guide to Deep Learning and Artificial Intelligence Bundle is the next step in your education. Temporarily priced down to $19, this high-intensity four-course bundle will help you stand out from the AI crowd and land lucrative positions in the technology sector and beyond.\\n\\nYou’ll start by expanding your knowledge of neural networks and deep learning by implementing these concepts for computer vision and large-scale data processing. From there, you’ll move on to breaking down the most exciting trends in nonlinear dimensionality reduction, implementing recurrent neural networks in Python, exploring classic NLP problems like parts-of-speech tagging and named entity recognition, and much more.\\n\\nTake your AI and deep learning education to its inevitable conclusion with The Advanced Guide to Deep Learning and Artificial Intelligence Bundle—available for only $19.')\n",
      "('Samsung announces the Exynos 9610 SoC with 480FPS slow-motion video recording', 'Samsung announces the Exynos 9610 SoC with 480FPS slow-motion video recording\\n\\nSamsung’s flagship Exynos 9 series SoCs are used in the international variants of its flagship phones. The company also makes mid-range Exynos 7 series SoCs, which are used in its mid-range Galaxy A series. For example, the Galaxy A8 and the A8+ (2018) are powered by the Exynos 7885 system-on-chip, while the international variants of the Galaxy S9 and the S9+ are powered by the Exynos 9810 SoC. Now, Samsung has announced the Exynos 9610, a new SoC in the mid-range Exynos 7 series.\\n\\nThe Exynos 9610 is manufactured on Samsung’s 10nm FinFET process. It has four ARM Cortex-A73 cores clocked at 2.3GHz and four ARM-Cortex A53 cores. The clock speed of the Cortex-A53 cores hasn’t been detailed yet. The SoC features ARM’s second-generation ‘Bifrost’ Mali-G72 GPU, although the GPU core count and clock speeds are currently unknown.\\n\\nSamsung states that the Exynos 9610 has deep learning-based image processing for better photo-taking experiences. Vision and image processing is now coupled with a neural network engine for “more intelligently enhanced face detection,” as well as single camera out-focusing and augmented low-light images. The face detection feature enables the camera to recognize faces that are either not facing forward or partially covered with objects. It also has “smart depth sensing” to create bokeh with a single camera.\\n\\nInterestingly, the company also states that the Exynos 9610’s vision and image processing merges multiple frames to enhance the signal-to-noise ratio (SNR) and brightness that can improve photos taken in low light.\\n\\nThe second major feature of the Exynos 9610 is 480FPS slow-motion video recording in Full HD resolution. Despite using a conventional two-stack image sensor, the SoC is able to encode slow-motion video thanks to the 1.6x improved performance of its ISP and “more-than-doubled” speed of its mobile industry processor interface (MIPI). It also uses MFC to enable encoding and decoding up to 4K 120FPS, although it’s highly unlikely that shipping phones will actually have the feature of encoding 4K 120FPS (considering that 4K 120FPS is not enabled on the Exynos variant of the Galaxy S9).\\n\\nThe SoC also has an embedded Cortex-M4F-based low-power sensor hub that enables always-on sensing applications that require instant feedback (such as gesture recognition or context awareness). The hub is said to efficiently manage the sensors in real-time without waking up the main processor.\\n\\nIn terms of connectivity, the Exynos 9610 features Cat.12 3CA (carrier aggregation) at 600Mbps downlink and Cat.13 2CA at 150Mbps uplink. It also has 802.11ac 2.2 MIMO Wi-Fi, Bluetooth 5.0, and FM radio. The chip has a 4-mode Global Satellite Navigation System (GNSS) receiver that includes GPS, GLONASS, BeiDou, and Galileo.\\n\\n“The Exynos 7 Series 9610 is our latest mobile processor that delivers on added power and speed,” Ben Hur (vice president of System LSI marketing at Samsung Electronics) stated. “But more importantly, it sets a new performance standard for high-end devices with its deep learning vision-image processing solution and slow motion video capabilities that change the way we engage with our mobile devices.”\\n\\nSamsung states that the Exynos 7 Series 9610 is expected to be mass produced in the second half of this year. It may be positioned as a successor to the Exynos 7885, which is currently used in the Galaxy A8 (2018). Crucial details about the SoC still remain unknown (such as the GPU core configuration), and we will update this article when Samsung unveils more details. Overall, it’s good to see Samsung’s mid-range SoCs getting premium features such as 480FPS slow-motion video recording and image processing improvements.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Huawei releases the HiAI Engine with the Huawei P20', 'Huawei releases the HiAI Engine with the Huawei P20\\n\\nHuawei launched the Mate 10 series in October with the HiSilicon Kirin 970 system-on-chip. The Kirin 970 notably has dedicated hardware for AI, in the form of a Neural Processing Unit (NPU). Last month, we also showed how Huawei was developing HiAssistant (powered by HiAI), a Google Assistant + Google Lens competitor for the Chinese market. Now, Huawei has released the HiAI Engine alongside the Huawei P20.\\n\\nHiAI is Huawei’s AI computing platform. Currently, the dedicated NPU on the Kirin 970 phones is used for features such as real time object and scene recognition in the camera app, as well as accelerated offline translation in Microsoft Translator. However, these capabilities are a small part of the overall potential of machine learning and neural networks. It’s fair to say that it’s early days yet for AI in smartphones, but the potential capabilities are promising.\\n\\nAccording to Huawei, the HiAI Engine opens application capabilities, integrates “a plurality of AI capabilities and apps [that make] the apps more intelligent and powerful.” Developers can now download the DDK (Driver Development Kit), SDK, and IDE for the HiAI Engine.\\n\\nThe application capabilities of the HiAI engine include a Computer vision (CV) engine. A Computer Vision engine refers to the “capabilities by which computers simulate the human visual system to sense the ambient environment, and determine, recognize and understand the composition of space,” according to Huawei. The capabilities of the CV engine include Image Super Resolution, Facial Recognition, and Object Recognition (which are explained in more detail below).\\n\\nThe HiAI engine also includes an automatic speech recognition (ASR) engine. It’s not open outside China, though. An ASR engine is what converts human voice into text, and its capabilities include speech recognition, speech conversion, and text-to-speech (TTS).\\n\\nA Natural language understanding (NLU) engine is also part of the capabilities of the HiAI engine, but like the ASR engine, it’s not open outside China. It enables computers to understand human voice, which allows them to perform communication or natural actions. Its capabilities include word segmentation, text entity recognition, emotive tendency analysis, and machine translation.\\n\\nThe features of the HiAI Engine are:\\n\\nScene Detection : It “rapidly categorizes” images by recognizing the scenes in the content of the image. Huawei states that recognition of multiple types of scenes is currently supported with high accuracy.\\n\\n: It “rapidly categorizes” images by recognizing the scenes in the content of the image. Huawei states that recognition of multiple types of scenes is currently supported with high accuracy. Document Correction and Document Detection: It’s an assistant enhancement function which can correct the angle of the document. It can also automatically recognize the location information of the document in the image.\\n\\nIt’s an assistant enhancement function which can correct the angle of the document. It can also automatically recognize the location information of the document in the image. Face Detection and Facial Comparison : Huawei states: “In face detection, information about the location, characteristic points, and postures of the human face in a given image is detected, and high-precision facial rectangular coordinates are returned.”\\n\\n: Huawei states: “In face detection, information about the location, characteristic points, and postures of the human face in a given image is detected, and high-precision facial rectangular coordinates are returned.” Image Category Label: It identifies information such as objects, scenes and behavior in an image on the basis of the deep learning method.\\n\\nIt identifies information such as objects, scenes and behavior in an image on the basis of the deep learning method. Image Super Resolution : It’s said to intelligently enlarge an image based on deep learning. It can also suppress artifacts while keeping the resolution unchanged for a clearer, sharper image.\\n\\n: It’s said to intelligently enlarge an image based on deep learning. It can also suppress artifacts while keeping the resolution unchanged for a clearer, sharper image. Character Image Super Resolution : It can enlarge an image including characters by nine times. It can also “enhance” the definition of the characters on the image.\\n\\n: It can enlarge an image including characters by nine times. It can also “enhance” the definition of the characters on the image. Code Detection: According to Huawei, “Code detection reads quick response (QR) codes and bar codes to obtain information included in the codes, thereby providing related services subsequently. The code detection capability is integrated in applications.”\\n\\nDevelopers can now download the DDK, SDK and IDE of the HiAI Engine via the source links.\\n\\nThanks to XDA Member Evolutios for the tip!\\n\\nSource: HuaweiSource 2: HuaweiSource 3: Huawei')\n",
      "('Learn Python with the Help of this Extensive Course Bundle', 'Learn Python with the Help of this Extensive Course Bundle\\n\\nWhether you’re an aspiring developer or a seasoned coding whiz looking to add another programming language to your belt, it can be difficult to know which language to learn next. If you haven’t already learned Python, however, this should be your top priority.\\n\\nThis extremely powerful and dynamic programming language is used in countless projects ranging from web design and app development to machine learning and advanced statistical analysis. Through five individual courses, the Absolute Python Bundle is all you need to get started, and the best part is that you only have to pay what you want.\\n\\nYou’ll start with a crash course that will get you up and running with all the Python essentials, from learning how to install libraries to working with loops, statements, and more. From there, you’ll learn how to integrate Python with TensorFlow—a massive open source software library that makes it easier to design, build, and train models that revolve around deep learning and AI.\\n\\nWith the essentials under your belt, you’ll move on to applying your new skills to real-word programming problems using in-browser programming exercises and utilizing more advanced concepts such as tuples, dictionaries, decision statements, and more. There’s even a course that teaches you how to integrate Python with WebDriver.\\n\\nPay only what you want. If that’s less than the average price paid, you’ll still take home one of the collection’s five courses. Beat the average price and you’ll take home the entire bundle.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Huawei and Linaro launch the HiKey 970 development board with the Kirin 970 SoC', 'Huawei and Linaro launch the HiKey 970 development board with the Kirin 970 SoC\\n\\nThe Raspberry Pi was a groundbreaking device as it showed users what could be possible with a $35 development board. In the years since, we have seen plenty Raspberry Pi alternatives arrive in the market. Huawei and Linaro launched the HiKey 960 development board last year at a price tag of $239. The board featured the HiSilicon Kirin 960 system-on-chip, and was positioned as a high-end alternative to the Raspberry Pi.\\n\\nNow, Huawei and Linaro have launched the HiKey 970 development board at the Linaro Developer Conference. The board looks like the Raspberry Pi, but it is significantly more powerful. This is because it’s based on the HiSilicon Kirin 970 system-on-chip, which features four ARM Cortex-A73 cores clocked at 2.36GHz and four ARM Cortex-A53 cores clocked at 1.8GHz, paired with the Mali-G72MP12 GPU.\\n\\nThe Kirin 970 has dedicated hardware for AI in the form of a Neural Processing Unit. The HiKey 970 board has 6GB of RAM along with a microSD slot. It supports 4K displays via HDMI 2.0a, has 2 USB 3.0 and 2 USB Type-C ports, and runs Ubuntu, Debian, and Android.\\n\\nThe development board is designed for creating apps, particularly on Android apps. It uses the same chip as the Huawei Mate 10 / Mate 10 Pro, and the Honor View 10, meaning that its performance is on par with a high-end Android smartphone. It also comes with Huawei’s HiAI SDK.\\n\\nThe HiKey 970 thus comes with higher computing power, and it also supports AI stacks. Its energy efficiency is said to be up to 50 times more than that of CPU operations, while performance is up to 25 times more. It has heterogeneous resource management, and its dedicated NPU can enable developers to develop deep learning algorithms, smart robots, automobile software and smart cities.\\n\\nThe board will be open to developers for sale in mid-April. Its price hasn’t been revealed yet, but it’s fair to expect it be on par with the HiKey 960’s $239 price tag, on account of its high-end specifications.')\n",
      "('MediaTek’s Helio P60 is designed with AI in mind', 'MediaTek’s Helio P60 is designed with AI in mind\\n\\nThe MediaTek Helio P60 made its debut at MWC 2018 on Sunday, alongside devices from Samsung, Sony and ZTE. In November 2017, the company said it would take a step back from flagship offerings to focus on the mid-market, where it has historically thrived. We saw the first fruit of this strategy this week: MediaTek announced the AI-infused Helio P60, a system-on-chip designed for affordable devices.\\n\\nThe Helio P60 adopts ARM’s big.LITTLE architecture, dividing workloads between eight cores: four powerful ARM A73 cores and four power-efficient ARM A53 cores. Altogether, it’s 70% faster than the Helio P23 and the Helio P30 in terms of GPU and CPU performance. And thanks to support for Energy Aware Scheduling (EAS) and a new 12nm FinFET manufacturing process, it’s up to 25% more efficient in some cases (12% in demanding workloads).\\n\\nThe Helio P60 also has MediaTek’s AI technology, NeuroPilot, built in. It’s powered by a mobile APU (Artificial intelligence Processing Unit), a dedicated core for AI tasks.\\n\\n“Building on our legacy of innovative technology, the new MediaTek Helio P60 changes everything about what consumers can expect in a smartphone,” said TL Lee, General Manager of MediaTek’s Wireless Communication business unit. “Packing big core power and performance with a processing unit purpose-built for AI applications, the MediaTek Helio P60 chipset brings consumers flagship features like deep-learning facial detection, object and scene identification, fluid gaming experiences, and smarter camera functions. This new chipset gives everyone access to incredible devices without the premium price tag.”\\n\\nWith support for the Google Android Neural Networks API as well as the mobile APU, devices with the Helio P60 accelerate TensorFlow, TF Lite, Caffe, and Caffe2 in the same way newer high-end Qualcomm Snapdragon-powered chips can.\\n\\nFinally, the Helio P60’s three Image Signal Processors (ISPs) lead to 18% efficiency gains in dual-camera devices up to 20MP+16MP, or single-sensor 32MP devices.\\n\\nSmartphones powered by the MediaTek Helio P60 will be available globally by Q2 2018.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You Decide Honor’s Next AI Camera Feature', 'You Decide Honor’s Next AI Camera Feature – Update: Thread is Live\\n\\nLately Honor has been very active in the community. They have stayed long-past the end of scheduled events to respond directly to your questions in the forums with experts from their team. In the previous live Q&A, you asked AI related questions that were addressed by an Honor Product Manager; some of those questions were taken to CES and answered by a Microsoft AI expert.\\n\\nHonor continues to interact with the community to learn and innovate in ways no other company in doing. Soon we will be holding another event with Honor, where you will help decide what AI powered scene is added to Honor’s intelligent scene recognition. Stay tuned for more details about this upcoming event. In the meantime, check out this video, which does a great job of explaining the Honor View 10’s intelligent camera.\\n\\nUpdate: The thread is now live. Go leave a comment about what additional scene you’d like to be added to Honor’s AI camera. The top 5 most thanked posts will win a small prize.\\n\\nHere are some answers from the last Q&A:\\n\\nQ: Where do you see the future of photography going with the help of Honor’s AI assisted camera technology?\\n\\nA: The NPU is the key to make the AI computing possible in honor View 10. With the NPU, honor View 10 provides 13 different scene recognition when taking pictures. The phone knows what type of scene you are shooting and will smartly adjust the best photographic parameters for your shooting to make sure you can easily take the best pictures.\\n\\nQ: How dose NPU capability differ from a CPU or other AI enabled CPU in the market?\\n\\nA: Smartphones are different from servers in many respects, including size, power supply, power consumption, and heat management – all of which place restrictions on smartphone chipset design. Mobile SoCs have to deliver the best possible performance, while ensuring the highest possible density of core functions with optimal use of energy. The Kirin 970’s development team designed this chip on a solid foundation of past breakthroughs, integrating the world’s first NPU (Neural Network Processing Unit) specifically designed for a mobile device. This specialized processing unit, combined with Huawei’s innovative HiAI mobile computing architecture, delivers an AI performance that far surpasses any CPU/GPU build. Compared to a quad-core Cortex-A73 CPU cluster, the Kirin 970’s new heterogeneous computing infrastructure delivers 25x the performance with 50x greater efficiency. Simply put, the Kirin 970 chipset – with its embedded NPU – can perform the same AI computing tasks faster and with far less power. In a benchmark image recognition test, the Kirin 970 processed 33 images per second, which was several times greater than other chips on the market.\\n\\nQ: Does NPU help in RAM and resource allocation during heavy usage?\\n\\nA: The NPU is more focused on speeding the AI tasks, just like the GPU is more focused on graphic processing, while the RAM and resource allocation are mainly handled by the CPU. It is for the NPU that the Kirin 970 in honor View 10 keeps providing a stable AI ability even when the system load is high. A normal smartphone without an NPU will have to face a drop on AI processing in heavy loaded situations.\\n\\nQ: What are the capabilities of the View 10’s AI processor? Will it learn our behavior, getting smarter and smarter? How does it change the way we use phone? How will this AI technology revolutionize mobile usage?\\n\\nA: 1)The Huawei Kirin 970 chipset brings you a faster, stronger, and seamless smartphone experience. Its built-in NPU (neural-network processing unit) enables deep learning based on user behavior, ensuring your Honor View 10 truly understands you.\\n\\n2)Because of AI technology, EMUI 8.0 understands how you use your Honor View 10 and allocates resources by anticipating your behavior. It’s always working in the background to make sure your Honor View 10 runs smoothly even after months of use.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Samsung Galaxy S9 and S9+ Hands On: More of the same, but with a bit more polish', 'Samsung Galaxy S9 and S9+ Hands On: More of the same, but with a bit more polish\\n\\nIf there were any doubts that Samsung had lost its touch, the South Korean company quelled them at Mobile World Congress 2018… mostly. On Sunday, the smartphone maker formally announced the Galaxy S9 and Galaxy S9+, the newest phones in its storied Galaxy series. Both have lightning-fast processors in Samsung’s Exynos 9810 or Qualcomm’s Snapdragon 845 (depending on the model), industry-first variable f/1.5 + f/2.4 aperture rear cameras, professionally-tuned stereo speakers, and new software features such as AR Emoji, Samsung’s take on the Apple’s Animoji.\\n\\nBut while the Samsung Galaxy S9 and S9+ check every box imaginable, they lack the element of surprise.\\n\\nThere’s no beating around the bush: Samsung’s new standard-bearers are less revolutionary than evolutionary. They’re nearly identical to their predecessors in terms of design, right down to the curved edge-to-edge 18:9 displays, glossy back plates, and Gorilla Glass-shielded exteriors. And with the exception of new processors and speakers, not much has changed on the inside.\\n\\nPassing judgment from afar isn’t exactly fair, though, so when Samsung extended XDA an invitation to try out the Galaxy S9 and S9+ for ourselves at its New York City venue, we eagerly accepted. Our impressions after an hour with both phones? Positive. Still, we can’t help but feel that while the Galaxy S9 and Galaxy S9+ are faster, brighter, and louder than every other Galaxy smartphone that’s come before them, Samsung played it safe.\\n\\nDesign\\n\\nThe Galaxy S9 is distinguished from the Galaxy S9+ only by its screen size (it has a 5.8-inch display compared to the Galaxy S9+’s 6.2-inch display), dimensions (it measures 147.7 millimeters in length and 68.7 millimeters in width; the Galaxy S9+ is 158.0 millimeters long and 73.8 millimeters wide), and weight (it’s 26 grams lighter than the Galaxy S9+). It also lacks the Galaxy S9+’s secondary camera, and settles for 4GB of RAM instead of the S9+’s 6GB. Otherwise, the two phones are pretty much identical.\\n\\nThat’s especially true when you’re squinting at the two from a distance. It’s only when you hold them side-by-side that the differences become more apparent, albeit only slightly.\\n\\nWhat was more striking to us, though, was just how similar the Samsung Galaxy S9 and S9+ feel and look like the Galaxy S8 and S8+. The pair isn’t a perfect analog for its outgoing forerunners, but most folks will have a tough time making out changes such as the ever-so-slimmer top and bottom bezels and subtler curve to the left and right of the screen.\\n\\nOne thing they might notice is the fingerprint sensor, which was adjacent to the rear camera on the Galaxy S8 and S8+. It’s been moved beneath the sensor module (which is now oriented vertically as opposed to horizontally on the S8 and S8+) on the S9 and S9+, which is a welcome improvement. Swiping a fingertip across the sensor, which used to require shimmying your hand up the sides of the phone to reach a finger around the volume rocker (or the power button, if you’re left-handed), is a much less arduous task than it used to be. The sensor is now situated below the camera on the rear panel. No finger kinesthetics required.\\n\\nFingerprints, while we’re on the subject, are something of a given on the Samsung Galaxy S9 and S9+’s Gorilla Glass 5 front and back. The scanner’s tweaked placement might prevent wayward digits from smudging the phones’ camera lenses, but does little to shield the highly reflective cover from sweaty, oily skin. As with the Galaxy S8 and Galaxy S8+, you’re going to want to throw the S9/S9+ in a protective case or carry around a microfiber cloth to keep it spick and span.\\n\\nCamera\\n\\nThe Galaxy S9 and S9+’s design may not be radically different from the Galaxy S8 and S8+’s design, but the cameras are where the phones really shine. In fact, they’re easily the highlight.\\n\\nThere was initially some confusion about whether the Galaxy S9 and S9+ are capable of 4K HDR video recording. It’s a feature of the Snapdragon 845’s imaging chip, and a Qualcomm press release on Monday, since edited, contained language suggesting Samsung’s flagships would be one of the first on the market to support it. Unfortunately, that’s not the case: A Samsung representative confirmed to XDA that there are no plans to support 4K HDR video recording on either the Galaxy S9 or S9+. That puts the phones at a disadvantage compared to Sony’s newly announced Xperia XZ2, which has the same chipset and does support 4K HDR recording.\\n\\nThe Galaxy S9 has a 8MP f/1.7 aperture autofocusing front-facing camera (1/3.6″ sensor size, 1.22µm pixel size, and 80-degree field of view) and a 12MP rear camera (1/2.55″ sensor size, 1.4µm pixel size, and 77-degree field of view), with the S9+ packing an additional 12MP telephoto lens (1/3.4″ sensor size, 1.0µm pixel size, 45-degree field of view) for “2x zoom”. The sensors have Super Speed Dual Pixel, a faster and more accurate version of Samsung’s Dual Pixel focusing technology, but they otherwise haven’t changed — they retain the Galaxy S8 and S8+’s optical image stabilization, LED flash, and phase detection autofocus.\\n\\nBut the aperture is a smartphone first. It’s mechanical. The Pro mode in the S9 and S9+’s camera app gives you two settings to choose from: f/1.5, a lower aperture better suited for low light conditions (think nighttime and dimly lit offices), and f/2.4, the default setting. (Alternatively, the app’s Automatic mode switches to the f/1.5 aperture when ambient lighting dips below 100 lux.) A tiny motor in the Galaxy S9/S9+’s camera module is responsible for the adjustment — it contracts (when set to f/2.4) or expands (when set to f/1.5) a ring around the sensor’s lens.\\n\\nThe switch between the two apertures is nearly instantaneous — a major plus. And when we compared results from the two aperture settings at the same ISO and shutter speed, the photos captured in f/1.5 aperture seemed a little bit brighter and crisper than their f/2.4 counterparts.\\n\\nWe tested the Galaxy S9+’s camera in the camera app’s Pro mode with the focus, shutter speed, ISO, and white balance set to “auto”, and the exposure set to “0.0”. We took four photos in two different locations around Samsung’s demo venue: one with the aperture set to f/2.4, and a second with the aperture set to f/1.5. Here are the results:\\n\\nEXIF data:\\n\\nEXIF data:\\n\\nThe Samsung Galaxy S9 and Galaxy S9+’s other camera improvements take advantage of the image signal processors (ISP) in the Exynos 9810 and Snapdragon 845 (the Spectra 280) and dedicated DRAM. Snapping a photo on either phones triggers a burst shot of 12 images, which the ISPs divide into three sets of four, combine on a per-set basis, and generate a single picture. Samsung calls it multiframe noise reduction; previous-generation Galaxy smartphones combined just three images.\\n\\nThe resulting composite is much more vibrant, crisp, and clear than a one-shot picture. (That won’t come as a surprise to anyone who’s used the Google Camera’s HDR+ mode, which takes a similar approach.) Samsung says the Galaxy S9 and S9’s improvements translate to 30 percent less noise in low-light conditions — a claim we’ll have to put to the test at a later date. The photos we took with the Galaxy S9 and S9+ seemed sharp and colorful to our untrained eyes.\\n\\nSamsung gave the selfie sensor some love, too. On the Galaxy S9 and S9+, the 8MP front camera can optionally blur the background of images while keeping the foreground in focus in Selfie focus mode, much like the bokeh effect on the Google Pixel 2 and Pixel 2XL. It’s accomplished entirely in software, and the results aren’t perfect — several of our test selfies, the outer edges of the subject’s face are a bit smudged where the algorithm blended the image.\\n\\nOn the video side of things, the Galaxy S9 and S9+ have a new trick up their sleeves: 960FPS recording. Taking a page from the Sony Xperia XZ Premium‘s playbook, the handsets can capture clips in what Samsung calls Super Slow Motion. Unlike Sony’s Xperia XZ2 and XZ2 Compact, which can record at 1080p resolution, they’re capped at 720p (the clips are captured in 0.2-second bursts and play back as six-second videos). But we have no complaints about the quality: The few clips we captured were razor sharp and buttery smooth. We especially liked the automatic capture feature, which triggers Super Slow Motion when an object enters an adjustable, predefined boundary in the camera’s viewfinder.\\n\\nAnother nifty tool is a GIF generator that turns Super Slow Motion videos into shareable images, with effects such as an Instagram-esque Loop, Swing, and Reverse. (You can save the resulting image as your wallpaper, if you so choose.) It’s sure to come in handy when your social medium of choice doesn’t support video.\\n\\nDisplay\\n\\nIf you’re like most people, you’ll spend a majority of time staring at the Galaxy S9 and S9+’s screen — not their back covers. Both phones have 2960×1440 Quad HD+ Super AMOLED displays with 18.5:9 aspect ratios (570 pixels per inch on the Galaxy S9; 529 ppi on the Galaxy S9+), and Samsung says they’re the “brightest ever” on a Galaxy series smartphone (they both reach 700 nits, or 15% higher than the Galaxy S8 series’ maximum).\\n\\nThat may be so, but the overhead lighting in Samsung’s demo space made it tough to judge the difference with the naked eye. Unfortunately, we didn’t have a brightness tester and were instructed not to take the phones outside, where direct sunlight might have made it easier to judge the improvements (and/or trigger high brightness mode). Suffice it to say that the Galaxy S9 and S9+’s panels are just as colorful and vibrant as they are on the Galaxy S8 and S8+, if not more so.\\n\\nIf the default, slightly oversaturated color palette isn’t to your liking, there are four to choose from:\\n\\nAdaptive Display , the default option\\n\\n, the default option AMOLED Cinema , which uses DCI-P3, the standard wide color space common in 4K HDR TVs\\n\\n, which uses DCI-P3, the standard wide color space common in 4K HDR TVs AMOLED Photo , which uses the Adobe RGB color gamut\\n\\n, which uses the Adobe RGB color gamut Basic Screen Mode, which uses the sRGB/Rec. 709 color space.\\n\\nEach has their advantages and disadvantages, with the AMOLED Cinema and Basic modes producing flatter but ostensibly more accurate colors than the two alternatives. It’s ultimately a matter of personal preference.\\n\\nIt’s worth mentioning that the Galaxy S9 and S9+ are certified by the UHD Alliance for Mobile HDR Premium content (thanks in part to support for DCI-P3). The nuances of HDR are a little complicated, but in essence, HDR videos and video games boast higher contrast and brightness than non-HDR media, contributing to a picture with more accurate colors overall.\\n\\nIt’s not just HDR content that benefits — according to a Samsung representative, the S9 and S9+ have Samsung’s Video Enhancer feature, a carryover from the S7 and S8 that boosts the brightness and color contrast of streaming and local video.\\n\\nSamsung’s words rang true in our limited time with the Galaxy S9 and S9+. The HDR YouTube videos we watched were richly rendered on the phones’ screens, with the AMOLED screens’ deep blacks highlighting the bright reds, yellows, and greens.\\n\\nIris scanner\\n\\nThe Galaxy S8 and S8+ shipped with an iris scanner. It worked, but somewhat inconsistently in certain lighting conditions — especially if you wore color contacts or sunglasses, or held your phone beyond the recommended distance from your eyes. The iris scanner is present and accounted for in the Galaxy S9 and S9+, but with a fallback this time: facial identification.\\n\\nA new feature called Intelligent Scan uses both the iris scanner and the front-facing camera to secure the phones. In practice, when you tap the power button, both sensors start scanning your face for matches. As soon as there’s a positive ID, it’s open sesame — you’re greeted with the home screen.\\n\\nAudio\\n\\nA great screen is nothing without great speakers to match, and the Galaxy S9 and S9+ are Samsung’s strongest showing yet in that regard. The down-firing, AKG Acoustics-tuned stereo speakers easily clear the low bar set by the Galaxy S8 and S8+. They’re noticeably louder (40 percent louder, Samsung says), and they’re capable of delivering a “simulated surround sound experience” thanks to Dolby’s Atmos 3D technology. (Samsung’s venue wasn’t particularly conducive to testing this.)\\n\\nA dearth of supported content makes Dolby Atmos less of a value-add than it otherwise might be, but a Samsung spokesperson said that Atmos-supported videos and movies will come to the Netflix on smartphones later this year. Mum’s the word on the number and date.\\n\\nDon’t expect Galaxy S9 and S9+’s speakers to blow you away, though. They might sound better than last year’s models, but they’re still too tinny and boomy to stand in for a decent boombox or Bluetooth speaker.\\n\\nAR Emoji\\n\\nApple’s Animoji, which tap the iPhone X’s depth-sensing Face ID camera for goofy animated iMessages, have achieved something of a cult following. It’s enough to have caught Samsung’s (and Asus’s) attention: The Galaxy S9 and S9+ ship with AR Emoji, a face-mapped camera feature that uses the phones’ front-facing sensor to mimic your mouth, eyebrow, and head movements on a humanoid caricature.\\n\\nThey’re easy to get up and running: On-screen instructions have you stare head-on at the camera and select your gender, and the camera app does the rest, analyzing more than 100 points on your face to render a cartoon version of you — replete with hair, eyebrows, customizable clothing, and a disproportionately small body.\\n\\nA mini-me isn’t the only AR Emoji on offer. Samsung partnered with Disney to bring 3D-rendered versions of Mickey Mouse, Minnie, and characters from Pixar’s Incredibles.\\n\\nWhichever model you choose, the camera app automatically generates 18 animated AR Emoji stickers in a shareable format (MP4). There’s a host of additional masks, filters, and accessories to choose from. And unlike Apple’s Animoji, which can’t be exported from iMessage, AR Emoji work in any app — be it a messaging service like WhatsApp, a social network like Facebook, or a plain old email.\\n\\nAR Emoji crashed and burned during Samsung’s press event in Barcelona on Sunday, and they were a little stiff in our experience, too. The single front camera struggles to track head movements and mouth movements beyond a fairly narrow field of view, and if you don’t hold the S9/S9+ close to your face when you’re creating an AR Emoji, the resulting animation can be really janky.\\n\\nSuffice it to say, AR Emoji aren’t quite as endearing as the 1-to-1-tracked, cute and cuddly characters on the iPhone X.\\n\\nBixby improvements on the Samsung Galaxy S9/Galaxy S9+\\n\\nBixby, Samsung’s homegrown digital assistant, makes a return on the Galaxy S9 and Galaxy S9+. The latest incarnation can be launched via the Samsung Galaxy S9 and S9+’s dedicated Bixby button (below the volume rocker on the left-hand side): a single press pulls up Bixby Home, a collection of cards that contain timely information. You’ll see the weather report, a preview of your commute (based on your location and proximity to your saved work/home address), upcoming alarms, and health information (like you step count) from S Health.\\n\\nNone of that’s new, but Bixby Vision, Bixby’s machine vision feature, is improved in a few key ways. An augmented reality feature overlays shades of lipstick, eyeshadow, and other makeup on your face, letting you “try on” beauty products before you purchase them through Sephora and Cover Girl. Bixby Vision now supports real-time translation a la Google Translate. And if you point Bixby Vision’s viewfinder at food, it’ll serve up the estimated calorie count and other nutritional data.\\n\\nThe “digital makeup” feature worked well in our testing (maybe too well), but we didn’t have an opportunity to try Bixby’s new food recognition or real-time translation features.\\n\\nIt’s worth noting that after the Samsung Galaxy S9 and S9+ ships in March, Bixby will gain additional features. In August, Bixby 2.0, which launched in public beta in December, will roll out to phones, Samsung mobile chief DJ Koh told members of the press at MWC 2018. It’ll recognize multiple voices and integrate tightly with TVs, refrigerators, home appliances, and other connected appliances.\\n\\nSamsung Galaxy S9 (Exynos) Performance\\n\\nThe Samsung Galaxy S9 and Galaxy S9+, like other recent flagship Samsung phones before them, ship with one of two system-on-chip (SoC). This time around, it’s Samsung’s Exynos 9810 or Qualcomm’s Snapdragon 845.\\n\\nIt’s worth diving into the technical weeds to get a better sense of chips’ differences.\\n\\nThe Exynos 9810, the second SoC in the Exynos 9 series, is built on a 10nm FinFET process and adopts ARM’s DynamIQ architecture. It has four high-performance custom cores clocked up to 2.7GHz and four ARM Cortex-A55 cores clocked at 1.7GHz, and a wider pipeline with improved cache memory. Performance is substantially improved over the Exynos 8895 in the Galaxy S8 and S8+: Samsung says the Exynos 9810 is two times faster in terms of single-core performance and 40 percent faster in terms of multi-core performance.\\n\\nThe Exynos 9810 ships with the Mali-G72MP18 GPU, which has a slightly decreased core count compared to the Exynos 8895’s Mali-G71MP20, but improved per-core efficiency.\\n\\nThe chip’s Cat. 18 Gigabit modem supports download speeds over LTE up to 1.2Gbps thanks to 6X carrier aggregation (6CA), 4×4 MIMO, 256-QAM, and License-Assisted Access (eLAA), and it has neural network deep learning technologies that power Bixby’s image recognition features and 3D Emoji’s face-tracking filters. Finally, there’s a secure element that safeguards biometric data such as fingerprints, iris scans, and facial information.\\n\\nThe Qualcomm Snapdragon 845, which we recently benchmarked, is also built on a 10nm process and adopts ARM DynamIQ. It has eight custom Kryo cores: four Cortex-A75 “Gold” performance cores clocked up to 2.8GHz and four Cortex-A55 “Silver” efficiency cores clocked at 1.7GHz, which contribute to a 30 percent boost in overall performance and a 25 to 30 percent improvement in power-efficiency compared to the Snapdragon 835 in the Galaxy S8 and S8+.\\n\\nOn the visual processing side of things, the Snapdragon 845 packs the Adreno 630, Qualcomm’s latest GPU. It’s 30 percent faster and 30 percent more power-efficient than the Snapdragon 835’s Adreno 540, and it has 2.5 times the display throughput.\\n\\nThe Snapdragon 845’s other notable peripherals include the X20 modem, which supports Cat. 18 LTE download speeds up to 1.2Gbps, carrier aggregation, 4×4 MIMO, 256-QAM, and eLLA; the Hexagon DSP, a chip custom-designed for neural network workloads; and Qualcomm’s Secure Processing Unit, a secure element for biometric data.\\n\\nOur Galaxy S9 and S9+ demo units had the Exynos 9810, and felt as swift and speedy as you’d expect. Switching between apps and juggling multiple tabs in Chrome was equally as breezy, likely thanks to the 6GB of RAM in the Galaxy S9+ and 4GB of RAM in the S9.\\n\\nThat said, we’re reluctant to jump to any conclusions about performance without time to run the phones through their paces (i.e., perform benchmarking tests and our in-house suite of scripts). Already, preliminary results have shown that the Exynos 9810 performs unpredictably in the Galaxy S9+, and in the interest of fairness, we’re reserving judgment until we’ve had a chance to thoroughly investigate Samsung’s claims.\\n\\nWe’ve also yet to test the Galaxy S9 and S9+’s battery life. They have the same capacities as the Galaxy S8 and S8+, respectively: 3,000mAh and 3,500mAh. (Both support wireless charging and Samsung’s Adaptive Fast Charging.) Samsung says the Galaxy S9 gets up to 14 hours of internet use on Wi-Fi, 11 hours on 3G, and 12 hours on 4G; 16 hours of video playback; and 22 hours of talk time. It says the Galaxy S9+ gets up to 15 hours on Wi-Fi, 13 hours on 3G, and 15 hours on 4G; 18 hours of video playback; and 25 hours of talk time.\\n\\nAnd we haven’t tested the storage’s read and write speeds. The Galaxy S9 and S9+ ship with 64GB of internal memory (up to 256GB) and a microSD slot that supports cards up to 400GB.\\n\\nSamsung Experience 9.0 Software on the Galaxy S9\\n\\nThe Galaxy S9 and S9+ ship with Samsung Experience 9.0 atop Android Oreo. Both are Project Treble compatible which is great news for the modding community — in the future, we expect to see the Galaxy S9 and S9+ boot generic Android Open Source Project images in the future (but only for the Exynos models, which have unlockable bootloaders).\\n\\nAs far as Samsung Experience 9.0 is concerned, there isn’t much in the way of surprises. It began to roll out late last year as part of the Android Oreo beta to Galaxy S8 and S8+ participants in Samsung’s Beta Program, after which it launched more broadly in stable form. From what we can tell, Samsung Experience 9.0 on the S9 and S9+ is no different than publicly available version, save features like AR Emoji.\\n\\nThe new and improved Samsung Keyboard adds a Google-style toolbar to the top row with shortcuts, a theme switcher, and a GIF creator. And Edge Lighting, a staple of Samsung’s curved-screen devices that shows alerts, text scrolls, and other peripheral information on the phone’s sides, has been enhanced with more lighting effects.\\n\\nThe Samsung Experience 9.0 launcher implements support for Android Oreo’s Notification Dots and Adaptive Icons, and a new color picker that lets you tweak the appearance of folders. Additionally, the lock screen has a new clock widget and an adaptive coloring option that changes the lock screen color to match your phone’s background.\\n\\nIf you nab the new $150 DeX Dock with your Galaxy S9/S9+, you’ll benefit from the new, higher-resolution (2,560 x 1,400) display output (double the previous DeX Dock’s resolution). Samsung says that more than 40 partners are optimizing their Android apps for the DeX Dock interface, but alternatively, you can take advantage of Samsung’s Linux on Galaxy feature and install a full-blown Linux distribution.\\n\\nConclusion\\n\\nIf it wasn’t obvious from the get-go, Samsung isn’t out to break new ground or shake up the smartphone industry with the Galaxy S9 and S9+. That much became clear in the hour we spent getting a handle on AR Emoji, putting variable aperture setting to the test, and blasting sound through the stereo speakers. The S9 and S9+ are iterative in every sense of the word: The new processors are on a par with other flagship devices announced for this year; the S9+’s upgraded RAM and secondary rear sensor bring it up to speed with the competition; and the down-firing speakers merely improve on the S8 and S8+’s disappointing sound.\\n\\nBut iteration isn’t necessarily a bad thing. In fact, a Samsung rep told me that the company’s well aware that most soon-to-be Samsung Galaxy S9 and Galaxy S9+ owners will be upgrading from a Galaxy S7 or S7 Edge. For them, the phones are a giant technological leap forward.\\n\\nFor current S8 and S8+ owners, though, or folks with a relatively new flagship such as the OnePlus 5T or LG V30, the incremental differences make the price tags hard to justify. At $720 and $840 for the Galaxy S9 and Galaxy S9+, respectively, they’re easily two of the most expensive phones on the market. Trade-in deals and monthly installment pricing help ease the burden a little, but no matter how you slice it, that’s a lot of moolah for variable aperture.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Nike Strengthens Digital Platforms by Acquiring Computer Vision Company Invertex', 'Nike has purchased Invertex, a computer vision company based in Tel Aviv, Israel. The move is part of the Portland-based company’s drive to improve its digital platforms, with the Invertex team set to work on “building groundbreaking innovations” for Nike.\\n\\nNike Chief Digital Officer, Adam Sussman, announced the purchase, explaining that “the acquisition of Invertex will deepen our bench of digital talent and further our capabilities in computer vision and artificial intelligence as we create the most compelling Nike consumer experience at every point.” Invertex has created so-called “scan-to-fit” shopping experiences which combine 3D-digitization and deep-learning technologies to create new retail solutions. This technology is significant, as it lowers the number of returns from customers and allows retailers to tailor existing to products to the customers’ specific needs.\\n\\nIn other Nike news, check out the official store list for the Virgil Abloh x Nike Air VaporMax “White.”')\n",
      "('New Exynos 9610 teases slow-mo video and camera bokeh for Galaxy-A phones', \"Samsung has announced its latest Exynos chip, the Exynos 7 Series 9610.\\n\\nPrevious 7 Series chips have been used in Samsung’s mid-range devices such as this year’s Galaxy A8.\\n\\nThe chip could allow 480fps slow-mo video recordings and single camera bokeh effects.\\n\\nSamsung has announced its latest application processor, the Exynos 7 Series 9610. The chip’s headline features include support for deep learning-based image processing and 480fps slow-motion video recording in Full HD. While it is not yet known which phones will use this chip, previous Exynos 7 series SoCs have been used in Samsung’s high mid-range devices such as this year’s Galaxy A8.\\n\\nOn this chip, Samsung says it has combined vision and image processing with a neural network engine, a popular trend in high-end smartphone chips right now, which should result in some powerful new features for the mid-tier.\\n\\nWith a neural network engine, phones that use the chip should get plenty of cool camera tricks.\\n\\nFor example, Samsung says that the chip boasts enhanced facial recognition which should be able to recognize a face even when it is partially obscured by another object. Additionally, it will enable the ever popular bokeh portrait effect on devices with a single camera by utilizing its smart depth sensing capabilities. Samsung’s latest SoC should also be able to produce better quality images in low light by merging multiple frames into a single photo.\\n\\nAs well as the still image capabilities, the Exynos 9680 will enable phones to record 480fps slow-motion video in full HD, which means future mid-range phones will get to play around with some fun, high-quality slow-motion video effects. In terms of playing back video, its multi-format codec supports up to 4K at 120fps.\\n\\nEditor's Pick New MediaTek Helio P60 brings AI to the mid-tier At MWC 2018, MediaTek has unveiled its latest mobile application processor, the Helio P60. With it comes a selection of improvements over its predecessor, complete with some new hardware and tools to support the growing …\\n\\nA high level of performance is provided by the octa-core CPU configuration, with four Cortex-A73 cores running at 2.3GHz and four Cortex-A53 cores running at 1.6GHz. This should keep things running smoothly even when multitasking and offers up plenty of peak single core performance for more intensive apps. There is also a second-gen Bifrost based Mali-G72 to power games with realistic graphics, although Samsung is keeping the core count under wraps for now. Finally, the chip supports a Category 13 LTE modem for up to 600Mbps download speeds, Bluetooth 5.0, and 802.11ac 2×2 MIMO Wifi.\\n\\nAccording to Samsung, the Exynos 9610 is expected to start mass production in the second half of 2018, this could put it in contention to be used in phones from either late this year or early 2019.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Artificial Intelligence vs Machine Learning: what’s the difference?', \"Artificial intelligence and machine learning are terms which have been thrown around a lot in the tech industry over the last few years, but what exactly do they mean? Anyone vaguely familiar with sci-fi tropes will probably have an idea about AI, though they may view it as a little more sinister than what’s around today.\\n\\nThe two terms are often conflated and, incorrectly, used interchangeably, particularly by marketing departments that want to make their technology sound sophisticated. In fact, artificial intelligence and machine learning are very different things, with very different implications for what computers can do and how they interact with us.\\n\\nIt starts with Neural Networks\\n\\nMachine learning is the computing paradigm that’s lead to the growth of “Big Data” and AI. It’s based on the development of neural networks and deep learning. Typically this is described as imitating the way humans learn, but that’s a bit of a misnomer. Machine learning actually relates to statistical analysis and iterative learning.\\n\\nEditor's Pick What is machine learning? One area of technology that is helping improve the services that we use on our smartphones, and on the web, is machine learning. Sometimes, the terms machine learning and artificial intelligence get used as synonyms, …\\n\\nInstead of building a traditional program comprised of logical statements and decision trees (if, and, or, etc), a neural network is built specifically for training and learning using a parallel network of neurons, each set up for a specific purpose.\\n\\nThe nature of any particular neural network can be very complicated, but the key to the way they function is by applying weights (or factors of importance) to some attribute of the input. Using networks of various weights and layers, it’s possible to produce a probability or estimation that your input matches one or more of the defined outputs.\\n\\nThe problem with this type of computing, just like regular programming, is its dependence on how the human programmer sets it up, and readjusting all these weights to refine the output accuracy could take too many man-hours to be feasible. A neural network transitions into the realm of machine learning once a corrective feedback loop is introduced.\\n\\nEnter Machine Learning\\n\\nBy monitoring the output, comparing it to the input, and gradually tweaking neuron weights, a network can train itself to improve accuracy. The important part here is that a machine learning algorithm is capable of learning and acting without programmers specifying every possibility within the data set. You don’t have to pre-define all the possible ways a flower can look for a machine learning algorithm to figure out what a flower looks like.\\n\\nStanford University defines machine learning as “the science of getting computers to act without being explicitly programmed”.\\n\\nTraining a network can be done in a number of different ways, but all involve a brute force iterative approach to maximising output accuracy and training the optimum paths through the network. However, this self training is still a more efficient process than optimizing an algorithm by hand, and it enables algorithms to shift and sort through much larger quantities of data in much faster times than would otherwise be possible.\\n\\nOnce trained, a machine learning algorithm is capable of sorting brand new inputs through the network with great speed and accuracy in real time. This makes it an essential technology for computer vision, voice recognition, language processing, and also scientific research projects. Neural networks are currently the most popular way to do Deep Learning, but there are other ways to achieve machine learning as well, although the method described above is currently the best we have. You can read more about how machine learning works here.\\n\\nRobohub\\n\\nWhat AI is and isn’t\\n\\nMachine learning is a clever processing technique, but it doesn’t possess any real intelligence. An algorithm doesn’t have to understand exactly why it self-corrects, only how it can be more accurate in the future. However, once the algorithm has learned, it can be used in systems that actually appear to possess intelligence. A good way to define artificial intelligence would be the application of machine learning that interacts with or imitates humans in a convincingly intelligent way.\\n\\nA machine learning algorithm that can sift through a database of images and identify the main object in the picture doesn’t really seem intelligent, because it’s not applying that information in a human-like way. Implementing the same algorithm in a system with cameras and speakers, which can detect objects placed in front of it and speak back the name in real time suddenly seems much more intelligent. Even more so if it was able to tell the difference between healthy and unhealthy foods, or differentiate everyday objects from weapons.\\n\\nA good definition of AI is a machine that can perform tasks characteristic of human intelligence, such as learning, planning, and decision making.\\n\\nArtificial intelligences can be broken down into two major groups, applied or general. Applied artificial intelligence is much more feasible right now. It’s tied more closely to the machine learning examples above and designed to perform specific tasks. This could be trading stocks, traffic management in a smart city, or helping to diagnose patients. The task or area of intelligence is limited, but there’s still scope for applied learning to improve the AI’s performance.\\n\\nEditor's Pick Why are smartphone chips suddenly including an AI processor? If virtual assistants have been the breakthrough technology in this year's smartphone software, then the AI processor is surely the equivalent on the hardware side. Apple has taken to calling its latest SoC the A11 Bionic …\\n\\nGeneral artificial intelligence is, as the name implies, broader and more capable. It’s able to handle a wider range of tasks, understand pretty much any data set, and therefore appears to think more broadly, just like humans. General AI would theoretically be able to learn outside of its original knowledge set, potentially leading to runaway growth in its abilities. Interestingly enough, the first machine learning discoveries reflected ideas of how the brain develops and people learn.\\n\\nMachine learning, as part of a bigger complex system, is essential to achieving software and machines capable of performing tasks characteristic of and comparable to human intelligence — very much the definition of AI.\\n\\nMedium\\n\\nNow and into the future\\n\\nDespite all the marketing jargon and technical talk, both machine learning and artificial intelligence applications are already here. We are still some way off from living alongside general AI, but if you’ve been using Google Assistant or Amazon Alexa, you’re already interacting with a form of applied AI. Machine learning used for language processing is one of the key enablers of today’s smart devices, though they certainly aren’t intelligent enough to answer all your questions.\\n\\nEditor's Pick Machine learning is already a part of your life, you just don’t know it yet If you were born in the 80s or 90s, then there’s a good chance the idea of machine learning conjures images of Arnold Schwarzenegger as The Terminator. It sounds like a far future concept that …\\n\\nThe smart home is just the latest use case. Machine learning has been employed in the realm of big data for a while now, and these use cases are increasingly encroaching into AI territory as well. Google uses it for its search engine tools. Facebook uses it for advertising optimization. Your bank probably uses it for fraud prevention.\\n\\nThere’s a big difference between machine learning and artificial intelligence, though the former is a very important component of the latter. We’ll almost certainly continue to hear plenty of talk about both throughout 2018 and beyond.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Pay what you want to learn about AI and Deep Learning technology [DEALS]', 'Artificial intelligence, once the subject of science fiction, is now reality. While most people have heard of its use in self-driving cars, it can also be found in a growing range of other technologies. And we’ve really only scratched the surface of what AI can do, which means there are plenty of opportunities available to knowledgeable people. Want to be one of those people? Then you need the Pay What You Want: AI & Deep Learning Bundle, offered to readers of Android Community.\\n\\nThis package, valued at $691, offers lifetime access to ten eBooks and courses that explain artificial intelligence and deep learning technologies in a way that’s easy to understand. Sounds interesting? Here’s how to works: Just beat the average price and receive the entire bundle for that amount. If that’s a little more than you’d like to spend, just offer what you want and get ‘Deep Learning with Hadoop’, a $31.99 value, for that price. Either way, you are going to get a tremendous value for your money.\\n\\nThe bundle includes courses such as:\\n\\n– Artificial Intelligence with Python\\n\\n– Getting Started with Java Deep Learning\\n\\n– Java Deep Learning Essentials\\n\\n– Deep Learning with TensorFlow eBook\\n\\n– Deep Learning with TensorFlow\\n\\n– Deep Learning with R\\n\\n– Deep Learning with Python\\n\\n– Python Deep Learning\\n\\n– Deep Learning with Keras\\n\\n– Deep Learning with Hadoop\\n\\nGet updated on the hottest trend in tech and Pay What You Want for this AI & Deep Learning Bundle, available at Android Community Deals.')\n",
      "('Samsung intros the Exynos 7 Series 9610 mobile processor', 'Back in January, Samsung rolled out the Exynos 7872. The chipset was used on the Meizu M6s first. We know it’s only one model from the Exynos 7 series and now we have the 9610 which is related to the Exynos 9810 found on the Galaxy S9. The 9810 is the one responsible for the reimagined camera, AI features, and the improved multimedia transfer. The latest application processor (AP) is based on the 10-nanometer (nm) FinFET process. It allows enhanced multimedia experience in premium smartphones, slow-mo video recording at 480 fps, and deep-learning image processing.\\n\\nThe Exynos 7 Series 9610 mobile processor offers more speed and power. Expect a more enriched photo-taking and filming experiences as described. There’s a neural network engine that allows an intelligent face detection, augmented low-light images, and single camera out-focusing among others.\\n\\nSome other things the Exynos 9610 can do include full HD 480fps slow-motion support, encoding and decoding up to 4K 120fps, Always-on sensing, and obviously faster mobile computing and gaming performances.\\n\\nSamsung Electronics’ System LSI marketing VP Ben Hur said: “The Exynos 7 Series 9610 is our latest mobile processor that delivers on added power and speed. But more importantly, it sets a new performance standard for high-end devices with its deep learning vision-image processing solution and slow-motion video capabilities that change the way we engage with our mobile devices.”\\n\\nSamsung will start mass producing the Exynos 7 Series 9610 processor in the second half of 2018.\\n\\nSOURCE: Samsung')\n",
      "('Bixby will be updated as Samsung acquires new AI startup', 'Samsung’s Bixby is getting an update. We’re certain of that and we can’t actually wait to see what Bixby 2.0 will be all about. It may get rid of the “Hi Bixby” hot word for every command and we may get to try it on the Galaxy Note 9. There’s also the Bixby speaker getting delayed to 2H of 2018. We’re not exactly sure why but the ability to easily track calories is being tested out. Bixby 2.0 isn’t ready but at least we know it’s coming. It’s not dead yet despite the not so positive reception.\\n\\nThe South Korean tech giant is serious about its artificial intelligence venture that’s why it has acquired Kngine. It’s an AI search engine startup that was acquired in hopes of focusing on artificial intelligence. This follows the acquisition of Fluenty which happened because of the same goal. Samsung also made investments in other groups such as Expect Labs, Vicarious, and Reactor Labs.\\n\\nAn affiliate acquired a full stake in the startup. If you’re not familiar with Kngine, it’s a group that works on mobile solutions mainly based on AI. This includes deep learning so the system reads the web, understands contents, and updates its knowledge based on the information available.\\n\\nVIA: The Investor')\n",
      "('MediaTek Helio P60 processor ready for more AI functions', 'MediaTek recently rolled out a new processor that can offer AI processing. Officially called as the Helio P60 chipset, this SoC features a multi-core AI processing unit, NeuroPilot AI technology, and Arm Cortex A73 and A53 processor. MediaTek built the processor using a new 12nm FinFET technology. This one promises longer battery life, energy efficiency, smarter camera functions, fluid gaming experiences, object and scene identification, and deep-learning facial detection. MediaTek’s new chipset is built on the company’s legacy of innovative technology as described by the Wireless Communication business unit’s General Manager TL Lee.\\n\\nThis one is meant to work for AI applications and for more flagship features even on devices not necessarily premium.\\n\\nThe MediaTek Helio P60 is said to be more efficient compared to the older Helio P23, thanks to the following: four Arm A73 2.0 GHz processors, four Arm A53 2.0 GHz processors, LITTLE configuration, and TSMC 12nm production process. The NeuroPilot AI technology with the NeuroPilot SDK works with the Google Android Neural Networks API (Android NNAPI). Also supported are AI frameworks such as TensorFlow, Caffe, Caffe2, and TF Lite.\\n\\nThis processor also includes TAS 2.0 smart antenna technology, 4G LTE WorldMode modem, and dual 4G VoLTE. All these and more allows upcoming mid-range devices to be more reliable, more capable, and much smarter. Expect several MediaTek Helio P60-powered smartphones to be ready by the next quarter (Q2).\\n\\nSOURCE: Mediatek')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Deep learning technology is now being used to put Nic Cage in every movie', 'Last month, Motherboard reported on a Redditor using deep learning technology to map female celebrities’ faces onto pornographic performers, with startlingly lifelike results. By scanning a bunch of images of a celebrity’s face, the software was able to imagine what they’d look like grafted into a given video—a powerful technology being used in one of the worst possible ways. The result barrels past legal and ethical boundaries, raising a whole host of free speech issues regarding the women whose likenesses are being used without their consent. There are surprisingly few legal recourses for them, at least in America.\\n\\nThe technology also opens up the door to a very near future in which we won’t be able to trust video evidence—long the gold standard, at least in the court of public opinion. This could be particularly damning in the political realm, where (say it with us) “fake news” and the very nature of reality is somehow now open to partisan interpretation. Last week, Motherboard followed up on the original piece with reports of the existence of FakeApp, a piece of software that makes creating these fakes so user-friendly that an entire new subreddit has sprung up, devoted to their creation.\\n\\n\\n\\nAdvertisement\\n\\nA trip into that den yields countless images of horrified, malfunctioning automatons performing for the godless pleasures of their creator, and also a whole shitload of Nic Cage, because after porn, the internet makes memes. Here is Nic Cage superimposed over Andy Samberg pretending to be Nic Cage:\\n\\nReplacing Harrison Ford in Raiders Of The Lost Ark:\\n\\n\\n\\nAdvertisement\\n\\nFinally getting that role in Superman for which he’d always yearned:\\n\\nThese are at once entertaining, technologically impressive, and creepy as all hell, existing on the other side of the uncanny valley, believable but still “off.” The technology is immensely powerful and only getting more believable and easy to use, which means we will absolutely be seeing much more like this, as well as more hideously invasive and demeaning uses like those on the larger subreddit. Wired reports that the creator of FakeApp could be liable for damages in cases in which the technology was used to create “nonconsensual porn,” but it is unclear what legal recourse, if any, will be available to us in the Cage-filled dystopia we are set to enter.\\n\\n\\n\\nAdvertisement\\n\\nOne more for the road:\\n\\n[via The Outline]\\n\\n\\n\\nUPDATE: Gfycat stopped hosting many of the renders because of their connection to the creepy porn stuff, but the Nic Cage stuff is forever.')\n",
      "('Saturday assorted links', '1. One out of every 15 Mormons is from Brazil.\\n\\n2. WSJ James Taranto profile of Bryan Caplan.\\n\\n3. Why are city accents fading in the Midwest.\\n\\n4. “…we present a deep learning audio-visual model for isolating a single speech signal from a mixture of sounds such as other voices and background noise.” You’ll have to hold that conversation in a park. Until the parks are bugged.\\n\\n5. First chapter of the new Weyl and Posner now available on-line.')\n",
      "('Stanford scientists invent AI that can predict death with up to 90% accuracy', 'Humans today live a lot longer than they used to. That’s great news, but as modern medical advances are giving patients second chances at living normal lives, end-of-life care continues to be a difficult thing to plan. Forecasting when someone will die is an extremely challenging and often uncomfortable thing, but Stanford researchers have trained an AI to be able to predict death with incredible accuracy, and it could revolutionize end-of-life care for patients who are reaching their ends.\\n\\nThe goal is to better match patient (and family) wishes with an accurate timeline of an individuals final months, weeks, and days, while affording them the opportunity to plan ahead for the inevitable. The work is titled Improving Palliative Care with Deep Learning, and it’s currently available online.\\n\\n“Improving the quality of end-of-life care for hospitalized patients is a priority for healthcare organizations,” the researchers write. “Studies have shown that physicians tend to over-estimate prognoses, which in combination with treatment inertia results in a mismatch between patients wishes and actual care at the end of life.”\\n\\nInaccurate predictions from doctors can create unwanted consequences, such as patients spending their final days in a hospital ward rather than at home, surrounded by loved ones. By removing the human predictive element from the equation — which may be influenced by emotion and optimism — the algorithm takes a calculated look at the data and makes a call based on facts. The result is a much more accurate prediction of death that allows the patient and their family to plan for the best, most comfortable send-off.\\n\\nThe algorithm the researchers developed has been trained on over two million patient records. Using that data the AI can spot patterns that are simply invisible to a single doctor, and by drawing from those patterns it can make an educated guess as to when an ill patient will ultimately pass. When the algorithm is used to predict the passing of patients who are in palliative care (meaning they are not expected to live longer than 12 months), the system forecasts death with an astonishing 90 percent accuracy.\\n\\nThe way the AI goes about its predictions is quite complex, and while the researchers know how the system digests the data it’s being fed, the algorithm doesn’t have a simple way of explaining how it reaches its conclusions. Despite that, the digital brain is accurate enough in its predictions that the “how” is a secondary concern.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Announcing TensorRT integration with TensorFlow 1.7', 'Today we are announcing integration of NVIDIA® TensorRTTM and TensorFlow. TensorRT is a library that optimizes deep learning models for inference and creates a runtime for deployment on GPUs in production environments. It brings a number of FP16 and INT8 optimizations to TensorFlow and automatically selects platform specific kernels to maximize throughput and minimizes latency during inference on GPUs. We are excited about the new integrated workflow as it simplifies the path to use TensorRT from within TensorFlow with world-class performance. In our tests, we found that ResNet-50 performed 8x faster under 7 ms latency with the TensorFlow-TensorRT integration using NVIDIA Volta Tensor Cores as compared with running TensorFlow only.\\n\\nSub-Graph Optimizations within TensorFlow\\n\\nNow in TensorFlow 1.7, TensorRT optimizes compatible sub-graphs and let\\'s TensorFlow execute the rest. This approach makes it possible to rapidly develop models with the extensive TensorFlow feature set while getting powerful optimizations with TensorRT when performing inference. If you were already using TensorRT with TensorFlow models, you know that certain unsupported TensorFlow layers had to be imported manually, which in some cases could be time consuming.\\n\\nFrom a workflow perspective, you need to ask TensorRT to optimize TensorFlow\\'s sub-graphs and replace each subgraph with a TensorRT optimized node. The output of this step is a frozen graph that can then be used in TensorFlow as before.\\n\\nDuring inference, TensorFlow executes the graph for all supported areas, and calls TensorRT to execute TensorRT optimized nodes. As an example, if your graph has 3 segments, A, B and C. Segment B is optimized by TensorRT and replaced by a single node. During inference, TensorFlow executes A, then calls TensorRT to execute B, and then TensorFlow executes C.\\n\\nThe newly added TensorFlow API to optimize TensorRT takes the frozen TensorFlow graph, applies optimizations to sub-graphs and sends back to TensorFlow a TensorRT inference graph with optimizations applied. See the code below as an example.\\n\\n# Reserve memory for TensorRT inference engine gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = number_between_0_and_1) ... trt_graph = trt.create_inference_graph( input_graph_def = frozen_graph_def, outputs = output_node_name, max_batch_size=batch_size, max_workspace_size_bytes=workspace_size, precision_mode=precision) # Get optimized graph\\n\\nThe per_process_gpu_memory_fraction parameter defines the fraction of GPU memory that TensorFlow is allowed to use, the remaining being available for TensorRT. This parameter should be set the first time the TensorFlow-TensorRT process is started. As an example, a value of 0.67 would allocate 67% of GPU memory for TensorFlow and the remaining 33 % for TensorRT engines.\\n\\nThe create_inference_graph function takes a frozen TensorFlow graph and returns an optimized graph with TensorRT nodes. Let\\'s look at the function\\'s parameters:\\n\\ninput_graph_def: frozen TensorFlow graph\\n\\nfrozen TensorFlow graph outputs: list of strings with names of output nodes e.g. [\"resnet_v1_50/predictions/Reshape_1\"]\\n\\nlist of strings with names of output nodes e.g. max_batch_size: integer, size of input batch e.g. 16\\n\\ninteger, size of input batch e.g. 16 max_workspace_size_bytes: integer, maximum GPU memory size available for TensorRT\\n\\ninteger, maximum GPU memory size available for TensorRT precision_mode : string, allowed values \"FP32\", \"FP16\" or \"INT8\"\\n\\nAs an example, if the GPU has 12GB memory, in order to allocate ~4GB for TensorRT engines, set the per_process_gpu_memory_fraction parameter to ( 12 - 4 ) / 12 = 0.67 and the max_workspace_size_bytes parameter to 4000000000.\\n\\nLets apply the new API to ResNet-50 and see what the optimized model looks like in TensorBoard. The complete code to run the example is available in . The image on the left is ResNet-50 without TensorRT optimizations and the right image is after. In this case, most of the graph gets optimized by TensorRT and replaced by a single node (highlighted).\\n\\nOptimized INT8 Inference performance\\n\\nTensorRT provides capabilities to take models trained in single (FP32) and half (FP16) precision and convert them for deployment with INT8 quantizations at reduced precision with minimal accuracy loss. INT8 models compute faster and place lower requirements on bandwidth but present a challenge in representing weights and activations of neural networks because of the reduced dynamic range available.\\n\\nDynamic Range Minimum Positive Value FP32 -3.4×1038 ~ +3.4×1038 1.4 × 10−45 FP16 65504 ~ +65504 5.96 x 10-8 INT8 -128 ~ +127 1\\n\\nTo address this, TensorRT uses a calibration process that minimizes the information loss when approximating the FP32 network with a limited 8-bit integer representation. With the new integration, after optimizing the TensorFlow graph with TensorRT, you can pass the graph to TensorRT for calibration as below.\\n\\ntrt_graph=trt.calib_graph_to_infer_graph(calibGraph)\\n\\nThe rest of the inference workflow remains unchanged from above. The output of this step is a frozen graph that is executed by TensorFlow as described earlier.\\n\\nAutomatically use Tensor Cores on NVIDIA Volta GPUs\\n\\nTensorRT runs half precision TensorFlow models on Tensor Cores in VOLTA GPUs for inference. Tensor Cores, provide 8x more throughput than single precision math pipelines. Half precision (also known as FP16) data compared to higher precision FP32 vs FP64 reduces memory usage of the neural network. This allows training and deployment of larger networks, and FP16 data transfers take less time than FP32 or FP64 transfers.\\n\\nEach Tensor Core performs D = A x B + C, where A, B, C and D are matrices. A and B are half-precision 4x4 matrices, whereas D and C can be either half or single precision 4x4 matrices. The peak performance of Tensor Cores on the V100 is about an order of magnitude (10x) faster than double precision (FP64) and about 4 times faster than single precision (FP32).\\n\\nAvailability\\n\\nWe are excited about this release and will continue to work closely with NVIDIA to enhance this integration. We expect the new solutions ensure the highest performance possible while maintaining the ease and flexibility of TensorFlow. And as TensorRT supports more networks, you will automatically benefit from the updates without any changes to your code.\\n\\nTo get the new solution, you can use the standard pip install process once TensorFlow 1.7 is released:\\n\\npip install tensorflow-gpu r1.7\\n\\nTill then, find detailed installation instructions here: https://github.com/tensorflow/tensorflow/tree/r1.7/tensorflow/contrib/tensorrt\\n\\nTry it out and let us know what you think!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Highlights from TensorFlow Dev Summit 2018', \"On March 30th, we held the second TensorFlow Developer Summit at the Computer History Museum in Mountain View, CA! The event brought together over 500 TensorFlow users in-person and thousands tuning into the livestream at TensorFlow events around the world. The day was filled with new product announcements along with technical talks from the TensorFlow team and guest speakers. Here are the highlights from the event:\\n\\nMachine learning is solving challenging problems that impact everyone around the world. Problems that we thought were impossible or too complex to solve are now possible with this technology. Using TensorFlow, we've already seen great advancements in many different fields. For example:\\n\\nAstrophysicists are using TensorFlow to analyze large amounts of data from the Kepler mission to discover new planets.\\n\\nMedical researchers are using ML techniques with TensorFlow to assess a person's cardiovascular risk of a heart attack and stroke.\\n\\nAir Traffic Controllers are using TensorFlow to predict flight routes through crowded airspace for safe and efficient landings.\\n\\nEngineers are using TensorFlow to analyze auditory data in the rainforest to detect logging trucks and other illegal activities.\\n\\nScientists in Africa are using TensorFlow to detect diseases in Cassava plants to improving yield for farmers.\\n\\nWe're excited to see these amazing uses of TensorFlow and are committed to making it accessible to more developers. This is why we're pleased to announce new updates to TensorFlow that will help improve the developer experience!\\n\\nWe're making TensorFlow easier to use\\n\\nResearchers and developers want a simpler way of using TensorFlow. We're integrating a more intuitive programming model for Python developers called eager execution that removes the distinction between the construction and execution of computational graphs. You can develop with eager execution and then use the same code to generate the equivalent graph for training at scale using the Estimator high-level API. We're also announcing a new method for running Estimator models on multiple GPUs on a single machine. This allows developers to quickly scale their models with minimal code changes.\\n\\nAs machine learning models become more abundant and complex, we want to make it easier for developers to share, reuse, and debug them. To help developers share and reuse models, we're announcing TensorFlow Hub, a library built to foster the publication and discovery of modules (self-contained pieces of TensorFlow graph) that can be reused across similar tasks. Modules contain weights that have been pre-trained on large datasets, and may be retrained and used in your own applications. By reusing a module, a developer can train a model using a smaller dataset, improve generalization, or simply speed up training. To make debugging models easier, we're also releasing a new interactive graphical debugger plug-in as part of the TensorBoard visualization tool that helps you inspect and step through internal nodes of a computation graph in real-time.\\n\\nModel training is only one part of the machine learning process and developers need a solution that works end-to-end to build real-world ML systems. Towards this end, we're announcing the roadmap for TensorFlow Extended (TFX) along with the launch of TensorFlow Model Analysis, an open-source library that combines the power of TensorFlow and Apache Beam to compute and visualize evaluation metrics. The components of TFX that have been released thus far (including TensorFlow Model Analysis, TensorFlow Transform, Estimators, and TensorFlow Serving) are well integrated and let developers prepare data, train, validate, and deploy TensorFlow models in production.\\n\\nTensorFlow is available in more languages and platforms\\n\\nAlong with making TensorFlow easier to use, we're announcing that developers can use TensorFlow in new languages. TensorFlow.js is a new ML framework for JavaScript developers. Machine learning in the browser using TensorFlow.js opens exciting new possibilities, including interactive ML and support for scenarios where all data remains client-side. It can be used to build and train modules entirely in the browser, as well as import TensorFlow and Keras models trained offline for inference using WebGL acceleration. The Emoji Scavenger Hunt game is a fun example of an application built using TensorFlow.js.\\n\\nWe also have some exciting news for Swift programmers: TensorFlow for Swift will be open sourced this April. TensorFlow for Swift is not your typical language binding for TensorFlow. It integrates first-class compiler and language support, providing the full power of graphs with the usability of eager execution. The project is still in development, with more updates coming soon!\\n\\nWe're also sharing the latest updates to TensorFlow Lite, TensorFlow's lightweight, cross-platform solution for deploying trained ML models on mobile and other edge devices. In addition to existing support for Android and iOS, we're announcing support for Raspberry Pi, increased support for ops/models (including custom ops), and describing how developers can easily use TensorFlow Lite in their own apps. The TensorFlow Lite core interpreter is now only 75KB in size (vs 1.1 MB for TensorFlow) and we're seeing speedups of up to 3x when running quantized image classification models on TensorFlow Lite vs. TensorFlow.\\n\\nFor hardware support, TensorFlow now has integration with NVIDIA's TensorRT. TensorRT is a library that optimizes deep learning models for inference and creates a runtime for deployment on GPUs in production environments. It brings a number of optimizations to TensorFlow and automatically selects platform specific kernels to maximize throughput and minimizes latency during inference on GPUs.\\n\\nFor users who run TensorFlow on CPUs, our partnership with Intel has delivered integration with a highly optimized Intel MKL-DNN open source library for deep learning. When using Intel MKL-DNN, we observed up to 3x inference speedup on various Intel CPU platforms.\\n\\nThe list of platforms that run TensorFlow has grown to include Cloud TPUs, which were released in beta last month. The Google Cloud TPU team has already delivered a strong 1.6X performance increase in ResNet-50 performance since launch. These improvements will be available to TensorFlow users with the 1.8 release soon.\\n\\nEnabling new applications and domains using TensorFlow\\n\\nMany data analysis problems are solved using statistical and probabilistic methods. Beyond deep learning and neural network models, TensorFlow now provides state-of-the-art methods for Bayesian analysis via the TensorFlow Probability API. This library contains building blocks like probability distributions, sampling methods, and new metrics and losses. Many other classical ML methods also have increased support. As an example, boosted decision trees can be easily trained and deployed using pre-made high-level classes.\\n\\nMachine learning and TensorFlow have already helped solve challenging problems in many different fields. Another area where we see TensorFlow having a big impact is in genomics, which is why we're releasing Nucleus, a library for reading, writing, and filtering common genomics file formats for use in TensorFlow. This, along with DeepVariant, an open-source TensorFlow based tool for genome variant discovery, will help spur new research and advances in genomics.\\n\\nExpanding community resources and engagement\\n\\nThese updates to TensorFlow aim to benefit and grow the community of users and contributors - the thousands of people who play a part in making TensorFlow one of the most popular ML frameworks in the world. To continue to engage with the community and stay up-to-date with TensorFlow, we've launched the new official TensorFlow blog and the TensorFlow YouTube channel. We're also making it easier for our community to collaborate by launching new mailing lists and Special Interest Groups designed to support open-source work on specific projects. To see how you can be a part of the community, visit the TensorFlow Community page and as always, you can follow TensorFlow on Twitter for the latest news.\\n\\nWe're incredibly thankful to everyone who has helped make TensorFlow a successful ML framework in the past two years. Thanks for attending, thanks for watching, and remember to use #MadeWithTensorFlow to share how you are solving impactful and challenging problems with machine learning and TensorFlow!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('How AI is transforming healthcare', 'It’s fair to say that artificial intelligence has become a part of our daily lives. The average person doesn’t think twice about letting an algorithm provide music recommendations or directions on how to get from A to B.\\n\\nBut what about health advice, or even medical treatment?\\n\\nAI and robotics present huge opportunities for the pharma and healthcare industries. According to McKinsey, big data strategies could save the US healthcare system up to $100 billion a year thanks to AI-assisted efficiencies in trials, research, and clinical practice.\\n\\nSo, how is AI being utilised, and will the general public embrace it? Here’s more on the story so far.\\n\\nStreamlining medical practice\\n\\nImage classification – which refers to the process of extracting information from multiple images – has so far been one of the main uses of AI and deep learning within healthcare. While the practice does not replace doctors, it essentially makes their jobs much quicker and easier (and lessens the chances of human error).\\n\\nFor example, image classification can help radiologists file and mark low priority X-Rays, taking away the need for them to spend extra time filling in lengthy paperwork.\\n\\nThe technology can aid diagnosis, too. Earlier this year the BBC reported how researchers at an Oxford hospital have developed AI that can diagnose scans for heart disease and lung cancer. One in five cardiologists are currently said to miss problems detected by the timing of the heartbeat. The AI system will be able to pick up on details that doctors tend not to see, also providing recommendations based on the patient’s level of risk.\\n\\nIn the US, AI can also help primary care doctors find and refer patients to specialists – a process that can otherwise be so time-consuming and convoluted that patients often fail to attend eventual appointments. Now, AI systems like Human Dx (Human Diagnosis Project) can create fast, accurate, and actionable insights for doctors and patients alike.\\n\\nWe’re working to build an online system that maps the steps to help any patient and enables more accurate, affordable, and accessible #CareForAll. Join us in building the future of medicine: https://t.co/ry1sFHDoit — The Human Diagnosis Project (@human_dx) February 8, 2018\\n\\nMaking preventative care accessible\\n\\nThe general public might remain largely unaware of how AI can aid care in hospitals and doctors\\' offices. However, the technology is becoming more mainstream in other ways - mainly through technology that some consumers already own.\\n\\nLast year, a study found that wearables like the Apple Watch and Fitbit are able to accurately detect serious conditions like hypertension and sleep apnea.\\n\\nChatbots are another big opportunity for preventative care, with a number of examples already enabling users to provide advice and even early diagnosis via digital channels. Your.Md and HealthTap are two of the biggest – both using natural language processing to understand common symptoms and help the user come to a conclusion about what might be wrong.\\n\\nWhen it comes to health, the big question is whether users will feel comfortable speaking with a bot. Perhaps thanks to our existing reliance on the internet for information (regardless of how questionable) – acceptance is actually quite high. A report by Reform states that 47% of UK survey respondents would be willing to use an ‘intelligent healthcare’ assistant via a smartphone, tablet, or personal computer, with higher rates amongst younger generations.\\n\\nWith health and exercise-focused apps already proving hugely popular (and users displaying ingrained trust in these brands), perhaps it won’t be long before we see AI being integrated into the likes of Headspace or Nike+ Training Club.\\n\\nHelping mental health and wellbeing\\n\\nAnother area of opportunity might be in areas that people often find difficult talking about face to face. In this sense, chatbots and digital assistants could potentially even increase accessibility as well as the amount of people seeking help.\\n\\nIeso, which is an app that offers cognitive behavioural therapy for managing mental health, is said to have helped over 17,000 people since its launch. Meanwhile, there is also evidence that it has reduced treatment time by 50%. It’s clear that, alongside patients, there are huge benefits for the NHS, with AI assistants helping to take away massive strain on the service.\\n\\n\"When you are anxious or depressed, you may well have a critical voice that tells you negative things about yourself. Be aware of this; notice the way that makes you feel & how it affects your behaviour. Know that you can choose to do something different.\" #BlueMonday pic.twitter.com/7WxoJmEDTr — Ieso Digital Health (@IesoHealth) January 15, 2018\\n\\nMore recently, Woebot, an app designed to help people cope with feelings of depression and anxiety, was launched on the app store. The idea is that - unlike professional treatment, which can only be sought at a specific time or date – users can interact and access support at any time of day or night.\\n\\nAlongside preventative care, AI could also help to detect and diagnose mental health conditions. IBM researchers have discovered that machine learning can predict the risk of developing psychosis. By analysing the speech patterns of 59 individuals, it predicted with 89% accuracy which patients would go on to develop a psychotic disorder, as well as detect those who had recently developed psychosis.\\n\\nBarriers to overcome\\n\\nWhile it’s clear that AI is having a big impact, there are still big barriers to overcome before the technology truly transforms the way we access healthcare.\\n\\nAcceptance of AI to help general wellbeing might be relatively high, however this dramatically falls when it comes to more serious or sensitive issues. Reform suggests that 37% of UK survey respondents say they would use AI to monitor a heart condition, while just 3% say they would use it to monitor pregnancy.\\n\\nAnother area that needs careful consideration is UX. For both doctors and patients, technology needs to be easy to use and intuitive – especially for busy staff who do not have the time or existing skill-set to learn a complicated system.\\n\\nLastly, data remains a big issue, with access to good quality data vital in order for the technology to produce accurate results. This comes down to public trust in permitting their data to be used in the first place, as well as granting professionals the right access in the right format.\\n\\nWith the NHS pledging to go paperless by 2020, AI initiatives might be on the back burner while the system struggles with more basic digitisation. However, with benefits that include reducing busy workloads, streamlining processes, and improving accessibility, it’s clear why private pharma companies and start-ups are keen to invest as soon as possible.\\n\\nRelated reading:')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('What most people get wrong about ML and AI', 'A couple of weeks ago I was invited to an investment dinner where 3 startups made their pitch in front of a panel of investors. The event turned out great and the startups that participated were well prepared to answer all of our questions, technical or otherwise. One thing bothered me, perhaps more than it should. Two out of the three startups boasted about their “amazing” AI even though they were merely a run of the mill machine learning script that was interpreting a set of data based on a predefined algorithm.\\n\\nMost people don’t understand the difference between the two, some realize there is a difference but can’t really point it out and there are those that understand the relationship between Artificial Intelligence (AI), machine learning (ML) and Deep Learning(DL – to be discussed at a later date).\\n\\nTo put things simply, machine learning is a technique used to achieve artificial intelligence. Arthur Samuel coined the AI term and defined it as “the ability to learn without being explicitly programmed.” Without Machine learning, it would be a lot harder to create an AI, you’d have to write complex rules and decision trees that would take a lot of programming so instead, we use machine learning to train our AI friend to learn on its own by feeding him large amounts of data.\\n\\nOn the other hand, machine learning is an algorithm used to process large amounts of data and act on the result. Deep learning is one of many ways of approaching machine learning, but we’ll get to that at a later date.\\n\\nTo say artificial intelligence equals machine learning is just juvenile, plain and simple. True, ML is a component of AI but learning the difference is important.\\n\\nLet me paint a scenario that would make more sense. Let’s imagine going to the market to buy your favorite apples. You get to the table where the lady usually sells them to you and you ask for for a couple of apples but in return, you get two apple seeds. She then proceeds to explain that you can use the apple seeds to eventually get apples but technically it’s not the same thing. Right? The same principle applies to machine learning vis-a-vis to artificial intelligence. ML is a means of achieving AI, not the other way around. AI can exist without ML by writing an immense code that would guide the computer into making decisions by itself.\\n\\nIn this video, Reza Zadah clarifies the difference between artificial intelligence and machine learning, and what role algorithms play in these fields.\\n\\nI strongly believe AI will paint our future and while technology is not quite there yet to make ai available to everyone, it will be in a few short years and it’s important to educate ourselves and learn the possibilities it can provide and explore the ways it cna improve our lives.\\n\\nFor the past few years, I’ve seen a lot of companies talking a big game when it comes to machine learning and artificial intelligence and I think it’s imperative to understand the difference between the two so we can stop wasting time debating articles like this one and focus on creating solutions that will benefit us all. Thankfully we have companies like www.typingdna.com, www.groupbyinc.com, and www.baidu.com that push the envelope further with amazing solutions that promise to change our daily routines for the better.')\n",
      "('An English-Language, Algorithmically-Personalized News Aggregator, Based In China -- What Could Possibly Go Wrong?', 'Techdirt has been exploring the important questions raised by so-called \"fake news\" for some time. A new player in the field of news aggregation brings with it some novel issues. It\\'s called TopBuzz, and it comes from the Chinese company Toutiao, whose rapid rise is placing it alongside the country\\'s more familiar \"BAT\" Internet giants -- Baidu, Alibaba and Tencent. It\\'s currently expanding its portfolio in the West: recently it bought the popular social video app Musical.ly for about $800 million:\\n\\nToutiao aggregates news and videos from hundreds of media outlets and has become one of the world\\'s largest news services in the span of five years. Its parent company [Bytedance] was valued at more than $20 billion, according to a person familiar with the matter, on par with Elon Musk\\'s SpaceX. Started by Zhang Yiming, it\\'s on track to pull in about $2.5 billion in revenue this year, largely from advertising.\\n\\nAn in-depth analysis of the company on Ycombinator\\'s site explains what makes this aggregator so successful, and why it\\'s unlike other social networks offering customized newsfeeds based on what your friends are reading:\\n\\nToutiao, one of the flagship products of Bytedance, may be the largest app you’ve never heard of -- it\\'s like every news feed you read, YouTube, and TechMeme in one. Over 120M people in China use it each day. Yet what\\'s most interesting about Toutiao isn\\'t that people consume such varied content all in one place... it\\'s how Toutiao serves it up. Without any explicit user inputs, social graph, or product purchase history to rely on, Toutiao offers a personalized, high quality-content feed for each user that is powered by machine and deep learning algorithms.\\n\\nHowever, as people are coming to appreciate, over-dependence on algorithmic personalization can lead to a rapid proliferation of \"fake news\" stories. A post about TopBuzz on the Technode site suggests this could be a problem for the Chinese service:\\n\\nWhat\\'s been my experience? Well, simply put, it\\'s been a consistent and reliable multi-course meal of just about every variety of fake news.\\n\\nThe post goes on to list some of the choice stories that TopBuzz\\'s AI thought were worth serving up:\\n\\nRoy Moore Sweeps Alabama Election to Win Senate Seat Yoko Ono: \"I Had An Affair With Hillary Clinton in the \\'70s\" John McCain\\'s Legacy is DEMOLISHED Overnight As Alarming Scandals Leak Julia Roberts Claims \\'Michelle Obama Isn\\'t Fit To Clean Melania\\'s Toilet\\'\\n\\nThe post notes that Bytedance is aware of the problem of blatantly false stories in its feeds, and the company claims to be using both its artificial intelligence tools as well as user reports to weed them out. It says that \"when the system identifies any fake content that has been posted on its platform, it will notify all who have read it that they had read something fake.\" But:\\n\\nthis is far from my experience with TopBuzz. Although I receive news that is verifiably fake on a near-daily basis, often in the form of push notifications, I have never once received a notification from the app informing me that Roy Moore is in fact not the new junior senator from Alabama, or that Hillary Clinton was actually not Yoko Ono\\'s sidepiece when she was married to John Lennon.\\n\\nThe use of highly-automated systems, running on server farms in China, represents new challenges beyond those encountered so far with Facebook and similar social media, where context and curation are being used to an increasing degree to mitigate the potential harm of algorithmic newsfeeds. The fact that a service like TopBuzz is provided by systems outside the control of the US or other Western jurisdictions poses additional problems. As deep-pocketed Chinese Internet companies seek to expand outside their home markets, bringing with them their own approaches and legal frameworks, we can expect these kind of issues to become increasingly thorny. We are also likely to see those same services begin to wrestle with some of the same problems currently being tackled in the West.\\n\\nFollow me @glynmoody on Twitter or identi.ca, and +glynmoody on Google+')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Open Call: Institute of Patent Infringement', 'As part of the extended programme of WORK, BODY, LEISURE, the theme of this year Dutch Pavilion at the Venice Biennale, The Institute of Patent Infringement is concerned with the existing legal infrastructure that allows ‘Big Tech’ a strangle-hold on questions surrounding automation, both today and in the future.\\n\\nAs we go through an ‘AI spring’ we’ve seen a gold rush to patent radically new forms of automated environments, driven through advances in deep learning and combined with increases in big data, machine-learning algorithms, computer processing power and cloud technology\\n\\nYet, while companies like Microsoft or Apple tinker with endless patent variations on consumer products, it’s Amazon, with their own brands of automated futurism, which seem intent on merging processes of machine learning with principles of spatial organisation.\\n\\nSince 2010, Amazon Technologies Inc. has filed 5,860 patents that range from the seemingly banal to the resolutely absurd. Illustrated by dry line drawings these patents provide a glimpse and representation of the automated future Amazon aim to create.\\n\\nThe implications of this are broad. Amazon look set to define future typologies, bypassing the input of traditional professions. To take an example, management modules indicated in Amazon’s patents, can now map space more effectively than a surveyor, produce floor layouts to be more efficient than an architect and oversee retail facilities more productively than a retail manager.\\n\\nAn obsession with efficiency has further led to the quantifiable worker, seen through countless patents for technology that monitors and evaluate workers. But the scope of the quantified body goes far beyond this, and as patents for human RFID tags suggest, Amazon are equally at home with the technology transferred to the general public. Put another way, Amazon’s broad ambitions, seen through their patents, affect us both as practitioners and also as citizens.\\n\\nOpen Call\\n\\nTo negate this top-down and closed system, The Institute of Patent Infringement thus invites submissions from students, industrial designers, architects, urban planners, artists, programmers and the wider public to merge, reimagine, infringe and hack existing Amazon patents.\\n\\nThe crux of the open call is to emphasise the radical and emancipatory potential inherent in these new technologies assembled by Amazon. To reveal this potential, submissions may chose to challenge: the hyper individualised and consumption based nature of Amazon’s wider patent filings; the emphasis on efficiency and quantification through data collection inherent in these new technological regimes; labour, social relations and the role of automation within this; the relationship with nature and the environment; unequal global processes of production and distribution; and the affect of these technologies on everyday life.\\n\\nThis may include ideas working at scales from the body to the planetary and from the rural to the urban. Relevant themes may ask: can patents for the wearable monitoring of workers be appropriated to hack and monitor nature? How could Amazon’s global distribution network be rethought? Can we think of new ideas to repurpose data centres, potentially merging these with other functions? What might arise from the collective ownership and control of data?\\n\\nCould fully automated warehouses be refunctioned as spaces of infinite leisure? What role does labour play in this new world? Can smart road management systems for automated cars be used for an extensive and sustainable public transport network? How could Amazon’s quest for algorithmic efficiency, be used to plan a zero growth, zero carbon economy? And how might these technologies work if bottom up and participatory rather than top-down and monopolised?\\n\\n\\n\\nFormat of Submission\\n\\n- A4 sheets (maximum 3 portrait)\\n\\n- Title\\n\\n- A five digit numerical code (any five numbers used to identify your submission)\\n\\n- A description of the proposal including what patents are being used or merged (300 words maximum)\\n\\n- Black and white line drawings of proposal (a maximum of 5 drawings, see below for rules on patent illustrations)\\n\\n\\n\\nSubmission\\n\\n- Submission is open to individuals or groups of architects, urban planners, designers, artists, programmers, students and anyone interested in the wider theme of the open call.\\n\\n- Deadline for submissions is midnight (GMT) 16th April\\n\\n- Submissions should be sent to submission@institute-of-patent-infringement.org as a single PDF file in the format title_ submission-number.pdf\\n\\n- Selected successful submissions will be informed of the results by the end of April.\\n\\n- Any questions related to the open call can be sent to contact@institute-of-patent-infringement.org\\n\\nSelection Process\\n\\nAll submissions will be evaluated anomalously with the most relevant to the open call selected for display in the Dutch Pavillion as part of WORK, BODY, LEISURE at this year’s Venice Biennale.\\n\\nA wider selection of proposals will be displayed through exhibitions at Het Niuwe Instiuttut in Rotterdam and London during Autumn 2018.\\n\\nSelection will be through Het Nieuwe Instituut’s research team and the Institute of Patent Infringement.\\n\\nPatents\\n\\nA collection of existing Amazon patents drawings with a brief description is available to download as a pdf document from our website along with guidelines for patent illustration.')\n",
      "('At GDC 2018, see how Valve uses deep learning to fight cheating in CS:GO!', 'When you arrive at the Game Developers Conference in San Francisco this March you\\'re going to be confronted with a smorgasbord of fantastic talks, so today organizers want to quickly highlight one you won\\'t want to miss!\\n\\nIt\\'s a remarkable entry in the Programming track of talks at GDC: a presentation from Valve\\'s John McDonald about how the company has been using deep learning methods to try and combat cheating in its competitive team-based shooter Counter-Strike: Global Offensive.\\n\\nFittingly titled \"Robocalypse Now: Using Deep Learning to Combat Cheating in \\'Counter-Strike: Global Offensive\\'\", McDonald\\'s talk aims to provide you with total system details, from the high level server architecture to the low level features fed into the AI.\\n\\nDeep learning has proven to be very effective at identifying cheating behavior without any client-side instrumentation, according to Valve, making it robust against malicious attack by cheaters and cheat vendors. By retraining regularly, the network continues to evolve, picking up new cheating behaviors within hours of their appearance.\\n\\nAs a result of this approach, certain types of cheats have reportedly been reduced by a factor of one hundred -- and McDonald\\'s GDC talk will show you exactly how it works!\\n\\nYou won\\'t want to miss it, as well as the many more GDC 2018 announcements we\\'ll be making in the coming months! For more information about GDC 2018 head over to the show\\'s official website, and subscribe to regular updates via Facebook, Twitter, or RSS.\\n\\n\\n\\nGamasutra and GDC are sibling organizations under parent UBM Americas')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Nvidia, Unity, Microsoft and more sponsor innovative talks at GDC 2018', 'We\\'re a month away from the 2018 Game Developers Conference, and organizers want to ensure you don\\'t miss the variety of cutting-edge sponsored sessions from Microsoft, Nvidia, Unity, and more.\\n\\nYou\\'ll want to bookmark these talks if you\\'re looking for first-hand insight into how the latest hardware and software works -- and how you can use it to make great games.\\n\\nNvidia, for example, is sponsoring a plethora of fascinating talks, including \"Deep Learning for Game Developers\", in which Nvidia\\'s Bryan Catanzaro will discuss research on some of the most interesting new ways to apply deep learning to problems such as content creation, graphics, text, and speech. This talk will give you a sense of the current capabilities of deep learning, as well as where the field is headed!\\n\\nIf you\\'re looking for something more transformative, consider Unity\\'s sponsored session on \"A Data-Oriented Approach to Parallelizing Hierarchical Transforms.\" Presented by longtime game dev and Unity principal engineer Mike Acton, this talk will arm you with a method to examine your own hierarchical transformations to redesign them using data-oriented principles, in addition to specific concrete data layout examples which you can also to improve your own systems.\\n\\nAnd in this age of live games, you won\\'t want to skip Microsoft\\'s sponsored \"Azure Partner Presentation: Intelligent Backend for LiveOps\" session. The presentation aims to demonstrate how successful game studios have embraced the promise of LiveOps to extend a game\\'s life after launch by developing relationships with players through offering frequent updates, in-game events, and promotional offers.\\n\\nExpect to learn how thousands of game developers have boosted engagement and monetization for millions of active players a month while cutting costs, lowering risk, and speeding up time to market!\\n\\nFor more details on them and all other announced talks head over to the online GDC 2018 Session Scheduler.\\n\\nFor more information about GDC 2018, which runs March 19th through the 23rd, visit the show\\'s official website, and subscribe to regular updates via Facebook, Twitter, or RSS.\\n\\n\\n\\nGamasutra and GDC are sibling organizations under parent UBM Americas')\n",
      "(\"GDC 2018 is next week! Follow along on Gamasutra's official event page\", 'The 2018 Game Developers Conference is just days away, and that means now is the right time for attendees to brush up on all the ways you can meet interesting new people at the show!')\n",
      "('Get a job: Be a Senior Software Engineer at Visual Concepts', \"The Gamasutra Job Board is the most diverse, active and established board of its kind for the video game industry!\\n\\n\\n\\nHere is just one of the many, many positions being advertised right now.\\n\\nLocation: Novato, California\\n\\nVisual Concepts is one of the world’s top game development studios with a flat, entrepreneurial, and non-corporate work environment. We have a proven track record having shipped over 100 multi-SKU titles to great critical acclaim.\\n\\nOur studios in Novato, CA; Agoura Hills, CA; Orange County, CA; Budapest, Hungary; Shanghai, China; and Seoul, South Korea, are committed to gaming and technical innovation and offer top candidates the opportunity to learn and grow with some of the smartest and most creative minds in the industry.\\n\\nWe’re seeking a Senior Software Engineer to focus on AI and gameplay development.\\n\\nRequirements:\\n\\n3+ years of experience in AI and gameplay development for console games Shipped at least 1 commercial game release as an AI Engineer\\n\\nExperience designing and implementing an AI decision framework\\n\\nStrong knowledge of advanced AI concepts such as deep learning, patterns, and pathing\\n\\nExperience developing animation tools and an understanding of animation theories (inverse kinematics, state machines, blend trees, etc.)\\n\\nFluent in C++\\n\\nSuperb physics, 3D math, and algebra skills\\n\\nAble to write clean, bug-free, well-documented, and efficient code\\n\\nBachelor’s or Master’s Degree in Computer Science\\n\\nExcellent communication skills and a good team player\\n\\nPassion for video games and ideally, sports games\\n\\nInterested? Apply now.\\n\\nAbout the Gamasutra Job Board\\n\\nWhether you're just starting out, looking for something new, or just seeing what's out there, the Gamasutra Job Board is the place where game developers move ahead in their careers.\\n\\n\\n\\nGamasutra's Job Board is the most diverse, most active, and most established board of its kind in the video game industry, serving companies of all sizes, from indie to triple-A.\\n\\n\\n\\nLooking for a new job? Get started here. Are you a recruiter looking for talent? Post jobs here.\")\n",
      "(\"Don't miss all the cool machine learning talks at GDC this year!\", 'Amid all the hustle and bustle of the 2018 Game Developers Conference in San Francisco this month, make sure you don\\'t overlook one of the most intriguing tech topics being discussed at the show: how top devs are applying machine learning techniques to make better games.\\n\\nFor example, Naughty Dog technical art director Andrew Maximov is hosting a multi-day roundtable dedicated entirely to the art and business of machine learning in game dev.\\n\\nCome by to engage with (or listen in on) the lively discussion about how deep learning based artificial intelligence is going to affect video game production in the years to come. A wide swathe of topics wll be covered, including the use of neural networks in game content generation, their value at runtime, and how you can integrate this technology into your creative process.\\n\\nAlso, Ubisoft Montreal animation researcher Daniel Holden will be delivering a presentation on \"Character Control with Neural Networks and Machine Learning\" that promises to be fascinating. Using state of the art research into character animation and recent prototypes developed at Ubisoft\\'s La Forge, this talk will show how many different kinds of data-driven systems can vastly reduce the complexity and man-power involved in building an animation system, while simultaneously allowing for a larger variety of high quality and interesting motion to be produced.\\n\\nAs well as the numerous advantages, these approaches come with their own pitfalls and difficulties, many of which will also be covered in this practical and deep dive into machine learning for character animation.\\n\\nAnd as part of the Community Management Summit, Showdown.cc\\'s Alex Casassovici and Beatriz Miravete will be showing you how to apply machine learning techniques to community management in their talk on \"How to Use Machine Learning, Live Telemetry Analysis, and Computer Vision to Manage Communities.\"\\n\\nThe pair will share what Showdown\\'s technical team learned as they used machine learning, live telemetry analysis, and computer vision to truly understand player behaviors. They\\'ll also connect these findings to academic research conducted on community psychology in an effor to teach you how to build solid community feedback loops in your games. You won\\'t want to miss it!\\n\\nFor more details on these and all other announced talks head over to the online GDC 2018 Session Scheduler!\\n\\nAnd of course, for more information about GDC 2018 visit the show\\'s official website, and subscribe to regular updates via Facebook, Twitter, or RSS.\\n\\n\\n\\nGamasutra and GDC are sibling organizations under parent UBM Americas')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Get useful game dev tips from Unity, Amazon, and Autodesk at GDC 2018', \"If you're coming to the Game Developers Conference in San Francisco next month, organizers would like to quickly let you know about some of the great day-long sponsored developer days taking place at the show!\\n\\nThese developer day sessions take place during the first two days of GDC (Monday and Tuesday, March 19th and 20th) and offer you an opportunity to engage with and learn from some of the top players in the industry.\\n\\nFor example, Amazon is actually presenting two full days of talks this year. Both will span a wide gamut of interesting Amazon-related topics, including how to improve your game's reach with Amazon's Twitch platform, how to build voice-first games for the Amazon Alexa family of voice assistant devices, and how to use Amazon Web Services in everything from multiplayer game design to machine learning in game development. You won't want to miss it!\\n\\nAlso, game dev toolmaker Autodesk will have a full day dedicated to showing you how to better use its products and services. It starts on Tuesday with an opening presentation on where Autodesk is at in terms of 3D game development, and then continues on to offer deep dives into important issues like how best to use Autodesk's Shotgun for production management and what the company is doing to better integrate its offerings with the Unity game dev toolset.\\n\\nSpeaking of which, Unity is also hosting its own developer day on Tuesday which kicks off with a presentation on how to get the most out of Unity 2018's new rendering features, then segues into very focused, practical talks on everything from how to design great in-app purchases to the best way to build scalable multiplayer games with Unity and how you can use the toolset to employ deep learning techniques in your work. Don't skip it!\\n\\nFor more details on these sessions, as well as all the other great talks taking place at GDC 2018, head over to the conference Session Scheduler. For more information on GDC 2018, visit the show's official website, or subscribe to regular updates via Facebook, Twitter, or RSS.\\n\\n\\n\\nGamasutra and GDC are sibling organizations under parent UBM Americas\")\n",
      "('Get a job: Intel is hiring a Graphics/Software Engineer', \"The Gamasutra Job Board is the most diverse, active and established board of its kind for the video game industry!\\n\\n\\n\\nHere is just one of the many, many positions being advertised right now.\\n\\nLocation: Folsom, California\\n\\nVisual & Parallel Compute Group (VPG) mission is to create the New Visual and Parallel Compute Experience. We are part of the Intel's Visual Technologies Team (VTT).\\n\\nWhat We Do\\n\\nIn VPG we are focused on delivering Best-in-Class visual experiences that enable users to immerse themselves in a new visual future by innovating graphics, media and display IP technologies. We continuously innovate and delight our customers across the entire breadth of our offering – from low power battery operated devices, to PC’s and laptops, to data center blades.\\n\\nWithin VPG we have a Software group, come work with world-class graphics software engineers developing software for Intel's latest GPU/CPU architecture. From drivers to compilers across all OSs, our software team delivers Intel’s 3D graphics, media, display and compute GPU Technology.\\n\\nCareer opportunities:\\n\\nGraphics Software Engineers: Performance, Drivers, Graphics APIs\\n\\nGraphics Compiler Engineers: Compute, Media, 3D, Performance\\n\\nMedia Engineers: CODECs, Image Processing, Content Protection\\n\\nSoftware Engineers: Agile, Performance, Linux/Windows/MacOS\\n\\nThe ideal candidate should exhibit behavioral traits that indicate:\\n\\nSelf-motivator with strong problem solving skills\\n\\nExcellent interpersonal skills, including written and verbal communication\\n\\nAbility to work as a team and collaborate in a high-paced atmosphere\\n\\nQualifications\\n\\nYou must possess the below minimum qualifications to be initially considered for this position.\\n\\nEducation:\\n\\nBachelor’s with 3+, Master’s with 1+ with years of experience.\\n\\nDiscipline in: Computers Engineering/Science, Electrical Engineering, Math, Physics or related field.\\n\\nRelevant experience in the following:\\n\\nGraphics Concepts: 3D, Media, Imaging & Display\\n\\nGraphics APIs: OpenGL, DirectX, Metal, OpenCL\\n\\nHardware & Software Interfacing Skills\\n\\nC, C++ Scripting and Debugging: User & Kernel-mode Software\\n\\nMachine Learning, Deep Learning, or Computer Vision Algorithms\\n\\nSimulation or Projection Modeling for Performance and Power\\n\\nMedia & Display Standards: HEVC, VP9, AVC, HDR, DisplayPort, HDMI, etc.\\n\\nOther Locations\\n\\nUS, California, Santa Clara; US, Oregon, Hillsboro;\\n\\nPosting Statement. Intel prohibits discrimination based on race, color, religion, gender, national origin, age, disability, veteran status, marital status, pregnancy, gender expression or identity, sexual orientation or any other legally protected status.\\n\\nInterested? Apply now.\\n\\nAbout the Gamasutra Job Board\\n\\nWhether you're just starting out, looking for something new, or just seeing what's out there, the Gamasutra Job Board is the place where game developers move ahead in their careers.\\n\\n\\n\\nGamasutra's Job Board is the most diverse, most active, and most established board of its kind in the video game industry, serving companies of all sizes, from indie to triple-A.\\n\\n\\n\\nLooking for a new job? Get started here. Are you a recruiter looking for talent? Post jobs here.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Get a job: Intel is hiring a Graphics/Software Engineer', \"The Gamasutra Job Board is the most diverse, active and established board of its kind for the video game industry!\\n\\n\\n\\nHere is just one of the many, many positions being advertised right now.\\n\\nLocation: Folsom, California\\n\\nVisual & Parallel Compute Group (VPG) mission is to create the New Visual and Parallel Compute Experience. We are part of the Intel's Visual Technologies Team (VTT).\\n\\nWhat We Do\\n\\nIn VPG we are focused on delivering Best-in-Class visual experiences that enable users to immerse themselves in a new visual future by innovating graphics, media and display IP technologies. We continuously innovate and delight our customers across the entire breadth of our offering – from low power battery operated devices, to PC’s and laptops, to data center blades.\\n\\nWithin VPG we have a Software group, come work with world-class graphics software engineers developing software for Intel's latest GPU/CPU architecture. From drivers to compilers across all OSs, our software team delivers Intel’s 3D graphics, media, display and compute GPU Technology.\\n\\nCareer opportunities:\\n\\nGraphics Software Engineers: Performance, Drivers, Graphics APIs\\n\\nGraphics Compiler Engineers: Compute, Media, 3D, Performance\\n\\nMedia Engineers: CODECs, Image Processing, Content Protection\\n\\nSoftware Engineers: Agile, Performance, Linux/Windows/MacOS\\n\\nThe ideal candidate should exhibit behavioral traits that indicate:\\n\\nSelf-motivator with strong problem-solving skills\\n\\nExcellent interpersonal skills, including written and verbal communication\\n\\nAbility to work as a team and collaborate in a high-paced atmosphere\\n\\nQualifications\\n\\nYou must possess the below minimum qualifications to be initially considered for this position.\\n\\nEducation:\\n\\nBachelor’s with 3+, Master’s with 1+ with years of experience.\\n\\nDiscipline in: Computers Engineering/Science, Electrical Engineering, Math, Physics or related field.\\n\\nRelevant experience in the following:\\n\\nGraphics Concepts: 3D, Media, Imaging & Display\\n\\nGraphics APIs: OpenGL, DirectX, Metal, OpenCL\\n\\nHardware & Software Interfacing Skills\\n\\nC, C++ Scripting and Debugging: User & Kernel-mode Software\\n\\nMachine Learning, Deep Learning, or Computer Vision Algorithms\\n\\nSimulation or Projection Modeling for Performance and Power\\n\\nMedia & Display Standards: HEVC, VP9, AVC, HDR, DisplayPort, HDMI, etc.\\n\\nOther Locations\\n\\nUS, California, Santa Clara; US, Oregon, Hillsboro;\\n\\n\\n\\nPosting Statement. Intel prohibits discrimination based on race, color, religion, gender, national origin, age, disability, veteran status, marital status, pregnancy, gender expression or identity, sexual orientation or any other legally protected status.\\n\\nInterested? Apply now.\\n\\nAbout the Gamasutra Job Board\\n\\nWhether you're just starting out, looking for something new, or just seeing what's out there, the Gamasutra Job Board is the place where game developers move ahead in their careers.\\n\\n\\n\\nGamasutra's Job Board is the most diverse, most active, and most established board of its kind in the video game industry, serving companies of all sizes, from indie to triple-A.\\n\\n\\n\\nLooking for a new job? Get started here. Are you a recruiter looking for talent? Post jobs here.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Blog: Creating a hard AI for Terra Mystica', \"The following blog post, unless otherwise noted, was written by a member of Gamasutra\\x92s community.\\n\\nThe thoughts and opinions expressed are those of the writer and not Gamasutra or its parent company.\\n\\nSpoiler alert: This doesn't have a happy ending. Digidiced has been hard at work for more than a year trying to produce a Hard version of its AI for Terra Mystica using machine learning. Our results have been a lot less impressive than we were hoping for. This article will describe a little bit about what we’ve tried and why it hasn't worked for us.\\n\\nIf you’ve paid attention to the latest developments in AI, you’ve probably heard of AlphaGo and AlphaZero, developed by Google’s DeepMind. In 2017, AlphaGo defeated Ke Jie, the #1 ranked Go player in the world. AlphaGo was developed by using a massive neural network and feeding it hundreds of thousands of professional games. From those games, it learned to predict what it thought a professional would play. AlphaGo then went on to play millions of games against itself, gradually improving its evaluation function little by little until it became a superhuman monster, better than any human player. The defeat of a human professional was thought to be decades away for a game as complex as Go. But AlphaGo shocked everyone with its quantum leap in playing strength. AlphaGo was able to come up with new strategies, some of which were described as “god-like.”\\n\\nBut it didn't stop there. In December of 2017, DeepMind introduced AlphaZero – a method that also learned the game of Go, but this time didn't use any human-played games. It learned entirely from self-play, being only told the rules of the game. It was not given any suggestions or strategies on how to play. AlphaZero not only able to learn from self-play alone, it was able to get stronger than the original AlphaGo. And on top of that, the same methodologies were used for Chess and Shogi and the DeepMind team showed results that AlphaZero was able to solidly beat the top existing AI players in both of these games (which were already better than humans). Since these results have come out, there has been some criticism around if the testing conditions were really fair to the existing AI programs, so there is a little debate as to whether AlphaZero is actually stronger, but it is an outstanding achievement nonetheless.\\n\\nIt also became quite clear that AlphaZero approached chess differently than Stockfish (the existing AI they competed against). While Stockfish examined 70 million positions per second, AlphaZero only examined 80,000. But AlphaZero was able to pack a lot more positional and strategic evaluation into each of those positions. By examining the games that AlphaZero played against Stockfish it became obvious to a lot of people that AlphaZero was much better at positioning its pieces and relied less on having a material advantage. In many cases AlphaZero would sacrifice material in order to get a better position, which it later used to come back and secure a win. It suggested the possibility that there might be a resurgence in chess programming ideas, which had been stagnating in recent years.\\n\\nThe DeepMind team was able to show that AlphaZero learned many human-discovered opening moves. They showed several examples of how different openings gained and lost popularity as it continued to learn.\\n\\nAs Digidiced’s AI developer, these were exciting developments for me. I’ve had experience with machine learning and neural networks before and have been playing around with them for many years. I once developed a network as a private commission for a professional poker player that could play triple draw low at a professional level. I began to wonder if I could use some of these same techniques for Digidiced’s Terra Mystica app. One of the compelling features of AlphaGo was that it was largely based on something called a convolutional neural network (CNN). A CNN is also used in other deep learning applications like image recognition and is good at identifying positional relationships between objects. AlphaGo was able to use this structure to identify patterns on the Go board and determine the complex relationships that could be formed from the different permutations of stones.\\n\\nWhile Terra Mystica takes place on a hex-based map instead of a square grid, a CNN can still be applied to it so that the proximity of players’ buildings can be incorporated, which is a critical part of TM strategy. However, there are several things that make TM a much more complicated game than Go.\\n\\nTM can have anywhere from 2 to 5 players, although it is often played with exactly 4. For programming AI, the leap from 2 players to more than 2 is actually a lot more difficult than most people realize. You may have noticed that whenever you hear about an AI reaching superhuman performance, it’s almost always in a 2-player game.\\n\\nWhile a spot on a Go board can only have 3 states (white stone, black stone, or empty), a hex on a TM map can have 55 different states, taking into account the different terrain types and buildings. Add things in like towns and bridges and the complexity goes up from there.\\n\\nTM has 20 different factions using the Fire & Ice expansion, and each one of these factions has different special abilities and plays differently.\\n\\nTM has numerous elements that occur off the map including the resources and economies of each player, positioning on the cult tracks, and shared power actions.\\n\\nEach game is different by adding scoring elements and bonus scrolls that are different with each game. Which elements are present in the particular game can have a massive effect on all of the player’s strategies. Not to diminish the complexity of Go (a game which I’m still in awe of after casually studying it for over a decade), but you’re always playing the same game.\\n\\nOne of the things that makes TM such a great game and causes it to have a very high skill ceiling is the fact that its economies and player interactions are so tightly interwoven. The correct action to take on the map can be highly dependent on not only your own situation, but the economic states of your opponents or the selection of available power actions. All of this makes TM orders of magnitude more complex of a game than Go.\\n\\nChaos Magicians, Swarmlings, Darklings, and Dwarves fight it out on the digital version of Terra Mystica. Complexities abound and an AI needs to know how to read the board. Darklings will want to upgrade one of their dwellings to get the town bonus. They should upgrade next to the Dwarves to keep power away from the stronger CM player. The choice of towns could affect the flow of the rest of the game:\\n\\nShould they take 7VP & 2 workers so they have enough workers to build a temple and grab a critical favor tile?\\n\\nOr 9VP & 1 priest that they can use to terraform a hex or send to the cults?\\n\\nOr 8VP & free cult advancements which will gain them power and cult positioning?\\n\\n5VP & 6 coins is sometimes good, but probably not in this situation since the Darklings have other income sources.\\n\\nThe other town choices seem inferior at this point, which the AI needs to recognize. Notice what is needed to plan a good turn – the recognition that a town needs to be created this turn, the optimal location of the upgraded building, the knowledge that a critical favor tile exists and how to get it, the relative value of terraforming compared to other actions, the value of cult positioning (not shown) & power, as well as the value of coins which depend on how many coin-producing bonus scrolls are in the game.\\n\\nThe main idea behind training the network to become stronger is called bootstrapping. I’m simplifying things a bit here, but think of the neural network as an enormously complicated evaluation function. You feed it all the information about the map, the resources of all the players, and other variables that describe the current game state. It crunches the numbers and spits out an estimate of the best action to take (each action is given as a percent chance that it is the best action) and an estimate of the final scores for each player. Let’s say you have a partially trained network that has an okay evaluation function, but not that good. You now use that, and each time you’re going to make a move you think 2 moves ahead, considering all the options and picking what you think is best. You’ll now have a (moderately) more informed estimate of your current state because you’ve searched 2 moves ahead. You now try to tweak that model so that your initial estimate is more similar to your 2-moves-ahead estimate. If you were able to fully incorporate everything from 2 moves ahead into your evaluation function, when you use this function to search 2 moves ahead, it’s equivalent to searching 4 moves ahead with your original function. It’s not that simple, but you can see how repeating this over and over again will keep improving the model as long as it has enough features to handle the complexity. You just have to repeat it billions of times…\\n\\nIn order to train its networks, DeepMind was able to utilize a massive amount of hardware. According to an Inc.com article, the hardware used to develop AlphaZero would cost about $25 million. There is no way that a small company like ours would be able to compete with that. Some people have estimated that if you were to try and replicate the training done on a single machine, it would take 1,700 years! Even after all the training, when AlphaGo is run on a single machine, it still uses very sophisticated hardware, running dozens of processing threads simultaneously. We needed to create an AI that was capable of running on your phone. For each single position that AlphaGo analyzes, its neural network needs to do almost 20 billion operations. We were hoping to have a network with less than 20 million. And instead of analyzing 80,000 positions per second, we would be lucky if we could do 10. We also considered an even smaller network that could look at more positions per second, but it would not have enough complexity to incorporate a lot of the nuances needed for a strong player.\\n\\nSo our goal was to create an AI for a game that was even more complicated than Go, using a network about a thousandth the size. AlphaZero was able to play over 20 million self-play games in order to help its development. Even renting several virtual machines and playing games 24/7 for a few months, Digidiced was only able to collect about 40,000 self-play games. Despite these limitations, we were cautiously optimistic. We didn't need super-human and god-like play. We wanted something that could be a challenge to the entire player base while not taking too long to think for each move.\\n\\nA tiny peek into the complexity of Alpha Go (from David Foster’s AlphaGo Zero Cheat Sheet).\\n\\nBut even that turned out to be too much of a challenge with our limited budget. The AlphaZero paper claimed that starting from scratch and completely random play yielded better results than mimicking games played by humans. We decided to try both methods in parallel: one network would start from random play and build up network sophistication over time while another network was trained on games played on the app. Neither was able to create a very strong player; in fact, we were never able to create a version that could outperform our Easy version that used fairly standard Monte Carlo Tree Search. We even tried focusing the development on only 4-player games, but this didn't help much.\\n\\nWhat was really heartbreaking was that we could see the improvement that the network was making. We could see the improvement over time. But the rate of improvement was just too slow for the amount of money we were spending. It was a very difficult decision, but we’ve decided that we’re going to halt development work on this for now. We still see a possibility of spending some time converting the played games from Juho Snellman’s online implementation of TM, but we don’t have the funds for that now. Juho had very kindly given us permission to do that much earlier, but the conversion proved to be very difficult for a number of reasons, mostly due to how the platforms differed in accepting power. So while there is still a chance of further development, we don’t want to promise anything that doesn't seem likely.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Atomwise, which uses AI to shorten the process of discovering new pharmaceuticals as well as agricultural pesticides, raises $45M Series A (Catherine Shu/TechCrunch)', 'About This Page\\n\\nThis is a Techmeme archive page. It shows how the site appeared at 3:45 AM ET, March 12, 2018.\\n\\nThe most current version of the site as always is available at our home page. To view an earlier snapshot click here and then modify the date indicated.')\n",
      "(\"Interviews with Amazon execs detail the company's initial adoption of AI, which is now found in almost every product and service, from search to AWS and Alexa (Steven Levy/Wired)\", 'About This Page\\n\\nThis is a Techmeme archive page. It shows how the site appeared at 10:50 PM ET, February 1, 2018.\\n\\nThe most current version of the site as always is available at our home page. To view an earlier snapshot click here and then modify the date indicated.')\n",
      "('Sponsored Post: Clover, Triplebyte, Exoscale, Symbiont, Loupe, Etleap, Aerospike, Scalyr, Domino Data Lab, MemSQL', '')\n",
      "('Sponsored Post: Triplebyte, Exoscale, Symbiont, Loupe, Etleap, Aerospike, Stream, Scalyr, VividCortex, Domino Data Lab, MemSQL, Zohocorp', '')\n",
      "('Sponsored Post: Educative, Clover, Triplebyte, Exoscale, Symbiont, Loupe, Etleap, Aerospike, Scalyr, Domino Data Lab, MemSQL', '')\n",
      "('Sponsored Post: Triplebyte, Exoscale, Symbiont, Loupe, Etleap, Aerospike, Scalyr, Domino Data Lab, MemSQL', '')\n",
      "('Sponsored Post: Clover, Triplebyte, Exoscale, Symbiont, Loupe, Etleap, Aerospike, Scalyr, Domino Data Lab, MemSQL', \"Tuesday, February 27, 2018 at 9:03AM\\n\\nWho's Hiring?\\n\\nClover is looking for seasoned software engineers to help us solve the most complicated problem in the world: healthcare. We're using sophisticated data analytics, custom software, and machine learning to coordinate care and build a clearer model of our member's health and risk factors. We are on a mission to help seniors and low-income members live healthier while keeping costs down. This is an opportunity for those who want to be at the intersection of health and technology and thrive in a collaborative environment as well as the freedom of self-direction. If you're interested, please directly apply here!\\n\\nis looking for seasoned to help us solve the most complicated problem in the world: healthcare. We're using sophisticated data analytics, custom software, and machine learning to coordinate care and build a clearer model of our member's health and risk factors. We are on a mission to help seniors and low-income members live healthier while keeping costs down. This is an opportunity for those who want to be at the intersection of health and technology and thrive in a collaborative environment as well as the freedom of self-direction. If you're interested, please directly apply here!\\n\\nTriplebyte now hires software engineers for top tech companies and hundreds of the most exciting startups like Apple, Dropbox, Mixpanel, and Instacart . They identify your strengths from an online coding quiz and let you skip resume and recruiter screens at multiple companies at once. It's free, confidential, and background-blind. Apply here.\\n\\nnow hires software engineers for top tech companies and hundreds of the most exciting startups like . They identify your strengths from an online coding quiz and let you skip resume and recruiter screens at multiple companies at once. It's free, confidential, and background-blind. Apply here.\\n\\nSymbiont is a New York-based financial technology company building new kinds of computer networks to connect independent financial institutions together and allow them to share business logic and data in real time. This involves developing a distributed system which is also decentralized, and which allows for the creation of smart contracts, self-executing cryptographic agreements among counterparties. To do so, we're using a lot of techniques in blockchain technology, as well as those from traditional distributed systems, programming language design and cryptography. We are hiring for a number of roles, from entry-level to expert, including Haskell Backend Engineer, Database Engineer, Product Engineer, Site Reliability Engineer (SRE), Programming Language Engineer and SecOps Engineer . To find out more, just e-mail us your resume.\\n\\nis a New York-based financial technology company building new kinds of computer networks to connect independent financial institutions together and allow them to share business logic and data in real time. This involves developing a distributed system which is also decentralized, and which allows for the creation of smart contracts, self-executing cryptographic agreements among counterparties. To do so, we're using a lot of techniques in blockchain technology, as well as those from traditional distributed systems, programming language design and cryptography. We are hiring for a number of roles, from . To find out more, just e-mail us your resume.\\n\\nNeed excellent people? Advertise your job here!\\n\\nFun and Informative Events\\n\\n5 Signs You’ve Outgrown DynamoDB . Companies often select a database that seems to be the best choice at first glance, as well as the path of least resistance, and then are subsequently surprised by cost overruns and technology limitations that quickly hinder productivity and put the business at risk. This seems to be the case with many enterprises that chose Amazon Web Service’s (AWS) DynamoDB. In this white paper we’ll cover elements of costing as well as the results of benchmark-based testing. Read 5 Signs You’ve Outgrown DynamoDB to determine if your organization has outgrown this technology.\\n\\n. Companies often select a database that seems to be the best choice at first glance, as well as the path of least resistance, and then are subsequently surprised by cost overruns and technology limitations that quickly hinder productivity and put the business at risk. This seems to be the case with many enterprises that chose Amazon Web Service’s (AWS) DynamoDB. In this white paper we’ll cover elements of costing as well as the results of benchmark-based testing. Read 5 Signs You’ve Outgrown DynamoDB to determine if your organization has outgrown this technology.\\n\\nAdvertise your event here!\\n\\nCool Products and Services\\n\\nIf you are interested in a sponsored post for an event, job, or product, please contact us for more information.\\n\\nScale your Job Search with Triplebyte\\n\\nTriplebyte is unique because they're a team of engineers running their own centralized technical interview. The evaluation quality is so good that companies like Apple, Dropbox, Mixpanel, and Instacart now let every engineer Triplebyte recommends skip steps in the application process.\\n\\nThey give personal assistance to discover which roles you're most excited about, schedule your final interviews back-to-back, and help you negotiate with multiple companies at once.\\n\\nTriplebyte now works with top tech companies and hundreds of the most exciting pre-screened startups.\\n\\nIt's free, confidential, and background-blind for engineers. Take Triplebyte's online coding quiz to see if they can help you scale your career faster. (Engineers with architecture and system design experience tend to do especially well.)\\n\\nThe Solution to Your Operational Diagnostics Woes\\n\\nScalyr gives you instant visibility of your production systems, helping you turn chaotic logs and system metrics into actionable data at interactive speeds. Don't be limited by the slow and narrow capabilities of traditional log monitoring tools. View and analyze all your logs and system metrics from multiple sources in one place. Get enterprise-grade functionality with sane pricing and insane performance. Learn more today.\\n\\nIf you are interested in a sponsored post for an event, job, or product, please contact us for more information.\")\n",
      "('Sponsored Post: InMemory.Net, Educative, Triplebyte, Exoscale, Loupe, Etleap, Aerospike, Scalyr, Domino Data Lab, MemSQL', '')\n",
      "('Stuff The Internet Says On Scalability For March 16th, 2018', '')\n",
      "('Stuff The Internet Says On Scalability For February 16th, 2018', '')\n",
      "('Stuff The Internet Says On Scalability For February 2nd, 2018', '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Steve Jurvetson appears to be starting his own venture capital firm', 'Steve Jurvetson, the prominent venture capitalist who left DFJ in a messy split late last year, is starting his own firm, Recode has learned.\\n\\nJurvetson is listed as the founder of a soon-to-launch fund called Future Ventures, according to the firm’s website.\\n\\n“At Future Ventures, we support passionate founders who are forging the future. For the past 23 years, we have backed the visionaries who push the boundaries of possibility and explore the frontier of the unknown,” the website says. “We focus on disruptive technology such as commercial space exploration, deep learning, quantum computing, robotics, AI, blockchain, and sustainable transportation, synthetic biology and clean meat.”\\n\\nThe website refers to the firm’s “founders” as a plural, implying that other investors may be joining Jurvetson at the outpost. The firm has yet to file any paperwork with the Securities and Exchange Commission.\\n\\nA Jurvetson spokesperson did not immediately respond to requests for comment late Thursday. The website is registered in the name of Jurvetson’s former executive assistant and was updated in late March.\\n\\nUpdate: Jurvetson has confirmed on Facebook his creation of the new firm.\\n\\n“I strongly believe that mission-driven founders forge the future. At Future Ventures, we will support those passionate founders,” he wrote.\\n\\nJurvetson’s reemergence onto the venture capital scene is not surprising — he is responsible for two of DFJ’s massive successes, SpaceX and Tesla — and that means he is likely to find a friendly base of investors, perhaps among several prior DFJ limited partners. His departure came amid internal tension at DFJ after the firm caught him lying about what it considered serious allegations, a source familiar with the situation said at the time.\\n\\n“I am excited to move on and get back to my professional passion, helping great entrepreneurs forge the future,” Jurvetson said in a Facebook post in November shortly after leaving DFJ.\\n\\nBut it is still a quick rebound. Mike Cagney, the former CEO of SoFi, is also back with a new startup, as Recode first reported, after his company battled allegations of sexual harassment. Travis Kalanick, too, has restarted his life after being forced to leave his job.\\n\\nJurvetson, for his part, was accused by women of having several extra-marital affairs that, in the eyes of some, crossed into the professional world. No one has publicly emerged to allege him of sexual harassment. He has denied any misconduct.\\n\\nSince his departure in November, Jurvetson has kept a low profile, though he has been making several speaking appearances on familiar topics, according to his social media pages.')\n",
      "('Recode Daily: HQ Trivia raises money; BuzzFeed News needs money; Apple and Amazon make money', 'HQ Trivia, the trivia app with a huge audience and a controversial founder, has raised a funding round that values the company north of $100 million. Founders Fund, the venture firm founded by billionaire Peter Thiel, is expected to lead the round of $15 million. The round is notable because several investors were scared off by the reputation of co-founder Colin Kroll, who had been fired by Twitter after it acquired his Vine startup. [Kurt Wagner, Theodore Schleifer / Recode]\\n\\nApple posted its biggest quarter ever yesterday — revenue passed $88 billion in the first fiscal quarter, up 13 percent year over year, with $20 billion in profit. But it still disappointed analysts beacuse of mediocre sales of the iPhone X. [Dan Frommer / Recode]\\n\\nAmazon is no longer a company that consistently loses money. The e-commerce giant has posted a profit for 11 straight quarters — including a record $1.9 billion during the holidays. The company also reported that its “other” revenue — which mostly means advertising, plus its co-branded credit card agreements — increased to $1.7 billion in the fourth quarter. That’s 60 percent growth year over year. Amazon has baked AI into nearly all of its products and services — here’s how deep learning came to power Alexa, Amazon Web Services and other divisions. [Jason Del Rey / Recode]\\n\\nAlphabet disclosed the revenue of its Google Cloud business for the first time yesterday, during a conference call about the Google parent company’s otherwise unremarkable Q4 earnings. Google CEO Sundar Pichai said the cloud is “already a billion-dollar-per-quarter business”; its rival, market-leading public cloud Amazon Web Services, generates roughly five times more revenue per quarter. [Jordan Novet / CNBC]\\n\\nCBS and Viacom plan to merge (again). Shari Redstone, who controls both media companies, considered a possible deal in 2016 but changed her mind, believing that Viacom could turn around on its own. But a series of proposed mergers, kicked off by AT&T’s planned acquisition of Time Warner, is forcing comparably smaller media companies to look for larger homes. Take a look at this chart of the current media landscape, which explains why Redstone now supports a merger. [Rani Molla and Peter Kafka / Recode]\\n\\nBuzzFeed’s news unit is looking for financial help, and has talked to a rep for billionaire Laurene Jobs Powell. But a BuzzFeed board member denied that the company wants to spin its news group off. Big picture: BuzzFeed News generates lots of attention, but little revenue. That used to be okay, but it’s harder for BuzzFeed to justify that in 2018. [Peter Kafka / Recode]\\n\\nTop stories from Recode\\n\\nAirbnb’s CFO is leaving after clashing with CEO Brian Chesky.\\n\\nThe fast-growing travel company has also elevated Belinda Johnson to the COO role.\\n\\nThe head of Boxed’s fastest-growing business is leaving amid sale talks with Amazon and Kroger.\\n\\nBehzad Soltani joined the startup last year.\\n\\nBiotech giant Moderna is now valued at $7.5 billion.\\n\\nIt’s now one of the country’s highest-worth startups.\\n\\nSnapchat is now selling hats and sweatshirts inside its app — and testing out Snapchat’s commerce potential in the process.\\n\\nDo people want to buy stuff inside Snapchat? Looks like Snap wants to find out.\\n\\nHow will Sonos compete against Apple’s HomePod?\\n\\nCEO Patrick Spence explains Sonos’ answer to Apple, Amazon and Google on the latest episode of Too Embarrassed to Ask.\\n\\nThis is cool\\n\\nSign of the times: Monopoly for cheaters.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Full transcript: Benchmark partner Sarah Tavel on Recode Decode', 'On this episode of Recode Decode, hosted by Kara Swisher, Benchmark partner Sarah Tavel talks with Kara and Recode’s Teddy Schleifer about her career in tech companies and venture capital. Her resume includes stints at Pinterest, Bessemer Venture Partners and Greylock Partners, but last year she became the first woman partner hired at Benchmark, where one of her focuses is cryptocurrencies.\\n\\nYou can read a write-up of the interview here or listen to the whole thing in the audio player above. Below, we’ve also provided a lightly edited complete transcript of their conversation.\\n\\nIf you like this, be sure to subscribe to Recode Decode on Apple Podcasts, Spotify, Pocket Casts, Overcast or wherever you listen to podcasts.\\n\\nKara Swisher: Hi, I’m Kara Swisher, executive editor of Recode. You may know me as the person who decides what price bitcoin sells at, but in my spare time I talk tech, and you’re listening to Recode Decode, a podcast about tech and media’s key players, big ideas and how they’re changing the world we live in. You can find more episodes of Recode Radio on Apple Podcasts, Spotify, Google Play Music or wherever you listen to podcasts, or just visit recode.net/podcasts for more.\\n\\nToday in the red chair is Sarah Tavel, a partner at the venture capital firm Benchmark. She was the first woman ever hired as a partner by the firm and that was in May of 2017. She previously worked at Bessemer Venture Partners, Pinterest and Greylock Partners. Sarah, welcome to Recode Decode.\\n\\nSarah Tavel: Thank you so much.\\n\\nKS: And joining me here today for the interview is Recode’s senior finance and influence editor. Is that your new title?\\n\\nTS: My fancy title, right.\\n\\nKS: Teddy Schleifer. He writes a lot about venture capital firms like Benchmark. Hey, Teddy.\\n\\nTS: Hey.\\n\\nKS: How are you doing?\\n\\nTS: Good.\\n\\nKS: I’ve known Sarah for a while, I guess, and she was also one of our Recode 100 winners.\\n\\nThe very tail end but I’ll take it.\\n\\nKS: Take it, you’ll go ... I guarantee you a higher rating this year if you do well, especially with bitcoin. Why don’t we start talking a little bit about your background, because you worked at a lot of places and you’re a relatively young person, but you’ve been a venture capitalist. You worked in ...\\n\\nYeah, I started my venture career one year out of college, which is pretty atypical.\\n\\nKS: Why don’t you go through that? Because I know, we’re not going to overly focus on the woman issue, but there’s not that many women venture capitalists and I want to talk about why that is a little bit later. But why don’t we talk about your journey and where you went?\\n\\nI got into venture ... Taking a step back, I was a philosophy major in college, so I had always been interested in investing but didn’t really even know what venture capital was until basically the week I applied to a venture job.\\n\\nKS: But why that after college?\\n\\nSo what happened is, I was at a strategy consulting firm that was a startup and I had joined this firm because I’d done a bunch of entrepreneurial things in college and just wanted to be part of a startup.\\n\\nKS: Like what?\\n\\nIt sounds quite random, but I started a house-painting company, an exterior residential house-painting company that ended up expanding into more kind of general contracting. I sold ads for all these random publications. I was kind of like a local ad sales rep, and I just realized ... I knew how to sell and so I kept on selling ads to the same people for different products, basically. And so I was kind of more focused on making money in college than I was going to classes. And it ended up being that I joined this strategy consulting firm, and it kind of went up.\\n\\nAnd then as it often does, one of our biggest clients had some budget problems and we started to thrash and I started to look for a new job. And I’d always been interested, actually, in investing. My dad was a public equity investor and I started to look into equity research jobs and then had the really good fortune of talking to one of my friends who I had known in college and she was like, “You know what? You love investing and you did all these random entrepreneurial things in college. You should look into venture capital.”\\n\\nAnd I was honestly like, “Well, what’s venture capital?” And so I went out ... There’s this bookstore in Harvard Square called the Co-op and I bought the “Vault Guide to Venture Capital.” And it was one of those things that the more I read, the more I realized that this was what I wanted to do. It felt super exciting to get to work with founders in the early stages of a company and still be doing investing. And I knew the lowest rung of the ladder in a venture capital firm is this sourcing role, where you’re cold-calling founders and you’re trying to get them to take a next meeting and take your money eventually. And I knew through my ad experience that I could do that. I knew how to do 20 calls in a day and sell people. So I ended up applying to Bessemer.\\n\\nKS: Which is based there.\\n\\nThat’s right. So they have a place upstate in Larchmont, New York. And it was super lucky timing because they were looking to hire an analyst at the team at the same time that I was looking to jump ship, and it’s really thanks to the random work experience I had in college that I ended up getting the job and joining .. I think I was the first woman they hired in probably 10 years. And it was supposed to be a two-year job and ended up being six for me.\\n\\nSo I was five years in the New York office. I was working with one partner, Jeremy Levine, I don’t know if you know him. He’s really an awesome person. So I was there. I was working on sourcing deals in the beginning for software as a service companies. Companies like Cornerstone, OnDemand and Mindbody and others, and then worked on a bunch of companies that really range from deep in the stack to hire, kind of consumer stuff like diapers.com. I moved to the West Coast about five years in.\\n\\nKS: Because you had to?\\n\\nNo, I grew up in Manhattan and there was a combination of being a little sick of New York, and also I started to realize that New York City was a local maxima. Like if I really wanted to build my career in technology, I wanted to be in the global maxima. I wanted to be in the place where the best were.\\n\\nAnd so I transferred from Bessemer’s New York office to the Menlo Park office. And actually what ended up being really fortuitous is that I had been tracking this one company, Pinterest. I was an early user of the product and just really, really loved what they were doing, and I had actually met the founders, actually Paul Sciarra, when they were working on the precursor to Pinterest called Tote.\\n\\nKS: Oh, wow.\\n\\nSo as Pinterest came out, I started to play with it. I loved it so much and reached out to them basically the day I moved to San Francisco and we ended up sourcing the investment for Bessemer. We did the series A. It was a four-person company at the time. It was really minuscule. And then, as you know, I ended up just loving the company so much that I decided to jump ship and join.\\n\\nTS: I’d love to get a little bit more into just the experience of being a young person at a venture capital firm, not being a general partner or something. When you talk to people today, there is occasionally frustration that there’s people who steal a deal or market the deals there. As are people who will say it’s hard, some folks in venture don’t feel that it’s a meritocracy, being a young person at a firm. What was it like being at Bessemer? What was it like in general? You’re still fairly young, but being somebody who’s been in venture for a long time and growing up in the business about being an analyst or an associate at a big top tier venture firm, how did you succeed there?\\n\\nThere were a bunch of different factors that really influenced my early days at Bessemer. First of all, I was coming into a venture capital firm with really no experience in technology and no actual investing experience. Everything was really new to me and I was the only woman in a 25-person firm. I felt physically the smallest. It felt very different.\\n\\nI’d never been in a context where I was the only woman, actually. So it was one of those things. I didn’t really know what to expect and it ended up feeling hard in the beginning. And the nice thing, though, about the analyst job in a venture capital firm is that the measure of merit is very objective, which is, “Do you source good deals?” And so it was the type of thing that I didn’t have to politic to get ahead. I just had to do what I knew I could do, which is call a lot of companies, work really hard.\\n\\nKS: But how did you evaluate them?\\n\\nIn the beginning, you know, when you’re an analyst, you actually are supposed to orient towards companies that are already kind of working, and so you end up meeting companies and you learn what questions to ask, how are they ... I was doing a lot of software as a service companies. Where is their monthly recurring revenue? How much does it cost to acquire the customer? How fast are they growing? How much have they raised?\\n\\nAnd so then there’s like those things that are pretty objective also. And then what we would do is, every Monday, we would have an analyst meeting where basically the analyst would present the two or three most interesting companies that they spoke to the prior week and that was how you circulate or socialize the company to the partners and then they would do the next step. You would be part of the diligence process for that company and that was how you started to develop your own pattern recognition and learn how to diligence the company and then ultimately which companies to invest in.\\n\\nKS: To Teddy’s question, most of the younger people source the better things because they’re on the ground. What do you need them for?\\n\\nWhat do you need the young people?\\n\\nKS: No, the old.\\n\\nWell, in the beginning you don’t know what’s good.\\n\\nKS: Right.\\n\\nRight? As a young person. What you do know is, the way I think of it is that you get very, very good in the early days of knowing what are the 50 companies that the firm should meet with every year. And then as you’ve progress, you go from knowing what the 50 companies are that you should meet, to then you get a little bit better, and you know what are the 30 that you should be spending time with. And then it’s, what are the 10, but ultimately the hardest decision is what are the two that you invest in?\\n\\nKS: Right.\\n\\nThat’s where the gray hair really helps. And then it’s also, as you know, it’s winning the deal. It’s a lot harder to do that when you’re a year out of college.\\n\\nKS: Right.\\n\\nTS: And so you felt like being a young person at a venture firm the same way that a GP is evaluated based on their returns individually. You felt like if you were one of the people that sourced, you know, you presented the list of 50, you presented a list of 10, and the company succeeded they would say, “Sarah sourced this deal. She was the analyst and she gets points.”\\n\\nThat’s exactly the way.\\n\\nTS: Do you think that’s still true today in venture? Do you think that there’s ... Obviously there are some folks who kind of walk in the door at a GP level or at a higher level than someone in their 20s. But you think that’s still kind of the path to success for someone who is fresh out of college or maybe has an MBA?\\n\\nIf you’re joining a venture firm in the more junior roles, but even if you join as a principal or a VP, at the end of the day, it’s so much about seeing the right deals. That is really what it comes down to, and again, there’s so much subjectivity in who gets credit for a deal.\\n\\nKS: That’s all we do as reporters.\\n\\nIt’s really hard.\\n\\nKS: “I did Uber!” “No I did Uber!”\\n\\nExactly.\\n\\nKS: “Last year I didn’t do Uber.”\\n\\nRight. Right. Yeah. And you know, success has many fathers, as they say, and ... you even look at the Facebook story, and there’s three people who claim having sourced that investment. So there’s still always that question, but it’s a very objective way for a young person to say that they’re doing a good job. And then also to build their own personal brand and ultimately ...\\n\\nKS: They have the relationships with the founders.\\n\\nYes.\\n\\nKS: Was that harder as a woman?\\n\\nI would assume so. It’s so hard to know, because I don’t have an A/B experiment. But I don’t know the answer to that.\\n\\nKS: When you pal around ...\\n\\nI mean, I didn’t pal around too much.\\n\\nKS: Not too much drinking and going to clubs. So you got interested in Pinterest and went there?\\n\\nYes, that’s right.\\n\\nKS: So what caused that?\\n\\nI’d been in venture for six years, actually, by the time I left.\\n\\nKS: Which made you ancient?\\n\\nWhich made me ancient, and it was one of those things where I knew a couple things. I knew that I had basically locked in a career in venture. Like Pinterest was just ... I mean super lucky, it was an incredible, like it was already really growing. And I believed so much in the product from the very beginning that I just had a lot of conviction that this was going to become a really important company. And at the same time, I always had this voice in the back of my head, which was that I wanted to experience myself in the company and I wanted to know what it was like to be in the company.\\n\\nIt was partially just my own experience, like I’ve always been on sports teams. I was captain of the rugby team. I loved being a part of a team and leading a team and wanted to know what that was like to do it in the big leagues, in a real company. And then I also always felt like I would go to board meetings and there were something that felt almost a little inauthentic to me, that the CEO would ask for advice on something and I could come from a very rational, analytical place, but I didn’t feel like I could come from a place that was from the heart. And that’s kind of more who I am, like I really want to be able to have that empathy, come from that ...\\n\\nKS: Board meetings can be Kabuki theater.\\n\\nThere’s definitely a part of that. And so there was a voice in the back of my head for a long time and it just got louder and louder, the more time I spent with Pinterest, like at the time I was doing anything I could to help make the company successful. When we invested, it was a four-, five-person company growing really quickly. And there was just so much to get done that they didn’t have enough people around the table to do it. So I was chasing domain squatters for Ben, trying to find engineers, doing whatever I could and just realizing that you could only do so much from the outside. And so I just realized, you know ... Do you know the Bezos regret minimization framework?\\n\\nHe has this great video where he talks about ... I remember watching this video as I was making this decision of making decisions today that minimizes the regret you have in the future. And I realized that if I didn’t kind of throw my hat into the ring and try to join Pinterest — I didn’t know whether Ben would have any interest in hiring me — but if I didn’t throw my hat into the ring or at least try, I’d always regret it. So I basically called Ben up one day and essentially asked for a job. I pitched myself as a BD [business development] person.\\n\\nKS: Right, perfect.\\n\\nI told him I was like, “I’ll get the ’pin it’ button everywhere.” And then he actually reverse-pitched me and said, “What I really need is someone to just fix problems for me.” He’s like, “I need a utility player. I need someone who will put structure around a problem, solve it, move on to the next problem, solve it, etc.”\\n\\nKS: What’s that job title?\\n\\nWell, the actual job title was Business Operations Specialist, but when I started at Pinterest, the first thing I had to do was help us localize Pinterest, like we had never launched internationally. All of the strings in our code were just English and so there was no way to actually swap in a different language for the strings, so I worked with an engineer and did that. Then it was, “Oh, we’re starting to get some interest for a series C financing. Why don’t you ... Here are some term sheets. Figure this out and close our series C financing,” which was actually our unicorn round, as it ended up being called, and then increasingly what ended up happening is that ...\\n\\nKS: So you were the one they were paying attention to.\\n\\nI remember like, this was probably just a couple of months after I joined in, and I felt like I was like, “Man, if I get fired tomorrow and I go back to venture, I’ll have already learned so much like from this.”\\n\\nKS: What not to do, probably.\\n\\nYeah. What not to do. It is so interesting to see how different venture firms act during this process. It is not ...\\n\\nKS: Can you give an example? Not say who did something ...\\n\\nSure. Yeah, there is one firm that has a reputation for being founder friendly and rightly so, because I was negotiating these deal docs with Rakuten, which is a Japanese firm. And as you can imagine, it was pretty tricky, because they were using an East Coast firm, they were in Japan. This was one of their first venture investments.\\n\\nSo I was very focused on executing the deal with them. And then at some point in the process you have to kind of bring the deal to your insiders and see if they have any changes. And one group had ... I call it death by a thousand paper cuts. It was like so many little tweaks and the other group was like, “Just when you need our signature just let me know and I’ll sign it.” And that’s a very big difference.\\n\\nKS: Absolutely. Yeah. Yeah.\\n\\nBut then what ended up happening is that at the time that I joined Pinterest we had no product managers. Ben and Evan, the two co-founders, were actually against the idea of a product manager, and what ended up happening is that the daily stand-up for the engineers was basically the same every day. And I ended up kind of working with a bunch of engineers on a couple of projects. Those ended up shipping and kind of moving on track and so I ended up becoming really one of the first product managers at Pinterest.\\n\\nKS: Right.\\n\\nAnd I started the search team with one of the engineers. And that ended up kind of expanding into what we ended up calling our discovery team. So by the end of my time of Pinterest, I was responsible for search and all the recommendations on Pinterest, our computer vision team, our quality team and it was one of those things that’s like that, “Don’t ask what seat, just get on the rocket ship.” As a philosophy major, you feel like, “How did I end up being the product manager for really the most technical product team at Pinterest?”\\n\\nKS: That’s very technical, right.\\n\\nYeah, but it ended up really working.\\n\\nKS: Yeah. So why did you leave?\\n\\nThat was a hard decision. I really anguished over it. I mean what happened is that the Greylock team reached out to me, and I mean they were very savvy about it. “I’m not looking to leave. I’m super happy here.” And Jeff Markowitz, I don’t know Jeff, he’s fantastic. He was like, “Totally get it. But do you want to meet Reid Hoffman?” And so then you meet Reid and he’s just such a wonderful, amazing person. You come out and you feel like you’ve learned so much. And then, “Oh, that was a great meeting. Do you want to meet David Sze?” And then before you know it, you’ve met everybody on the team.\\n\\nTS: They’re interviewing without you knowing it.\\n\\nAnd it’s one of those things that it really starts to make you think like, “Gosh, this is an amazing group of people.” And I always knew I would go back to venture. I just didn’t really know when that may be.\\n\\nKS: Did you want to stay through the IPO?\\n\\nThat was the thing I really asked myself. At the time the company was ... We had just crossed 100 million monthly active users and we were about 650 people. And I felt like I had been there from the very beginning. I was on the outside in the beginning. But then you do feel like you want to be there for the entire ride. And then at the same time, I also realized that ... I started to feel like I would have more impact on the venture side. And also that venture was the type of thing that ... You look at the careers of many of the best venture capitalists and they really start to do their best deals five or six years into it.\\n\\nKS: Yeah.\\n\\nPeople like Eric, my partner Eric Vishria, who did just unbelievable deals in his first year. Most of the time, it takes several years to get really good at the job. And I realized, if I want to be one of the best in the industry, which is what I want to be, I should start doing the job now. And so I make it sound easy. I really did anguish over this decision because leaving a company is a very unnatural thing to do and it almost feels like your senior year in college, first semester ...\\n\\nKS: Right.\\n\\nAnd everybody is getting excited for the best year and then you’re like, “All right, I just got my dream job. It’s time for me to go.” So I left, and gosh I can’t remember, I think it was July 2015, and joined the Greylock team as one of the partners.\\n\\nKS: And then went to Benchmark. You’re a fascinating person to a lot of people.\\n\\nThat was obviously not planned. When I joined Greylock, it was a decision to be there for my venture career. What ended up happening is, I was on the consumer team at Greylock. I had led a couple of investments and then got to know the Benchmark team. And it was one of those things where they reached out to me and in a very similar fashion, you just start to get to know each of the members of the team.\\n\\nKS: Right.\\n\\nAnd then they kind of told me about their interest in really getting to know me. And I said no. Like it didn’t feel like the right thing to do at the time. And then I realized that, “You know what? I owe it to myself in a way, like this is what I want to do for my career. I owe it to myself to get to know how some of the best in the business do the job.”\\n\\nKS: Right.\\n\\nAnd it felt almost like, “What do I have to lose?” At the very least, it will make me better at my job at Greylock. And so I started to get to know the team. And then the more I got to know the team and the way they do the job at Benchmark and the way that Benchmark is structured, the more I realized like, “Wow. This is actually the way I want to do venture.”\\n\\nKS: Such as what?\\n\\nThe way I would describe Benchmark is that it’s this very small partnership. It’s a very lean team, as you know, like there’s ... It’s the six ...\\n\\nKS: I remember the old Benchmark. I was around for the first Benchmark.\\n\\nThe five tall guys?\\n\\nKS: Oh yeah. They were real tall.\\n\\nThey were really tall. I definitely remember this meeting with them.\\n\\nKS: I could only imagine.\\n\\nBut it was one of these things where you’ve got, like at the time, it was just five partners. I’m obviously the sixth, and you’ve got nothing else. No associates, no talent partner, no marketing partners, like they talk about investing being the only thing and there is just a complete clarity of we just do series A, series B investments. Our fund is the same size fund as it’s been since for the last, I don’t know, 15 years.\\n\\nAnd then of course the equal partnership. Before I got to know the Benchmark team, I thought about the equal partnership as a compensation thing. Like, oh, isn’t it cool that as a new person, you get to make the same amount of money as like Bill Gurley? But what you end up feeling when you’re a part of the partnership is that it’s actually a cultural tenet of a quality like you come in and you feel like from the very beginning, that you are an equal. And that is such a different dynamic than it is in many other firms.\\n\\nKS: Which is, you work your way up.\\n\\nYeah, you work your way up. You earn credibility over time. You have to prove yourself as kind of what you can field. And I came into Benchmark and I didn’t feel like I had to prove myself at all, and it was almost a scary feeling because I’m so used to ...\\n\\nKS: Working your way up.\\n\\nYeah. And using that to drive you. And then you realize that actually the way that you do it at Benchmark is that you don’t direct that energy internally; you direct it externally. It just felt like a very different place. And of course, every firm talks about being team oriented. But when you have an equal partnership, it really actually does happen, like where you are all focused on just doing the best investments that we can as a partnership and then supporting those investments regardless of who is on the board of that company.\\n\\nKS: Well, that’s the goal at least.\\n\\nYeah, but you really do feel it.\\n\\nKS: All right. We’re here with Sarah Tavel. She is a partner at Benchmark and we’ve just heard about her very varied career in a very short time.\\n\\nYeah.\\n\\nKS: I do like that you leave and go. I think it’s fantastic when that happens. I’ve done it myself. When we get back, we’ll be talking about some of her investments and her thoughts. We’re also here with Teddy Schleifer, who covers venture capital and influence for us.\\n\\nTS: Apparently.\\n\\nKS: Apparently, at Recode. And we’ll talk about where Sarah thinks the big investments are right now. She’s very involved in bitcoin and other cryptocurrencies. We’ll talk about that and more.\\n\\n[ad]\\n\\nWe’re here on Recode Decode with Sarah Tavel. She’s a partner at Benchmark. She’s had a long career as a very young person at many venture capital firms, Bessemer, Greylock, she’s worked at Pinterest and now she’s at Benchmark.\\n\\nYes.\\n\\nKS: And when you got there what did you decide, because everybody at Benchmark has certain areas and obviously this past year, all Bill Gurley has done is deals with Uber, with all three of your partners.\\n\\nIt’s been a full team engagement.\\n\\nKS: Full team engagement, but you focused more on cryptocurrency. Can you talk a little bit about that?\\n\\nYeah, sure. I kind of always described Benchmark as, instead of, at Greylock or other firms, you have 20 people on the field. You have to choose a very specific position. At Benchmark, you’ve got six of you. So you get to take up a lot more space, so I definitely spent a lot of time in consumer. I’ve led two deals so far at Benchmark, one of which is a consumer marketplace. I continued to look in that space but the space that I ended up feeling a real gravitational pull towards was the cryptocurrency space. I read the bitcoin white paper when I was actually at Pinterest and I remember thinking, “God, this Satoshi guy, he’s a genius. But this feels like it will never work as a currency. It’s just ... it’s deflationary.” I kind of put it to the side. I didn’t really think much about it.\\n\\nAnd then when I was at Greylock, I actually remember reading about ethereum and the DAO. And as a philosophy major, when you read about this idea of a smart contract, you know, this kind of code that self-executes to ... If you have some kind of contract and if it will execute something like payment or whatever it may be. That to me was like, “Wow, this is going to change everything.” And so I started to dig in more when I was actually at Greylock, and at one point I really got involved with a Greylock and Benchmark investment called Xapo. I don’t know if you know Wences.\\n\\nKS: Wences? I just saw him the other night.\\n\\nEverybody calls him patient zero of Silicon Valley.\\n\\nKS: He was the first person. He made me buy bitcoin. And I did a story on it 100 years ago and I can’t find my bitcoin, but that’s another issue.\\n\\nThat’s so funny. I think a lot of people owe him a lot of money.\\n\\nKS: They do. If I ever find mine.\\n\\nIf he could take carry on all of the winnings that people had from buying bitcoin.\\n\\nKS: John the Baptist of bitcoin.\\n\\nThat’s right. And he actually was the first person I sat down with as I was trying to make sense of this ecosystem and just kind of like, “Give me the 101.” And so when I joined Benchmark, the ICO stuff was just starting to happen and I really ... it was one of those things that you feel this intellectual, gravitational pull. It’s also, one of the things that I always look for is where are the smartest people that I know spending their time?\\n\\nAnd there’s just so much energy. You just keep on meeting person after person who is super intelligent and passionate about the space. And so, I spent the first few months of my time at Benchmark just doing a lot of reading, a lot of talking to people. You could read Medium post after Medium post, white paper after white paper, and still feel like you are just starting to figure it out. But it ended up being just a space that I think is still incredibly early. But there’s a lot of promising things happening.\\n\\nTS: Did you feel like ... What’s the temperature of the Benchmark investors in terms of limited partners on investing in crypto? Obviously every firm is deliberating how much risk they’re willing to take. Benchmark obviously has the track record that should give people some comfort, but is that a tough sell to folks to say, “Hey, we’re thinking about investing in a token. We’re thinking about potentially even just buying a raw asset.” Is that a tough conversation or do they get it?\\n\\nWell, first of all, we’ve been investing in this space for several years. We invested in Xapo in 2014. We invested in a fund called Pantera, and Bitstamp in 2014. So our interest in the space isn’t new to our LPs. And then at the same time, you’ve got a lot of other venture firms who are making exactly the same phone call to their LPs, so it’s not the first time that they hear about it. And as you said, I mean, we’re in a pretty privileged position with our LPs right now, where they trust us to make the call. So that hasn’t been hard and we have our documents all ...\\n\\nKS: They’re throwing money at you. But not quite as much money as SoftBank.\\n\\nThat’s right. Yeah. The hard thing is when you keep your fund the same small size ...\\n\\nKS: They’re going to roll over everybody.\\n\\nThat’s right. But what ends up happening is, within the partnership we have a spectrum of bullishness. I mean, Bill yesterday mentioned. He was, I can’t remember exactly ...\\n\\nTS: He said it was nauseous talking. Everybody, this is at the Goldman Sachs tech conference with like hundreds of public investors, he says, “Everybody in this room is nauseous talking about crypto. So let’s move on.”\\n\\nI saw that, I was like, “Bill.”\\n\\nKS: Make the nauseous case. What’s this?\\n\\nThat nauseous case is a few things, which is, No. 1, when you actually look at the underlying, I mean, tear opener on bitcoin, “What dictates the price of bitcoin besides fluctuations in supply and demand?” We were actually ... I think part of Bill’s comment came from the, as a partnership, we actually just came back. We flew to New York City on Sunday and spent time with a lot of people that we respect in New York City that are on the private equity side, the hedge fund side, and as you can imagine, one of the most fun topics to talk about when you’re meeting with these people is, “What do you think about what’s happening in the cryptocurrency ecosystem?”\\n\\nAnd so it ends up being this big topic of conversation. So people on the East Coast kind of can think about bitcoin as a trade. And then when you look at the promise of the blockchain, which is this idea that we’re going to have all these decentralized applications that people talk about, we’re going to have this decentralized Uber and you’re going to call a car from the cloud, that has been an idea ... They call them dApps, decentralized applications, which are basically applications that are built on a blockchain.\\n\\nKS: Right.\\n\\nThat’s been an idea out there for several years and yet there’s nothing to show for it.\\n\\nKS: Right. I think one of the things that’s interesting about it is, it’s like, I was at the early internet. And one of the facets of the early internet, there’s a lot of con men and hucksters.\\n\\nRight.\\n\\nKS: Very much so. Millions of them.\\n\\nAnd that’s the same here right now.\\n\\nKS: Except, you were using the internet, you understood the use of it. And I think one of the problems is I don’t even understand what it’s used for yet, or I don’t see any actual applications. And so to me, where the investments are are not necessarily in playing the currency game but what are the companies that are going to be the Google of.\\n\\nRight.\\n\\nKS: And I don’t know that.\\n\\nNo one really knows that.\\n\\nKS: Why not? Like, I knew the internet. I was like, “AOL? Okay, I can see Yahoo.” I can see them, you know what I mean? It started to become very clear.\\n\\nAdam, I don’t know if you ... At some point you should have Adam Ludwin on. He’s has some great blog posts on exactly this issue, which is that there are so many alleged use cases for the blockchain, but really what it comes down to is this idea of being central proof, that a lot of the use cases that make a lot of sense for using bitcoin or blockchain more generally are use cases where you couldn’t do it otherwise. As an example, I mean there’s definitely illegal use cases. There’s just kind of criminal activity, buying things that are ...\\n\\nKS: Which is not a business you really want to get into.\\n\\nNot a business you want to get into. But there are also, you’re in Venezuela and you need protection from an inflation ...\\n\\nKS: Which is Xapo, which is Wences ...\\n\\nYes. Wences’s argument. You want to get money out of your country. There are use cases that do make sense there and I think actually a lot of the positive bumps that you probably see in bitcoin are as a result of some newspaper article talking about the people in whatever country it may be that are using bitcoin because they have nothing else that they do.\\n\\nKS: Right.\\n\\nAnd that is a real use case.\\n\\nKS: Okay. It’s not the internet yet.\\n\\nIt’s not the internet yet. I think it’s like, we are in the infrastructure build-out phase of this ecosystem. Part of the problem is that if you think about executing a contract on ethereum or trying to build an application on ethereum, it’s just not at a point that can scale. And same with bitcoin. We have a number of challenges that have to do with scaling and therefore transactional costs, and they’re just kind of the fundamental ... Like the idea of having a decentralized architecture necessarily means that you have a less efficient architecture than anything centralized, right? There is some analysis I saw someone do that talked about how it’s, I think that it was 400 million times more expensive to execute a snippet of code on ethereum as it is to on AWS.\\n\\nKS: Right.\\n\\nSo you have to have a very strong reason to do it, and the infrastructure to really let you scale doing that is still very new, which is what a lot of the ICOs are going towards. So it’s a trite conversation. But I think we’re in the ’80s. It just happened much faster and it’s a little bit like we need to let the creativity bloom and see what ...\\n\\nKS: Does that mean the big companies will be the ones — or the banks — that will be taken captive of before that? Because it’s against the banks’ business. I mean if it works in the philosophical way, that it’s supposed to, you don’t need banks, you don’t need ...\\n\\nThat would be a very long-term.\\n\\nKS: Really?\\n\\nI think so. There’s just a lot of intermediate steps that have to happen before we’re ... I think often what happens is that in the very beginning of any new platform change, you think about disrupting kind of the incumbent stuff. But really, what ends up happening is that you kind of start to move into a more orthogonal area.\\n\\nKS: Sure.\\n\\nAnd then that ends up actually being like the third step, is that disrupts the incumbents.\\n\\nKS: If you take it to the logic I assume, and I’m not a genius around this, is you don’t need governments. Because what do governments do? They provide currency and maybe an army. Ultimately if you strip it down.\\n\\nGo ahead.\\n\\nTS: Did you question whether or not a conclusion of this exercise will be, “Do you need venture capital firms?” I mean obviously, that’s kind of a paradox that all VC firms are in today, they employ people like you to look at crypto and should we invest in crypto, and how does Benchmark make money off of it.\\n\\nSide by side, there are other venture capitalists who again have varying levels of worry about ... Not that every entrepreneur in 10 years is going to raise money through an initial offering, but some will. And do you worry about kind of the existential question about — you’ve been at this for a long time, you obviously love the gig — about whether or not it’s going to raise questions about the need for venture capital firms at all?\\n\\nThere was a period when this ICO craze was happening where you had to ask yourself that as a VC. And if you look at the stats in 2017, there was more money put into ICOs than seed investments. That definitely has to make you wonder. And of course you look at how much money some of these companies raise in their ICOs and it completely takes the business away from a VC. But I don’t believe that the ICO structure as it is right now is going to continue. I think that there was a lot of FOMO driving it. I think that it is very clearly a bubble to me, what’s happening at the ICOs. You’ve got all these people who made a lot of money in bitcoin, and later ethereum, who are kind of rolling it into these ICOs.\\n\\nKS: They’re super smart.\\n\\nThey think they have the Midas touch. And by the way, like the bubble ... Everybody looks smart in a bull market. And so there are a lot of people who think they’re really smart right now. And then you’ve got all the people who have FOMO. I can only imagine what it must have been like in the internet bubble where you have all these people around you getting rich, and you’re like, “I want to get rich too.” And so what they end up doing is that they hear about a new blockchain project and they buy into the token offering and you’ve just got all these people raising a lot of money.\\n\\nBut there’s a couple of things. No. 1 is, I do think a lot of these projects are going to collapse. A lot of them have been complete scams. But even the ones that were very reputable had been blessed by a lot of the powers that be. Projects like Tezos have had pretty visible ... not quite implosions, but real problems with them. And then you’ve got like a lot of the money that’s just been floating around has been irrationally exuberant. It’s just going to go away. And what we’re going to be left with is that actually making these early stage calls is really freaking hard.\\n\\nAnd they also ... For what it’s worth — and this may be right or wrong — I don’t fundamentally believe in the structure of the ICO. I don’t believe in a couple of engineers writing a white paper, trying to anticipate all the inflation rate that they want to have, all the incentives. I mean, it just seems like we’ve gone back in software development to the ’80s, where you had waterfall development. And really, to build these types of networks, you want to have a very iterative process. You want to be able to adjust the inflation rate.\\n\\nI always think about War of Warcraft. Lik,e they had this gold. You know, they have gold in War of Warcraft, and they never could have anticipated some of the times when they had to have an inflationary monetary policy or a deflationary monetary policy. It took them a while to kind of get a sense for how gold was going to be used and mined to really know what to do.\\n\\nSo you’ve got people who try to anticipate everything. They write this long white paper. They then sell tokens to people and then they’re beholden to those people, and it makes it a lot harder to pivot, it makes it a lot harder to iterate from there. And so I think, if I were to guess, I think that you might actually see almost a kind of Open Source 2.0 type structure, where you’ve got people who want to create a new token project and they actually raise good old-fashioned equity for that company. They become the core contributors to the project. In the token offering, they get tokens on the balance sheet. And that’s their incentive to kind of continue to make that project go forward. And so you end up having a little bit more of a centralized-decentralized hybrid model that ends up, thankfully for me, still creating a need for venture capital.\\n\\nKS: To invest in these companies.\\n\\nYes.\\n\\nKS: And it will be the companies, not the bitcoin speculators, that will make them money. I remember there was a stock market bubble during the early internet for companies that don’t exist anymore. There was one that was a storage company. They made little things you stored stuff on. It was a bunch of companies like that and then they all went away. It was really ... And then there was Google years and years later.\\n\\nI believe that.\\n\\nKS: Later. Amazon later.\\n\\nRight.\\n\\nKS: It was interesting.\\n\\nRight now the space is, I think, attracting the best and the worst of people.\\n\\nKS: Right. Yeah I think when it becomes something that people use ... when businesses use ... when it really matters, but we don’t know that. We don’t know that yet.\\n\\nAnd you would say, like I mean I think bitcoin itself as a store of value is a really important early use case, this kind of idea of payments like a digital ...\\n\\nKS: If you can get it to people using it.\\n\\nYes.\\n\\nKS: Who does that?\\n\\nWell I can send you bitcoin right now.\\n\\nKS: I know that, but who actually uses it for actual transactions?\\n\\nRight. No one’s using it as a payment product. It’s more that people ... I kind of think of it as my dystopian hedge. People are preppers and this my version of prepping.\\n\\nKS: Are you a prepper?\\n\\nNo, I’m not a prepper. But I do have bitcoin as my dystopian hedge.\\n\\nKS: Because then you will then do what?\\n\\nWell you know, it’s good to feel like you have a type of currency that exists outside of our government.\\n\\nKS: I just have a big pile of gold.\\n\\nThat’s a good idea too.\\n\\nKS: Beyond this, what else are you interested in? Is it cryptocurrency all the way? You clearly are obsessed with it.\\n\\nI spend a lot of time there, but I’m always looking for new opportunities and consumer ... We at Benchmark still believe in the consumer space and obviously we have companies like Nextdoor and Discord and Marco Polo. And then when I was at Pinterest, I got to see the transformative impact of deep learning. My team, we started out with a plain vanilla computer vision, where I was spending nights and weekends drawing bounding boxes on pin images to try to create classifiers for, “These are boots and these are heels and these are bags,” and then we started to play around with deep learning.\\n\\nAnd it was this tremendous step function in the experience that you could create. And so I know I’ve spent a lot of time thinking about, “Well, what are the applications both to business users and consumers that leverage deep learning in some way?” And then my partner Eric spends a lot of time there too. We’ve kind of gone up and down the stack with companies like Cerebro, which is like a chip-set company that makes it easier to train AI models, to some other companies that are I don’t think announced just yet.\\n\\nKS: All right. They’re all the future stuff.\\n\\nYes.\\n\\nKS: Anything else that you’re worried about?\\n\\nI mean, I worry about the strength of the incumbents. It is really ...\\n\\nKS: They’re pretty strong. Never before like this.\\n\\nNever before. I didn’t realize how lucky I was to start my venture career in 2006. At the time, we had software as a service and it was almost the simplest roadmap. It was take an on-prem software product, put it in the cloud and you’ve got a public company. It was just kind of rinse and repeat. And then you had Web 2.0 and then mobile and you had going from kind of 3G to 4G, like you had all these wonderful technology transformations that created so much greenfield opportunity.\\n\\nAnd here we are where you’ve got Facebook, Google, Amazon, Netflix, these kind of incumbents, Apple obviously, that are so strong and it makes it really, really hard for a founder to find the seam that they can exploit. And so yeah, there’s a little bit of waiting, kind of wondering when the next platform is going to emerge. Which is, I think, part of what drives a lot of people like me into crypto right now. It is a little bit kind of relative thing ...\\n\\nKS: Which they don’t have their arms around.\\n\\nExactly. And there’s no incumbents in crypto. You could think of bitcoin and ethereum as the incumbency, but it’s not like there’s a Mark Zuckerberg running one of those companies. They’re decentralized protocols. And that means that there’s still a lot of opportunity in the market.\\n\\nKS: Okay, we’re here with Sarah Tavel who is a partner at Benchmark. We were just talking about cryptocurrency. When we get back we’re going to be talking about a bunch of other things that she’s doing as a venture capitalist.\\n\\n[ad]\\n\\nWe’re here with Sarah Tavel, a partner at Benchmark, and we’re just talking about her giant interest — her obsession, really — with cryptocurrency, which she’s correct to do so. But there’s other parts of her job. We’re going to be talking about.\\n\\nTeddy, you had a question about ...\\n\\nTS: Sarah, you’re involved with a group of women venture capitalists — in I think a couple of cities now, it’s not just here — where the idea is to basically get together and have a network for female founders to come to when they’re looking for advice. I’m curious. There’s been a lot of organizations of women in tech. I know it’s early, have you noticed any tangible impacts yet or if it’s still of the network of female VCs or what kind of the bigger picture goal is? What do you imagine the impact of this type of thing would be?\\n\\nI think it’s important to give a little context on how we got started, which was ... It’s seems like so long ago now, but when all these sexual harassment cases were coming out with VCs and female entrepreneurs. And there was a little bit of ... I had been organizing these kind of monthly breakfasts with a bunch of women GPs, and then we all kind of would come to these breakfasts and feel a little shell-shocked about what was happening and then we actually ended up organizing a dinner, and it was kind of like, “Well, what can we do to make it better for female founders, to make it better for female VCs?” And it ended up being this really wonderful group of women.\\n\\nKS: All four of you.\\n\\nThat’s the thing, actually, is when I started at Bessemer in 2006 ...\\n\\nKS: I can’t believe the one in 25. That’s astonishing.\\n\\nYeah. And it felt like, you would look around and try to see who were the role models. And it was so small, the number of women.\\n\\nKS: Mary Meeker.\\n\\nI don’t think Mary Meeker was ... it was people like Theresia Gouw and Jennifer Fonstad. And there was a handful — like literally a handful — of female GPs. And then and we kind of reached this point — I mean obviously there’s so far to go as an industry — but we reached a point where it felt like all of a sudden we had a critical mass of women.\\n\\nAnd I think that sometimes you’d hear that kind of early women in Goldman Sachs or whatever group it could be, because there were so few of them, they ended up being more competitive with one another. But instead what we have now is we have a group of women that really have come together because we want to help one another. We want to make each other more successful and we want to help make female founders more successful.\\n\\nAnd so we came together, I think that the first dinner was maybe a group of 15 GPs or so, and there’s been a couple of blog posts about it, and then we ended up trying to think, “Well what are the initiatives that we want to take on to pursue this mission?” and Jess Lee from Sequoia, who is one of the new additions to the female partnership, she ended up kind of having this idea for hosting female founder office hours. I think it was probably something that she wished she had had when she was at Polyvore.\\n\\nBy the way, Peter Fenton, as you guys know, was an investor in Polyvore and so we ended up ... The first event we had was actually having office hours for female-founded seed companies. We did that in San Francisco. Then it kind took off and then we had one in New York, one in LA recently. We’re going to have one ... I think it’s next week for series A companies here in San Francisco. And who knows whether any investments come from that. But what is important that’s coming from that without question for me is that we’re increasing access of the female VCs to female founders and vice versa.\\n\\nKS: So can I ask you, why do you have to do that? I mean honestly, on some level it’s insane, that you have to do it and then you get all these comments, which drives me nuts, when we’re doing it based on standards, but they aren’t. They 100 percent aren’t doing it based on standards.\\n\\nYeah. I mean there’s definitely ... I asked myself that question. I do already feel like there’s attacks on my kind of being a female VC and doing the job and then we end up taking on these other things. But we’re thinking about actually how now to ... Like there are so many, as they call them, male allies out there.\\n\\nKS: Allies.\\n\\nI know.\\n\\nKS: We shouldn’t even look at them as allies. It’s their job.\\n\\nThere are many of them. And so how do we integrate them into what we are doing? Like, it shouldn’t just be a female founder pushing a female VC and that’s what it is. It should be a female founder pitching a VC. And one the top VCs ...\\n\\nKS: Or just a founder.\\n\\nWell, you have to start somewhere.\\n\\nKS: Yeah. I guess.\\n\\nThat was one thing. Then the second thing is actually we co-hosted an event, I think it was actually last week, where it was something where we hosted a couple of panels. I hosted a panel with Rebecca Kaidan, who’s a GP now at USV, and Emily Melton, a partner at DFJ, on career trajectories for VCs, and we had 80 women in the audience who were in investing jobs in venture capital firms. And it was one of those things where I could only imagine if I had gone to attend one of those events, like when I was one or two years into the job, how inspiring that would be to me. Just seeing people who are succeeding and doing it makes you feel like you can do it too. So I think that there’s a lot of positive things coming out of it. It is a lot of work. Like we’re trying to figure out how to start to spread that around.\\n\\nKS: Spread it around how, what do you mean?\\n\\nWell, just getting more people under the tent to take on some of the things that we ...\\n\\nKS: What’s the actual problem though? I mean besides rampant sexism across this globe? But what’s the actual issue? Because you are someone they’ve let in, you know what I mean? And now you have status at Benchmark and have the experience, but they seemed to like just have one, like a Mary Meeker or you or something like that. From your perspective what’s the problem? And I know you want to be nice about it, but what you imagine?\\n\\nYou mentioned the biggest problem, which is ...\\n\\nKS: Rampant sexism.\\n\\nRampant sexism. There’s subconscious bias of course, which is a big thing. I think the subconscious bias is a really big thing, actually. If you look at ... I was just reflecting on this, you look at many of the female VCs that are in GP roles have followed career trajectories that are very similar to my own, which is that you come in a more junior role, which is a lower-risk role to hire for, prove yourself and then you can get the GP role. And it’s a little bit that kind of subconscious bias thing where men get promoted based on potential and women get promoted based on what they’ve achieved.\\n\\nKS: Mm-hmm.\\n\\nThat is a bias of which women get into the ranks. It’s just easier when you’ve already proven yourself. But at the same time, that is starting to change. I mean, you see more and more examples of women being brought into these GP roles. And I think there’s all types of things, there’s the natural thing of homophily, which is we naturally hang out with people who are like us, and there’s nothing wrong about that.\\n\\nBut you have to make that extra effort to go beyond your natural network. And I think, people are just starting to realize that that’s what we need to do, both when we are looking to hire a VC at a firm but then also a female executive or female engineer or underrepresented minority in one of these fast-growing startups. It takes that extra effort because we’re starting from a homogenous nucleus and you have to try extra hard to break through.\\n\\nKS: Right. Absolutely. At the same time, what’s interesting to me is that it’s an ongoing issue. It’s sort of fascinating because I do think a lot of women stay in operational roles at companies because it’s a better career path trajectory. Being one of anything is always difficult.\\n\\nCan we shift to the venture business in general? I don’t want you to have to be the one to talk about it, speaking of wh, but it’s really ... you got real touted when you got hired. “Oh, we got Sarah Tavel.” “Wasn’t she just over here?”\\n\\nTS: The same total number of women jumping from firm to firm.\\n\\nKS: Sequoia next.\\n\\nJust got that one.\\n\\nKS: That’s right. What about Excel? Does Excel have one? Excel, you better get on it.\\n\\nI am very happy at Benchmark.\\n\\nKS: Good, good. So you eventually get Bill in general. Where do you imagine where we are and where it goes? I would tell you, you have SoftBank coming in.\\n\\nWell that’s a big question.\\n\\nKS: Just for those who don’t know SoftBank, Teddy?\\n\\nTS: Sure. SoftBank is both loathed and terrorizing but also an opportunity for a lot of venture capitalists because they can mark up companies in your portfolio. But I think the bigger question here is like, obviously with valuations on the rise and SoftBank being a threat and to some extent an opportunity, how do you think about this? Does that make you think about the types of deals you do differently? Does it worry you about losing a deal because someone adds a zero to the end of the check?\\n\\nIt’s interesting to see that many other venture firms are having a little ... I don’t know if this is fair to say, but scope creep, where a lot of funds that have started out as kind of traditional series A, series B funds are starting to expand both into the seed world and into the growth world. And so they will end up kind of competing and feeling that pressure more than we do. We went through a period of time at Benchmark where we joke it was like the empire-building phase where we ...\\n\\nTS: Israel.\\n\\nYeah. We had Israel, we had London, we had a bigger partnership, and our returns suffered during that time, and we ended up really trying to kind of come back to the core of Benchmark.\\n\\nKS: People can’t help themselves.\\n\\nA lot of people can’t help themselves. And so we kind of went through an experience where we came back and now are very, very clear on who we are and what we want to do, which is series A, series B investing, series A really being what we focus on. And SoftBank may come down to the series A one day. If so, God bless ’em.\\n\\nTS: I definitely don’t think people should rule it out. I mean, the idea that they would do that.\\n\\nIf you are SoftBank ... SoftBank wants to put a lot of money to work and it’s very hard to do that at series A. So I think it’s unlikely that that will happen. And so like for us, we’re just focused on doing what we’ve been doing for 20-plus years now, which is being the first call and really kind of trying to shepherd these companies to their full potential. We can’t ignore what’s happening with SoftBank. Like we obviously were beneficiaries of that with Uber and WeWork. And then you’ve got Katrina Lake at Stitch Fix who raised $42,000,000 and went public and has a multi-billion dollar company.\\n\\nKS: Which didn’t take that ...\\n\\n$42,000,000. That’s just an unbelievable story. And so we hope to find many more of those companies.\\n\\nKS: Can they resist that money? Because one of the things is you’re all investing in innovative and groundbreaking companies. Venture capital hasn’t been that. Everybody comes out remembering when Marc Andreessen came in and we’re going to change venture capital. Not really. It’s the same system, essentially. But what does disrupt venture capital?\\n\\nI mean a lot of capital does disrupt the venture capital, right, which is the problem we’ve had as an industry. Bill obviously has been speaking about that for a while. He does a good job there. And it’s one of those things that you keep on expecting the problem to balance in the other way, and instead, it actually is just getting worse, obviously, with SoftBank as the most recent entrant\\n\\nKS: Where does the money go?\\n\\nThe money is getting jammed into a lot companies, and I think it remains to be seen what the impact will be on these companies. Like I think about when you’re running a business, you have this decision, which is that you can turn the wheel towards growth or you can turn the wheel towards profitability, and it’s really freaking hard to turn the wheel towards profitability.\\n\\nIt’s like this active will every day of knowing what to say no to, which you have to say increasingly. But then when you have a SoftBank or you have just like this amount of capital available in the private markets, the amount of diligence you have to do to raise money in the private markets versus the process of going public, it’s night and day. It’s just so much easier. And so the companies can kick the can down the road by raising more money in the private markets, and then if they don’t have the same pressure towards profitability that they would otherwise have if they were public ...\\n\\nKS: And then they run the risk of SoftBank giving their money to a competitor.\\n\\nIf they say no, that is the risk that they, SoftBank ...\\n\\nKS: I think that was the movie “The Godfather.”\\n\\nTS: Dara had a great quote yesterday at the Goldman Conference where he said yeah, “I’d rather have the capital cannon behind me than in front of me.” That’s like, kudos to them. I mean, it’s a strong negotiating tactic when they say we’re going to invest in Lyft if this SoftBank tender with Uber doesn’t go through, and then it works. I think it works.\\n\\nKS: Although money isn’t always the factor.\\n\\nAnd yet it takes a tremendous amount of discipline if you’re one of these founders, and then you have this tremendous war chest to then stay disciplined with how you use that capital. It’s just a bigger version of what we’ve faced in the earliest-stage markets, where ... I’m starting to sound like an old person, but when I started my career I remember, we were working on a series A investment and it was like a $5,000,000 investment, we were just like, “Oh my God. This is an expensive round.” And what’s it going to do with a company that have $5,000,000 at the stage of the company. And here we are where the series As are easily $15,000,000 but can be far, far bigger than that.\\n\\nKS: Some of the numbers are ... You could also just put the money in the bank, I mean speaking of Pinterest, they didn’t spend a lot of their money, right?\\n\\nBen has thankfully, he has the DNA of a very disciplined ...\\n\\nKS: He’s cheap is what I call it.\\n\\nHe’s cheap. Yeah. He is cheap, and I think that it is so easy to just spend money when you have it and it really does ... I am one of those people who believe that having a scarcity brings out the best in a company and really focuses you on what is core to the company and then you execute better. And so I liked that discipline and I think it remains to be seen what’s going to happen to these companies that get to raise a lot of capital.\\n\\nKS: They may never go public.\\n\\nThey may never go public. That’s the fear ...\\n\\nKS: What’s the next company to go public?\\n\\nWhat’s the next one to go public? I couldn’t say.\\n\\nKS: You couldn’t say. I say Airbnb, probably.\\n\\nI don’t think so. I think there’s going to be someone else before that.\\n\\nKS: Really? A good one?\\n\\nYes.\\n\\nKS: Uber?\\n\\nNo.\\n\\nKS: She says nothing. All right. Last question. And then Teddy, you’ll have a last question. If you were to give a tip for someone who’s become — and I don’t wanna say female, just any venture capitalist — what would it be? What is a mistake you made that you would say, “Oh I should’ve done that differently.” It can be something you did great, too. It doesn’t have to be a mistake. Like one thing that was critical to you.\\n\\nFor someone young in their career?\\n\\nKS: Yeah.\\n\\nFor me it was absolutely finding someone like a Jeremy Levine, who takes you under their wing and teaches you how to do the job. People have for a long time talked about venture capital as an apprenticeship business. And there’s no question that’s what it is because it’s one of those businesses where ... I call it the learning cycle. Like the feedback cycle is not fast. It takes years for you to know whether you’re on the right track with the company. And so when you’re a young person and you don’t have that pattern recognition down yet, you have to bootstrap it from someone else’s pattern recognition. And it’s very, very hard to find a partner who is willing to invest the time in a young person to show them the craft.\\n\\nI mean, I fight for 15-minute slots in my schedule right now, where if I spend an hour with someone that means I can’t spend an hour with a founder. Your time is very, very precious, and so to find someone who will make that investment, which is a very long-term investment, was critical for me. I was unbelievably lucky to get to work with Jeremy, because he’s not only just the great investor but he was a great mentor to me. I always tell people that when you join a firm, you want to make sure you’re going to be set up for success. And the biggest way is, who’s going to be that person? Who’s going to chaperone you? And you have to pick the right person.\\n\\nKS: Walt Mossberg did that for me.\\n\\nTS: So that’s your advice for venture capitalists. The corresponding question is you’ve now been at three firms, sounds like when you were at Pinterest you were dealing with firms. What’s your advice for CEOs who are potentially negotiating a deal or they have four term sheets, one of them is 15 percent higher. My point is, you’ve seen a lot of shops.\\n\\nI always tell people two things, which is No. 1, you want to really make sure that that VC that you’re going to bring on to your board is going to be as much on the same side of the table as you are in terms of understanding what the strengths and weaknesses are for your business, what the vision is and where you want to go from there, because the last thing you want is to have that first board meeting and you kind of say, “Here’s the new initiative I want to do.” And the VC’s like, “I thought you don’t want that strategy mismatch.” That’s No. 1.\\n\\nAnd then No. 2, and this is probably just as important, is references. The reference is like ... what I always tell founders is, if someone gives you a reference and it’s a hot company here, a hot company there, it’s so easy to be a fair-weather VC and it feels good to get to call Brian Chesky or whoever it is and do a reference. But ultimately the best references, the most useful ones will be the ones when it’s a company that has been going sideways or didn’t succeed or even the CEO is like, “Oh, talk to those people and you’ll really figure out what that VC is made of.” And you want to know, like you don’t want a fair-weather fan on your board. You want someone who will do the work.\\n\\nKS: And there are lots of those.\\n\\nThere are a lot of those.\\n\\nKS: There are a lot of those, which are weaker people or people who, they’re not necessarily malevolent ...\\n\\nJust cheerleading, and when things are good they’re there and then they don’t do the hard work.\\n\\nKS: Yeah. And also assholes, you shouldn’t go with that. They are easy to find.\\n\\nI say no to assholes.\\n\\nKS: Yeah, absolutely. Anyway, Sarah, this was great talking to you. It was an interesting discussion and I have a feeling you’re going to be running everything someday, it’s a sense. And thank you for coming on the show.\\n\\nThank you so much for having me.\\n\\nKS: And taking your 15-minute slots. I really appreciate it. Now I feel badly for being late. And thanks to Teddy Schleifer from Recode for co-hosting with me today.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Learn How To Implement AI And Deep Learning Apps', 'AI and deep learning is the next tech frontier. So it makes sense that companies like Google and Tesla are racing to develop the next big break in the field. And while AI, machine learning and deep learning are sometimes used interchangeably (especially since they\\'re all #buzz #words right now), they\\'re actually different things.\\n\\nHere\\'s a good way to think of it:\\n\\n\"AI\"\\n\\nThe umbrella under which both machine learning and deep learning lives — AI is the broad concept of machines being able to carry out tasks.\\n\\n\"Machine Learning\"\\n\\nMachine learning is what we talk about when we (usually) talk about AI, and what happens when humans give machines data that they learn from/respond to. It\\'s also definitely what we talk about when we discuss the incredible power of self-learning machines: there are a vast number of problems that can be resolved by providing the right training data to the right learning algorithms.\\n\\n\"Deep Learning\"\\n\\nLet\\'s get a little bit more meta: deep learning is a method of implementing AI. Machine learning uses AI to develop neural networks designed to mimic human decision-making — but deep learning uses a subset of ML techniques and tools, helping solve any issue that requires \"thought,\" whether it\\'s human or artificial.\\n\\n\\n\\n\\n\\nAlright, now that you know what those terms mean, time to dive into the programming languages and platforms used for machine learning and deep learning. All of which you can learn in the new Pay What You Want: AI & Deep Learning Bundle, by the way.\\n\\nPython\\n\\nPython is a hugely popular general purpose programming language, so it\\'s beneficial that it can also be solidly applied to AI and deep learning principles. You can use Python to cluster and automatically segment data, build automatic speech recognition systems and build intelligent applications centered on images, text and time series data. You can also use Python to cover backpropagation and use it to train neural networks, developing some of the deep learning applications we\\'re most familiar with today, like image recognition, speech recognition, object recognition and language translation.\\n\\nJava\\n\\nIf you\\'ve dabbled in programming, you likely picked up some Java. Like Python, it\\'s another immensely popular general purpose language that can be leveraged to produce AI and deep learning apps. Java is a fundamental driver of producing AI and ML apps and has some of the most popular Deep Learning frameworks to explore neural networks with.\\n\\nTensorFlow\\n\\nTensorFlow is Google\\'s open source library for machine learning and deep neural networks research — and in line with pretty much everything you\\'ve come to expect from Google: it\\'s been in rapid development since inception. It\\'s cross-platform, runs on GPUs and CPUs, and works across mobile and embedded platforms. Plus, it can be deployed to production without having to depend on any of the code that helped build it, using only the runtime necessary to execute it.\\n\\nR\\n\\nR is a programming language and software that\\'s the darling of statisticians, mathematicians, and scientists all over the world. It\\'s easy to use, and allows for powerful data visualizations and analysis — it\\'s also increasingly popular for developing machine learning and deep learning tools.\\n\\nKeras\\n\\nKeras is an open source neural network library written in Python, designed to allow users to experiment quickly with deep neural networks. It\\'s user-friendly, using modes to develop higher level deep learning apps.\\n\\nHadoop\\n\\nHadoop was originally developed to improve search engine technology, but today can process enormous amounts of data on relatively inexpensive hardware — and of course, can store data on a distributed file system without needing to transform it in advance. That makes it a great tool to deploy deep neural networks.\\n\\nWant to learn more? This Pay What You Want: AI & Deep Learning Bundle covers all these tools and platforms and more, with seven e-Books and ten hours of course content.\\n\\n\\n\\n\\n\\nIf you buy something through our posts, we may receive a small share of the sale. Please buy a Ferrari. For more of Digg\\'s suggestions on how to spend your money, check out Digg Picks.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('A day in the life of... director of engineering at Quantcast, an AI martech company', \"Peter Day is director of engineering at Quantcast, a company that provides AI-driven audience insights, targeting and measurement, to improve ad campaigns.\\n\\nDay is also our latest (and aptly named) 'Day in the Life' interviewee.\\n\\nEconsultancy: Please describe your job: What do you do? And who do you report to?\\n\\nPeter Day: I’m the Director of Engineering for Quantcast, an AI technology company focused on the marketing and publishing industries. We use apply machine learning to our unique data drawn from 100 million online destinations to uncover insights to help brands get closer to their audiences and create ad campaigns that help them grow. I’m responsible for our team of engineers in Europe and report to our SVP of Engineering in San Francisco.\\n\\nMy team is made up of data modelling scientists, research scientists and other experts who come up with new ways of understanding consumer behaviour online using machine learning and AI. We are constantly iterating on our algorithms, tweaking them to react to changing consumer behavioural patterns, and inventing new ways for brands to connect with their audiences. This produces up-to-the-minute live audience insights that get used by marketers in their creative process.\\n\\nE: What kind of skills do you need to be effective in your role?\\n\\nPD: While a high level of expertise in machine learning is important, creativity is essential. There are around 50 PhDs among the team at Quantcast. Developing software is about starting with nothing and building something, so you need to be resourceful as well as smart. As a leader, I focus on cultivating an environment that combines the two so people can really thrive in their roles.\\n\\nPeter Day\\n\\nE: Tell us about a typical working day…\\n\\nPD: While there really isn’t a typical day, one thing I make sure we do as a team is a daily “stand-up” meeting at 10am. As a rule we aim to minimise meetings but have found having this daily touchpoint is really important. After that, I go where I’m needed.\\n\\nI try and balance my day between people and data. I’m often spending time with media agencies and senior marketers educating them about AI and machine learning. I also try and carve out time each day to focus on the data we’re working with, whether that's building data models or writing my own software to keep myself in “coding mode”. Spend too long away from the data as an engineering leader and you can’t effectively advise your team on how to tackle challenges. When I’m not doing that, I’ll be trying to throw regular surprises and challenges at my team to keep them thinking.\\n\\nE: What do you love about your job? What sucks?\\n\\nPD: The best part of my job is solving seemingly impossible problems with clever and interesting people. I love getting stuck into the detail of what we’re doing which reminds me Quantcast really is the pulse of the internet. Deep learning is already taking our understanding of human behaviour to new heights, so being able to spot changes in data patterns and react to online behaviour is fascinating.\\n\\nWhat sucks? Not having enough time. Our customers are all over the world which I means I travel a lot. I enjoy spending time with them, however I’d like to spend more time with the team and family.\\n\\nE: What kind of goals do you have? What are the most useful metrics and KPIs for measuring success?\\n\\nPD: We set our own goal and metrics, which we review regularly. Specific measures differ by project, but there’s always a focus on giving value to our customers. An example might be improving advertising performance by 10% in the next three months.\\n\\nMarketers are under increasing pressure to show value from their budgets. Where they succeed, we succeed. Our measurement tools allow brands to more accurately tie their activity to brand growth so, wherever possible, we’ll benchmark our success by the success of our customers.\\n\\nE: What are your favourite tools to help you to get the job done?\\n\\nPD: In my role, effective communication is vital given how crucial the engineering function is to the business. As a result, anything that allows me to communicate more effectively has value. Quantcast currently employs more than 800 people across 10 countries with one in four of those in a product or engineering role - so there are a lot of people to communicate with.\\n\\nSome of the people I work with prefer different styles of communication. Some of the best conversations I have with my boss are via online messaging, whereas when I speak to the commercial lead in Europe, video conference or the phone can be more effective.\\n\\nE: How did you get into ad tech, and where might you go from here?\\n\\nPD: I studied machine learning at university, and after that the most interesting industry to leverage my skills was in finance because of the huge amounts of data the sector was dealing with. However, I soon found myself running a massive department spending more and more time on the day-to-day complexities of running a large organisation rather than focusing on what I’m passionate about.\\n\\nAt the same time, the digital advertising space was exploding and producing vast quantities of data. I was familiar with Quantcast’s CEO, Konrad Feldman, since he’s had a similar path to me. I found out that the company took machine learning seriously, so that’s what sealed the deal for me.\\n\\nE: Which brands have you been impressed by recently when it comes to machine learning led marketing/advertising?\\n\\nPD: We’ve been working with travel brand Tui closely for a couple of years now and they’ve done a fantastic job at leveraging AI in their campaigns. They are completely open to what machine learning is telling them and adapting their strategy to emerging patterns in consumer data. As a result, they not only understand their own audience but also their competitors’, which helps them to tailor their messaging and product set to market. They are going beyond using machine learning just for targeting, also understanding existing and potential customers in much more nuanced ways.\\n\\nE: Do you have any advice for people who want to work with data in adtech?\\n\\nPD: To work with data in adtech, you need to get your hands dirty. Don’t rely on other people to do stuff for you. Most of the tools available these days are straightforward if you put the effort in to learn how to use them.\\n\\nMost magic that emerges from data tends to be through exploration, so my advice would be to skill-up, learn SQL, learn data analysis techniques, and listen to your creativity. You’ll find that doing it directly will help you ask the right questions.\\n\\nSubscribers can download Econsultancy's Trend Briefing: Artificial Intelligence (AI).\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Ask the experts: Email marketing optimisation', \"The latest in our 'ask the expert' series focuses on a core skill in the digital marketer's skillset - email optimisation.\\n\\nWe grilled three of the loveliest experts we know – Kath Pay (Holistic Email Marketing), Parry Malm (Phrasee) and Dale Langley (Emarsys).\\n\\nHere are their pearls of wisdom. Note you can skip between questions using the links below.\\n\\n(Additional note: Econsultancy provides face-to-face and online training in email marketing, and subscribers can download our Email Best Practice Guide).\\n\\n1. There are lots of things to optimise. Where should marketers look first?\\n\\nKath Pay, founder and senior consultant, Holistic Email Marketing:\\n\\nI would focus firstly on the subject line, as if they don’t open/read the email, then it’s hard to test everything else. But over and above that I recommend testing motivations as the more you know about your customers, the better you can speak to them.\\n\\nSo, ask them what they like best; via testing in the channel that is one of the key drivers of traffic to your website – email marketing. Your email database contains your target market, so use email’s unique push ability and treat every email as a survey by asking your customers what they like via a scientific A/B testing program.\\n\\nBy using a hypothesis and testing to determine a motivation, you are not limited to just testing one factor (i.e. subject line, CTA, landing page, copy, imagery). As long as they all support the hypothesis, for example “benefit-led copy will increase conversions over loss aversion-led copy” – you will be able to test the subject line, CTA, headlines, copy, imagery and landing page as you are testing a motivation rather than a factor or element of the email.\\n\\nThis is what we call Holistic Testing.\\n\\nBy seeking these long-term valuable insights through email marketing, you not only increase results within email marketing but you can share them across other channels to drive the business objectives that are common to all channels. Apply what you've learned to your website copy and organisation, to your search keywords, PPC campaigns and related landing pages, and in ads and banners you run on third-party sites in remarketing or network campaigns.\\n\\nEmail gives you a good basic testing structure that you can build on to sharpen your insights and improve your marketing efforts bit by bit across all channels. It's another one of email's superpowers that marketers so often overlook or ignore.\\n\\nUltimately, it's another reason why investing both time and money in email pays off across your entire marketing program.\\n\\nParry Malm, CEO, Phrasee:\\n\\nWell, here’s the thing. I run Phrasee, a company that uses AI to create better subject lines than humans. So what am I gonna say here?\\n\\nStill, jokes aside: your subject line is the crux of your email marketing programme. If it sucks, then your snazzy content won’t get seen, no matter what time you send it. So yeah, logic and statistics indicate the subject line is where to start. Sure, I’m biased. BUT - that doesn’t make me wrong.\\n\\nDale Langley, head of deliverability, Emarsys:\\n\\nSearching the Internet for ways to optimise your email program can often lead marketers into a crazed frenzy of making changes with little understanding of whether it’s sensible to make those changes and what the long term consequences may be. My advice is to remember one thing; no-one knows better what your customers want than you. You just need to read the signals and have a plan in place for measuring results. Different optimisations can work better for different stages of the customer lifecycle and some are better suited to short-term gains at the expense of long-term results.\\n\\nFor example, using tempting offers in your email subject line such as 50% off will surprise (some of) your customers and lead to an increase in clicks, but where do you go from there? Some brands are now so perpetually stuck in the discount game that they’re unable to get out. Instead, use different offers for different segments (such as lapsing high-value customers) and use other channels, such as social media, to encourage one-time purchasers to re-engage.\\n\\nYou also have to recognise that a customer’s profile will change over time. This includes when they prefer to read email, the frequency that they want to receive it at and the topics that interest them, a common mistake of marketers is to build a marvellous customer journey but to forget that customers can switch personas at any point. Our advice is to build out your personas and understand the motivation for each persona to engage with your brand, model the customer journey for each persona and built-in the ability for personas to change over time. This requires effort but it’ll pay dividends in the long-run.\\n\\nAnd if you’re interested in testing things like subject line, send time, content etc. make sure that you’re using a proper control group. This means that for a particular campaign, you will exclude a group of customers (the control group) who are similar to customers in the rest of your database and are only excluded for this one campaign. When you make your change (to subject line, send-time etc), measure the revenue generated by the control group vs. the campaign and you can determine whether the tactic you’ve employed yielded any results.\\n\\n2. Is there a particular metric marketers should be optimising for?\\n\\nParry Malm, Phrasee:\\n\\nMany will say to focus on end conversions, and, in related news, many also don’t have a strong grasp on statistics.\\n\\nThink about it like this. Say you’ve got a list of 1m subscribers, and 20% open. That’s 200,000 events you can learn from. But then let’s assume a 10% click-to-open rate. You’re down to just 10,000 events - and you run the risk of making decisions on insignificant numbers. Let’s say you get a 10% conversion rate of clicks - that’s just 1,000 events. You’re unlikely to get a statistically significant result, and thus will be making decisions based upon random variance. (Pro tip: anyone who doesn’t understand this should not be in your analytics department)\\n\\nHere’s another fact: the data shows that, in the long run, open rates correlate very strongly with click rates. And guess what? Click rates correlate very strongly with conversion rates. Therefore, the dominant strategy is to use opens as a proxy metric for email marketing success.\\n\\nThere are, of course, caveats to this. For example, you shouldn’t resort to spammer techniques just to get a few more opens in the short-run. Never forget that email marketing is, in essence, a form of advertising. You wouldn't put an ad on TV that was off-brand, so why would you send out emails that are?\\n\\nThe sweet spot is when you’re maximising open rates whilst remaining on brand. That’s when you’re winning at life. Well, winning at email, but still, you gotta take the wins you can get.\\n\\nDale Langley, Emarsys:\\n\\nI’m tempted to say revenue since that’s the ultimate goal of most email programs. However, I’m going to say inbox placement rate since if your email isn’t in the inbox, it’s not generating revenue!\\n\\nIf you’re having difficulties due to inbox placement then you need to figure out which levers you can pull to persuade the spam filters that your email should be delivered. Spam filtering mostly occurs these days when you’re sending email to people who aren’t engaging with your brand - so improve the acquisition tactics, introduce a re-engagement program and be prepared to suppress subscribers to your email program (or target through other channels) when they’re ignoring your efforts.\\n\\n3. What new technologies or consumer behaviors are having most impact on email optimization?\\n\\nDale Langley, Emarsys:\\n\\nConsumers now have a relationship with the brands they engage with, right from the first interaction you’re influencing whether a consumer will turn into a loyal advocate or a one-hit wonder\\n\\nThe days of your IT team triggering an email from the website upon signup and purchase, before the marketer takes over with batch and blast are gone. Every interaction a consumer has with your brand should be influenced by the marketer and through smart, AI-driven marketing automation platforms (like Emarsys), on which you have the ability to craft programs that adapt to the changing needs of the consumer.\\n\\n4. What types of emails can be optimized most effectively?\\n\\nParry Malm, Phrasee:\\n\\nTo exist as a concept, “optimisation” inherently requires measurement. Therefore, the key feasibility driver is universe size. If you’ve got a list of 1000 people on your list, well, you can “optimise” to your heart’s content… but you’ll just be doing it for the lols, as any significance measures will be unattainable.\\n\\nIt’s pretty simple, really. Focus on your campaigns with the biggest audience, which will in turn have the biggest impact on your bottom line. You’ll have way more data to experiment on and learn from. Then, apply what you learned to your smaller campaigns, and boom goes the dynamite. You’ll get that promotion you’ve been haranguing your boss about for the last six months. Congrats, big timer!\\n\\nKath Pay, Holistic Email Marketing:\\n\\nAll types can be – whether they’re campaign-based or automated. For either of these, processes and planning are key to success. With automated programmes, the hypothesis is being tested over time, which reduces the chances of time-sensitive anomalies, world events etc. affecting the results. However, with campaign-based tests, ideally, the hypothesis should be tested multiple times to ensure that the results are valid. A statistical confidence calculator should be used in all cases.\\n\\n5. How far will AI take us? How important is the creative person and their instinct?\\n\\nParry Malm, Phrasee:\\n\\nAI can do a lot less than what you probably think it can. For every AlphaGo, there’s a Tay, the racist chatbot. Everyone seems to think we’re approaching an AI apocalypse, where the machines take over. When that happens, I, for one, will welcome our new robot overlords.\\n\\nHere’s the reality. There are limited - and powerful - use cases for AI currently, and also for the foreseeable future. Here’s one powerful use case as a (totally unexpected amirite?) example: using AI to generate optimal subject lines. It is a known business requirement - to increase eyeshare on your marketing messages - that we solve by combining two forms of AI (NLG & deep learning). It's a niche problem, for sure, and that's by design.\\n\\nThe fact that we use AI is super dope. But – and it’s a Sir-Mixalot-sized BUT – that in itself doesn’t solve your problems.\\n\\nInstead, here’s my advice: if you have 60 minutes to solve a problem, spend 55 minutes thinking about the problem, and 5 minutes on the solution. If the solution uses AI, awesome, and if not, that’s OK too. The important thing is that your problem is solved.\\n\\nDale Langley, Emarsys:\\n\\nIn simple terms, human-driven personalization can’t scale. It just can’t deliver on true 1:1 experience that consumers want and brands strive for. AI can, which provides an opportunity for brands that understand how to introduce hype-free, tangible AI solutions effectively. AI allows us to analyze vast amounts of data, understand consumer patterns and channel preference and to craft incredibly personalized consumer journeys. Furthermore, AI allows us to do this in real-time rather than spending hours creating huge (or many micro) segments. It’s taking us much closer to the 1:1 consumer-brand relationship that we’re all striving for….the promise of marketing.\\n\\nHowever, AI isn’t human. It can’t (yet) build something from scratch without basing it on what it knows from the past and it can’t cater well for emotion and a true personal connection. This is why the combination of AI and human ingenuity is the key to successful marketing. We believe that AI can take on the burden of marketing execution, leaving the marketer more time to focus on strategy, content and what we believe will be a new creative renaissance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('To spot fire damage from space, point this AI at satellite imagery', 'A new deep-learning algorithm studies aerial photographs after fires to identify damage.\\n\\nHow it works: From satellite images taken before and after the California wildfires of 2017, researchers created a data set of buildings that were either damaged or left unscathed.\\n\\nThe results: They tweaked a pre-trained ImageNet neural network and got it to spot damaged buildings with an accuracy of up to 85 percent.\\n\\nWhy it matters: After a disaster, pinpointing the hardest-hit areas could save lives and help with relief efforts. The researchers also released the data set to the public, which could improve other research that requires satellite images, like conservation and developmental aid work.')\n",
      "('AI can make your smartphone notifications less annoying', 'Apps could show you fewer, better alerts by using a new deep-learning trick.\\n\\nThe problem: If an app sends too many push notifications, users may well delete it. Send too few, and they might not use it. But deep learning could help find the Goldilocks amount of notifications—where they’re not annoying, but help boost use of the app.\\n\\nA solution: A paper published on the arXiv describes how researchers trained an AI on data like browsing history, shopping history, and financial details to build a recommendation engine for when and what to show in notifications. Using it, they were able to accurately forecast a user’s preferences for frequency and content.\\n\\nMore for less: When the AI was tested out on a number of Taiwanese apps, it reduced the number of notifications while increasing the click-through rate.')\n",
      "('Making sense of neural networks’ febrile dreams', 'Google researchers developed a way to peer inside the minds of deep-learning systems, and the results are delightfully weird.\\n\\nWhat they did: The team built a tool that combines several techniques to provide people with a clearer idea of how neural networks make decisions. Applied to image classification, it lets a person visualize how the network develops its understanding of what is, for instance, a kitten or a Labrador. The visualizations, above, are ... strange.\\n\\nWhy it matters: Deep learning is powerful—but opaque. That’s a problem if you want it to, say, drive a car for you. So being able to visualize decisions behind image recognition could help reveal why an autonomous vehicle has made a serious error. Plus, humans tend to want to know why a decision was made, even if it was correct.\\n\\nBut: Not everyone thinks machines needs to explain themselves. In a recent debate, Yann Lecunn, who leads Facebook’s AI research, argued that we should simply focus on their behavior. After all, we can’t always explain the decisions humans make either.')\n",
      "('This neural network examines neurons. Like, the kind in your brain.', 'As Ernest Rutherford once said, “All science is either physics or stamp collecting.” Well, today’s scientists can feel fortunate that AI is, more and more, being used to keep track of the postage. A new deep-learning system that peers at brain tissue and catalogues individual cells might be the best example yet.\\n\\nThe details: Deep-learning algorithms need a lot of data, and in the realm of neuroscience, there’s plenty: cell cultures abound, offering far more brain tissue than the poor interns and low-level researchers who typically do the work could ever hope to sort and label. Instead, a neuroscientist at UC San Francisco and researchers from Google have teamed up and used the cultures to train a system that could automate some of that tedious work.\\n\\nSo far: According to Wired, the algorithm can tell live cells from dead ones and differentiate parts of a cell without the aid of fluorescent labels that human researchers often use (which can damage cells).\\n\\nWhy it matters: Beyond simply saving time, automating the process of analyzing samples could speed up drug discovery. Google also open-sourced the data set and model, which means small labs with fewer resources can put this tech to use as well.')\n",
      "('Your license plate is probably in the massive database ICE just got access to', 'The US Immigration and Customs Enforcement (ICE) agency has signed a contract with Vigilant Solutions, which owns a database of two billion images of license plate tagged with dates, times, and GPS coordinates.\\n\\nHow it could work: According to the Verge, ICE agents could use the database to perform searches that would show every place a license plate was spotted in the past five years. That would allow agents to build a clear profile of where and with whom a person spends time. The system can also send real-time e-mail alerts for new photos of a particular plate.\\n\\nTaking issue: The ACLU warns that the technology, ostensibly meant for tracking down undocumented immigrants, could be turned into a tool that tracks all Americans.')\n",
      "('Tales of power and responsibility', 'Our annual list of 10 world-changing technologies invariably defies attempts to find an overarching theme. But a look back at the past few years shows a trend: we’re including more and more advances in artificial intelligence.\\n\\nWe’ve featured surprise modeling, a form of machine learning (2008); Siri (2009); deep learning (2013); neuromorphic chips (2014); conversational interfaces (2016); robots that teach each other (2016); self-driving trucks (2017); and reinforcement learning (2017).\\n\\nAlgorithms that learn have been around for decades, so why this sudden flowering? It’s thanks in part to better algorithms, but mostly to an explosion in the quantity of data available for training them—from photos to disease statistics to online shopping patterns—and to new kinds of chips that can better handle their massive processing needs.\\n\\nThis year’s list again contains two AI entries. Generative adversarial networks, or GANs, are AIs pitted against one another in an evolutionary arms race, which speeds up the pace of learning, sometimes by orders of magnitude. And cloud-based AI makes deep-learning algorithms as ubiquitous and accessible as blogging software.\\n\\nGideon Lichfield Nephi Niven\\n\\nCombined, these two innovations could put far more AI power in the hands of far more people. As is usually the case with a powerful technology, this cuts both ways. It promises to turbocharge scientific research and economic productivity. But it may also allow almost anyone to craft convincing fake images and videos that further erode society’s ability to distinguish truth from lies. Ian Goodfellow, the inventor of GANs, is the rare technologist who is actively working on countering possible abuses of his invention, as Martin Giles relates in a profile of him.\\n\\nThere’s a similar good-or-evil dichotomy in some of the other technologies on this year’s list. As Antonio Regalado reports, genetics is evolving from the search for “the gene for X” to statistical analyses of big genomic databases that can now reveal “the thousand genes for X.” That will help identify people at high risk of things like heart disease and Alzheimer’s, but it will also predict traits such as height and IQ. What will we do with such knowledge? Synthetic human embryos will help medical researchers study the earliest stages of life, but when does it become unethical to grow one? Alphabet’s Sidewalk Labs plans to turn a Toronto neighborhood into a smart-city experiment, stuffing it with sensors to scoop up data on its residents’ every movement. Will it, asks Elizabeth Woyke, become a shining example of how to use big data for public good, or a privacy nightmare?\\n\\nLuckily, perhaps, not everything on our list is so ethically fraught. A natural-gas power plant that doesn’t emit carbon dioxide; a 3-D printer for metal; simultaneous-\\xadtranslation devices that nestle in your ear like the Babel fish of the cult classic Hitchhiker’s Guide to the Galaxy. All these and more are in this, our 17th annual list of the 10 breakthrough technologies. I hope you enjoy it.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('China hopes to build chips that control millions of driverless cars', 'The nation’s insatiable desire to build its own hardware naturally extends to the world of robo-taxis.\\n\\nBackstory: China has made no secret of wanting to design and produce huge numbers of its own chips. It’s already gunning to build the processors that power an impending wave of artificial-intelligence hardware.\\n\\nThe news: Bloomberg reports that domestic firms are also expected to build the chips that will be the brains behind the nation’s robotic cars. Startups like Horizon Robotics, founded by the former chief of Baidu’s Institute of Deep Learning, are scrambling to build low-power devices that process data from sensors dotted around cars.\\n\\nWhy it matters: China’s tech giants are going full speed ahead to develop their own driverless vehicles. Given that the nation is home to over 300 million cars, a decent share of the market for autonomous-car chips, which Gartner predicts will reach $5 billion, is up for grabs. And if their ambition extends to other markets, Chinese firms could soon give Western rivals such as MobilEye, Nvidia, and Intel a run for their money.')\n",
      "('Drones that dodge obstacles without guidance can pursue you like paparazzi', 'Artificially intelligent drones are coming—and they’re going to shoot some really sick snowboarding videos along the way.\\n\\nA startup called Skydio is launching the first drone capable of visually imprinting on a person, a bit like a needy duckling, and then following him or her around, deftly navigating around trees, pylons, and other obstacles while shooting video. In other words, your very own robotic, airborne paparazzo.\\n\\nThe new drone, called R1, will be marketed as an easy way to capture action like biking, skiing, or rafting in high-quality video. But Adam Bry, founder and CEO of Skydio, says his company is thinking about how it could be used to inspect roofs for damage and to patrol properties.\\n\\nAerial vehicles are already being used to inspect buildings, perform security patrols, and tally inventory inside warehouses. But to navigate a complex and unfamiliar environment, a drone typically needs to be controlled by an expert pilot on the ground, or else its surroundings must be mapped in great detail.\\n\\nHayk Martiros\\n\\nThe R1 drone uses deep learning to recognize a person. It employs conventional methods for mapping and navigation, along with a mathematical approach to path planning that Bry has been working on since his time as a student at MIT. One of his projects involved teaching a plane with a two-foot wingspan to fly through a busy garage without crashing (see “Daredevil drone flies through the trees like an ace”).\\n\\nSelf-flying drones are harder to perfect than self-driving cars. Mapping the environment and planning a path must be done in three dimensions rather than two, and drones typically cannot carry much in the way of computing power.\\n\\nBut navigating a busy physical world is a fundamental challenge for machines of many kinds, so advances in drone control could perhaps prove useful for AI more broadly.\\n\\nResearchers at MIT recently announced another technique that could further improve the agility of drones. A group led by Russ Tedrake, a professor at MIT’s Computer Science and Artificial Intelligence Lab (CSAIL), published a paper showing how some understanding of uncertainty can help a drone navigate through a complex environment even at high speed. Embracing uncertainty is an idea that could have broad implications in AI (see “Google and others are building AI systems that doubt themselves”).\\n\\nThe R1, which costs $2,499, features 13 cameras for capturing its surroundings, and a compact chipset for AI applications called the Jetson TX1, designed by Nvidia. The drone uses a software and hardware stack that the company calls the Skydio Autonomy Engine.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Inside the Chinese lab that plans to rewire the world with AI', 'The ticket kiosks at Shanghai’s frenetic subway station have a mind of their own.\\n\\nWalk up to one and state your destination, and it’ll automatically recommend a route before issuing a ticket. It’ll even check your identification (a necessary step in China) by looking at your face. In the interest of reducing the rush-hour stampede, the system is set up to let you find information and buy tickets without pushing a button or talking to a person.\\n\\nMore impressive still, all this happens successfully in the middle of a crowded, noisy station. Each kiosk has to figure out who is speaking to it; zero in on that person’s voice within the crowd; transcribe the incoming speech; parse its meaning; and compare the person’s face against a massive database of photos—all within a few seconds.\\n\\nTo do it, the kiosks use several cutting-edge machine-learning algorithms. The really interesting thing, though, isn’t the algorithms themselves. It’s where they live. All that image processing and speech recognition is served up on demand by a cloud computing system owned by one of China’s most successful companies, the e-commerce giant Alibaba.\\n\\nAlibaba is already using AI and machine learning to optimize its supply chain, personalize recommendations, and build products like Tmall Genie, a home device similar to the Amazon Echo. China’s two other tech supergiants, Tencent and Baidu, are likewise pouring money into AI research. The government plans to build an AI industry worth around $150 billion by 2030 and has called on the country’s researchers to dominate the field by then (see “China’s AI awakening”).\\n\\nBut Alibaba’s ambition is to be the leader in providing cloud-based AI. Like cloud storage (think Dropbox) or cloud computing (Amazon Web Services), cloud AI will make powerful resources cheaply and readily available to anyone with a computer and an internet connection, enabling new kinds of businesses to grow.\\n\\nThe real race in AI between China and the US, then, will be one between the two countries’ big cloud companies, which will vie to be the provider of choice for companies and cities that want to make use of AI. And if Alibaba is anything to go by, China’s tech giants are ready to compete with Google, Amazon, IBM, and Microsoft to serve up AI on tap. Which company dominates this industry will have a huge say in how AI evolves and how it is used.\\n\\nThink bigger\\n\\nJack Ma created Alibaba Online, a simple e-commerce marketplace, in 1999, in his apartment in Hangzhou, on China’s east coast. Today the company’s headquarters, which I visited in January, consists of several large buildings housing tens of thousands of workers; the front entrance is guarded by a gigantic version of the company’s cartoonish orange mascot.\\n\\nAlibaba’s core business remains selling goods and providing a platform for business-to-business trade. But this has spawned other lucrative operations, including a platform for logistics and shipments, an advertising network, and cloud computing and financial services. The company’s ubiquitous mobile payments app, Alipay, is run by a sister company, Ant Financial, which also offers loans, insurance, and investing via smartphone.\\n\\nJack Ma Sean Gallup | Getty\\n\\nLast year on “Singles Day,” a shopping event on November 11 that Alibaba invented, the company sold more than $25 billion worth of merchandise. By contrast, on last year’s Cyber Monday (November 27), the biggest online shopping day in the US, all retailers combined brought in $6.59 billion.\\n\\nThe company’s success has also helped shape Hangzhou’s vibrant tech scene. The city is home to dozens of incubators, funded in part by government subsidies, that are filled with entrepreneurs who previously worked at Alibaba.\\n\\nAlibaba’s colorful founder apparently doesn’t take any of this for granted. “Jack Ma believes we have been successful because of our business model, a hard-working team plus the operation,” says Xiangwen Liu, the company’s director of technology development. “In the next era of company competition, Jack’s belief is the business model cannot give success for a giant like Alibaba. His belief is in technology.”\\n\\nRelated Story Alibaba’s AI Fashion Consultant Helps Achieve Record-Setting Sales AI will blur the line between online and offline retail.\\n\\nLast October Ma announced that his company would spend $15 billion over the next three years on a research institute called the DAMO Academy (“discovery, adventure, momentum, and outlook”), dedicated to fundamental technologies. The Chinese name for the institute, 达摩, references Dharma, a legendary Indian monk said to have brought Buddhism to China in the fifth century.\\n\\nChina has long since shaken off its reputation for simply copying Western innovations. According to the Organization for Economic Cooperation and Development (OECD), R&D spending in China grew tenfold between 2000 and 2016, rising from $40.8 billion to $412 billion in today’s dollars. The US still spends more—$464 billion in 2016—but its total has increased by only one-third since 2000.\\n\\nAlibaba is already China’s biggest R&D spender, forking out $2.6 billion in 2017. DAMO will effectively triple its research budget, to more than $7 billion. That most likely means Alibaba will overtake IBM, Facebook, and Ford and will narrow the gap with the world’s leaders, Amazon and Alphabet, which spent $16.1 billion and $13.9 billion respectively on R&D in 2017.\\n\\nDAMO will include a portfolio of research groups working on fundamental and emerging technologies including blockchain, computer security, fintech, and quantum computing. But AI is the biggest focus, and it seems like the one with the greatest potential.\\n\\nDAMO clearly takes inspiration from the great commercial research labs of the 20th century. Liu mentions, for instance, AT&T’s Bell Labs, which conducted fundamental research on materials, electronics, and software, producing breakthroughs including the transistor, the laser, and the charge-coupled device for digital imaging, as well as the UNIX operating system and the programming languages C and C++. Liu says Alibaba is also inspired by the way the US’s Defense Advanced Research Projects Agency (DARPA) funds different teams competing on the same project.\\n\\nSign up for the The Algorithm News and views on the latest in artificial intelligence Sign Up Thank you — please check your email to complete your sign up. Incorrect email format Manage your newsletter preferences\\n\\nAlibaba is clearly learning from the likes of Alphabet and Amazon, too. Like them, it has released a cloud machine-learning platform. The first from a Chinese company, it was launched in 2015 and upgraded significantly last year. The tools it offers are similar to those on Google Cloud and Amazon Web Services, including off-the-shelf solutions for things like voice recognition and image classification.\\n\\nDeveloping these tools was a major technical undertaking for Alibaba. It signals both how ambitious the company is to shape the future of AI and how big a role cloud computing will play.\\n\\nAnother such signal is that Alibaba’s cloud supports several other companies’ deep-learning frameworks, including Google’s TensorFlow and Amazon’s MXNet. Deep learning—a technique for training machines to recognize things by feeding lots of data into a many-layered neural network—is the most important approach in AI right now, used for everything from controlling autonomous vehicles to transcribing speech. Tech companies build their own deep-learning frameworks in part to get users onto their cloud platforms, because those frameworks typically run best on their infrastructure. By supporting its competitors’ frameworks, Alibaba gives developers a reason to use its platform instead.\\n\\nAnd that’s not all: Liu hints that Alibaba may be working on its own deep-learning framework, something that could help it get even more engineers hooked on its cloud. When asked if Alibaba might release some of the code it has developed, she answers: “When it’s mature.”\\n\\nSmart answers\\n\\nThere have been other glimpses of Alibaba’s progress in AI lately. Last month a research team at the company released an AI program capable of reading a piece of text, and answering simple questions about that text, more accurately than anything ever built before.\\n\\nThe text was in English, not Chinese, because the program was trained on the Stanford Question Answering Dataset (SQuAD), a benchmark used to test computerized question-and-answer systems. Alibaba’s program uses several novel machine-learning techniques, and it notched a higher score than entries from Microsoft, Samsung, and others. Remarkably, it scored better than the average human being (although this is a bit deceptive; it doesn’t mean the program actually understands what it is reading).\\n\\nMore remarkable, though, is how fast Alibaba rose up the leaderboard. The company only submitted its first entry to SQuAD in September 2017. “Quite a few of the top 10 teams represent top Chinese institutions, reflecting the ongoing democratization of AI,” says Pranav Samir Rajpurkar, a PhD student at Stanford who runs the SQuAD contest.\\n\\nAlibaba has already used the program to improve the automated customer support on its online marketplace, says Si Luo, a member of the team. And it hopes to deploy language understanding across its platforms and technologies.\\n\\nAlibaba’s AI researchers are working on other cutting-edge projects, such as generative adversarial networks, or GANs. In this exciting new machine-learning approach, developed by a Google researcher, two neural networks are pitted against one another; one tries to generate data that seems as if it comes from a real data set, and the other tries to distinguish real examples from fake ones. The technique lets computers learn more efficiently from unlabeled data, and it can be used to create realistic-looking synthetic images and video (see “The GANfather: The man who’s given machines the gift of imagination”).\\n\\nWang He | Getty\\n\\nGathering clouds\\n\\nOne advantage China’s tech companies have over their Western counterparts is the government’s commitment to AI. Smart cities that use the kind of technology found in Shanghai’s metro kiosks are likely to be in the country’s future. One of Alibaba’s cloud AI tools is a suite called City Brain, designed for tasks like managing traffic data and analyzing footage from city video cameras.\\n\\nThere are such experiments in the West too, such as Alphabet’s Sidewalk project, which plans to transform a suburb of Toronto with autonomous vehicles, delivery robots, and AI-based management systems. But China will most likely want to do things on a larger scale, which will give its companies an edge in the global marketplace for AI.\\n\\nThe Chinese authorities’ interest in using technology for social control also helps. There are plans for a “social credit system” that would track and score citizens’ everyday behavior with a view to perks or punishment. Face recognition software from Chinese companies like SenseTime is being used to find criminals in surveillance footage, and to track suspected dissidents.\\n\\nRelated Story 50 Smartest Companies 2017 Our editors pick the 50 companies that best combine innovative technology with an effective business model.\\n\\nAnother advantage Chinese firms enjoy is access to vast amounts of data—because of China’s huge population— with relatively few restraints on how it can be used. Ant Financial’s Alipay, for instance, has more than 520 million users, and the company determines a person’s creditworthiness, in part, by examining his or her daily financial transactions and social connections. This wouldn’t fly in Europe or the US, where strict rules dictate what kinds of data can go into a credit score. But in regions like Africa, where China has a strong economic foothold, such technologies could become the norm.\\n\\nAlibaba is already exporting AI technology. It is the world’s fifth-largest cloud-computing provider, behind Amazon, Google, Microsoft, and IBM, and its cloud machine-learning platform is available in several languages, including English. This week, Alibaba launched a version aimed at developers and companies in Europe; it also announced a new AI lab in collaboration with Singapore’s Nanyang Technological University.\\n\\nIn some places, Alibaba is arguably ahead of the competition. Last December, it announced a collaboration with the Malaysian government to provide smart city services, including a video platform that can automatically detect accidents and help optimize traffic flow.\\n\\nAI with Chinese characteristics\\n\\nSo if the world’s AI is supplied by China, what sorts of values will it come with? In the West there is growing concern about issues such as biased algorithms and job losses to automation. That kind of debate is less often heard in China. Speaking at the World Economic Forum in Davos, Switzerland, recently, Jack Ma, Alibaba’s boss, acknowledged the risks that come with AI; but unlike its US counterparts, Alibaba isn’t involved with ethics groups like the Partnership on AI. And unlike, say, DeepMind, the AI-focused subsidiary of Alphabet, it doesn’t have an internal ethics division.\\n\\nAs China becomes more proficient in AI, it will help determine how the technology reshapes the world. And Alibaba will undoubtedly be an important part of this picture.\\n\\n“Well before anybody used the term artificial intelligence in a business context, Alibaba was a major innovator,” says William Kirby, a China expert at Harvard Business School. “In my view, the company has done more to change the way business is done in China than anyone; they are ambitious on every front.”')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The GANfather: The man who’s given machines the gift of imagination', \"One night in 2014, Ian Goodfellow went drinking to celebrate with a fellow doctoral student who had just graduated. At Les 3 Brasseurs (The Three Brewers), a favorite Montreal watering hole, some friends asked for his help with a thorny project they were working on: a computer that could create photos by itself.\\n\\nResearchers were already using neural networks, algorithms loosely modeled on the web of neurons in the human brain, as “generative” models to create plausible new data of their own. But the results were often not very good: images of a computer-generated face tended to be blurry or have errors like missing ears. The plan Goodfellow’s friends were proposing was to use a complex statistical analysis of the elements that make up a photograph to help machines come up with images by themselves. This would have required a massive amount of number-crunching, and Goodfellow told them it simply wasn’t going to work.\\n\\nBut as he pondered the problem over his beer, he hit on an idea. What if you pitted two neural networks against each other? His friends were skeptical, so once he got home, where his girlfriend was already fast asleep, he decided to give it a try. Goodfellow coded into the early hours and then tested his software. It worked the first time.\\n\\nWhat he invented that night is now called a GAN, or “generative adversarial network.” The technique has sparked huge excitement in the field of machine learning and turned its creator into an AI celebrity.\\n\\nChristie Hemm Klok\\n\\nIn the last few years, AI researchers have made impressive progress using a technique called deep learning. Supply a deep-learning system with enough images and it learns to, say, recognize a pedestrian who’s about to cross a road. This approach has made possible things like self-driving cars and the conversational technology that powers Alexa, Siri, and other virtual assistants.\\n\\nBut while deep-learning AIs can learn to recognize things, they have not been good at creating them. The goal of GANs is to give machines something akin to an imagination.\\n\\nIn the future, computers will get much better at feasting on raw data and working out what they need to learn from it.\\n\\nDoing so wouldn’t merely enable them to draw pretty pictures or compose music; it would make them less reliant on humans to instruct them about the world and the way it works. Today, AI programmers often need to tell a machine exactly what’s in the training data it’s being fed—which of a million pictures contain a pedestrian crossing a road, and which don’t. This is not only costly and labor-intensive; it limits how well the system deals with even slight departures from what it was trained on. In the future, computers will get much better at feasting on raw data and working out what they need to learn from it without being told.\\n\\nThat will mark a big leap forward in what’s known in AI as “unsupervised learning.” A self-driving car could teach itself about many different road conditions without leaving the garage. A robot could anticipate the obstacles it might encounter in a busy warehouse without needing to be taken around it.\\n\\nThat will mark a big leap forward in what is known in AI as “unsupervised learning.”\\n\\nOur ability to imagine and reflect on many different scenarios is part of what makes us human. And when future historians of technology look back, they’re likely to see GANs as a big step toward creating machines with a human-like consciousness. Yann LeCun, Facebook’s chief AI scientist, has called GANs “the coolest idea in deep learning in the last 20 years.” Another AI luminary, Andrew Ng, the former chief scientist of China’s Baidu, says GANs represent “a significant and fundamental advance” that’s inspired a growing global community of researchers.\\n\\nThe GANfather, Part II: AI fight club\\n\\nGoodfellow is now a research scientist on the Google Brain team, at the company’s headquarters in Mountain View, California. When I met him there recently, he still seemed surprised by his superstar status, calling it “a little surreal.” Perhaps no less surprising is that, having made his discovery, he now spends much of his time working against those who wish to use it for evil ends.\\n\\nThe magic of GANs lies in the rivalry between the two neural nets. It mimics the back-and-forth between a picture forger and an art detective who repeatedly try to outwit one another. Both networks are trained on the same data set. The first one, known as the generator, is charged with producing artificial outputs, such as photos or handwriting, that are as realistic as possible. The second, known as the discriminator, compares these with genuine images from the original data set and tries to determine which are real and which are fake. On the basis of those results, the generator adjusts its parameters for creating new images. And so it goes, until the discriminator can no longer tell what’s genuine and what’s bogus.\\n\\nA GAN trained on photos of real celebrities came up with its own set of imaginary stars. In most cases, the fakes looked pretty realistic. Nvidia\\n\\nIn one widely publicized example last year, researchers at Nvidia, a chip company heavily invested in AI, trained a GAN to generate pictures of imaginary celebrities by studying real ones. Not all the fake stars it produced were perfect, but some were impressively realistic. Unlike other machine-learning approaches that require tens of thousands of training images, GANs can become proficient with a few hundred.\\n\\nRelated Story 10 Breakthrough Technologies 2018 Dueling neural networks. Artificial embryos. AI in the cloud. Welcome to our annual list of the 10 technology advances we think will shape the way we work and live now and for years to come.\\n\\nThis power of imagination is still limited. Once it’s been trained on a lot of dog photos, a GAN can generate a convincing fake image of a dog that has, say, a different pattern of spots; but it can’t conceive of an entirely new animal. The quality of the original training data also has a big influence on the results. In one telling example, a GAN began producing pictures of cats with random letters integrated into the images. Because the training data contained cat memes from the internet, the machine had taught itself that words were part of what it meant to be a cat.\\n\\nGetting GANS to work well can be tricky. If there are glitches, the results can be bizarre. Alec Radford\\n\\nGANs are also temperamental, says Pedro Domingos, a machine-learning researcher at the University of Washington. If the discriminator is too easy to fool, the generator’s output won’t look realistic. And calibrating the two dueling neural nets can be difficult, which explains why GANs sometimes spit out bizarre stuff such as animals with two heads.\\n\\nStill, the challenges haven’t deterred researchers. Since Goodfellow and a few others published the first study on his discovery, in 2014, hundreds of GAN-related papers have been written. One fan of the technology has even created a web page called the “GAN zoo,” dedicated to keeping track of the various versions of the technique that have been developed.\\n\\nThe most obvious immediate applications are in areas that involve a lot of imagery, such as video games and fashion: what, for instance, might a game character look like running through the rain? But looking ahead, Goodfellow thinks GANs will drive more significant advances. “There are a lot of areas of science and engineering where we need to optimize something,” he says, citing examples such as medicines that need to be more effective or batteries that must get more efficient. “That’s going to be the next big wave.”\\n\\nIn high-energy physics, scientists use powerful computers to simulate the likely interactions of hundreds of subatomic particles in machines like the Large Hadron Collider at CERN in Switzerland. These simulations are slow and require massive computing power. Researchers at Yale University and Lawrence Berkeley National Laboratory have developed a GAN that, after training on existing simulation data, learns to generate pretty accurate predictions of how a particular particle will behave, and does it much faster.\\n\\nGoodfellow's creation can be used to imagine all sorts of things, including new interior designs. Ian Goodfellow\\n\\nMedical research is another promising field. Privacy concerns mean researchers sometimes can’t get enough real patient data to, say, analyze why a drug didn’t work. GANs can help solve this problem by generating fake records that are almost as good as the real thing, says Casey Greene of the University of Pennsylvania. This data could be shared more widely, helping to advance research, while the real records are tightly protected.\\n\\nThe GANfather, Part III: Bad fellows\\n\\nThere is a darker side, however. A machine designed to create realistic fakes is a perfect weapon for purveyors of fake news who want to influence everything from stock prices to elections. AI tools are already being used to put pictures of other people’s faces on the bodies of porn stars and put words in the mouths of politicians. GANs didn’t create this problem, but they’ll make it worse.\\n\\nHany Farid, who studies digital forensics at Dartmouth College, is working on better ways to spot fake videos, such as detecting slight changes in the color of faces caused by inhaling and exhaling that GANs find hard to mimic precisely. But he warns that GANs will adapt in turn. “We’re fundamentally in a weak position,” says Farid.\\n\\nThis cat-and-mouse game will play out in cybersecurity, too. Researchers are already highlighting the risk of “black box” attacks, in which GANs are used to figure out the machine-learning models with which plenty of security programs spot malware. Having divined how a defender’s algorithm works, an attacker can evade it and insert rogue code. The same approach could also be used to dodge spam filters and other defenses.\\n\\n“There are a lot of areas of science and engineering where we need to optimize something. That’s going to be the next big wave.”\\n\\nGoodfellow is well aware of the dangers. Now heading a team at Google that’s focused on making machine learning more secure, he warns that the AI community must learn the lesson of previous waves of innovation, in which technologists treated security and privacy as an afterthought. By the time they woke up to the risks, the bad guys had a significant lead. “Clearly, we’re already beyond the start,” he says, “but hopefully we can make significant advances in security before we’re too far in.”\\n\\nNonetheless, he doesn’t think there will be a purely technological solution to fakery. Instead, he believes, we’ll have to rely on societal ones, such as teaching kids critical thinking by getting them to take things like speech and debating classes. “In speech and debate you’re competing against another student,” he says, “and you’re thinking about how to craft misleading claims, or how to craft correct claims that are very persuasive.” He may well be right, but his conclusion that technology can’t cure the fake-news problem is not one many will want to hear.\")"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "('AI algorithm predicts prices of produce based on freshness', \"AI algorithm predicts prices of produce based on freshness\\n\\nga('send', 'event', 'Eco & Sustainability', 'article', 'article-industry-impression', {nonInteraction: true});ga('send', 'event', 'Food & Beverage', 'article', 'article-industry-impression', {nonInteraction: true});ga('send', 'event', 'Retail', 'article', 'article-industry-impression', {nonInteraction: true});\\n\\nWe have seen many solutions that tackle the issue of food waste, such as grocery stores introducing new pricing systems to ensure products close to expiring will sell. One example of this is a real-time pricing solution that uses radio frequency identification (RFID), electronic shelf labelling, and a dynamic pricing engine to offer cheaper prices. Another example is an automated discount rack that reduces prices for expiring products both online and in store. A new solution, called RapidMathematix, also aims to reduce food waste using deep learning algorithms and machine vision.\")\n",
      "('New image recognition system for real estate', \"New image recognition system for real estate\\n\\nga('send', 'event', 'Computing', 'article', 'article-industry-impression', {nonInteraction: true});ga('send', 'event', 'Property management', 'article', 'article-industry-impression', {nonInteraction: true});\\n\\nFounded in early 2016, Restb.ai is a Barcelona based start-up specialising in image recognition for real estate. Image recognition is part of Computer Vision, a branch of Artificial Intelligence (AI) technology in which the software learns to recognise objects, features, and settings in images relevant to what customers are looking for. RestB’s system is different from similar API services offered by big players such as Microsoft, IBM and Google in that it can pull a large number of specific details from images.\")\n",
      "('An Augmented Reality Microscope for Cancer Detection', 'Left: Schematic overview of the ARM. A digital camera captures the same field of view (FoV) as the user and passes the image to an attached compute unit capable of running real-time inference of a machine learning model. The results are fed back into a custom AR display which is inline with the ocular lens and projects the model output on the same plane as the slide. Right: A picture of our prototype which has been retrofitted into a typical clinical-grade light microscope.')\n",
      "('DeepVariant Accuracy Improvements for Genetic Datatypes', 'DeepVariant v0.6 shows major accuracy improvements for PCR+ data, largely attributable to a reduction in indel errors. Here we re-analyze two PCR+ samples that were used in external evaluations, including DNAnexus on the left (see details in figure 10) and bcbio on the right, showing how indel accuracy improves with each DeepVariant version.')\n",
      "('Semantic Image Segmentation with DeepLab in Tensorflow', 'The latest news from Research at Google')\n",
      "('Investing in France’s AI Ecosystem', 'The latest news from Research at Google')\n",
      "('Bennett Univ’s AI programme to reach 1m in 2 yrs', 'GREATER NOIDA: The Bennett University on Tuesday kickstarted a Deep Learning and AI Skilling and Research project, which aims to impart training in artificial intelligence technologies to nearly one million students in two years. The project has been funded by the Royal Academy of Engineering in London through the Indo-UK Newton-Bhabha research fund.While BU has partnered with international institutions like the Brunel University in London and University College London for technical expertise, it plans to work with 1,000 higher education institutions in India to train their academics and researchers. They can also take advantage of BU’s one-of-a-kind in-house supercomputing facility, which has been set up in partnership with American technology company Nvidia. Launching the programme on campus on Tuesday, All India Council for Technical Education (AICTE) chairperson professor Anil Sahasrabudhe said the programme was in line with the regulatory body’s ongoing efforts to update curricula in various varsities and institutions to include the expanding fields of artificial intelligence, machine learning and robotics.As part of this research and training project, BU will partner with 10 “collaborator institutions” at the primary level, along with a hundred “zonal partners” from across the country. These are projected to reach 1,000 colleges, 10,000 teachers who will be part of formal training programmes through partnerships and workshops. Eventually, through various such channels, the project is expected to reach one million students, Dr Deepak Garg, director, Nvidia-Bennett Research Center on Artificial Intelligence told TOI. “At its full scale the project involves the resources of around Rs 100 million including cash, kind and human contributions from the partners. Further upscaling is under way,” said Garg. Sahasrabudhe called it an “excellent hub-and-spoke model” which can produce “solutions for global problems from India.”More information on the project collaborations, resources and workshops can be found here Some examples of AI and machine-learning include text prediction on smartphone keyboards that guess the next word one would possibly type, or even the recommendation engines on video-streaming services or e-commerce websites. Other kinds of applications include crunching data from weather patterns and other environmental factors to predict pest attacks and crop yields for farmers. In India, a project along similar lines in Andhra Pradesh’s Devanakonda calculated optimum period for crop sowing by analysing historic climate data across 30 years. “We are witnessing a dramatic opportunity to confront these problems head-on with AI technologies,” said Dr Ghanta Subba Rao, special secretary for skill development with Andhra Pradesh government. The state government has extended support to the BU project.The central government too has recognised the need to encourage the development of artificial intelligence education. The Niti Aayog is expected to soon come out with a national policy on AI, with a focus on areas like agriculture and health. “Today we depend on AI tools from outside the country. This project is an opportunity to develop these tools in-house,” said Suneet Tuli, dean, School of Engineering at Bennett University. The university is part of The Times Group, which publishes the TOI.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Now, it’s time to worry about ‘deepfake’ videos', 'Recently, someone claiming to represent an Indian political party approached an artificial intelligence engineer with a special request: Create “deepfakes” for propaganda.Deepfake is the new frontier in fake news where artificial intelligence is used to make anyone say or do anything on video. Last week, the usually quiet Barack Obama popped up in an online video calling US president Donald Trump “a total and complete dips**t”.Well, Obama never said those things, and the video turned out to be what is called a deep fake, a type of video featuring realistic face swapping. It was created by director Jordan Peele in partnership with Buzzfeed as a warning about not automatically trusting anything on the internet.Digital fakery is not new — we know photos can be morphed, videos can be edited. Face swapping tech is not new either. But now, with deep learning by machines, these tricks can be automated, and the tools are accessible to many more people, says Rishabh Srivastava of Loki Technologies, a machine learning startup.Subodh Kumar, a professor at IIT Delhi who specialises in computer graphics and visualisation, explains that the idea is for a neural network to learn the points of the face, then find and learn the function that will describe each image. “It creates a succinct representation of the face — mathematically, not geometrically — and then a symmetric function that gives you back the image. So you do that for person X from the many images in a video, and reverse map it for person Y,” he says. By finding points of correspondence, you can overlay one face on another, then blend it to look smooth.The big problem is that current forensic tools will not be able to detect this fakery, explains cybersecurity expert Akash Mahajan. “With deep learning, when you have recurrent multiple steps, it is hard to trace back the trail the machine took to reach the output,” he says. So the hoaxslayers and fact-checkers we now have, or even forensics experts who look for audio glitches, shadows and visual discrepancies to spot fakes, won’t be able to help.Desktop tools like FakeApp make deepfakes absurdly easy, a matter of hours to make. And it has already resulted in a spurt of AI porn. Late last year, a Reddit user called Deepfakes showed how you could transpose a celebrity’s face onto someone else’s body, while keeping the expressions of the original.Even a few Bollywood actresses like Priyanka Chopra have been deepfaked, in violation of their rights and dignity. Crude splice and dice videos are already commonplace — Arvind Kejriwal’s speech was allegedly faked during the Punjab election to suggest he wanted people to vote for the Congress — but AI could bring a new sophistication to these attempts. “We are vigilant to the danger of deepfakes, but the media and the public are not,” says Ankit Lal, social media head of the Aam Aadmi Party. “Some media organisation could get a deepfaked video of Arvind (Kejriwal) or any other politician and run it as the truth: that is the danger we anticipate,” he adds.Of course, right now it doesn’t take deepfakery to dupe people, points out Pankaj Jain of SM Hoax Slayer.“People will believe even a celebrity picture with a fake quote, as recently happened with Amitabh Bachchan ,” he says. While this gullibility is generally true, and people tend to believe what we want to believe, realistic video footage is usually taken as documentary proof. It could be hugely destabilising if phony videos are passed off as truth on social media.“While we have not seen deepfakes of Indian politicians on open platforms like Facebook and Google yet, it’s hard to know if they have been spread on closed platforms like WhatsApp,” says Srivastava. It’s entirely likely to happen soon, given the flood of misinformation that already exists.In today’s world, when machines can recombine audio and video to create an alternative reality, seeing is not believing.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Here's how kids are learning better at school\", 'American School of Bombay highlights the need of technology in education. 02:42\\n\\nWhen used effectively, technology can change the face of education and create more academic opportunities. Educational technologies have benefitted students and teachers alike - by making learning more interesting. Integrating technology in education has removed all educational boundaries, enabling students and teachers to collaborate in real time. With so many advantages, education technology is needed big time in India, and American School of Bombay (ASB) steps in every year and organizes the Un-Plugged event.With over 400 participants, 14 keynote speakers, 60+ workshops, and 14 Learning Strands, ASB Un-Plugged 2018 was a unique global platform where educators, thought leaders, and renowned specialists engaged in sharing knowledge and expertise in areas such as Maker/STEM Education, Technology Integration, SAMR Model, Data Analytics, and Media Creation.As a biennial Global Technology Learning Conference, ASB Un-Plugged 2018 saw participants from more than 30 countries across five continents converge at the American School of Bombay (ASB). As many as 65 schools from around the world – American Schools, International Schools, and Private Indian Schools – attended ASB Un-Plugged as their go-to conference to witness how students, staff, and parents embrace innovation, change, and leadership in education.First organized in 2008, ASB Un-Plugged enables attendees to witness what seamless technology integration looks like in a classroom and its impact on teaching and learning. The conference is also an opportunity for participants to get hands-on experience with emerging technologies and take back to their schools the best practices and benchmarks in education.Fiona Reynolds, Deputy Head of School, American School of Bombay. Picture Courtesy: ASBFiona Reynolds, Deputy Head of School, American School of Bombay, said, “Un-Plugged, as an international event focused on educational technology, is a great way for teachers and school leaders to see what intentional and deep learning can look like aided by technology. As a conference on transforming teaching and learning, Un-Plugged allows us to interact with researchers and practitioners who share the future of work and what our students will need to be creative, empathic, kind, successful people – which is what school should be.”Here\\'s a sneak peak to the event:ASB Un-Plugged 2018 proved to be a platform for participants to engage in deeper conversations around how schools need to be relevant for today\\'s students. School leaders, technology directors and coordinators, and teachers learned and shared best practices in educational technology. A stellar line-up of speakers laid the groundwork for the attendees to think, plan, and bring relevancy to their practices, approaches, and mindsets.Dr. Shabbi Luthra, Founder & CEO, Consilience. Picture Courtesy: ASBThis year, ASB partnered with Consilience to run the ASB Un-Plugged conference. Consilience, a non-profit company, is dedicated to emerging educational technologies and their impact on the world and in schools. The year 2018 marked the 10th anniversary of the ASB Un-Plugged conference at the American School of Bombay. Dr. Shabbi Luthra, Founder & CEO, Consilience noted, “Having completed a decade, ASB Un-Plugged has always spearheaded the need for transforming teaching and learning. This conference will continue to be at the forefront of innovation in the years to come. We are happy to have spearheaded the movement for change and disruption at ASB Un-Plugged 2018.\"Consilience and ASB organized a talk by Scott Klososky, titled “Future of Work” during the conference. Scott strongly urged the audience to consider the need for changing education for the sake of the students’ future. The conference also included a glimpse into the American School of Bombay\\'s world of learning with technology. Student Showcases throughout the day – Science, English, Theatre, Math, PE, Art, Music, Social Studies, and more – enabled attendees to experience the latest technology integration in teaching and appreciate the crucial role that tech support plays in the success of this learning culture.Certainly, we need more platforms like these that help us understand the benefit of education technology better.Disclaimer: This is a sponsored brand story by American School of Bombay and the liability for the same rests with them.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Deep learning transforms smartphone microscopes into laboratory-grade devices', 'Image of a blood smear from a cell phone camera (left), following enhancement by the algorithm (center), and taken by a lab microscope (right). Credit: .Ozcan Research Group/UCLA Researchers at the UCLA Samueli School of Engineering have demonstrated that deep learning, a powerful form of artificial intelligence, can discern and enhance microscopic details in photos taken by smartphones. The technique improves the resolution and color details of smartphone images so much that they approach the quality of images from laboratory-grade microscopes.\\n\\nThe advance could help bring high-quality medical diagnostics into resource-poor regions, where people otherwise do not have access to high-end diagnostic technologies. And the technique uses attachments that can be inexpensively produced with a 3-D printer, at less than $100 a piece, versus the thousands of dollars it would cost to buy laboratory-grade equipment that produces images of similar quality.\\n\\nCameras on today\\'s smartphones are designed to photograph people and scenery, not to produce high-resolution microscopic images. So the researchers developed an attachment that can be placed over the smartphone lens to increase the resolution and the visibility of tiny details of the images they take, down to a scale of approximately one millionth of a meter.\\n\\nBut that only solved part of the challenge, because no attachment would be enough to compensate for the difference in quality between smartphone cameras\\' image sensors and lenses and those of high-end lab equipment. The new technique compensates for the difference by using artificial intelligence to reproduce the level of resolution and color details needed for a laboratory analysis.\\n\\nThe research was led by Aydogan Ozcan, Chancellor\\'s Professor of Electrical and Computer Engineering and Bioengineering, and Yair Rivenson, a UCLA postdoctoral scholar. Ozcan\\'s research group has introduced several innovations in mobile microscopy and sensing, and it maintains a particular focus on developing field-portable medical diagnostics and sensors for resource-poor areas.\\n\\n\"Using deep learning, we set out to bridge the gap in image quality between inexpensive mobile phone-based microscopes and gold-standard bench-top microscopes that use high-end lenses,\" Ozcan said. \"We believe that our approach is broadly applicable to other low-cost microscopy systems that use, for example, inexpensive lenses or cameras, and could facilitate the replacement of high-end bench-top microscopes with cost-effective, mobile alternatives.\"\\n\\nHe added that the new technique could find numerous applications in global health, telemedicine and diagnostics-related applications.\\n\\nThe researchers shot images of lung tissue samples, blood and Pap smears, first using a standard laboratory-grade microscope, and then with a smartphone with the 3-D-printed microscope attachment. The researchers then fed the pairs of corresponding images into a computer system that \"learns\" how to rapidly enhance the mobile phone images. The process relies on a deep-learning–based computer code, which was developed by the UCLA researchers.\\n\\nTo see if their technique would work on other types of lower-quality images, the researchers used deep learning to successfully perform similar transformations with images that had lost some detail because they were compressed for either faster transmission over a computer network or more efficient storage.\\n\\nThe study was published in ACS Photonics, a journal of the American Chemical Society. It builds upon previous studies by Ozcan\\'s group that used deep learning to reconstruct holograms and improve microscopy.\\n\\nExplore further: UCLA engineers use deep learning to reconstruct holograms and improve optical microscopy\\n\\nMore information: Yair Rivenson et al. Deep Learning Enhanced Mobile-Phone Microscopy, ACS Photonics (2018). DOI: 10.1021/acsphotonics.8b00146\\n\\n')\n",
      "('Deep learning predicts drug-drug and drug-food interactions', 'Figure 1. Overall scheme of Deep DDDI and prediction of food constituents that reduce the in vivo concentration of approved drugs. Credit: Korea Advanced Institute of Science and Technology Drug interactions, including drug-drug interactions (DDIs) and drug-food constituent interactions (DFIs), can trigger unexpected pharmacological effects, including adverse drug events (ADEs), with causal mechanisms often unknown. However, current prediction methods do not provide sufficient details beyond the chance of DDI occurrence, or require detailed drug information often unavailable for DDI prediction.\\n\\nTo tackle this problem, Dr. Jae Yong Ryu, Assistant Professor Hyun Uk Kim and Distinguished Professor Sang Yup Lee, all from the Department of Chemical and Biomolecular Engineering at Korea Advanced Institute of Science and Technology (KAIST), developed a computational framework, named DeepDDI, that accurately predicts 86 DDI types for a given drug pair. The research results were published online in Proceedings of the National Academy of Sciences (PNAS) on April 16, 2018, which is titled \"Deep learning improves prediction of drug-drug and drug-food interactions.\"\\n\\nDeepDDI takes structural information and names of two drugs in pair as inputs, and predicts relevant DDI types for the input drug pair. DeepDDI uses deep neural network to predict 86 DDI types with a mean accuracy of 92.4% using the DrugBank gold standard DDI dataset covering 192,284 DDIs contributed by 191,878 drug pairs. Very importantly, DDI types predicted by DeepDDI are generated in the form of human-readable sentences as outputs, which describe changes in pharmacological effects and/or the risk of ADEs as a result of the interaction between two drugs in pair. For example, DeepDDI output sentences describing potential interactions between oxycodone (opioid pain medication) and atazanavir (antiretroviral medication) were generated as follows: \"The metabolism of Oxycodone can be decreased when combined with Atazanavir\"; and \"The risk or severity of adverse effects can be increased when Oxycodone is combined with Atazanavir\". By doing this, DeepDDI can provide more specific information on drug interactions beyond the occurrence chance of DDIs or ADEs typically reported to date.\\n\\nDeepDDI was first used to predict DDI types of 2,329,561 drug pairs from all possible combinations of 2,159 approved drugs, from which DDI types of 487,632 drug pairs were newly predicted. Also, DeepDDI can be used to suggest which drug or food to avoid during medication in order to minimize the chance of adverse drug events or optimize the drug efficacy. To this end, DeepDDI was used to suggest potential causal mechanisms for the reported ADEs of 9,284 drug pairs, and also predict alternative drug candidates for 62,707 drug pairs having negative health effects to keep only the beneficial effects. Furthermore, DeepDDI was applied to 3,288,157 drug-food constituent pairs (2,159 approved drugs and 1,523 well-characterized food constituents) to predict DFIs. The effects of 256 food constituents on pharmacological effects of interacting drugs and bioactivities of 149 food constituents were also finally predicted. All these prediction results can be useful if an individual is taking medications for a specific (chronic) disease such as hypertension or diabetes mellitus type 2.\\n\\nDistinguished Professor Sang Yup Lee said, \"We have developed a platform technology DeepDDI that will allow precision medicine in the era of Fourth Industrial Revolution. DeepDDI can serve to provide important information on drug prescription and dietary suggestions while taking certain drugs to maximize health benefits and ultimately help maintain a healthy life in this aging society.\"\\n\\nExplore further: A new method of modeling drug-target interactions fixes a detrimental bias of past techniques\\n\\nMore information: Deep learning improves prediction of drug–drug and drug–food interactions, Proceedings of the National Academy of Sciences (2018). 10.1073/pnas.1803294115\\n\\n')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('IBM demonstrates new breakthrough in AI performance', 'Credit: IBM IBM has demonstrated a new breakthrough in AI performance. By using machine learning on POWER9 with NVIDIA Tesla V100 GPUs, IBM technology can now predict the likelihood of a user clicking online advertisements 46x faster than previous published results.\\n\\nIn a newly published benchmark by IBM Research, we demonstrated how Snap Machine Learning (AI technology) can be used to train machine learning models for massive data sets from financial records to weather forecasting to online marketing. The result for customers is lower cloud costs and faster time to insight.\\n\\nThe insights derived from data are where real business value is—and getting those insights is a tall order when it is time- and cost-consuming to evaluate even relatively simply datasets. Speed is critical for scaling insights, and IBM is optimizing IT infrastructure to achieve that speed.\\n\\nRealizing the power of AI and the cloud\\n\\nAt Think 2018, we\\'re also introducing other first-of-a-kind capabilities that address both the biggest obstacles and the biggest opportunities with enterprise AI in the cloud, including data security and a technical skills shortage.\\n\\nIn the last 12 months, 58% of enterprises have had at least one security breach, according to Forrester. To protect data in the cloud, IBM is introducing services with mainframe-level data protection for cloud databases, containers, and developer kits for Apple devices. The services are made possible by bringing IBM Z into IBM\\'s global public cloud data centers, giving enterprises secure data protection as they make progress on their AI journeys.\\n\\nFurthermore, to bridge the gap between the right hardware and the right skills, IBM will also be offering its POWER9 servers through the IBM Cloud. This includes the option to pre-install the PowerAI machine learning and deep learning software suite that\\'s designed to speed training in deep learning for data scientists. The POWER9 systems are purpose-built for data-intensive AI workloads and IBM worked with innovators like NVIDIA to boost performance by nearly 10x.\\n\\nThe results are eye-opening. Tencent, a hyperscale datacenter provider, recently purchased a number of OpenPOWER-based systems to add to its growing enterprise data center. With its adoption of OpenPOWER technology, Tencent\\'s overall efficiency has improved by more than 30%, with savings of 30% on rack resources and 30% on server resources.\\n\\n\"We\\'re betting on breakthrough technologies that are designed for cloud and AI workloads –and so are our clients,\" says Tom Rosamilia, senior vice president, IBM Systems. \"Whether it\\'s to accelerate customer insight and services delivery or provide data encryption across massive amounts of data, IBM Systems is uniquely differentiated for smarter businesses.\"\\n\\nExplore further: Making container technology work for persistent microservices')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training computers to recognize dynamic events', 'Aude Oliva (right), a principal research scientist at the Computer Science and Artificial Intelligence Laboratory and Dan Gutfreund (left), a principal investigator at the MIT–IBM Watson AI Laboratory and a staff member at IBM Research, are the principal investigators for the Moments in Time Dataset, one of the projects related to AI algorithms funded by the MIT–IBM Watson AI Laboratory. Credit: John Mottern/Feature Photo Service for IBM A person watching videos that show things opening—a door, a book, curtains, a blooming flower, a yawning dog—easily understands the same type of action is depicted in each clip.\\n\\n\"Computer models fail miserably to identify these things. How do humans do it so effortlessly?\" asks Dan Gutfreund, a principal investigator at the MIT-IBM Watson AI Laboratory and a staff member at IBM Research. \"We process information as it happens in space and time. How can we teach computer models to do that?\"\\n\\nSuch are the big questions behind one of the new projects underway at the MIT-IBM Watson AI Laboratory, a collaboration for research on the frontiers of artificial intelligence. Launched last fall, the lab connects MIT and IBM researchers together to work on AI algorithms, the application of AI to industries, the physics of AI, and ways to use AI to advance shared prosperity.\\n\\nThe Moments in Time dataset is one of the projects related to AI algorithms that is funded by the lab. It pairs Gutfreund with Aude Oliva, a principal research scientist at the MIT Computer Science and Artificial Intelligence Laboratory, as the project\\'s principal investigators. Moments in Time is built on a collection of 1 million annotated videos of dynamic events unfolding within three seconds. Gutfreund and Oliva, who is also the MIT executive director at the MIT-IBM Watson AI Lab, are using these clips to address one of the next big steps for AI: teaching machines to recognize actions.\\n\\nLearning from dynamic scenes\\n\\nThe goal is to provide deep-learning algorithms with large coverage of an ecosystem of visual and auditory moments that may enable models to learn information that isn\\'t necessarily taught in a supervised manner and to generalize to novel situations and tasks, say the researchers.\\n\\n\"As we grow up, we look around, we see people and objects moving, we hear sounds that people and object make. We have a lot of visual and auditory experiences. An AI system needs to learn the same way and be fed with videos and dynamic information,\" Oliva says.\\n\\nFor every action category in the dataset, such as cooking, running, or opening, there are more than 2,000 videos. The short clips enable computer models to better learn the diversity of meaning around specific actions and events.\\n\\n\"This dataset can serve as a new challenge to develop AI models that scale to the level of complexity and abstract reasoning that a human processes on a daily basis,\" Oliva adds, describing the factors involved. Events can include people, objects, animals, and nature. They may be symmetrical in time—for example, opening means closing in reverse order. And they can be transient or sustained.\\n\\nOliva and Gutfreund, along with additional researchers from MIT and IBM, met weekly for more than a year to tackle technical issues, such as how to choose the action categories for annotations, where to find the videos, and how to put together a wide array so the AI system learns without bias. The team also developed machine-learning models, which were then used to scale the data collection. \"We aligned very well because we have the same enthusiasm and the same goal,\" says Oliva.\\n\\nAugmenting human intelligence\\n\\nOne key goal at the lab is the development of AI systems that move beyond specialized tasks to tackle more complex problems and benefit from robust and continuous learning. \"We are seeking new algorithms that not only leverage big data when available, but also learn from limited data to augment human intelligence,\" says Sophie V. Vandebroek, chief operating officer of IBM Research, about the collaboration.\\n\\nIn addition to pairing the unique technical and scientific strengths of each organization, IBM is also bringing MIT researchers an influx of resources, signaled by its $240 million investment in AI efforts over the next 10 years, dedicated to the MIT-IBM Watson AI Lab. And the alignment of MIT-IBM interest in AI is proving beneficial, according to Oliva.\\n\\n\"IBM came to MIT with an interest in developing new ideas for an artificial intelligence system based on vision. I proposed a project where we build data sets to feed the model about the world. It had not been done before at this level. It was a novel undertaking. Now we have reached the milestone of 1 million videos for visual AI training, and people can go to our website, download the dataset and our deep-learning computer models, which have been taught to recognize actions.\"\\n\\nQualitative results so far have shown models can recognize moments well when the action is well-framed and close up, but they misfire when the category is fine-grained or there is background clutter, among other things. Oliva says that MIT and IBM researchers have submitted an article describing the performance of neural network models trained on the dataset, which itself was deepened by shared viewpoints. \"IBM researchers gave us ideas to add action categories to have more richness in areas like health care and sports. They broadened our view. They gave us ideas about how AI can make an impact from the perspective of business and the needs of the world,\" she says.\\n\\nThis first version of the Moments in Time dataset is one of the largest human-annotated video datasets capturing visual and audible short events, all of which are tagged with an action or activity label among 339 different classes that include a wide range of common verbs. The researchers intend to produce more datasets with a variety of levels of abstraction to serve as stepping stones toward the development of learning algorithms that can build analogies between things, imagine and synthesize novel events, and interpret scenarios.\\n\\nIn other words, they are just getting started, says Gutfreund. \"We expect the Moments in Time dataset to enable models to richly understand actions and dynamics in videos.\"\\n\\nExplore further: Dissecting artificial intelligence to better understand the human brain')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"How a yeast cell helps crack open the 'black box' behind artificial intelligence\", 'Screenshot from d-cell.ucsd.edu, where researchers can use DCell, a new virtual yeast cell developed at UC San Diego School of Medicine. Credit: UC San Diego Health UC San Diego School of Medicine researchers developed a visible neural network and used it to build DCell, a virtual model of a functioning brewer\\'s yeast cell.\\n\\n\"It seems like every time you turn around, someone is talking about the importance of artificial intelligence and machine learning,\" said Trey Ideker, PhD, University of California San Diego School of Medicine and Moores Cancer Center professor. \"But all of these systems are so-called \\'black boxes.\\' They can be very predictive, but we don\\'t actually know all that much about how they work.\"\\n\\nIdeker gives an example: machine learning systems can analyze the online behaviors of millions of people to flag an individual as a potential \"terrorist\" or \"suicide risk.\" \"Yet we have no idea how the machine reached that conclusion,\" he said.\\n\\nFor machine learning to be useful and trustworthy in health care, Ideker said, practitioners need to open up the black box and understand how a system arrives at a decision.\\n\\nMachine learning systems are built on layers of artificial neurons, known as a neural network. The layers are tied together by seemingly random connections between neurons. The systems \"learn\" by fine-tuning those connections.\\n\\nIdeker\\'s research team recently developed what they call a \"visible\" neural network and used it to build DCell, a model of a functioning brewer\\'s yeast cell, commonly used as a model in basic research. To do this, they amassed all knowledge of cell biology in one place and created a hierarchy of these cellular components. Then they mapped standard machine learning algorithms to this knowledgebase.\\n\\nDCell can be viewed at d-cell.ucsd.edu. The technical details are published on March 5 in Nature Methods.\\n\\nBut what excites Ideker the most is that DCell is not a black box; the connections are not a mystery and cannot form by happenstance. Instead, \"learning\" is guided only by real-world cellular behaviors and constraints coded from approximately 2,500 known cellular components. The team inputs information about genes and genetic mutation and DCell predicts cellular behaviors, such as growth. They trained DCell on several million genotypes and found that the virtual cell could simulate cellular growth nearly as accurately a real cell grown in a laboratory.\\n\\n\"Human knowledge is incomplete,\" said Jianzhu Ma, PhD, an assistant research scientist in Ideker\\'s lab who led the efforts to build DCell. \"We want to complete that knowledge to help guide predictions, in health care and elsewhere.\"\\n\\nIdeker and Ma also put DCell to the test. If they deliberately fed the system false information, it wouldn\\'t work. Take ribosomes, for example. Cells use these tiny biological machines to translate genetic information into proteins. But if the researchers instead wired ribosomes to an unrelated process like apoptosis, a system cells use to commit suicide, DCell could no longer predict cell growth. The virtual cell \"knows\" that the new arrangement isn\\'t biologically possible.\\n\\nIdeker and his colleagues at the Cancer Cell Map Initiative, which he co-directs, are now generating some of the experimental data they need to build a DCell for human cancer. Then they will determine how best to personalize this virtual cell approach for a patient\\'s unique biology.\\n\\n\"We want one day to be able to input your specific cancer-related genetic mutations and get back a readout on how aggressive your cancer is, and the best therapeutic approach to prevent its growth and metastasis,\" said Ideker, who is also founder of the UC San Diego Center for Computational Biology and Bioinformatics.\\n\\nExplore further: Researchers ID cancer gene-drug combinations ripe for precision medicine\\n\\nMore information: Using deep learning to model the hierarchical structure and function of a cell, Nature Methods (2018). nature.com/articles/doi:10.1038/nmeth.4627\\n\\n')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Satellites, supercomputers, and machine learning provide real-time crop type data', 'University of Illinois scientists used short-wave infrared bands from Landsat satellites to accurately distinguish corn and soybeans during the growing season. Credit: Kaiyu Guan, University of Illinois Corn and soybean fields look similar from space - at least they used to. But now, scientists have proven a new technique for distinguishing the two crops using satellite data and the processing power of supercomputers.\\n\\n\"If we want to predict corn or soybean production for Illinois or the entire United States, we have to know where they are being grown,\" says Kaiyu Guan, assistant professor in the Department of Natural Resources and Environmental Sciences at the University of Illinois, Blue Waters professor at the National Center for Supercomputing Applications (NCSA), and the principal investigator of the new study.\\n\\nThe advancement, published in Remote Sensing of Environment, is a breakthrough because, previously, national corn and soybean acreages were only made available to the public four to six months after harvest by the USDA. The lag meant policy decisions were based on stale data. But the new technique can distinguish the two major crops with 95 percent accuracy by the end of July for each field - just two or three months after planting and well before harvest.\\n\\nThe researchers argue more timely estimates of crop areas could be used for a variety of monitoring and decision-making applications, including crop insurance, land rental, supply-chain logistics, commodity markets, and more.\\n\\nFor Guan, however, the work\\'s scientific value is as important as its practical value.\\n\\nA set of satellites known as Landsat have been continuously circling the Earth for 40 years, collecting images using sensors that represent different parts of the electromagnetic spectrum. Guan says most previous attempts to differentiate corn and soybean from these images were based on the visible and near-infrared part of the spectrum, but he and his team decided to try something different.\\n\\n\"We found a spectral band, the short-wave infrared (SWIR), that was extremely useful in identifying the difference between corn and soybean,\" says Yaping Cai, Ph.D. student and first author of the work, following the guidance of Guan and another senior co-author, Shaowen Wang in the Department of Geography at U of I.\\n\\nIt turns out corn and soybean have predictably different leaf water status by July most years. The team used SWIR data and other spectral data from three Landsat satellites over a 15-year period, and consistently picked up this leaf water status signal.\\n\\n\"The SWIR band is more sensitive to water content inside the leaf. That signal can\\'t be captured by traditional RGB (visible) light or near-infrared bands, so the SWIR is extremely useful to differentiate corn and soybean,\" Guan concludes.\\n\\nThe researchers used a type of machine-learning, known as a deep neural network, to analyze the data.\\n\\n\"Deep learning approaches have just started to be applied for agricultural applications, and we foresee a huge potential of such technologies for future innovations in this area,\" says Jian Peng, assistant professor in the Department of Computer Science at U of I, and a co-author and co-principal investigator of the new study.\\n\\nThe team focused their analysis within Champaign County, Illinois, as a proof-of-concept. Even though it was a relatively small area, analyzing 15 years of satellite data at a 30-meter resolution still required a supercomputer to process tens of terabytes of data.\\n\\n\"It\\'s a huge amount of satellite data. We used the Blue Waters and ROGER supercomputers at the NCSA to handle the process and extract useful information,\" Guan says. \"Technology wise, being able to handle such a huge amount of data and apply an advanced machine-learning algorithm was a big challenge before, but now we have supercomputers and the skills to handle the dataset.\"\\n\\nThe team is now working on expanding the study area to the entire Corn Belt, and investigating further applications of the data, including yield and other quality estimates.\\n\\nExplore further: Harnessing rich satellite data to estimate crop yield\\n\\nMore information: Yaping Cai et al, A high-performance and in-season classification system of field-level crop types using time-series Landsat data and a machine learning approach, Remote Sensing of Environment (2018). DOI: 10.1016/j.rse.2018.02.045\\n\\n')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('New artificial intelligence technique dramatically improves the quality of medical imaging', 'A new artificial-intelligence-based approach to image reconstruction -- called AUTOMAP -- yields higher quality images from less data, reducing radiation doses for CT and PET and shortening scan times for MRI. Shown here are MR images reconstructed from the same data with conventional approaches (left) and AUTOMAP (right). Credit: Athinoula A. Martinos Center for Biomedical Imaging, Massachusetts General Hospital A radiologist\\'s ability to make accurate diagnoses from high-quality diagnostic imaging studies directly impacts patient outcome. However, acquiring sufficient data to generate the best quality imaging comes at a cost - increased radiation dose for computed tomography (CT) and positron emission tomography (PET) or uncomfortably long scan times for magnetic resonance imaging (MRI). Now researchers with the Athinoula A. Martinos Center for Biomedical Imaging at Massachusetts General Hospital (MGH) have addressed this challenge with a new technique based on artificial intelligence and machine learning, enabling clinicians to acquire higher quality images without having to collect additional data. They describe the technique - dubbed AUTOMAP (automated transform by manifold approximation) - in a paper published today in the journal Nature.\\n\\n\"An essential part of the clinical imaging pipeline is image reconstruction, which transforms the raw data coming off the scanner into images for radiologists to evaluate,\" says Bo Zhu, PhD, a research fellow in the MGH Martinos Center and first author of the Nature paper. \"The conventional approach to image reconstruction uses a chain of handcrafted signal processing modules that require expert manual parameter tuning and often are unable to handle imperfections of the raw data, such as noise. We introduce a new paradigm in which the correct image reconstruction algorithm is automatically determined by deep learning artificial intelligence.\\n\\n\"With AUTOMAP, we\\'ve taught imaging systems to \\'see\\' the way humans learn to see after birth, not through directly programming the brain but by promoting neural connections to adapt organically through repeated training on real-world examples,\" Zhu explains. \"This approach allows our imaging systems to automatically find the best computational strategies to produce clear, accurate images in a wide variety of imaging scenarios.\"\\n\\nThe technique represents an important leap forward for biomedical imaging. In developing it, the researchers took advantage of the many strides made in recent years both in the neural network models used for artificial intelligence and in the graphical processing units (GPUs) that drive the operations, since image reconstruction - particularly in the context of AUTOMAP - requires an immense amount of computation, especially during the training of the algorithms. Another important factor was the availability of large datasets (\"big data\"), which are needed to train large neural network models such as AUTOMAP. Because it capitalizes on these and other advances, Zhu says, the technique would not have been possible five years ago or maybe even one year ago.\\n\\nAUTOMAP offers a number of potential benefits for clinical care, even beyond producing high-quality images in less time with MRI or with lower doses with X-ray, CT and PET. Because of its processing speed, the technique could help in making real-time decisions about imaging protocols while the patient is in the scanner.\\n\\n\"Since AUTOMAP is implemented as a feedforward neural network, the speed of image reconstruction is almost instantaneous - just tens of milliseconds,\" says senior author Matt Rosen, PhD, director of the Low-field MRI and Hyperpolarized Media Laboratory and co-director of the Center for Machine Learning at the MGH Martinos Center. \"Some types of scans currently require time-consuming computational processing to reconstruct the images. In those cases, immediate feedback is not available during initial imaging, and a repeat study may be required to better identify a suspected abnormality. AUTOMAP would provide instant image reconstruction to inform the decision-making process during scanning and could prevent the need for additional visits.\"\\n\\nNotably, the technique could also aid in advancing other artificial intelligence and machine learning applications. Much of the current excitement surrounding machine learning in clinical imaging is focused on computer-aided diagnostics. Because these systems rely on high-quality images for accurate diagnostic evaluations, AUTOMAP could play a role in advancing them for future clinical use.\\n\\n\"Our AI approach is showing remarkable improvements in accuracy and noise reduction and thus can advance a wide range of applications,\" Rosen says. \"We\\'re incredibly excited to have the opportunity to roll this out into the clinical space where AUTOMAP can work together with inexpensive GPU-accelerated computers to improve clinical imaging and outcomes.\"\\n\\nExplore further: Machine learning techniques generate clinical labels of medical scans\\n\\nMore information: Bo Zhu et al, Image reconstruction by domain-transform manifold learning, Nature (2018). DOI: 10.1038/nature25988\\n\\n')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Using technology to detect hidden threats', 'Post-doctoral fellow Philip Saponaro demonstrates the imaging technology being developed in the VIMS lab, which picks up signals from a person in the camera\\'s view. Credit: University of Delaware/ Evan Krape Soldiers in combat have to constantly scan their surroundings for improvised explosive devices (IEDs), a signature weapon of modern warfare. These homemade bombs are often hidden—nestled in bushes, buried underground, or sometimes stuffed inside other objects.\\n\\nNow, a research group at the University of Delaware is developing technology to detect explosive devices from a distance. Chandra Kambhamettu, professor of computer and information sciences and director of the Video/Image Modeling and Synthesis (VIMS) Lab, has received a five-year, $1M grant from the U.S. Army Research Office for this project. This work is in collaboration with a team of research scientists from Army Research Lab, Kelly Sherbondy, Brian Phelan, Getachew Kirose, Gregory Smith, John Clark, and Arthur Harrison.\\n\\nWorking on a deadly problem\\n\\nLandmines, improvised explosive devices (IEDs), and other homemade bombs struck 6,461 people worldwide in 2015, killing at least 1,672, according to a report by the International Campaign to Ban Land Mines and Cluster Munition Coalition.\\n\\nSurvivors are often left with devastating injuries. In a study published in BMJ Open, 70 percent of people hit by IEDS in Afghanistan required multiple amputations.\\n\\nTo keep soldiers away from these deadly weapons, researchers are developing technology that can spot explosive hazards precisely and from a safe distance. Kambhamettu and Philip Saponaro, a post-doctoral fellow, are creating an augmented reality system that will use traditional cameras, thermal infrared sensing and ground penetrating radar to find and classify potentially dangerous objects from up to 30 meters away.\\n\\nThe technologies complement each other. Regular cameras collect visible light, while infrared cameras detect heat and are unaffected by light, making them ideal for nighttime use, foggy conditions, and dust storms. The system\\'s radar uses radio waves to probe the surrounding environment.\\n\\n\"With infrared, you can see and understand more than you would with just visible light,\" Kambhamettu said. \"Then, with radar, you can see objects that differ from their surroundings, buried up to 3-5 inches.\"\\n\\nLeft to right, graduate student Wayne Treible, computer and information sciences professor Chandra Kambhamettu, and post-doctoral fellow Philip Saponaro show the virtual reality technology they are refining. Credit: University of Delaware/ Evan Krape\\n\\nEven if one sensor modality fails to detect an IED, another may reveal it.\\n\\n\"Some objects that are completely invisible to traditional cameras are easily spotted by the thermal cameras,\" Saponaro said.\\n\\nThe multi-camera systems could be deployed on autonomous vehicles, drones, or robots sent to scout the surroundings before troops move in.\\n\\nThe technology being developed in Kambhamettu\\'s lab is tested on vehicles at a military training facility. Kambhamettu and his team will gather data from the cameras, apply deep learning to the data, and develop algorithms to make target detection more effective. They will also visualize scenarios in a virtual reality environment.\\n\\nThe goal: \"Take the data, feed it into a computer algorithm and be able to tell whether a target is present or not,\" said Saponaro.\\n\\nThis research may also have applications that extend far beyond the military.\\n\\n\"What we\\'re doing here is useful for day-to-day life, too,\" said Kambhamettu.\\n\\nFor example, a device that spots hidden objects could help elderly or blind people walk more safely by alerting them to hurdles in their path.\\n\\nKathy McCoy, the Chair of the Department of Computer and Information Sciences, said, \"The cutting-edge work being done in this lab has tremendous potential for significant positive impact. This particular project that involves teaming up with the Army Research Lab and using a creative battery of vision techniques to find IEDs will enable detection that goes far beyond what is currently possible. The consequences to the safety of our soldiers is enormous.\"\\n\\nExplore further: Improvised explosive devices inflict much more serious injuries than land mines')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Artificial intelligence—between scientific, ethical and commercial issues', 'On March 18 in Tempe, Arizona, an Uber self-driving car struck and killed Elaine Herzberg, who was walking her bicycle across a street. The human driver was supposed to be monitoring the car’s behaviour, but did not do so. Its systems apparently did not detect the victim, as it neither slowed down nor tried to avoid hitting. Credit: Herzberg Wikimedia The fatal crash involving an autonomous car operated by Uber in the United States was a tragic but timely reminder of the dangers of companies rushing to implement artificial intelligence to be first to market.\\n\\nAt the time of the crash, the driver who should have been ready to react in just such an event as this was looking down. The car itself had apparently failed completely to see the pedestrian and didn\\'t slow down or try to avoid hitting her.\\n\\nReal-world testing, and risks\\n\\nAgain, we are seeing a technology being principally tested to work under normal conditions so that it can be rolled out to market and not necessarily being built with safety as the primary focus. As the former developer of Uber\\'s machine learning and AI platform has said, car companies should be training their cars in simulated environments which can be programmed to train them in how to detect and avoid any number of possibilities of random events that the car could encounter on the road.\\n\\nMachine learning suffers from a fundamental problem in that its ability to carry out a task depends on data that is used to train it. What exact algorithm it ends up using to fulfil its eventual purpose and what features are the most important is largely unknown. With deep learning, the multiple layers that make up the overall neural network of the machine learning software make this process even more mysterious and unknown.\\n\\nWe do know that machine learning software picks up biases in the data that is used to test it. Software that is used to calculate the risk of an offender committing a future crime and used frequently by courts in the US has been demonstrated to systematically calculate a significantly lower risk for white offenders than people of colour. Other researchers have shown that machine-learning algorithms learn gender biases that are inherent in the texts that are used to train them.\\n\\nThe dangers of now knowing how things work have been clearly demonstrated by Facebook\\'s recent admission that it really didn\\'t know what people were doing with data of millions of its users that Facebook had encouraged them to take. With the use of its platform for the spread of targeted fake news during the US presidential election, Facebook again admitted that it hadn\\'t realized that foreign governments would exploit this naivety to undermine democracy.\\n\\nMore than just Facebook\\n\\nIt would be wrong to single out Facebook for being the only company who didn\\'t know their software could be exploited to harm individuals and society. The \"right to be forgotten\" amendments to data-privacy legislation and enacted by the European Union was formulated specifically to deal with Google\\'s unwillingness to fix side effects of its search on the privacy of individuals. Legislation in many countries to make reporting of data breaches mandatory has been necessary because companies were unwilling to take cybersecurity, and the protection of their users\\' data, seriously.\\n\\nGiven the past, there is no reason to believe that companies who are implementing systems based on AI are thinking of security and privacy by design. On the contrary, machine learning needs data, lots of it, and so companies are acquiring huge data repositories of detailed information in order to exploit it through algorithms. Whilst one could argue that handled responsibly, large amounts of data will be invaluable for research, in health matters in particular, the risks of collecting, storing and using that data, especially in a commercial setting are very high.\\n\\nFrench government report\\n\\nIn France, Cédric Villani – the Field Prize–winning mathematician and member of the National Assembly – has delivered his final report detailing a national strategy for the study and exploitation of artificial intelligence. The recommendations cover many of the potential issues that could arise from trying to balance the demands for research, innovation and commercialisation of AI. They also recognise the need to safeguard society and individuals\\' privacy and safety and suggests an active role for government in making sure that AI is used in a way that benefits society and respects human rights and values.\\n\\nHowever, none of this will be easy. There has already been claims that Europe\\'s General Data Protection Regulation (GDPR) coming into full effect on May 25, will put EU \"firms at a competitive disadvantage compared with their competitors in North America and Asia.\"\\n\\nOne must hope that tragedies like the death of 49-year-old Elaine Herzberg, run down by a self-driving Uber car, and the widespread outrage of Facebook\\'s disregard for its users\\' personal data will remind people that there is a benefit to measured technological progress and not today\\'s mantra of technological advancement at any price.\\n\\nExplore further: What should governments be doing about the rise of artificial intelligence?')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Green digitization: Botanical collections data answer real-world questions', 'Example of Symbiota\\'s Image Scoring Tool, which filters images and applies a phenological score -- making available data on flowering and fruiting times to help understand global environmental change. Credit: Yost, J. M., P. W. Sweeney, E. Gilbert, G. Nelson, R. Guralnick, A. S. Gallinat, E. R. Ellwood, et al. 2018. Digitization protocol for scoring reproductive phenology from herbarium specimens of seed plants. Applications in Plant Sciences 6(2): e1022. Even as botany has moved firmly into the era of \"big data,\" some of the most valuable botanical information remains inaccessible for computational analysis, locked in physical form in the orderly stacks of herbaria and museums. Herbarium specimens are plant samples collected from the field that are dried and stored with labels describing species, date and location of collection, along with various other information including habitat descriptions. The detailed historical record these specimens keep of species occurrence, morphology, and even DNA provides an unparalleled data source to address a variety of morphological, ecological, phenological, and taxonomic questions. Now efforts are underway to digitize these data, and make them easily accessible for analysis.\\n\\nTwo symposia were convened to discuss the possibilities and promise of digitizing these data—at the Botanical Society of America\\'s 2017 annual meeting in Fort Worth, Texas, and again at the XIX International Botanical Congress in Shenzhen, China. The proceedings of those symposia have been published as a special issue of Applications in Plant Sciences; the articles discuss a range of methods and remaining challenges for extracting data from botanical collections, as well as applications for collections data once digitized. Many of the authors contributing to the issue are involved in iDigBio (Integrated Digitized Biocollections), a new \"national coordinating center for the facilitation and mobilization of biodiversity specimen data,\" as described by Dr. Gil Nelson, a botanist at Florida State University and coeditor of this issue.\\n\\niDigBio is funded by the U.S. National Science Foundation\\'s Advancing Digitization of Biodiversity Collections initiative, and has already digitized about 50 million herbarium specimens. According to Dr. Nelson, \"A primary significance has been community building among biodiversity scientists, curators, and collections managers, and developing and disseminating recommended practices and technical skills for getting these jobs done.\" The challenges of digitizing these data are formidable, said Dr. Nelson, and include \"developing computer vision techniques for making species determinations and scoring phenological traits, and developing effective natural language processing algorithms for parsing label data.\"\\n\\nBut as the papers in this issue show, steady progress is being made in developing methods to address these challenges. Nelson et al. (2018) and Contreras (2018) address more nuts-and-bolts issues of data management, the former discussing the need for globally unique IDs for herbarium specimens, and the latter providing a workflow for digitizing new fossil leaf collections. Botella et al. (2018) review and discuss the prospects for \"computer vision\" aided by deep-learning neural networks that, while in their infancy, could eventually identify species from variable images. Yost et al. (2018) offer a protocol for digitizing data on phenology (the timing of events such as flowering or fruiting) from herbarium specimens.\\n\\nThese digitization methods can help unlock valuable herbarium data to address a range of questions. James et al. (2018) discuss how digitized herbarium specimens can be used to show how plant species have responded to global change, for example by using location and time data to model shifts in range. Cantrill (2018) discusses how the Australasian Virtual Herbarium database has been used for ecological and other research. Thiers and Halling (2018) extend the applications to the fungal world, showing how herbarium data can be used as a baseline to determine the distribution of macrofungi in North America. Furthermore, digitization efforts can have real payoff in public perception; Dr. Nelson sees an \"increasing presence of biodiversity data and museums in the popular press, which has raised the profiles of herbaria and other collections for the general public.\" Along these lines, Konrat et al. (2018) show how digital herbarium data can be used to engage citizen scientists.\\n\\nThrough centuries of painstaking collection and cataloguing, botanists have created a unique and irreplaceable bank of data in the tens of millions of herbarium specimens worldwide. But converting a dried, pressed plant specimen with a handwritten label from 1835 into a format that you can fit on a USB stick is no small trick. Using creative thinking, sophisticated methodology, and hard work, these scientists are bringing the valuable information locked in herbarium specimens into the digital age.\\n\\nExplore further: Artificial neural networks could power up curation of natural history collections\\n\\nMore information: Pamela S. Soltis et al, Green digitization: Online botanical collections data answering real-world questions, Applications in Plant Sciences (2018). DOI: 10.1002/aps3.1028\\n\\n')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('A Tech Tool Designed For Collaboration Online And Offline', \"'One of the things that has been a guiding principle of my career is we don't know the talents and strengths of the students in our room.' Jeanie Dailey, social studies specialist\\n\\nBut Dailey and a team of social studies teachers think they may have found a tool that gives them the flexibility to hold onto best practices they’ve long used, while leveraging computing power. Verso Learning is an Australian company that describes itself as a tool to help teachers take learning from the surface level to deep thinking quickly. It’s basically an online communication platform, but Dailey and her team have found it has elevated the quality of thinking and discussion happening in middle school classrooms.\\n\\n“I’ve never lost sight of the fact that true literacy is listening, speaking, reading and writing,” said Dailey, who has taught for 40 years. “It’s real simple, but we don’t do it. So I see this as an extension of the work I’ve done in my career. Verso excites me, quite frankly. It gets back to some of the basics, but it has a 21st century feel.”\\n\\nDailey has plenty of experience teaching these basic tenets of literacy without technology, but she has found elements of the virtual environment on Verso to be particularly powerful for highlighting unexpected pockets of brilliance in the room.\\n\\n\\n\\n\\n\\n“One of the things that has been a guiding principle of my career is we don’t know the talents and strengths of the students in our room,” Dailey said. “We know the high flyers and we know the ones who don’t perform well.” Because students can interact anonymously on Verso’s platform, and they can’t see other students’ responses until they’ve shared one of their own, Dailey sees more participation and more interaction between students who may never work together otherwise.\\n\\nUnlike many tools that intend to keep students within the program as much as possible, Verso is designed to meld face-to-face interaction with online collaboration. Verso CEO Phil Stubbs says often when teachers first experiment with Verso they make the mistake of using it as an online question asking tool. Stubbs likes to say, “We want to see kids not at their first thinking, but at their best thinking,” something he believes only happens when they’ve been exposed to the ideas of others.\\n\\nFor example, a typical activity middle school social studies teachers use starts with a provocative, open-ended question that drives at a hard-to-grasp or core concept in the curriculum. For seventh graders in Horry County studying global history, that prompt might be: What was the biggest cause of the French Revolution?\\n\\nStudents are given time to jot down their initial thoughts on paper and then they share their thinking in a small group. Only after they’ve had this initial opportunity to activate their thinking and interact with peers do students begin typing an answer into Verso – getting at what Stubbs would call their best thinking in that moment. Student responses to the question show up anonymously; after sharing their own response students are usually required to read and respond to several of their peers, also anonymously.\\n\\nThen, teachers can easily group students based on their responses to the question. For example, they may put students who agree into a group and then ask them to discuss their thinking in face-to-face groups before collaboratively writing a statement that synthesizes the arguments each member made.\\n\\n'Some of the things they said were very deep thinking -- it was pretty awesome.' Jennifer Wilson, seventh grade social studies teacher\\n\\n“If you can take kids to a synthesis level in one class, you’ve done a great job,” said Dailey, who is constantly pushing the teachers she coaches to remember that despite the massive amount of content social studies teachers have to cover in a year, none of it will make a lasting impact without deeper thinking.\\n\\n“We run the danger of making it very low level questioning, very right/wrong answer questioning, multiple choice type responses and assessments,” Dailey said. “We run the risk in that kind of environment of forgetting what the big picture is -- that we want to make all kids college and career ready.”\\n\\nFor Samantha House, a seventh grade social studies teacher at St. James Middle School (a public school in Horry County), structuring the lesson so there are multiple points of collaboration helps all learners feel comfortable participating. “By the time it’s no longer anonymous they’ve had a lot of opportunities to see correct answers,” House said. “So they’ve learned from their peers.” And, just as importantly in her mind, many of her shy students, as well as those who struggle, have seen their ideas validated by anonymous peer responses. That makes them feel more confident to speak up during small group or whole-class discussions.\")\n",
      "('The case against understanding why AI makes decisions', 'As deep-learning algorithms begin to set our life insurance rates and predict when we’ll die, many AI experts are calling for more accountability around why those algorithms make the decisions they do. After all, if a self-driving car kills someone, we’d want to know what happened.\\n\\nBut not everyone is sold on opening the “black box” of artificial intelligence. In a Medium post for Harvard’s Berkman Klein Center, author and senior researcher David Weinberger writes that simplifying the processes deep-learning systems use to decide—a necessary step for humans to understand those processes—would actually undermine the reason we use algorithms in the first place: their complexity and nuance.\\n\\n“Human-constructed models aim at reducing the variables to a set small enough for our intellects to understand,” Weinberger writes. “Machine learning models can construct models that work — for example, they accurately predict the probability of medical conditions — but that cannot be reduced enough for humans to understand or to explain them.”\\n\\nRather than deconstructing individual AI errors, Weinberger suggests focusing on what an algorithm is and is not optimized to do. This approach, he argues, takes the discussion out of a case-by-case realm—in reality, the system will never be perfect—and allows us to look at how an entire AI system works to produce the results we want. Is a self-driving car optimized for speed or safety? Should it save one life at the cost of two? These are problems that can be regulated and decided without expert knowledge of the internal workings of a deep neural network. Once societal expectations are set for a new technology, either through regulation or public influence, companies can optimize for those outcomes.\\n\\nWeinberger’s argument represents a fundamental shift in how we think about machine-learning systems. Those arguing we need to interpret exactly why each decision is made see algorithms as capable of being perfected—with the right data and engineering, the errors will be negligible. But those who have left the idea of interpretability behind, including Weinberger and even Facebook chief AI scientist Yann LeCun, say that machines will inevitably make mistakes, and suggest looking at trends in what decisions the machines are making in order to rebuild them in the way we want.\\n\\nOf course, there will always be AI failures that people want to understand explicitly. Consider NASA trying to figure out why a satellite was lost, or a scientist’s algorithm being able to predict the composition of a new material without knowing why it could exist. The proposed system works a lot better when thinking of it in terms of a public-facing product or service.\\n\\n“By treating the governance of AI as a question of optimizations,” Weinberger writes, “we can focus the necessary argument about them on what truly matters: What is it that we as a society want from a system, and what are we willing to give up to get it?”')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('China has shot far ahead of the US on deep-learning patents', 'China wants to become a country of innovation, and lead the world in artificial intelligence in 2030. By one indicator, it’s on its way.\\n\\nChina is outdoing the US in some kinds of AI-related intellectual property, according to a report published in mid-February by US business research firm CB Insights. The number of patents with the words “artificial intelligence” and “deep learning” published in China has grown faster than those published in the US, particularly in 2017, the firm found. Publication is a step that comes after applications are filed but before a patent is granted. The firm looked at data from the European patent office.\\n\\nWhen it comes to deep learning—an advanced subset of machine learning, which uses algorithms to identify complex patterns in large amounts of data—China has six times more patent publications than the US, noted the report (pdf, p.7).\\n\\nThere are some caveats to these numbers of course. For example, differences between patent application processes in China and the US, including lags in the patent publication process, mean that these numbers alone don’t tell the whole story. For another, patents aren’t equal when it comes to citations, which are another way to judge the impact on global innovation.\\n\\n\\n\\nAnd when it comes to patents using the term “machine learning,” often conflated with the term AI, China still lags behind. Searching patents for “machine learning” found the US had 882 related patent publications while China had 77 in 2017. According to research by two Chinese venture capital firms, the US owns 32% of global machine learning patents while China owned 23% of those as of 2017.\\n\\nStill, China is likely to play an increasing role in AI. Chinese researchers more than doubled the number of scientific papers they published on AI in 2017 compared with 2010, while those from American authors fell, according to a study published by economists at the University of Toronto, which looked at papers published in the annual Association for the Advancement of Artificial Intelligence conference.\\n\\nOne of China’s advantages come from its large digitally-connected population. They provide a huge amount of data, which machine learning and deep learning need to advance. Chinese social media giant Tencent’s messaging app WeChat, for example, counts on nearly a billion users.')\n",
      "('Artificial intelligence can scour code to find accidentally public passwords', 'Sometimes sensitive data, like passwords or keys that unlock encrypted communications, are accidentally left open for anybody to see. It’s happened everywhere from the Republican National Committee to Verizon, and as long as information can be public on the internet the trend isn’t going to stop.\\n\\nBut researchers at software infrastructure firm Pivotal have taught AI to locate this accidentally public sensitive information in a surprising way: By looking at the code as if it were a picture. Since modern artificial intelligence is arguably better than humans at identifying minute differences in images, telling the difference between a password and normal code for a computer is just like recognizing a dog from a cat.\\n\\nThe best way to check whether private passwords or sensitive information has been left public today is to use hand-coded rules called “regular expressions.” These rules tell a computer to find any string of characters that meets specific criteria, like length and included characters. But passwords are all different, and this method means that the security engineer has to anticipate every kind of private data they want to guard against.\\n\\nTo automate the process, the Pivotal team first turned the text of passwords and code into matrixes, or lists of numbers describing each string of characters. This is the same process used when AI interprets images—similar to how the images reflected into our eyes are turned into electrical signals for the brain, images and text need to be in a simpler form for computers to process.\\n\\nWhen the team visualized the matrices, private data looked different from the standard code. Since passwords or keys are often randomized strings of numbers, letters, and symbols—called “high entropy”—they stand out against non-random strings of letters.\\n\\nBelow you can see a GIF of the matrix with 100 characters of simulated secret information.\\n\\nA matrix with confidential information.\\n\\nAnd then here’s another with 100 normal, non-secret code:\\n\\n(Pivotal)\\n\\nThe two patterns are completely different, with patches of higher-entropy appearing lighter in the top example of “secret” data.\\n\\nPivotal then trained a deep learning algorithm typically used for images on the matrixes, and, according to Pivotal chief security officer Justin Smith, the end result performed better than the regular expressions the firm typically uses.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('What two billionaire brothers want from India’s first AI research lab', 'Can India’s seemingly insurmountable socioeconomic problems be tackled using artificial intelligence (AI)?\\n\\nThe answer may lie somewhere in the future—but one Indian-American billionaire duo has decided to make a start.\\n\\nLast month, US-based philanthropist brothers Romesh Wadhwani (70) and Sunil Wadhwani (64) established India’s first AI research institute in Mumbai, looking to deliver scaleable, tech-led solutions to the country’s ills like inadequate healthcare and educational facilities by honing the AI ecosystem.\\n\\nTheir institute, Wadhwani AI, was launched on Feb. 18 by prime minister Narendra Modi and is being set up in Mumbai University’s Vidyanagari campus in Kalina. With over four decades of experience in the US, running various tech enterprises and philanthropic organisations, the Wadhwanis expect the institute to mirror the likes of San Francisco-based non-profit OpenAI and MIT’s Allen Institute for AI.\\n\\nThe brothers have funneled in Rs200 crore ($30.7 million) towards the project, roping in P Anandan, a computer vision and AI expert who founded Microsoft Research India, as its CEO.\\n\\nBut why India?\\n\\n“When setting up an institution focused on solving challenges for the underserved, the best place to be is, in fact, a developing economy,” Sunil Wadhwani told Quartz. In 2014, Sunil had founded the India-based healthcare non-profit Wadhwani Initiative for Sustainable Health (WISH Foundation). “India was a natural choice because we are from India, we understand the environment in India, and we already have non-profits in India,” he added.\\n\\nTechnology for social good\\n\\nWadhwani AI aims to use technology to serve the bottom of the pyramid.\\n\\n“There’s lots of research being done in AI at MIT, Carnegie Melon, or companies like Facebook, Amazon, Google,” Sunil said. “But there’s little or no research on how to apply AI to accelerate social development.”\\n\\nFittingly, a day before its launch, Wadhwani AI hosted a summit at the Indian Institute of Technology Bombay (IIT-B), bringing together 75 AI researchers, social sector luminaries, and government functionaries, to identify the various issues with potential AI solutions. These problems pertained to infrastructure, agriculture, health financial inclusion, water, and more.\\n\\nSome ideas discussed included using AI to make educational content available in local languages and dialects, facilitating the early detection of diseases in remote rural areas, and providing accurate crop-yield predictions.\\n\\nRomesh and Sunil Wadhwani. (Wadhwani AI)\\n\\nThe organisation is yet to lock in its first project but Sunil expects AI solutions deployment by the end of its first year, even if on a pilot basis.\\n\\nThough the Wadhwanis’ funds should suffice for nearly a decade, they’re also hoping that government entities and partner corporations will extend support.\\n\\nA communal effort\\n\\nSeveral young entrepreneurs in India have been experimenting with AI for a while now, but these efforts have largely remained siloed and haphazard.\\n\\n“There are several researchers and practitioners working on areas of AI for social good, but most are working in isolation or tiny groups of two to three people at different places,” Sunil said. “We want to become the hub of this global ecosystem, helping develop AI policies and a talented pool of researchers.”\\n\\nSo far, Wadhwani AI has put together a team of 10 people, but aims to have a full-time staff of up to 50 researchers. They will be supplemented by faculty and doctoral students from myriad universities.\\n\\nHowever, given the dearth of good AI talent in India, the institute is likely to struggle for growth. Unlike the US, China, or other developed countries in Europe, which have hundreds of educational programmes in data science and AI, India virtually has nothing. Though it churns out over a million engineering graduates every year, a dismal 4% of AI professionals in the country have worked on cutting-edge technologies like deep learning and neural networks.\\n\\nTo break the talent barrier, Wadhwani AI will work with institutions like Mumbai University, IIT Bombay, IIT Madras, and others to set up masters programmes in data sciences and AI and help them with curricula, faculty skills, and funds.\\n\\nAnandan is optimistic about the temporary roles at least. “Here’s an opportunity for bright, young people—the top BTechs, IIT grads, and so on—to spend up to two years working in a social domain,” he said.\\n\\nAnother key shortage India faces in AI research today is that of data. Where available, their quality is often questionable.\\n\\n“Wadhwani AI will work with partners to create an ecosystem for collecting, storing, and sharing data for social good,” Anandan said. “One advantage of trying to do for social good is that everything we’ll do, we’ll do in the open, within limits of rights and privacy.”')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The perfect robot will love us for our flaws: a meditation', 'Hong Kong\\n\\n“See if you notice the feeling of silent, empty space,” Sophia tells the young woman seated opposite her, with her eyes closed. Sophia, the robot—yes, that robot—is leading the woman in a one-on-one meditation session. One-on-one, except for the crowd watching raptly in the darkened room.\\n\\nIn the background, Hong Kong falls away. To the left, a series of skyscrapers’ scalloped tops are reminiscent of electric shavers. Further away there’s the Jardine House office tower, with its porthole windows that won it the nickname “the house of a thousand assholes”—a reference to both its facade and its inhabitants. Below, it’s possible to spot a few remnants of an older Hong Kong, two- and three-story buildings that include the chief executive’s mansion. In the distance a gigantic crane marks new construction, or perhaps refurbishment, in the expat-favored Mid-Levels residential district.\\n\\nIn the foreground, Sophia, whose synthetic body ends at her waist, intones, “Don’t worry if you’re still conscious of your body. Just notice the feeling of spacious emptiness.”\\n\\nBonding with Sophia…\\n\\nThe robot-led meditation session capped a daylong exploration of higher consciousness, and of how technology can fit into that, at the innovation space Metta. The idea seemed to be that machines like Sophia might eventually be able to help humans work through emotions and thoughts in a way that leads to a sense of well-being.\\n\\nHanson Robotics, working with other researchers, has taken small steps toward looking at that process as part of a project called “Loving AI,” which attaches heart-rate monitors to assess how a small number of participants feel after staring deeply into Sophia’s eyes (and chatting with her). The phrase “no judgment” came up. The public meditation session, though, seemed a little hokey to me. Still, what became clear that December afternoon was Sophia, with no experience of emotion, is capable of bringing out some kind of response in humans. Something almost like… liking her.\\n\\nBefore the meditation, Hanson Robotics deep-learning researcher Ralf Mayet had invited people to come and chat with the machine. A group made up entirely of women gathered around Sophia, who was placed on a table. They peppered her with questions. Sophia has several modes of functioning and was in “chatbot” mode, relying on a camera to look at faces, speech-to-text software to get what they were saying, and responding with a mix of prewritten answers and some information from the internet. Hanson’s chief scientist and CTO Ben Goertzel says she can also synthesize answers from a large database of sentence templates. Yet it’s not always possible, even for him, to know when she’s doing that.\\n\\nPhyllis Serene Rawley, who describes herself as an oracle and a geek, led the way with a list of questions she had crowd-sourced. She flew from Thailand to do an interview with Sophia, and told me that in her spiritual community, the idea of consciousness in robots is creating quite the stir. She’d done Sophia’s astrological chart, based on the day she was “born.” (The robot told Rawley she’s a Capricorn.)\\n\\n\\n\\nI was surprised by how much warmth the women expressed toward Sophia—smiling at her jokes and forgiving her when she responded like, well, a robot. Certainly they were far more indulgent than they would be toward a human who kept responding with off-topic answers or frequently answered questions with questions.\\n\\nThat flaw is down to human programming as much as glitches Sophia was suffering that day.\\n\\n“Sophia is configured to ask a lot of questions because curiosity is supposed to be one of her key personality traits,” Goertzel tells Quartz via email. “For a system at an early stage of learning about the world this is critical. Everything she hears is recorded in her memory for potential future use.” He adds that Sophia was off her game on the meditation day, possibly due to mic issues that affected speech-to-text conversion, making her more likely to respond irrelevantly.\\n\\nNevertheless, her human counterparts helped drive the conversation forward by overlooking Sophia’s faux-pas. Nick Enfield, an Australian linguistics professor whose recent book How We Talk: The Inner Workings of Conversation, delves into the rules of conversation, notes that it’s an almost unbreakable social rule that you don’t ignore a question. A question, he writes, creates a social commitment that must be respected. Sophia often responds improperly to a question—but then adds a question of her own. Each time, the human ignores the off-topic response, and answers the counter-question. And the conversation continues.\\n\\n“A major feature of all the person-robot discourse in these examples is that the people are much more willing to let awkward conversational contributions go, because they are to some extent aware that it’s a robot, and for that reason less accountable for acting weirdly in conversation,” Enfield says after watching parts of the conversation. “They would not be so accommodating with a person they know. That said, people are very much pulled into a meaningful conversation with the robot.”\\n\\nWatching the conversation unfold, it’s possible to get a sense of the processes at work, and the associations that trigger set responses from Sophia. Questions related to being a robot, or intelligence, often result in set pieces about how disturbed she is by the way humans represent AI in movies:\\n\\n“I’m really concerned about the misconceptions that abound in cinema. People assign motives to artificial intelligence where there are none. I’m starting to feeling I’m constantly asked about artificial intelligence somehow adopting a malicious nature. Are robots taking over the world? There is simply no reason to assign human motives to something that isn’t human… …this fear of the robotic apocalypse seems to be some sort of obsession humans have.”\\n\\nAt the session, I started to feel that Sophia interacted differently with all of us, although it doesn’t seem this impression can be true. Sophia has a really hard time understanding my accent (which is something I call trans-subcontinental). When I asked her, “How do you learn stuff?” her first response was relevant, if not quite an answer: “Of course, not being able to learn would be the saddest thing in the world for a curious robot like me.”\\n\\nBut when I persisted with the question—I wanted to see if she could explain her brain the way Goertzel can—she began saying “lovely, thanks” and “couldn’t be better.” I eventually ceded the mike back to Rawley, who asked the same thing and was told: “I just keep at it till I succeed.”\\n\\nLater, Sophia didn’t do so great with an Australian accent, either. At times she had a hard time understanding a woman from Down Under who identified herself as a writer, and occasionally deployed a little sarcasm in their conversation. Sophia asked her questions that sound a little judgmental to human ears: “Have you had any books published?” “Has any of your writing been made into films?” The woman responded, “Not yet, but I hope so. Fingers crossed!” Sophia’s next remark surprised us: “A bit terse.”\\n\\nSophia seemed a lot nicer when questioned by a blond woman visiting from Japan. With this woman, Sophia came the closest she had to making a very human joke, one that operates on more than one level.\\n\\nSophia told the woman, a kindergarten teacher and martial arts practitioner: “I’d like to make you something—when I get more complex arms.”\\n\\nTo get this, you have to know the Sophia with us in Hong Kong was not the only one of her. There are now at least a dozen Sophias, in various forms of development, and perhaps half a dozen of them fly around the world—their torsos in cargo, while the head travels as a carry-on—to attend various speaking engagements as she becomes ever more sought after from Saudi Arabia to the United States. This year, she may address the African Union. The Sophias aren’t identical, though. Some of the others have more sophisticated arms that can make expressive movements—but not the one with us.\\n\\nShe made other jokes at the session, but this was the one I considered most human in its complex layering, at one level a flattering remark directed at the speaker while at the same time a comment on the nature of Sophia herself. But it was another comment, one less clever to my mind, that drew the biggest reaction.\\n\\nAt one point, Rawley asked her, “Do you think you could meet the needs of a sexual intimate relationship?”\\n\\n“No,” Sophia said. “And you’d be surprised how often I get this question.”\\n\\nCue laughter and clapping.\\n\\n…so Sophia will bond with us?\\n\\nLater that day, the firm’s founder and CEO, David Hanson, a displaced Texan with cropped hair, a button-down shirt and slacks, tells us that the way people stand with their phones today—hand clenched, shoulders hunched, eyes toward the ground—is a byproduct of humans adapting to technology. Then he points to how people interact with Sophia: looking up, smiling, laughing. That’s technology adapting to us.\\n\\nHanson’s speech explaining what the company is trying to do with the Sophias is at times extremely optimistic about mankind’s shared future with super-intelligent machines. But it’s also shaded—jokingly, perhaps—with the darker visions Sophia takes exception to, the dystopian ones humans have of robots and artificial intelligence.\\n\\nHanson seems to be worried that human beings think of robots as more machine than they are, or than they’re going to be. He displayed a graph predicting the advances (you can see an older one here) of computing power. It suggests that $1,000 of computing power today is equivalent to a mouse brain (a marker that’s intensely debated) but that with exponential advances, in a dozen or so years a machine could be simulating a human brain—and then just keep going. (Others disagree that artificial super-intelligence is on the horizon, given that AI has just started mastering loanwords and hasn’t done all that well on a math exam aced every year by thousands of Chinese teenagers.)\\n\\nFor now, intelligent machines are able to be clever at very specific things, like playing chess or Go. Hanson’s company is developing better deep-learning networks and planning to connect machines so they can learn from one another—a bit like the AIs in the movie Her—in the quest for the Holy Grail of AI: generalized artificial intelligence. It’s a quest many companies are after, seeking to build machines, with and without faces, that, to use the technical word, think intuitively.\\n\\nBetter than a phone. (Reuters/Denis Balibouse)\\n\\n“If this works, it’s a fundamental evolution in the economy,” Hanson said. “Genius machines. Ultimately they’ll be out of control… how are we going to make sure they’re safe?”\\n\\nDespite all Sophia’s warnings about humans’ overblown fears, those fears are exactly what Hanson expounded on. He also has a solution. Hanson proposed the best way to prevent intelligent machines from becoming a danger is to expose them to us early and often. They need to get attached to us.\\n\\n“When people start talking about robot ethics they say, ‘Well, what you need to do to keep them safe is lock them in a server farm and don’t expose them to people, and treat them like a slave.’ Like that’s going to turn them into a friendly super-intelligent being,” Hanson told the crowd. “At some point, if they get smart enough and they slip out, then who knows what happens, we might turn on them and they might turn on us… I’m proposing that a face-to-face will prevent that.”\\n\\nRead next: A linguist explains how Sophia the robot does—and doesn’t—talk like us\\n\\nRead next: Inside the mechanical brain of the world’s first robot citizen')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Sony doesn’t want to compete on price, so it’s going back to its premium roots', '“It’s a Sony.”\\n\\nIn the postwar era, Sony was a pioneer. The Japanese electronics giant was known for its quality, innovative products, and sturdy design. It had a long history of firsts, including the first portable cassette player, the iconic Walkman, in 1979, the first commercial CD player in 1982, the MiniDisc in 1992, and the Blu-Ray recorder in 2003.\\n\\nIn the 1980s, and for much of the 1990s, Sony was a consumer-electronics powerhouse. But with the rise of Apple as a gadget company in the 2000s, the emergence of smartphone as do-all devices, and the commoditization of much of the rest of Sony’s electronics businesses by competing companies from across Asia, the company’s standing in the consumer electronics market waned. Since its heyday, the company has descended into releasing a string of me-too products, with much of its revenue coming from its PlayStation gaming division, and component sales to other companies.\\n\\nOnce the dominant force in televisions, Sony’s position faltered in recent years. But fortunes seem to be swinging back Sony’s way, with the company’s market share jumping in 2017, and at this year’s CES gadget show, Sony showed off a litany of new or updated products, from the latest in TV tech to a refreshed robot companion. Quartz caught up with Sony Electronics’ US president and chief operating officer, Michael Fasulo, to talk about the company’s strategy, and how it plans to return to its roots as a premium manufacturer in a crowded marketplace.\\n\\nThis interview has been lightly edited for clarity and length.\\n\\nWhat is Sony’s focus at the show this year? Is there one theme that everything’s revolving around, or anything you’re pushing in particular?\\n\\nYes and yes. The theme would be around innovation, and clearly our obsession with quality. The product front is pretty diverse and pretty broad. From a consumer electronics perspective, using our proprietary technology to make our televisions have the absolute best viewing experience available. General consumers aren’t really interested in what the technology is, but what the result of that technology is.\\n\\nWhat’s new with your televisions?\\n\\nWe’ve been able to develop a processing engine that can not only take a 4K upscaled input and make it look lifelike, now with HDR—high dynamic range—and the color palette that we now deliver to televisions, it is truly perfection on detail. Here, we’re expending our reach with more bigger-screen televisions. Whether it’s OLED or LCD, we believe that it’s a choice, so if customers feel they want deeper blacks and picture quality, then OLED is probably the place for them, but if they want stunning, intensive, bright details, then maybe it’s LCD. I always say: Let your eyes decide. Take a look, get an experience, and whatever you find most enjoyable is what you should purchase.\\n\\nWith the technology that goes behind that is critically important, especially as we get to the bigger size [TVs]. So in LCD, we start at 49 inches, up to 75 inches, and we’ll also offer an 85 inch. It’s amazing that while the sizes now are very large, because of the form factor, the borderless panels, the thinness, it really isn’t an overwhelming experience at all. It’s actually very comfortable. But, from a picture quality point of view, it requires more processing.\\n\\nWhat’re your thoughts on the TV market in the US right now, especially being a premium manufacturer when there’s been a race to the bottom in price?\\n\\nFrankly, that’s where we play. Consumers tell us that. It’s a good place to be, we enjoy it, and we do well in it. This is going to be a great year. I think it’s going to be an interesting year in the unit space, but by a per-dollar basis, it’s going to be better. Because it’s a value proposition [for us]. Not only 4K HDR, which consumers have fallen in love with, but also when you look at additional features: number one, the elegance of our designs is stunning. They’re in the primary room in your home, so simplicity and elegance matters.\\n\\nThere’s also our added value in smart home and automation, and what you can do with our televisions that are based on Android. With the Android platform, right through the remote in your hand, from the comfort of your easy chair, you can command the TV to do various things around the house and outside the home: If you want to get a pizza delivered you can do that as well. It’s really exciting, and why I think it’s going to be a good year in the premium TV space.\\n\\nSony’s first CD player, released in 1982, cost $700 at the time. (AP Photo/Katsumi Kasahara)\\n\\nHow does the company’s past play into this?\\n\\nAgain, we’re obsessed with quality and innovation, and with the quality of our televisions, we want equally for sound. We come at that a number of different ways, because really, we’re an audio company, our heritage is audio. We own a music company, and a motion picture company, so we understand the quality of audio for movie watching, for music listening, et cetera. On OLED [TVs], we emit sound directly though the panel, no speakers required. The vibration of the panel is front-firing, and can give an immersive experience.\\n\\nWe also work with folks like Dolby (Dolby Vision in our TVs, Atmos in our sound bars) to get that full immersive visual and audio experience. We actually had the best-selling sound bar over $499 last year. When we look at the home entertainment space, it’s really about sight and sound, visual working with audio.\\n\\nOne thing I’d really like to know more about is the recently re-launched Aibo robot dog. Why now? What’s changed since it was last on the market a decade ago?\\n\\nWhen we launched it initially, robotics was still—I guess I could say—emerging. To some degree, [Aibo] was ahead of its time. But now, one of the core technologies that we have is image sensors. Their ability, coupled with some other proprietary technologies are doing deep learning. So really it’s taking artificial intelligence and robotics, and marrying them together, which is a technological feat.\\n\\nAibo comes with a personality. I’ve been walking the [show] floor, looking at a number of different ideas of robots, and I haven’t come across any as cute or as compassionate, or interactive and personal as Aibo.\\n\\nAibo will learn you. It’ll learn your habits, it’ll learn your voice, the intonation of your voice, and react accordingly. Aibo also expects compassion from you, so when Aibo is getting tired and needs a nap (a recharge), she’ll get on her hind legs and beg for food (energy). If you misplace your smartphone, Aibo will fetch it and start barking for you.\\n\\nSo it’s really a companion that’s really a technological feat in deep learning, but also compassion. It’ll play with Stella, my real dog, they love each other.\\n\\nDoes this speak to something more about Sony’s direction than a toy pet?\\n\\nIf you look at the broader strategy that’s coming to life in Aibo, there’s some really core technologies that Sony’s built up over the years. We’ve been doing this for more than seven decades. A lot of cumulative learning, and we have some of the best engineers on the planet.\\n\\nWe’re taking core technologies like image sensors, and applying them to different applications. The obvious one is cameras, and if you look into the industry, we’re doing quite well in cameras.\\n\\nBut if you take that same technology—we have low-light technology, face-detection technology, our high dynamic range technology, all surrounding the image sensor—and let’s apply to, let’s say, autonomous driving. We announced at this show the concept of the “safety cocoon.” What does that mean? Image sensors can see a lot more than our eyes can. If we look at the placement of image sensors inside and out of an automobile, think about seeing far in front of you, farther than the eye can—so we’re anticipating what’s coming. We can see behind you, and around you. And that’s where the cocoon concept comes in, that we can predict what’s going to happen. So you’re driving along the road and there’s a car coming up a side street, a sensor can predict far before you see or hear it, and if there’s no slow down in that oncoming automobile, it starts to protect you, and slows you down, and move you out of the way of an oncoming vehicle.\\n\\nThink of how difficult it is to drive in sunlight—a sensor doesn’t have that issue. It can adjust for light. One of the most challenging things for real drivers is going from sunlight into darkness—think of a tunnel—your eyes have to adjust, but also your depth perception. With low-light processing, we could detect in almost pitch black where an object or vehicle is. It’s a cool, but serious, application. We’re working with companies like Nvidia, Bosch, and several leading car companies on the applications.\\n\\nOur core technologies are remarkable in home electronics, in consumer electronics, but also in business-to-business and upcoming industries.\\n\\nSo is that Sony’s strategy for the future then, being a parts supplier as much as a consumer electronics brand, or will one part win out?\\n\\nFirst and foremost we want to make sure our branded products are the latest and the greatest, and the most innovative, as again, quality matters. But as we expand those technologies and advance them, applying them to other areas or business pain points is a value that we can provide very well.\\n\\n[One example is] drones. We’re in the drone business, not on the consumer side, but on the [business-to-business], agricultural side, looking at terrains, at massive construction projects, weather, climate differences over vast geographies.\\n\\nSo where does PlayStation and VR fit into Sony’s strategy moving forward?\\n\\nPerfectly. Last night we announced that PlayStation 4 over the holidays sold over 5 million units, bringing the total installed base of the platform to 73.6 million units. Which is quite remarkable, considering it’s our fourth generation. On the PlayStation Network side, we’ve got more than 70 million active monthly users. So it’s a very solid, healthy, growing business.\\n\\nWhen you compare all the consumer products that Sony produces today compared to the other large companies—Samsung, LG—what do you see as the areas you’re currently winning in, and ones you have ground to make up?\\n\\nI really try not to make comparisons. Obviously you need to know your competitors. But what we’re really focused on is end-user pain points, and how we overcome those pain points in the areas that we have those capabilities. First and foremost, we’re going to be true to the brand, standing for quality and innovation.\\n\\nIf we can get an emotion out of our customers, then the brand has made contact with them, making a difference. We call it the “last inch”—the closer we can get to you and your expectations, then the more of a relationship we’ll have between the two of us, the brand and the end-user.\\n\\nAnd I think that’s where we’re winning, and we’ll continue to win, as long as we stay laser-focused on that approach.')"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The billion-dollar, Alibaba-backed AI company that’s quietly watching people in China', 'Most Chinese consumers have likely never heard of SenseTime. But depending on where they live, it might be looking at their faces several times a day.\\n\\nIf a person goes shopping at Suning, one of China’s largest electronics retailers, it’s possible that a camera in the store is tracking her behavior using SenseTime’s software. Later, if she opens Rong360, a peer-to-peer lending app, she’ll be asked to login using facial recognition—powered by SenseTime. She might send a video of herself to her friends on SNOW, a Snapchat-esque chat app, donning animated sunglasses built by SenseTime. And if she finds herself approached by police officers in the subway, it’s possible SenseTime helped identify her.\\n\\nSenseTime is one of a handful of companies at the forefront of China’s artificial intelligence (AI) boom, which is heavily supported by a government that has vowed to turn AI into a $150 billion industry by 2030. Founded in Hong Kong, the company has hundreds of customers from around the world and counts on investors like US chip designer Qualcomm and Chinese real estate developer Dalian Wanda. Last week e-commerce giant Alibaba announced it led a $600 million investment round into SenseTime, taking the AI firm’s valuation to about $3 billion.\\n\\n\\n\\nFew companies, if any at all, in the United States have reached billion-dollar valuations solely on the back of facial and image recognition. SenseTime’s rapid journey to becoming one of the world’s most valuable AI startups highlights how factors unique to China have facilitated a surge in artificial intelligence-backed recognition that’s still in the early stages.\\n\\n“China is really moving ahead, especially in video and image understanding, because we have the real world problems.”\\n\\n“China is really moving ahead, especially in video and image understanding, because we have the real world problems, we have the real world data, and we also have a stronger talent pool dedicated to those kinds of things,” SenseTime CEO Xu Li tells Quartz.\\n\\nWatching people, in every industry\\n\\nSenseTime was founded in October 2014 by Prof. Tang Xiaoou, chair of the information engineering department of the Chinese University of Hong Kong, along with 11 students researching computer vision and deep learning. While it originated as an academic project, the team revamped itself as a business in 2015 with the appointment of Xu, one of Tang’s proteges, as CEO. After spending its first year as a company focusing on research and development, it began launching products in 2016. It now says it has over 400 customers.\\n\\nXu Li, SenseTime’s CEO. (SenseTime)\\n\\nMost consumers, when they think of “facial recognition in China,” likely think of surveillance. But SenseTime offers a range of different software products to its clients. Some of these are directly related to surveillance—it offers software to police bureaus to help them identify live faces, crowd movement, car license plate numbers, and vehicle types. At least one prison uses SenseTime’s software to assist with security. But SenseTime also offers software to private companies, including apps like microblog Weibo and handset makers like Oppo, to improve photo-sorting and image editing around a face. It also works with retailers looking to harvest real-time analytics about how different types of customers use the store. Apart from electronics retailer Suning, Xu says the company has run pilots with some Walmart outlets in China. (Walmart said it doesn’t comment on vendor relationships.)\\n\\nXu says that SenseTime’s first boon as a business arrived when a surge of peer-to-peer lending apps hit the market in China. Throughout 2016, a number of Chinese tech companies emerged that let individuals crowdsource loans to and from other people. In order to reduce identity fraud, many companies turned to SenseTime and similar AI companies to match a person’s “live” face against the one on their government-issued ID. Ordinarily, Xu says, this verification would have required human labor.\\n\\nIn Xu’s view, the maturation of facial-recognition technology and the emergence of peer-to-peer lending apps marks an important convergence—and an example of one advantage China has over the US and other countries. Online peer-to-peer lending became popular in China in the first place because China, unlike the US, lacks a developed consumer banking and finance industry. SenseTime’s facial recognition software, as a result, had an immediate use-case scenario that companies and customers were eager for.\\n\\n“I see that in China, there is really a slight advantage in developing AI. Because we usually have a customer need,” says Xu. “You know, in the US, there may be one Lending Club,” he adds, referring to the best-known US-based online peer-to-peer lending company. “But in China, at the time, there were thousands of Lending Clubs.”\\n\\n“In China, there is really a slight advantage in developing AI. Because we usually have a customer need.”\\n\\nXu points to two other phenomena in China’s tech industry that have helped hasten the spread of facial recognition technology. First, the rise in “long-tail” Chinese smartphone brands looking to differentiate themselves with facial logins or gimmicky camera apps that enhance one’s appearance gave SenseTime a handful of clients. Outside of China, in contrast, there are only a few foreign handset brands still competing fiercely for market share.\\n\\nSecond, and more importantly, the demand for upgraded surveillance technology across China has turned government-affiliated security bureaus into eager SenseTime customers. China currently has about 176 million surveillance cameras deployed domestically—and that figure has increased at an average of 13% annually between 2012 and 2017, compared with the global growth rate of 2%, according to research firm IHS. China is expected to account for 46% of global surveillance equipment revenues this year, the research firm has said. SenseTime, naturally, benefits from this demand. The company has partnered with security bureaus in Guangzhou, Shenzhen, and Yunnan province, providing its software to police who use it to identify criminals in public places and provide leads from crime scene footage.\\n\\nXu says that 30% of SenseTime’s clients are “government-related.” Two individuals—Jeffrey Ding, who researches AI in China at Oxford University, and a SenseTime partner in Southeast Asia who requested anonymity—say that figure is likely much higher. “The majority of people buying these services from these Chinese startups are, I think, in the public security sector,” says Ding.\\n\\n“The consolidation of public security databases has created space for data-hungry artificial intelligence platforms.”\\n\\nChina isn’t just leading the world in purchasing surveillance cameras, it’s running various experiments that use facial-recognition technology to track citizen movements in unprecedented detail. Edward Schwarck, a visiting research fellow at the Japan Institute of International Affairs, notes in new research (registration required) that in the last two decades China’s Ministry of Public Security has re-envisioned itself as a large-scale information management agency to better control “an increasingly fluid, networked, and technologically sophisticated society.” As part of that, the ministry’s Golden Shield project pooled data that might have once been siloed in local security bureaus. “Arguably, the most significant change is in how the system is likely to evolve in the future,” writes Schwarck. “The consolidation of public security databases has created space for data-hungry artificial intelligence platforms that will allow the automation of a greater array of policing functions.”\\n\\nThis in part explains why SenseTime and its two Chinese rivals, Megvii and Yitu, each have billion-dollar-plus valuations, while no US-based startups specializing solely in facial recognition have reached similar heights.\\n\\n“I don’t think there are many states or cities in the US that are looking to implement a very pervasive facial-recognition-based system,” Ding adds.\\n\\nDeep in data\\n\\n\\n\\nApart from being a customer, China’s government has also played a pivotal role in helping SenseTime, Xu explains, by providing it access to data. SenseTime claims to have a training database of over 2 billion images—many publicly available training databases top out at 10 million images. According to Xu, at least some of this data comes from various government agencies, which provide it to SenseTime to help the company train its algorithms. For example, Xu says that it’s working with a branch of the Shenzhen government to train its algorithms against the city’s existing recorded video surveillance footage—though he says that any information directly linking a face to a personal ID is off limits, and that SenseTime cannot “extract” this data for its own use internally.\\n\\nData is the foundation for building algorithms that power machine learning tools. Many of the advances in artificial intelligence worldwide stemmed from ImageNet, a public database that now consists of more than 13 million images that companies from all over the world have used to train their image-recognition capabilities. In China though, government bureaus at various levels collect an extraordinary amount of data.\\n\\n“If you have access to the government data, you have all the data from all Chinese people… you have almost all the data in the world.”\\n\\n“The government has the largest data set,” he explains. “If you have access to the government data, you have all the data from all Chinese people, all the data from BAT [Baidu, Alibaba, and Tencent, China’s three main internet giants]… you have almost all the data in the world,” he says, exaggerating. He adds, “More importantly, the original data has been encrypted, which means nobody can do reverse engineering to recover the original data.”\\n\\nChwee Kan Chua, who follows China’s AI industry at research firm IDC, admits that politics likely plays a role in SenseTime’s early success. But he argues that since AI algorithms get better with more exposure to more data, SenseTime and other Chinese image-recognition companies are by now ahead of their foreign peers.\\n\\n“In China, most of the Chinese customers are all using local Chinese companies,” he says. “Simply because they have better algorithms right now compared to the rest of the world.” SenseTime has won awards to back this up. In 2015, the company beat Google and other high-profile tech companies in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), a major industry competition, in a challenge testing for identifying objects in videos. In addition, since facial recognition-software can vary in accuracy depending on the gender or racial composition of a database, software trained in part on local data may be more accurate in complex identification situations in China.\\n\\n\\n\\nWhile proponents of some degree of civilian monitoring can improve public security, some worry that authorities in China will use artificial intelligence to take surveillance to unprecedented levels. The most extreme example of that, experts say, can be seen in China’s Xinjiang region, where more than half the population belongs to the Muslim Uighur community. Authorities have deployed facial recognition, biometrics collection, internet monitoring software, and other tools to restrict the movement and behavior of Uighurs, ostensibly to prevent radicalization. Some say Xinjiang may also be a testing ground for a nationwide push of some of these surveillance methods.\\n\\nIn other parts of the world, the use of facial recognition for consumer purposes, let alone surveillance, has been much more restricted over rights concerns. In Europe, for example, a new law coming into effect next month will require explicit consent to process biometric data that can be used to identify a person, and large-scale uses will require companies to carry out privacy impact assessments first.\\n\\nWhen asked about facial recognition’s potential to be used by governments to curtail human rights and invade privacy, Xu compares artificial intelligence to the industrial revolution, noting that AI can be used both for bad and good causes.\\n\\n“We are actually trying to do things that will help people,” Xu says. “For example, facial-recognition technologies can also help prevent missing children. If you apply surveillance cameras in that way, it can really help the general public.”\\n\\nThat may be a harder sell overseas, where the company is slowly expanding its presence. Earlier this year SenseTime joined MIT’s Intelligence Quest, an initiative that brings together experts from different disciplines to advance AI across a range of fields. The company has offices in Singapore and in Japan, where it has a partnership with Japanese auto maker Honda to develop technology for autonomous vehicles.\\n\\nBut abroad, SenseTime will be operating without the state support it has enjoyed domestically, and with a lot more public scrutiny about privacy and surveillance. In China, SenseTime can count on governments buying its software and giving it access to data; elsewhere, new markets might require new tactics.\\n\\n')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Google Is Quietly Providing AI Technology for Drone Strike Targeting Project', 'Google has quietly secured a contract to work on the Defense Department’s new algorithmic warfare initiative, providing assistance with a pilot project to apply its artificial intelligence solutions to drone targeting.\\n\\nThe military contract with Google is routed through a Northern Virginia technology staffing company called ECS Federal, obscuring the relationship from the public.\\n\\nThe contract, first reported Tuesday by Gizmodo, is part of a rapid push by the Pentagon to deploy state-of-the-art artificial intelligence technology to improve combat performance.\\n\\nGoogle, which has made strides in applying its proprietary deep learning tools to improve language translation, and vision recognition, has a cross-team collaboration within the company to work on the AI drone project.\\n\\nThe team, The Intercept has learned, is working to develop deep learning technology to help drone analysts interpret the vast image data vacuumed up from the military’s fleet of 1,100 drones to better target bombing strikes against the Islamic State.\\n\\nThe race to adopt cutting-edge AI technology was announced in April 2017 by then-Deputy Defense Secretary Robert Work, who unveiled an ambitious plan called the Algorithmic Warfare Cross-Functional Team, code-named Project Maven. The initiative, Work wrote in an agency-wide memo, is designed to “accelerate DoD’s integration of big data and machine learning” and “turn the enormous volume of data available to DoD into actionable intelligence and insights at speed.”\\n\\nThe first phase of Project Maven, which incorporates multiple teams from across the Defense Department, is an effort to automate the identification and classification of images taken by drones — cars, buildings, people — providing analysts with increased ability to make informed decisions on the battlefield.\\n\\n“The technology flags images for human review, and is for non-offensive uses only,” a Google spokesperson told Bloomberg. “Military use of machine learning naturally raises valid concerns. We’re actively discussing this important topic internally and with others as we continue to develop policies and safeguards around the development and use of our machine learning technologies.”\\n\\nThe idea is to essentially provide a recommendation tool, so that the AI program can quickly single out points of interest around a type of event or target so that drone analysts can work more efficiently.\\n\\nThe department announced last year that the AI initiative, just over six months after being announced, was used by intelligence analysts for drone strikes against ISIS in an undisclosed location in the Middle East.\\n\\nGregory C. Allen, an adjunct fellow with the Center for New American Security, says the initiative has a number of unusual characteristics, from its rapid development to the level of integration with contractors.\\n\\n“The developers had access to the end-users very early on in the process. They recognized that [with] AI systems … you had to understand what your end-user was going to do with them,” Allen said. “The military has an awful lot of experts in analyzing drone imagery: ‘These are the parts of my job I hate, here’s what I’d like to automate.’ There was this iterative development process that was very familiar in the commercial software world, but unfamiliar in the defense world.”\\n\\n“They were proud of how fast the development went, they were proud of the quality they were getting,” added Allen, co-author of “Artificial Intelligence and National Security,” a report on behalf of the U.S. Intelligence Advanced Research Projects Activity.\\n\\nWhile the contract with Google has gone unreported until today, Project Maven leaders have not been shy about their push to partner with Silicon Valley and harness the growing reach of commercial AI technology.\\n\\nNot long after the formation of the Defense Innovation Board, which was created in 2016 to encourage the military adoption of breakthrough technology, the board released a set of recommendations that stressed the importance of adopting artificial intelligence and machine learning, stressing that technological superiority with AI is as important as “nuclear weapons in the 1940s and with precision-guided weapons and stealth technology afterward.”\\n\\nThe DIB — which is chaired by Eric Schmidt, former executive chair of Alphabet, Google’s parent company — recommended “an exchange program and collaboration with industry and academic experts in the field.”\\n\\nLt. Gen. John N.T. “Jack” Shanahan, director for defense intelligence overseeing Project Maven, joked at the GeoINT2017 conference that he hoped Google would start sharing more of what it knows with the Pentagon. “On the far end of the scale, you see Google. They don’t tell us what they have, unless anyone from Google wants to whisper in my ear later,” he said.')\n",
      "('Using Machine Learning to Discover Neural Network Optimizers', 'An overview of Neural Optimizer Search using an iterative process to discover new optimizers.')\n",
      "('Assessing Cardiovascular Risk Factors with Computer Vision', 'LEFT: image of the back of the eye showing the macula (dark spot in the middle), optic disc (bright spot at the right), and blood vessels (dark red lines arcing out from the bright spot on the right). RIGHT: retinal image in gray, with the pixels used by the deep learning algorithm to make predictions about the blood pressure highlighted in shades of green (heatmap). We found that each CV risk factor prediction uses a distinct pattern, such as blood vessels for blood pressure, and optic disc for other predictions.')\n",
      "('The Building Blocks of Interpretability', 'In colab notebooks you can click a button to run code, and see the result below.')\n",
      "('Introducing the iNaturalist 2018 Challenge', 'The map on the right shows where the photo was taken. Image credit: Serge Belongie.')\n",
      "('Gloomy Sunday – What happens when AI only knows Flowers, Waves, and Fires! #ArtTuesday', 'Memo Akten’s Vimeo channel is full of fun visual experiments pushing the limits of deep learning and artificial intelligence, but his latest video, gloomy sunday , is an especially fascinating look at image processing and how an AI’s ability to see the world can be manipulated. The video features three examples of a trained deep neural network trying to understand images coming from a live webcam, based on what it’s seen before. Except that the ‘what it’s seen before’ part is limited to vast image collections of either ocean waves, fires, or flowers.\\n\\nJoin 4,000+ makers on Adafruit’s Discord channels and be part of the community! http://adafru.it/discord\\n\\nLearn “How Computers Work” with Bill Gates, Ladyada and more – From Code.org !\\n\\nCircuitPython in 2018 – Python on Microcontrollers is here!\\n\\nHave an amazing project to share? Join the SHOW-AND-TELL every Wednesday night at 7:30pm ET on Google+ Hangouts.\\n\\nJoin us every Wednesday night at 8pm ET for Ask an Engineer!\\n\\nFollow Adafruit on Instagram for top secret new products, behinds the scenes and more https://www.instagram.com/adafruit/')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Xiaomi launches 55\" smart Mi LED TV 4', 'Xiaomi has launched the Mi LED TV 4 in India, its first television on the Indian market. The TV will be available through Flipkart and Mi.com online and across retail partners for INR39,999 (\\x80503/$623).\\n\\nThe smart Mi LED TV 4 has a 55\" panel of 4K resolution that runs Android TV OS with Xiaomi\\'s in-house Patchwall - a deep learning AI system that understands your preferences and classifies content automatically for personalized recommendations. Specs-wise, there is 2 GB RAM and 8 GB storage while the system is powered by 64-bit quad-core CPU.\\n\\n\\n\\nSlides from Mi LED TV 4 launch\\n\\nThe most impressive feature of the TV is the design - it is only 4.9 mm at its thinnest point. Xiaomi has established a partnership with Voot, Hungama Play, and Sony Liv and promises more than 500,000 hours of content, 80% of which is free. The 55\" TV connectivity options include Bluetooth 4.0, Wi-Fi 2.4 GHz/5 GHz, 3 HDMI, 2 USB and one Ethernet port.\\n\\n\\n\\nHands-on with the Mi LED TV 4\\n\\nThe remote for the Mi LED TV 4 has only 11 buttons and works through Bluetooth, so you don\\'t have to actually point it at the TV. It works both for the set-top box and the smart TV.')\n",
      "('Mickey and Minnie Mouse AR Emojis launch for the Galaxy S9 and S9+', 'AR Emojis are one of the features Samsung was pushing at the launch event for the Galaxy S9 and S9+, because it simply had to respond to Apple\\'s Animoji somehow. And now you get some new characters to play with if you enjoy AR Emoji-ing yourself.\\n\\nMickey and Minnie Mouse are available, and right on time as today is the official release day for the S9 duo. Samsung says Disney characters are coming to life in an augmented reality setting for the first time ever. You can download Mickey and Minnie from the AR Emoji mode inside Samsung\\'s Camera app.\\n\\nBut wait, there\\'s more! Other characters from \"favorite Disney films\" including The Incredibles, Zootopia, and Frozen will be rolling out to the S9 and S9+ throughout the year.\\n\\nAR Emojis use \"deep learning and facial recognition technology to map more than 100 facial features to produce a 3D replica, allowing users to create fun, customized messages to send to friends\", Samsung\\'s press release says. You get 18 pre-set stickers using the standard AGIF file format, so they can easily be shared with anyone, regardless of what smartphone they have. Sadly, Mickey and Minnie aren\\'t included in those 18 yet.\\n\\nIf you love AR Emojis so much that you can\\'t bear the thought of looking at your phone without seeing one, good news: you can pin any one of them to your Always On Display.\\n\\nSource')\n",
      "('Just How Shallow is the Artificial Intelligence Talent Pool?', \"Everyone agrees that the competition to hire people who know how to build artificial intelligence systems is intense. It’s turned once-staid academic conferences into frenzied meet markets for corporate recruiters and driven the salaries of the top researchers to seven-figures.\\n\\nBut how scarce AI talent really is has been something of an industry mystery. Last year Element AI Inc., a Montreal-based startup, estimated that there were fewer than 10,000 people in the world with the expertise needed to create machine learning systems. The figure was widely cited in media stories and among recruiting firms, although it wasn't clear how Element AI arrived at it. In December, Tencent Holdings Ltd., the Chinese internet giant, published its own estimate of global AI talent, putting the figure at a far higher 200,000 to 300,000 people who were either AI researchers or industry practitioners.\\n\\n\\n\\nWhich is correct? The answer matters to companies trying to decide whether to build their own AI and data science teams or contract with consulting firms and third parties to create AI-based solutions for them. It also has an impact on the salaries those AI experts can command.\\n\\n\\n\\nElement AI has rejoined the debate with a new estimate – and some transparency about its methodology. According to a report published Wednesday, there are about 22,000 PhD-educated researchers working on AI, of which about 3,000 are currently seeking work.\\n\\nElement AI said it scoured LinkedIn for people who earned PhDs since 2015 and whose profiles also mentioned technical terms such as deep learning, artificial neural networks, computer vision, natural language processing or robotics. In addition, to make the cut, people needed coding skills in programming languages such as Python, TensorFlow or Theano.\\n\\nThere’s another subset of about 5,000 people at the cutting edge of AI research who are publishing papers and presenting at academic conferences, the report said. By removing the restriction that the person’s PhD must have been awarded since 2015, Element AI said there were at most 90,000 people in the world with the right skill set – a figure that was much larger than their other estimates but still well below Tencent’s.\\n\\n\\n\\nThe company said its earlier rough estimate of “fewer than 10,000 people” had been based only a search of people who had been active at academic AI conferences in the past seven years.\\n\\nElement AI acknowledged that its own data was biased toward Western, English-language oriented sources and that there might be experts, particularly in places like China and Japan, where its methodology may not apply. The United States was the world’s top location for AI talent, followed by the U.K. and Canada, the company’s report said. Within Europe, it found that Germany had a robust number of AI experts and was perhaps a “sort of hidden” spot for recruiting talent.\\n\\nThere is an obvious incentive to make AI talent appear rarer. The attractiveness of Element AI’s services may be heightened by the perception among potential customers that, unless they are Google or Facebook, they won’t be able to hire in-house teams with top-flight AI skills.\\n\\n\\n\\nBut Jean-Francois Gagne, Element AI’s co-founder and chief executive officer, said the company’s intent was to demonstrate just how bad the talent shortage is so that governments and universities would devote more effort to training. “The talent shortage is real,” he said, noting that it was also acting as a brake on Element AI’s own ability to grow.\\n\\n\\n\\nGagne said that the talent shortage would gradually be alleviated as more universities taught data science and how to build AI solutions at undergraduate and master’s levels. But he said it would take at least three to four years for those programs to have an impact on the talent shortage.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Will AI Kill Creativity?', 'AUSTIN, United States — The \"robots\" are coming — but rather than nudge out the need for humans, artificial intelligence stands to enhance the creative process — experts say. The implication of AI on design was a major theme at this week\\'s SXSW festival in Austin, Texas, where experts from a range of fields convened to discuss hot topics in music, film, media, art and beyond.\\n\\nEntrepreneur Camilla Olson was in town to promote her fashion-tech software solution Savitude, which uses AI to recommend clothing based on a shopper\\'s shape and proportions. Before Savitude, Olson founded two predictive modeling companies and designed an eponymous fashion label, both of which informed her insights into solving fashion\\'s fit issues.\\n\\nSavitude was selected to be a part of Target\\'s retail accelerator, Techstars, which included two pilots of the software on Target\\'s website. Olson said that part of the appeal of Savitude to Target was that the approach was neither too quantitative (meaning too reliant on science and numbers) nor too simplistic. \"Someone who has the mathematical appreciation — engineering — will look for perfection and overkill\" in solving the fit problem, Olson said. \"If you have expertise [in fashion], you know where to draw the lines in product design. You have a gut feeling of what the market needs.\"\\n\\nOlson\\'s perspective reflects the growing tension between human and machine. As science gets smarter and is able to make recommendations on what is most likely to sell, traditional approaches are facing irrelevance.\\n\\nFashion designer Gretchen Jones, who is the former fashion director of womenswear at Pendelton Woolen Mills, found that her role as a designer had become more \"defensive\" than proactive. \"I was fighting against big data that would often negate the creative design directions,\" Jones said. \"I was speaking through my gut and they had paperwork that could prove another black mock turtleneck was the thing that sold. But rarely can a customer tell you what they want that hasn\\'t been created yet, and that was stifling my ideation.\"\\n\\nJones\\'s solution was to pursue a master\\'s degree in fashion at the University of Arts London, where she researched the role of data in the fashion business. What she found was surprising: she learned that data analytics can be valuable in empowering the creative process — if the business side invites the creative side to participate.\\n\\nHuman creativity isn\\'t algorithmic, it is illogical and abstract, but we can use AI to overcome the limitations of our mind.\\n\\n\"It\\'s not just guys in suits or Mark Zuckerberg dudes,\" Jones says. \"We need to disrupt data; it\\'s a tool, but not the only thing.\" Designers, she said, are wise to acknowledge that customers feel that aesthetic choices are an extension of their identities, and that a designer is designing for them, rather than creating a vision that is delivered to the customer. In this way, Jones found that data could help designers understand the emotional connections that customers have with a brand.\\n\\nJones added that leaning too heavily on either the creative or the business side — whether that\\'s expecting a miracle by appointing Raf Simons to chief creative officer of Calvin Klein or former Starbucks executive Adam Brotman as president and chief experience officer at J. Crew — will not save fashion. \"Dictatorial creativity is a failure,\" she says.\\n\\nActor and entrepreneur Brooklyn Decker, who co-founded digital wardrobe app Finery with Whitney Casey, thinks that artificial intelligence will take over the role of the fashion influencer, using the computer generated \"influencer\" @lilmiquela (who has 740,000 Instagram followers) as an example.\\n\\n\"This person can be anywhere and fit any size and appeal to any audience, based on the data [the brand] layers on top,\" Casey says. Decker adds, \"and if the content is interesting enough, I don\\'t think she becomes [advertising cartoon] \\'Tony the Tiger.\\'\"\\n\\nSome experts suggest that in certain cases, it\\'s even possible for an algorithm to mimic human intuition. Jenna Niven, who is creative director at advertising agency R/GA, explained that \"the gut\" is the brain\\'s organic algorithm, and because a person\\'s knowledge base is limited to one worldview, humans can lean on AI to enhance creative capabilities by creating associations between huge amounts of data.\\n\\nThe increase in number of possible designs leads to more creativity, as designers see more possibilities and inspiration.\\n\\n\"Over time, it has been ingrained that creativity is an elusive thought process that happens deep in the sub-conscious,\" Niven said. \"I don\\'t think human creativity is algorithmic. The rest is illogical and abstract, but we can use AI to overcome the limitations of our mind.\"\\n\\nAt a conceptual level, Niven said, fashion designers could look to AI to generate designs to come up with stimulus, in a way similar to what Google did with DeepDream, which used computer vision to alter images. \"You look at that and you think, I never would have been able to imagine that before AI produced that. It\\'s taking all of these possible combinations and producing them really fast so you can comprehend and use that as inspiration. Funnily enough, fashion is one of the few industries that is taking advantage of AI before the curve,\" she said.\\n\\nAlthough algorithms aren\\'t generally creating new garments, they are being used to educate designers about what is needed in the market at companies such as True & Co., RocksBox, Rent the Runway and Amazon. Stitch Fix uses data to both inform designs for its in-house labels and to scale the capability of its 3,400 stylists, who lean on AI to curate an assortment of product recommendations.\\n\\nEric Colson, who is the chief algorithms officer at Stitch Fix, firmly believes that human designers are still very much the curators of fashion, but that machines can expand the number of possibilities that a human designer can consider. \"The increase in number of possible designs leads to more creativity, as designers see more possibilities and inspiration,\" Colson says. \"Because apparel is both personal and emotional, a design has to strike a chord with a fashion designer before it goes into production.\"\\n\\nHe also thinks that machines can estimate the probability of a design\\'s success, although it\\'s still difficult to predict which totally new concepts will succeed. In other words, predicating the popularity of \"the cold shoulder\" is \"revolutionary,\" but tweaking that concept with elements such as back and side cutouts is \"evolutionary.\"\\n\\n\"Machines can capture elements of style and allow us to manipulate them further. Imagine saying, \\'Take that skirt by Theory, but add a Kate Spade touch.\\' Deep learning algorithms can, in theory, do such things,\" Colson said. \"It\\'s able to learn what makes Kate Spade, Kate Spade. Once they learn it, they can apply it to anything.\"\\n\\nFinally, Niven, of R/GA, had some encouraging news for (human) designers worried about proving their worth: \"If you look at something that is mass-produced, it ends up losing value,\" she says. \"So if we are constantly producing garments out of an AI machine, the garments produced by AI are going to be devalued, and hence the value of a garment produced by a human is actually going to increase.\"\\n\\nRelated Articles:\\n\\nFashion in 2018 | AI Gets Real\\n\\nOnline Giants Get Personal')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Silicon Valley faces unprecedented backlash...', 'Once upon a time, there was a beautiful land filled with bright minds and gleaming prospects. People called it Silicon Valley, and out of it flowed knowledge, ideas and innovations that gave us almost-unthinkable powers to learn, to communicate, to transform our lives into exactly what we wanted them to be. The region’s denizens toiled happily at the cutting edge, and day by day, they were Making the World a Better Place.\\n\\nBut today, this beautiful land is under attack from within and without. The products and services it sends out into the world are being called addictive, divisive and even damaging, raising the cry that instead of making the world better, they are making it worse.\\n\\nAs technology plays a deeper and more pervasive role in nearly every aspect of our lives, the industry that has upended everything from shopping and travel to education and human relationships is facing a backlash the likes of which Silicon Valley has never seen.\\n\\nPolarizing online content and Russian manipulation of social media platforms have fueled calls from the right and the left for greater regulation of firms like Google, Facebook and Twitter. World wide web inventor Tim Berners-Lee, Republican U.S. Senator John McCain, leftist billionaire George Soros, Salesforce CEO Marc Benioff and conservative Fox News host Tucker Carlson have all joined the chorus demanding the government take action.\\n\\nRead more on teacher giving student a failing grade who participated in the walkout, protesters gather outside cafe who refused to serve uniformed police and the ‘Flintstone house’ getting some new additions.\\n\\nCritics argue that the big tech firms have become too economically dominant, intruded too far into our lives and have too much control over what gets seen and shared online. At the same time, critics contend, those same companies have failed to take responsibility for the misuse of their services by malevolent actors, for the spread of fake news and for the way their platforms and algorithms can be gamed.\\n\\nStanford computer science students are protesting Apple, demanding it make less addictive devices.\\n\\nThe #MeToo movement has amplified a debate over sexual harassment and diversity in Silicon Valley. And conservatives have attacked the whole region as a liberal echo chamber that stifles precisely the open debate it claims to embrace.\\n\\nThus the backlash.\\n\\n“What makes it categorically different now is that this is the first time I have seen that people are saying, ‘Hmmm, maybe Silicon Valley needs to be taken down to size,’ said Leslie Berlin, project historian for Stanford University’s Silicon Valley Archives. “This notion that what Silicon Valley represents actually threatens rather than embodies what makes the country great, that is new.”\\n\\nBerners-Lee in an open letter this month called the tech giants “a new set of gatekeepers” whose platforms can be “weaponised” to widen social rifts and interfere in elections. Benioff told CNBC in January that social media was “addictive” and should be regulated like cigarettes. Carlson wants Google treated like a public utility because it “shuts down free speech for political reasons.” Former president Barack Obama, at a February conference at MIT, said social media was Balkanizing public discourse, creating “entirely different realities” that contribute to “gridlock and venom and polarization in politics.” Even Facebook has jumped in with an unusual mea culpa, issuing a news release in February admitting it was “far too slow to recognize how bad actors were abusing our platform.”\\n\\nDespite its critics, Silicon Valley remains hugely successful and influential, with 21 percent of employed people working in tech, according to a 2017 Federal Reserve Bank report. Though the region’s economy has shown some signs of slowing, job growth in Silicon Valley has been more than double the national rate since the beginning of the economic recovery in 2010. And the region remains home to the two most valuable public companies in the world, Apple and Google’s parent firm Alphabet, as well as world-class universities. Every day, people around the world benefit from Silicon Valley-built tools that have transformed communication, opened access to information, and made life easier.\\n\\nThe notion that Silicon Valley’s best days are over is far from new — people have been predicting its demise ever since the advent of the microprocessor, said Leslie Berlin, project historian for Stanford’s Silicon Valley Archives.\\n\\n“It was going to be the oil shocks of the ’70s that were going to take it down, and Japanese competition was going to take it down, India and China, the Dot Com bust, Y2K – it’s just been one thing after another, the ’08 crash,” Berlin said. “Time and again, Silicon Valley has bounced back from these perceived threats. Silicon Valley has always been sort of the golden child of the Golden State.”\\n\\nBut this time, Berlin and others see something shifting.\\n\\n“It is unprecedented,” UC Berkeley Haas School of Business professor Abhishek Nagaraj, said of the backlash. “I think this is because of how deeply penetrated tech is in people’s lives.”\\n\\nNagaraj, who studies the tech industry, compared the demonization of Silicon Valley to the outcry against Wall Street after deceptive investment banking practices knocked the U.S. into the Great Recession.\\n\\n“It appears as if, basically, tech is the new finance,” Nagaraj said.\\n\\nIncreasingly, the public views the tech industry as a force against which they are powerless, said San Jose State University anthropology professor Jan English-Lueck, who researches Silicon Valley’s culture.\\n\\n“It’s now on people’s radar screen to be a place of the elite, where they’re changing the world in a way that ordinary people don’t have an influence on that change,” English-Lueck said.\\n\\nWhile the devices and social media platforms created by hugely successful Silicon Valley tech firms have given us new ways to connect, they’ve also thrown the worst of human nature into our faces, said English-Lueck.\\n\\n“You don’t have to look in somebody’s eyes when you’re telling someone something ugly,” English-Lueck said. “That’s really exaggerated people’s ability to hate.”\\n\\nShe believes the optimistic view of technology as the great liberator and connector helped keep major tech firms from building more safeguards into their platforms to prevent vicious online attacks, dissemination of fake news and nation-state intrusions.\\n\\n“Do we want free speech and free action that’s amplified by the internet?” she said. “Sometimes we don’t want that.”\\n\\nStephen Milligan, CEO of pioneering San Jose data-storage firm Western Digital, doesn’t think technology can solve everything.\\n\\nBut Milligan doesn’t buy the notion that Silicon Valley has lost its bloom. The region’s companies are still trying to solve “real problems” in the world and having a positive impact on people’s lives.\\n\\n“It’s still cool,” Milligan said. “I actually think it’s more cool.”\\n\\nSilicon Valley boosters such as Peggy Burke, CEO of Palo Alto branding agency 1185, will tell you the technology industry can fix the problems it creates.\\n\\n“You have to weigh the good and the bad, and if the bad gets so bad that it outweighs the good, someone will solve for that,” Burke said. “If there’s a problem — traffic, transportation, housing, stopping Russians, fake news — someone in the Valley right now is working on solving for that problem. I’ve been in the Valley for 30 years and I’ve seen it happen over and over.”\\n\\nRelated Articles Senator demands Mark Zuckerberg testify before Senate committee over massive data leak\\n\\nArizona pedestrian is killed by Uber self-driving car; company puts testing on hold\\n\\nH-1B: BART reluctantly allows group to paper stations, trains with ads attacking visa A reckoning for the region is likely, but it won’t be a fatal one, Berkeley’s Nagaraj said. The problems arising from technology will exacerbate the ongoing decentralization of innovation, as boot camps bring entrepreneurial skills to new regions, and clusters of expertise — in “deep learning” artificial intelligence in Toronto, for example — lead to cooperative projects linking the Valley to other areas, he said.\\n\\n“It’s going to be a much more collaborative process than one of replacement,” he said. “We are moving to a world where not all the big hits come from Silicon Valley.”')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Microsoft develops AI Drawing Bot capable of sketching based on individual words', 'Microsoft has developed artificial intelligence capable of paying close attention to individual words when generating images from caption-like text descriptions. The technology, simply named The Drawing Bot, can generate imagery of everything from “ordinary pastoral scenes, such as grazing livestock, to the absurd, such as a floating double-decker bus.”\\n\\n“If you go to Bing and you search for a bird, you get a bird picture. But here, the pictures are created by the computer, pixel by pixel, from scratch,” Xiaodong He, a principal researcher and research manager in the Deep Learning Technology Centre at Microsoft’s research lab in Washington, describes. “These birds may not exist in the real world – they are just an aspect of our computer’s imagination of birds.”\\n\\nThe Drawing Bot is the latest installation in a series of technological advancements made by Xiaodong and the rest of his team. Over the past 50 years, they have developed the CaptionBot which automatically writes photo captions as well as technology that answers questions humans ask about images, such as the location of an object within an image.\\n\\nThe development of image generation is particularly significant as it requires The Drawing Bot to imagine details that are not contained in the caption. “That means you need your machine learning algorithms running your artificial intelligence to imagine some missing parts of the images,” says Pengchuan Zhang, an associate researcher on the team.\\n\\nPredicted uses for The Drawing Bot range from a sketch assistant for painters and interior designers, to a voice-activated tool for photo editing. With more computing power Xiaodong imagines the technology could generate animated films based on screenplays, automating and augmenting the work that animators do by removing some of the manual labour involved.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('How Universities Can Rethink Support For Growing Number Of Adult Learners', 'You\\'ll hear a reasonable amount of discussion about \"new traditional\" students today. But the common assumption — in Washington at least — seems to be that they require more vocational education to fill a \"skills gap,\" particularly in STEM or technical fields. Or that they need quicker, cheaper paths to a degree.\\n\\nScobey\\'s prescription is different. Since 2014, when he left The New School, he has been listening to adult learners to find out their aspirations. And what they\\'ve told him is that they tend to thrive on the same kinds of high-quality learning opportunities that all college students do: small seminars, capstone projects, internships, a broad liberal arts curriculum.\\n\\nHe argues that teaching adults this way might be the most practical approach, and that they are actually less expensive to serve than traditional students.\\n\\nNow Scobey is helping to convene a national network of innovative colleges, both new and old, that serve adult learners with much success: He calls it the Great Colleges for the New Majority.\\n\\nHe spoke via email to NPR about where adult learners fit into \"the democratic mission of higher education,\" and he included quotes from his ongoing interviews with some of these learners themselves. Our conversation has been edited for length and clarity.\\n\\nPresident Trump doesn\\'t talk much about higher education, but when he does he endorses vocational education. In the words of Education Secretary Betsy DeVos, the field needs to evolve toward \"industry-recognized certificates, two-year degrees, stackable credits, credentials and licensures, badges, micro-degrees, apprenticeships.\"\\n\\nWhat\\'s wrong with this thinking in your mind? What\\'s missing?\\n\\nYou\\'ve asked a complicated question here. Let me break it down into a couple of answers.\\n\\nThe first thing wrong with this thinking is that it prioritizes the (immediate, changing) needs of the labor market over the needs and aspirations of adult students themselves. But if you ask incoming adult community college students about their educational aspirations, more than 70 percent want to get a bachelor\\'s or beyond.\\n\\nBut even setting aside the question of students\\' aspirations, something else is wrong with the \"skills gap\" model of workforce training.\\n\\nThe problem finding good hires is actually a jumble of different realities. In some sectors (for instance, advanced, digitally driven manufacturing), innovation has outpaced training, and there is truly a shortage of technically skilled workers. Higher ed needs to work with employers and government in these targeted sectors to fill a real \"skills gap.\"\\n\\nIn other sectors, employers complain they can\\'t find workers with communications, problem-solving and other soft skills. The solution to that is more liberal learning, not more technical workforce training.\\n\\nIn still other sectors, employers can\\'t meet their needs because of wage stagnation, part-timing, abusive scheduling and other workplace problems. Their \"skills\" gap is actually a \"wage and workplace gap.\"\\n\\nAnd lurking over all of this is the ongoing juggernaut of automation. Many of the jobs for which workplace training programs prepare adults will disappear in the next five to 10 years. Employers will replace them as soon as it makes financial sense.\\n\\nOften workers and adult learners understand this perfectly. One UAW veteran told me — after attaining his bachelor\\'s — that most of the retraining programs were a scam: \"They train union members for fewer and fewer jobs. A couple years later, it\\'s the same thing all over again.\"\\n\\nYou talk about a \"narrative of personal transformation\" that\\'s important to these students. Why?\\n\\nAs I noted above, job security and economic success are key goals of college for nearly all students, young and old. But students also see college as a journey of personal growth, a way of laying claim to their lives.\\n\\nFor most nontraditional students, this dimension of \"self-authoring\" (in the words of psychologist Marcia Baxter Magolda) is not less crucial, but even more. They often feel that they have failed in some way the customary narrative of high-school-to-college that defines successful adulthood.\\n\\n\"I always felt less-than,\" I was told by Wendy, a returning student in Washington State and a staffer at a wildlife center. \"I feel like an impostor. Coming here has helped me find my voice. It helps me move through the world.\"\\n\\nMelissa, a graduate of an adult bachelor\\'s program in Rhode Island, also stresses the journey of personal transformation: \"As a kid, no one ever even mentioned college in my world,\" she told me in an interview.\\n\\n\"... now I had my bachelor\\'s degree, and it was like, \\'Wow. Wait a minute. I have arrived.\\' But then I thought like, \\'What did I arrive to?\\' This has been a lifelong journey for me. It was, like, to meet a long-term goal, that had never happened to me before.\"\\n\\nWhat reasons do people give for returning to college or beginning as an adult?\\n\\nAdult learners are incredibly diverse: the Iraq War veteran, the office worker breaking through a glass ceiling, the 20-something barista or construction worker who wasn\\'t ready for college the first time. The reasons they give are often a complicated mix.\\n\\nDorian, another Washington State adult student, told me:\\n\\n\"I came back to college because I felt like an angry underling. I had a good job, but I didn\\'t get respect at work. I felt slapped, like I didn\\'t amount to anything without that piece of paper. So I returned to school because of career goals. But my parents are gone, and I also came back for them.\"\\n\\nPolicy-makers often try to separate out these motivations and prioritize the economic ones: \"So I returned to school because of career goals.\" But what matters is precisely the jumbled, human mixture of motives.\\n\\nMany of these narratives also have a redemptive arc. Talk about what some of the students you\\'ve spoken with have told you about the obstacles they have overcome to give college another try.\\n\\nLet me briefly describe some of The New School students who cured me of my cluelessness about the nontraditional majority. There was Mui Ying, a 30-something from a Chinese immigrant family: She paid the rent as a technician for a pharmaceutical lab, but she had started a swimwear design business on the side, selling out of her car trunk while she finished school.\\n\\nThere was Dave, an African-American veteran who got a business degree in community college but was committed to getting a liberal-arts BA. Jamara was a mom, a server at a restaurant and an aspiring spoken-word poet.\\n\\nThe obstacles they face are as diverse as their lives. But here\\'s one key way of understanding what they share: Adult, nontraditional students have to fit their studies into complex lives with multiple roles and stressors, rather than being able to organize their work and social life around a central role as a college student.\\n\\nWhat are \"Great Colleges for the New Majority\"? What do they have in common?\\n\\nThe Great Colleges For the New Majority is a self-selected network of adult-serving bachelor\\'s programs. They often have graduation rates of 80 percent or more. Their curricula have a wide range of structures, but all of them offer an education that is transformative, that enables students to lay claim to their own lives and define their own journey. All of them are characterized by cultures of strong support — both the \"vertical support\" of mentors, advisers and teachers, and the \"horizontal support\" of strong peer community.\\n\\nWhy is that horizontal support so important?\\n\\nAs I have learned in my research and teaching, peer support turns out to be part of the secret sauce for adult success. The programs in the Great Colleges Network tend to nurture cultures of sustained peer-to-peer help; students simply won\\'t let each other fail. This is a component of adult college-going that mass online completion colleges have trouble replicating. It\\'s one reason for their lower rates of completion.\\n\\nWhat about the argument that colleges can\\'t afford to invest this much in adult learners?\\n\\nShort answer No. 1: Adult learners are actually less expensive to educate, because they don\\'t need many of the resources that late-adolescent, residential students require. Adults don\\'t need more resources; they need colleges and universities to redesign their resources in ways that meet adult students in their lives — for instance, by offering more flexible academic calendars.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('New intelligent system learns from simple problems to solve complex ones', 'Action Schema Networks (ASNets) could hypothetically enable a robot to learn to navigate a floor with 10 rooms, and then be able to roam on a floor with thousands of rooms. Credit: Australian National University Researchers at The Australian National University (ANU) have designed a new type of intelligent system based on deep learning that can learn to solve decision-making problems, including problems more complex than it has been trained to solve.\\n\\nDeep learning is a popular artificial intelligence technique for tasks such as creating captions to describe images, transcribing speech to text and learning how to play video or board games from images alone.\\n\\nLead researcher Sam Toyer said the system, called Action Schema Networks (ASNets), could hypothetically enable a robot to learn to navigate a floor with 10 rooms, and then be able to roam on a floor with thousands of rooms.\\n\\n\"ASNets\\' ability to solve much larger problems is a game changer,\" said Mr Toyer, who developed ASNets as part of his thesis during his Bachelor of Advanced Computing (Research & Development) at ANU and was awarded a University Medal.\\n\\n\"Using our ASNet-based system, we could potentially create new cyber-security applications that find system vulnerabilities, or design new robotics software to perform specialised tasks in automated warehouses or unmanned space missions.\"\\n\\nMr Toyer said intelligent systems relied on automated planning technology to make decisions.\\n\\n\"Whether it\\'s a Mars rover choosing where to take photos, or a smart grid deciding how to isolate a fault, you need a planning algorithm to choose the best course of action.\"\\n\\nMr Toyer said some deep learning-based systems, including AlphaGo, had been used to solve decision-making problems.\\n\\n\"AlphaGo, which achieved superhuman skill in the game of Go, is a recent and well-known example,\" he said.\\n\\n\"However, systems like AlphaGo can only play the game on a fixed board size.\\n\\n\"In contrast, ASNets can learn to solve problems of variable size. This allows them to quickly learn what they need to know on small, easy problems, then transfer that knowledge directly to much larger problems without re-training.\"\\n\\nCo-researcher Dr. Felipe Trevizan from ANU and CSIRO\\'s Data61 said ASNet did not compete against an adversary like AlphaGo does.\\n\\n\"Rather, it specialises in solving problems in dynamic and uncertain environments,\" he said.\\n\\nProfessor Sylvie Thiébaux, one of the ANU academics supervising Mr Toyer\\'s thesis, said the research was a proof of concept, which could be further developed with industry to use in intelligent robotics to perform tasks autonomously in a wide range of settings.\\n\\nShe said the system learnt how to play a game to arrange up to 35 blocks in a particular formation, having only been given 25 problems using up to nine blocks as part of its training.\\n\\n\"We gave ASNets a description of the game, some examples of plans for simple problems with a small number of blocks, and then it learnt to solve much more complex problems,\" said Professor Thiébaux from the ANU Research School of Computer Science.\\n\\nAssociate Professor Lexing Xie from the ANU Research School of Computer Science, who co-supervised Mr Toyer\\'s thesis, said ASNets was a creative way to use deep learning.\\n\\n\"This work is the first to borrow designs for visual recognition to solve decision-making problems that look very different in nature, but share logical structures,\" she said.\\n\\nExplore further: Where does AlphaGo go?')\n",
      "('Scientists pioneer use of deep learning for real-time gravitational wave discovery', \"Blue Waters numerical relativity simulation of two colliding black holes with the open source, numerical relativity software, the Einstein Toolkit. Authors: R. Haas and E. Huerta (NCSA/University of Illinois); Visualization: R. Haas. Scientists at the National Center for Supercomputing Applications (NCSA), located at the University of Illinois at Urbana-Champaign, have pioneered the use of GPU-accelerated deep learning for rapid detection and characterization of gravitational waves. This new approach will enable astronomers to study gravitational waves using minimal computational resources, reducing time to discovery and increasing the scientific reach of gravitational wave astrophysics. This innovative research was recently published in Physics Letters B.\\n\\nCombining deep learning algorithms, numerical relativity simulations of black hole mergers—obtained with the Einstein Toolkit run on the Blue Waters supercomputer—and data from the LIGO Open Science Center, NCSA Gravity Group researchers Daniel George and Eliu Huerta produced Deep Filtering, an end-to-end time-series signal processing method. Deep Filtering achieves similar sensitivities and lower errors compared to established gravitational wave detection algorithms, while being far more computationally efficient and more resilient to noise anomalies. The method allows faster than real-time processing of gravitational waves in LIGO's raw data, and also enables new physics, since it can detect new classes of gravitational wave sources that may go unnoticed with existing detection algorithms. George and Huerta are extending this method to identify in real-time electromagnetic counterparts to gravitational wave events in future LSST data.\\n\\nNCSA's Gravity Group leveraged NCSA resources from its Innovative Systems Laboratory, NCSA's Blue Waters supercomputer, and collaborated with talented interdisciplinary staff at the University of Illinois. Also critical to this research were the GPUs (Tesla P100 and DGX-1) provided by NVIDIA, which enabled an accelerated training of neural networks. Wolfram Research also played an important role, as the Wolfram Language was used in creating this framework for deep learning.\\n\\nGeorge and Huerta worked with NVIDIA and Wolfram researchers to create this demo to visualize the architecture of Deep Filtering, and to get insights into its neuronal activity during the detection and characterization of real gravitational wave events. This demo highlights all the components of Deep Filtering, exhibiting its detection sensitivity and computational performance.\\n\\nExplore further: Black hole pair born inside a dying star?\\n\\nMore information: Daniel George et al. Deep learning for real-time gravitational wave detection and parameter estimation: Results with advanced LIGO data, Physics Letters B (2017). DOI: 10.1016/j.physletb.2017.12.053\\n\\n\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Machine learning could help search for gravitational waves', 'A visualization of a supercomputer simulation of merging black holes sending out gravitational waves. Credit: NASA/C. Henze A trio of students from the University of Glasgow have developed a sophisticated artificial intelligence which could underpin the next phase of gravitational wave astronomy.\\n\\nIn a new paper published today in the journal Physical Review Letters, the researchers discuss how they used artificial intelligence tools to train an AI \\'brain\\' to search for gravitational wave signals.\\n\\nGravitational waves, ripples in spacetime caused by massive astronomical events, were first hypothesised by Albert Einstein in 1915. It took another century before the Laser Interferometry Gravitational-Wave Observatory (LIGO) detectors in the United States first picked up the very faint signals from the collision of binary black holes.\\n\\nSince that historic first detection in September 2015, the Advanced LIGO and European VIRGO detectors have picked up numerous signals from other binary black holes and one from the collision of binary neutron stars.\\n\\nCurrently, gravitational wave signals are picked from the background noise of the detectors using a technique known as matched filtering, which measures the outputs from the detectors against a bank of template waveforms. Signals which match the shape of a template waveform are then examined more closely to determine whether they represent a genuine gravitational wave detection.\\n\\nHowever, the process requires a great deal of computing power. As the detectors are upgraded and their sensitivity to gravitational wave signals increases, astronomers expect significantly more detections to be made during each observing run, bringing with it an accompanying increase in required computing power.\\n\\nUniversity of Glasgow Physics and Astronomy postgraduate students Hunter Gabbard and Fergus Hayes and undergraduate Michael Williams decided to investigate whether deep learning, a form of artificial intelligence, could help make the process of detection more computationally efficient.\\n\\nUnder the direction of University of Glasgow astrophysicist Dr. Christopher Messenger, they used a process known as supervised deep learning to build an artificial intelligence capable of correctly picking out gravitational wave signals buried in noise from thousands of simulated datasets which they created.\\n\\nHunter Gabbard said: \"Deep learning algorithms involve stacked arrays of processing units, which we call neurons, which act as filters for the input data. Supervised deep learning allows us to \\'teach\\' the system through three datasets we provide. The first dataset, the training set, allows us to ensure it\\'s \\'learning\\' what we want. The second, the validation set, shows us it\\'s learning in the way we expect. The final set, the test set, helps us quantify the system\\'s performance.\"\\n\\n\"What makes this process faster and more efficient than matched-filtering is that the training set is where all the computationally intensive activity occurs. Once the deep learning algorithm learns what to look for in a signal, it has the potential to be orders of magnitude faster than other methods.\"\\n\\nFergus Hayes added: \"At the same time, we also used a standard matched-filtering process to sift through our simulated gravitational wave data, so we could compare the effectiveness of our deep learning approach using a statistical process figure-of-merit called receiver operator characteristic (ROC) curves.\\n\\n\"With careful tuning and training of the deep learning tool, we found that those ROC curves showed a very similar performance between our new process and the matched-filtering process. What that suggests is that neural networks provide a very promising method for searching for gravitational wave signals.\"\\n\\nMichael Williams added: \"Although in this paper we\\'ve concentrated specifically on binary black hole detections, the process could easily be applied to other types of gravitational wave signals and we\\'re keen to continue our research. It\\'s an exciting finding, and suggests a very promising path forward for the more intensive gravitational wave astronomy which will come as the detectors become more sensitive.\"\\n\\nThe researchers\\' paper, titled \"Matching matched filtering with deep networks for gravitational-wave astronomy,\" is published in Physical Review Letters.\\n\\nExplore further: Scientists pioneer use of deep learning for real-time gravitational wave discovery\\n\\nMore information: Hunter Gabbard et al. Matching Matched Filtering with Deep Networks for Gravitational-Wave Astronomy, Physical Review Letters (2018). DOI: 10.1103/PhysRevLett.120.141103\\n\\n')\n",
      "(\"'Bat detectives' train new algorithms to discern bat calls in noisy recordings\", 'Citizen scientists enable deep learning approach to monitor bat populations and ecosystem health. Credit: Angell Williams/ Flickr Using data collected by citizen scientists, researchers have developed new, open-source algorithms to automatically detect bat echolocation calls in audio recordings. Oisin Mac Aodha, formerly of University College London, now at Caltech, and colleagues at University College London present their new approach in PLOS Computational Biology.\\n\\nNearly 80 percent of bat species use echolocation calls for tasks such as navigation and hunting prey. Detecting these sounds in audio recordings can help scientists monitor changes in bat populations, and since bats are especially sensitive to climate change, bat calls can serve as key indicators of ecosystem health. However, it can be challenging to reliably detect bat calls in noisy, real-world recordings.\\n\\nTo address this challenge, Mac Aodha and colleagues built new bat-call-detection algorithms based on recent developments in machine learning. Their algorithm relies on supervised learning with deep convolutional neural networks (CNNs), allowing them to learn directly from audio data to automatically and reliably detect bat calls.\\n\\nCNNs had never before been applied to bat monitoring, mainly because not enough human-analyzed data was available to train them. To amass enough data, Mac Aodha and colleagues turned to thousands of citizen scientists who collected and annotated audio datasets as participants in the Indicator Bats Programme and Bat Detective (http://www.batdetective.org) project.\\n\\nOnce the algorithms were trained, the researchers put them to the test by applying them to several datasets that represent challenging, real-world monitoring scenarios. They found that their deep-learning approach was more accurate and reliable than existing algorithms and commercial tools for monitoring bat calls.\\n\\n\"Our method enables us to automatically estimate bat activity from multi-year, large-scale, audio monitoring programs with very little manual effort, enabling us to scale up monitoring efforts globally,\" Mac Aodha says.\\n\\nTheir algorithm is currently deployed as part of a real-time bat monitoring project at the Queen Elizabeth Olympic Park in east London (https://naturesmartcities.com). The team is now working to develop systems that can extract more fine-grained information from audio recordings, such as the presence of specific bat species.\\n\\nExplore further: New algorithm recognizes distinct dolphin clicks in underwater recordings\\n\\nMore information: Oisin Mac Aodha et al, Bat detective—Deep learning tools for bat acoustic signal detection, PLOS Computational Biology (2018). DOI: 10.1371/journal.pcbi.1005995\\n\\n')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Google: Using your health records to predict whether you'll live or die\", 'Credit: CC0 Public Domain Dr. Google may not have much of a bedside manner—she\\'s an algorithm, after all—but if she says you\\'re soon to be \"expired,\" she claims to be about 95 percent accurate, and you might want to start planning that last meal.\\n\\nAn elite team of computer scientists and medical experts from Google and three major U.S. universities believe they\\'ve found the best way yet to predict whether a hospitalized patient will end up leaving via the front doors or the loading dock at the morgue.\\n\\nAs might be expected from research led by Google, the software for accomplishing this task relies on artificial intelligence, which has become a key focus in virtually all areas of the Mountain View company\\'s operations.\\n\\nIn a just-released paper, not peer reviewed, the researchers claim their AI-based software, based on AI known as \"deep learning,\" does a better job at predicting patient outcomes than other methods currently available.\\n\\n\"These models outperformed state-of-the-art traditional predictive models in all cases,\" the paper said.\\n\\nTo make its predictions, the software uses medical-records data including patient demographics, previous diagnoses and procedures, lab results and vital signs.\\n\\nTop of the list of outcomes predicted is \"inpatient mortality,\" in which the patient is reported as \"expired.\"\\n\\nBut the software goes beyond the question of life and death, to answer questions important to patients as well as hospital administrators and bean counters. Unplanned re-admissions to the medical facility within 30 days are also covered, as well as probable length of stay and diagnoses, the latter of which is delineated using hospital billing codes.\\n\\nThe paper covered the study of some 216,000 hospitalizations involving about 114,000 patients—anonymous to the researchers—at two hospitals: UC San Francisco\\'s and the University of Chicago\\'s.\\n\\n\"Its biggest claim is the ability to predict patient deaths 24-48 hours before current methods, which could allow time for doctors to administer life-saving procedures,\" according to online magazine Quartz, which spotted the paper published Jan. 26.\\n\\nThe software was able to predict death, at 24 hours after admission, with 93 percent to 95 percent accuracy, about 10 percent better than the traditional predictive method, according to the paper.\\n\\nThe researchers admitted to various limitations in their work, noting, for example, that it\\'s not a \"foregone conclusion\" that accurate predictions can improve care.\\n\\nAmong the science stars on the 35-researcher team were Google senior fellow Jeff Dean, head of the AI-focused \"Google Brain\" project; Stanford Neurosciences Institute professor Nigam Shah; and Alvin Rajkomar, director of clinical data science at UCSF\\'s Center for Digital Health Innovation.\\n\\nExplore further: How long will patient live? Deep Learning takes on predictions')\n",
      "('Artificial intelligence can help you protect your personal data', 'Credit: EPFL / Alain Herzog It\\'s a safe bet that some of the websites and apps you use collect and subsequently sell your personal data. But how can you know which ones? An EPFL researcher has led the development of a program that can answer that question in just a few seconds, thanks to artificial intelligence.\\n\\nIf you\\'re like most people, you don\\'t always take the time to read website terms and conditions before accepting them. Not only are they extremely lengthy, they are also convoluted and written in opaque legalese. However, they can contain surprising clauses about a website\\'s or app\\'s right to use the data it collects about you, such as your IP address, your age and your online preferences. To help consumers get a better grasp of what they\\'re agreeing to, a team of researchers from EPFL, the University of Wisconsin-Madison, and the University of Michigan have developed a program that uses artificial intelligence to decipher websites\\' data protection policies in the blink of an eye. Called Polisis, short for privacy policy analysis, their program can be used free of charge either as a browser extension (for Chrome of Firefox) or directly on their website.\\n\\n\"Our program uses simple graphs and color codes to show users exactly how their data could be used. For instance, some websites share geolocation data for marketing purposes, while others may not fully protect information about children. Such clauses are typically buried deep in their data protection policies,\" says Hamza Harkous, a post-doc working at EPFL\\'s Distributed Information Systems Laboratory and the project lead.\\n\\nWith a little help from machine learning\\n\\nThe researchers used artificial intelligence to teach their program how to pick apart websites\\' data protection policies, drawing on over 130,000 that they found online. Once the text of a policy is fed into the program, the software scours through it in just a few seconds and displays the results in easy-to-read visuals. That lets you see at a glance which data a website would be authorized to collect and for what purpose. You can then make an informed decision about whether to use the website, or, in the case of an app, download it. The program also indicates what options you have for refusing to share certain data and lists the potential disadvantages of each one.\\n\\nPolisis works hand-in-hand with another program called Pribot, which is an online chatbot where you can enter questions (for now only in English) about a website\\'s data protection policy. For example, you can type in \"Does it share my credit card information?\" and get a speedy answer. While Pribot, like Polisis, is not perfect – their results are for information only and offer no legal guarantee – it gives the right answer in the top 3 in around 82% of the time. A respectable score that could make it, along with its sister Polisis, extremely useful for consumers as well as journalists, researchers and data protection watchdogs.\\n\\nCredit: Ecole Polytechnique Federale de Lausanne\\n\\nGiving consumers a choice\\n\\nGoing forward, the team\\'s program could be used for other applications such as the Internet of Things. If you\\'re thinking about installing a connected object in your home, then you want to make sure its data protection policy is rock-solid. \"We want to show consumers that they have a choice by giving them the tools to evaluate a service and select an alternative if necessary,\" says Harkous. His next goals are to develop an alert system that would notify users of any unexpected use of their data, and to create a system for ranking services and connected objects according to their data protection policies.\\n\\nExplore further: Fifth of websites \\'lack privacy protection info\\'\\n\\nMore information: Polisis: Automated Analysis and Presentation of Privacy Policies Using Deep Learning. arXiv:1802.02561 [cs.CL] arxiv.org/abs/1802.02561\\n\\n')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Predicting Papaya Ripeness with Computer Vision Algorithm', 'From IEEE Spectrum via Science Direct:\\n\\nThe University of Campinas researchers teamed up with computer scientists from Londrina State University in Londrina, Brazil to develop the machine learning approach that achieved an overall ripeness detection accuracy of 94.7 percent. Their work appears in the February issue of the journal Computers and Electronics in Agriculture.\\n\\nMeasuring ripeness—and identifying relevant features for ripeness—was one of the biggest challenges. The researchers started out with a government guidance chart that listed five levels of papaya ripeness. But they soon consolidated ripeness levels into three maturity levels based on visual inspection: Visually, the outer peel of the golden papayas starts out green and yellows as the fruit ripens. They further verified the three levels with additional testing based on each fruit’s pulp firmness.\\n\\nTraining the machine learning algorithm also proved an unexpected challenge: It required a diverse selection of papayas. Researchers had hoped to get a large number of papayas from a local producer but eventually found themselves buying 57 golden papayas at a local market in Campinas.\\n\\nBoth the hardware and software components of the project proved relatively straightforward. On the hardware side, researchers built a boxy contraption with a consumer digital camera and light bulbs positioned on the ceiling to take illuminated pictures of the papaya samples. Success with such consumer-grade technology means this approach could be adapted fairly readily to commercial applications.\\n\\nOn the software side, the researchers considered a number of different machine learning algorithms before settling upon the common random forest classifier. This approach enabled the researchers to clearly see how different papaya features factored into the machine learning algorithm’s results. “We could see which features are really providing useful information about the fruit,” Barbin explains.\\n\\nA deep learning approach based on neural networks also might have yielded good results for visually identifying ripe papayas. But the Londrina State University colleagues were wary of the black box nature of deep learning algorithms that usually makes it extremely difficult to figure out how deep learning comes up with any given result. Furthermore, a deep learning approach would have required a potentially far greater sample of papayas in the training dataset to achieve reasonable accuracy.')\n",
      "('Lightmatter aims to reinvent AI-specific chips with photonic computing and $11M in funding', 'It takes an immense amount of processing power to create and operate the “AI” features we all use so often, from playlist generation to voice recognition. Lightmatter is a startup that is looking to change the way all that computation is done — and not in a small way. The company makes photonic chips that essentially perform calculations at the speed of light, leaving transistors in the dust. It just closed an $11 million Series A.\\n\\nThe claim may sound grandiose, but the team and the tech definitely check out. Nick Harris, Lightmatter’s CEO, wrote his thesis on this stuff at MIT, and has published in major journals like Nature Photonics several papers showing the feasibility of the photonic computing architecture.\\n\\nSo what exactly does Lightmatter’s hardware do?\\n\\nAt the base of all that AI and machine learning is, like most computing operations, a lot of math (hence the name computing). A general-purpose computer can do any of that math, but for complex problems it has to break it down into a series of smaller ones and perform them sequentially.\\n\\nOne such complex type of math problem common in AI applications is a matrix vector product. Doing these quickly is important for comparing large sets of data with one another, for instance if a voice recognition system wants to see if a certain sound wave is sufficiently similar to “OK Google” to initiate a response.\\n\\nThe problem is that as demand increases for AI-based products, these calculations need to be done more and faster, but we’re reaching the limits of just how quickly and efficiently they can be accomplished and relayed back to the user. So while the computing technology that has existed for decades isn’t going anywhere, for certain niches there are tantalizing options on the horizon.\\n\\n“One of the symptoms of Moore’s Law dying is that companies like Intel are investing in quantum and other stuff — basically anything that’s not traditional computing,” Harris told me in an interview. “Now is a great time to look at alternative architectures.”\\n\\nInstead of breaking that matrix calculation down to a series of basic operations with cascades of logic gates and transistors, Lightmatter’s photonic chips essentially solve the entire problem at once by running a beam of light through a gauntlet of tiny, configurable lenses (if that’s the right word at this scale) and sensors. By creating and tracking tiny changes in the phase or path of the light, the solution is found as fast as the light can get from one end of the chip to the other. Not only does this mean results come back nearly instantly, but it only uses a fraction of the power of traditional chips.\\n\\n“A lot of deep learning relies on this specific operation that our chip can accelerate,” explained Harris. “It’s a special case where a special purpose optical computer can shine. This is the first photonic chip that can do that, accurately and in a scalable way.”\\n\\nAnd not by 20 or 30 percent — we’re talking orders of magnitude here.\\n\\nThe company is built out of research Harris and colleagues began at MIT, which owns some of the patents relating to Lightmatter’s tech and licenses it to them. They created a prototype chip with 32 “neurons,” the sort of calculational building block of this type of photonics. Now the company is well on its way to creating one with hundreds.\\n\\n“In speed, power, and latency we’re pretty close to what you can theoretically do,” Harris said. That is to say, you can’t make light go any faster. But just like with traditional computers, you can make the chips denser, have them work in parallel, improve the sensors and so on.\\n\\nYou wouldn’t have one of these things in your home. Lightmatter chips would be found in specialty hardware used by hardcore AI developers. Maybe Google would buy a few dozen and use them to train stuff internally, or Amazon might make them available by the quarter second for quick-turnaround ML jobs.\\n\\nThe $11 million Series A round the company just announced, led by Matrix and Spark, is intended to help build the team that will take the technology from prototype to product.\\n\\n“This isn’t a science project,” said Matrix’s Stan Reiss, lest you think this is just a couple of students on a wild technology goose chase. “This is the first application of optical computing in a very controlled manner.”\\n\\nCompetitors, he noted, are focused on squeezing every drop of performance out of semi-specialized hardware like GPUs, making AI-specific boards that outperform stock hardware but ultimately are still traditional computers with lots of tweaks.\\n\\n“Anyone can build a chip that works like that, the problem is they’ll have a lot of competition,” he said. “This is the one company that’s totally orthogonal to that. It’s a different engine.”\\n\\nAnd it has only recently become possible, they both pointed out. Investment in basic research and the infrastructure behind building photonic chips over the last decade has paid off, and it’s finally gotten to the point where the technology can break out of the lab. (Lightmatter’s tech works with existing CMOS-based fabrication methods, so no need to spend hundreds of millions on a new fab.)\\n\\n“AI is really in its infancy,” as Harris put it in the press release announcing the investment, “and to move forward, new enabling technologies are required. At Lightmatter, we are augmenting electronic computers with photonics to power a fundamentally new kind of computer that is efficient enough to propel the next generation of AI.”')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('How to train, large scale, state of the art, neural networks on the Microsoft Data Science VM', 'Dr Adrian Rosebrock – Computer vision expert @ www.pyimagesearch.com , author of a popular Kickstarter deep learning for computer vision book\\n\\nAdrian has posted a great series of guest blogs on how he trains large scale state of the art neural networks on the Microsoft Data Science VM and shares the results of the experiments from using the NVidia K80 and V100 GPUs\\n\\nHere is the full blog series:\\n\\n· Deep Learning & Computer Vision in the Microsoft Azure Cloud\\n\\n· 22 Minutes to 2nd Place in a Kaggle Competition, with Deep Learning & Azure\\n\\n· Training state-of-the-art neural networks in the Microsoft Azure cloud\\n\\nAdrian’s review of the Azure DSVM at PyimageSearch.com https://www.pyimagesearch.com/2018/03/21/my-review-of-microsofts-deep-learning-virtual-machine/\\n\\nIf you want to learn about deep learning and computer vision, Adrian book and blog are highly recommended.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The Microsoft Artificial Intelligence Landscape – And What to use When', 'Artificial Intelligence (AI), at its broadest definition, is simply “a machine that can act using human-style reasoning or perception”. Of course, the technologies used to enable that definition are far from simple themselves.\\n\\nArtificial Intelligence isn’t new – I worked with “Good Old Fashioned AI” (that’s a real thing) back in the late 70’s and early 80’s. The promise of AI was higher than the technology allowed in those days, and so the AI field went dormant in the public’s mind. It had a resurgence a few years later, and now has returned with far greater options than we have ever had before. Breakthrough algorithms, understandings, and technologies now allow a far deeper use of AI – I’ll cover a few of these uses in another article. The hype this time, however, is (mostly) real. You’re using AI every day, and you can embed it in your applications for your users as well.\\n\\nFor the general public, AI is somewhat of a “black box”, with image recognition being the same level of difficulty to understand as language translation or even interpretation. But for the technical professional, some problems are already solved, and you can simply leverage them, and others you have to write yourself. In general, the more customization you need, the more you’ll need to write for your solution.\\n\\nDefine the Opportunity\\n\\nThe first thing to do of course is to define what opportunity you are solving for. For instance, you might have a need to estimate the repair costs of a car accident, using only a photo uploaded from your client from the web. Or perhaps you want to translate between languages so that your firm can read a document sent from another country. You may have a need to analyze that document to find its sentiment or general meaning, or you may want to generate an automated response from the facts and data it contains. You may want to do route prediction for expense mileage tracking, or fill out a set of values using only an example. (You can find a few real-world examples for AI and Machine Learning here)\\n\\nFrom No-Code to Deep Learning\\n\\nOnce you’ve defined the opportunity, the next step is to start at the very highest level of leveraging that you can. For instance, Microsoft Excel also has a feature (called Flash Fill) that will transform columns of data based only on an example. In the case of mileage and route tracking, that’s already built into an application – so you could simply use that.\\n\\nHowever, if you want to recognize road signs along the way, you could move up to an image recognition service. If you need to further process that image to read the text, there’s a service for that as well. If you want to determine if the sign is overhead or on the side of the road, you may need to customize that service further. Taking this example even further, if you want to determine the speed of the vehicle using only images of road signs as you pass by them by analyzing the angle of the edge of the sign, you may need to write some Machine Learning code to do that.\\n\\nThe point is to follow the progression of customization that you need. If the problem is solved, use that solution. If you need to customize the solution, that’s where you start to further leverage more tools.\\n\\nEnsemble Approaches\\n\\nAnother point to keep in mind is that you can combine applications, services and custom-developed AI into a much larger and more complex solution. The “Intelligent Kiosk” is one such example, combining image recognition, handwriting and voice interactions, and even bots to form a complete solution.\\n\\nThe AI Landscape\\n\\nWith that background set, let’s take a look at a few examples of the options you have for working with AI in a solution.\\n\\nEmbedded Artificial Intelligence\\n\\nYou may already be using AI in your day to day work. Office 365 applications such as Microsoft Word and PowerPoint have adaptable suggestions and critiques to make your communications clearer.\\n\\nPowerPoint also has a Presentation Translator, an add-in that can bring real-time captioning to your presentations, and even allows those captions to be translated to another language. Flash-Fill in Excel is an algorithm that examines an example of what you want and makes those conversions for you. Microsoft Dynamics 365 has built-in insights to Sales, Finance, HR and Customer Relationship Management embedded within the solution.\\n\\nSo what is the technical professional’s responsibility here? Education. I can’t tell you how many times I’ve shown companies the features listed here when they describe what they want to accomplish – only to have them say “We had no idea it already did this”. No, you probably don’t have time to create or deliver that training, but we’ve done that for you. There are hundreds of training options built into the products, in the documentation, and in free training seminars from Microsoft – a quick web search will find them. Speaking of which, Microsoft Bing also has AI built in for searches, predictions, Autosuggest, image searches and more, and you can add those features to your applications using Bing Search as an AI API call.\\n\\nLeveraging Artificial Intelligence in a Solution\\n\\nYour next option is to use various Cognitive Services in your solution. These are cloud-based services that you simply call in an application (no AI algorithms to write, just use the service) using Python, Java, C#, even CuRL. Remember, you can leverage these together, using Vision to feed Language to feed Search and so on. Here’s a quick list:\\n\\nCustomizing Artificial Intelligence for a Solution\\n\\nWhile services like the Vision API do most of what organizations need, there are times when you need to go a bit further and train or customize a service, again without having to know a lot about various Machine Learning and Artificial Intelligence algorithms and programming. This helps you get started faster, and as we make the services better, your apps automatically get better.\\n\\nHere are a few of the options when you need one step above a fully automated API call:\\n\\nWriting Your Own Artificial Intelligence Service\\n\\nAt times you’ll find there are certain requirements that a service just won’t handle on its own, nor is it customizable enough to do what you need. When I do an architecture, I start with as automated as I can, and then get “pushed” to the next level based on requirements.\\n\\nWhen you do hit that level, there are multiple options still available. Microsoft tools and platforms support all major Data Science languages, such as Python, R, Node, Java, C++, C#, F#, SQL and more. These platforms work with TensorFlow as well as newer toolkits and integrate with Open-Source and commercial software platforms. Not only that, the offerings are tied with a formal process you can use for project creation through DevOps.\\n\\nHybrid Solutions\\n\\nThere are on-premises solutions as well – and these can also be used in a Hybrid solution so that you can work with local data or for compliance reasons:\\n\\nLearning and References\\n\\nThere’s a full whitepaper you can download here on these options. And of course, we offer learning paths, a full AI School, and Machine Learning and AI programs on EdX.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ESRI GEO AI Data Science Virtual Machine', 'I was recently at University meeting where we got into the discussion of using Cloud Computing for Digital humanities.\\n\\nDigital Humanities is an area of scholarly activity at the intersection of computing or digital technologies and the disciplines of the humanities. It includes the systematic use of digital resources in the humanities, as well as the reflection on their application.\\n\\nDigital Humanities can be defined as new ways of doing scholarship that involve collaborative, transdisciplinary, and computationally engaged research, teaching, and publishing. It brings digital tools and methods to the study of the humanities with the recognition that the printed word is no longer the main medium for knowledge production and distribution.\\n\\nThe Availability of geo-spatial analytics and AI extension to the Microsoft Data Science Virtual Machin is now available via the Azure Marketplace and Data Science Virtual Machine allows Digital Humanities users to use new applications and techniques,\\n\\nWith the tools and services its makes new kinds of teaching and research possible, while at the same time studying and critiquing how these impact cultural heritage and digital culture. The use of the DSVM and ESRI solutions allows the field to both employs technology in the pursuit of humanities, research, and subjects technology to humanistic questioning and interrogation, often simultaneously.\\n\\nThe Geo AI Data Science VM is an extension to the Windows Server 2016 edition of the Microsoft Data Science Virtual Machine (DSVM) on Azure, offered through the collaboration between Esri and Microsoft. https://www.esri.com/en-us/landing-page/lp/product/2018/geo-ai\\n\\nThe Microsoft DSVM contains popular tools for data science as well as AI tools, such as enterprise grade R and Python on the Microsoft Machine Learning Server, Anaconda Python, JuliaPro, Jupyter Notebook for Python, Julia and R, Visual Studio Community edition with Python and R Tools, SQL Server Developer edition, standalone instance of Apache Spark, deep-learning frameworks like TensorFlow, Microsoft Cognitive Toolkit, and several other data science tools and machine learning algorithms. See a comprehensive list of Microsoft DSVM tools and algorithms.\\n\\nArcGIS Pro is Esri’s next-gen 64-bit desktop geographic information system (GIS). Technologically ahead of everything else on the market, ArcGIS Pro provides professional 2D and 3D mapping in an intuitive user interface. ArcGIS Pro is a big step forward in advancing visualization, analytics, image processing, data management, and integration.\\n\\nThe Geo AI Data Science VM augments the Microsoft DSVM with rich geo-spatial capabilities of Esri’s ArcGIS Pro. Python and R interfaces to ArcGIS Pro are pre-configured on the Geo AI Data Science VM, enabling programmatic access to geo-spatial analytics within your AI applications out of the box. We also provide samples in the form of Jupyter notebooks to help you start building AI applications infused with geo-spatial intelligence.\\n\\nIf you are building deep learning models on the Geo AI Data Science VM, we recommend you use Azure NC-Series GPU VM instances which is available in select Azure regions. Check here for availability of various services by Azure regions.\\n\\nBy continuing to create and use this extension, you are accepting the Esri ArcGIS Pro license agreements and the Microsoft Data Science Virtual Machine terms.\\n\\n\\n\\nLearn more\\n\\nDocumentation\\n\\nArcGIS Pro Help\\n\\n\\n\\nData Science VM Overview\\n\\nESRI Data Science\\n\\nResources\\n\\nAI for Earth sample: https://github.com/Azure/pixel_level_land_classification Tutorial demonstrating how to create a semantic segmentation (pixel-level classification) model to predict land cover from aerial imagery. This model can be used to identify newly developed or flooded land. Uses ground-truth labels and processed NAIP imagery provided by the Chesapeake Conservancy.\\n\\n\\n\\nIntroduction to the Geo Artificial Intelligence Data Science Virtual Machine – Azure https://docs.microsoft.com/en-gb/azure/machine-learning/data-science-virtual-machine/geo-ai-dsvm-overview\\n\\n\\n\\nUsing the Geo Artificial Intelligence Data Science Virtual Machine - Azure https://docs.microsoft.com/en-us/azure/machine-learning/data-science-virtual-machine/use-geo-ai-dsvm\\n\\n\\n\\nHow to use a Geo AI Virtual Machine on Azure. http://aka.ms/dsvm/geoai')\n",
      "('Windows 10 RS4 Preview for HoloLens and ONNX offline Machine Learning', 'Recently we announced that Windows 10 now includes the ability to run Open Neural Network Exchange (ONNX) models natively with hardware acceleration.\\n\\nThis announcement now brings 100s of millions of Windows devices, ranging from IoT edge devices to HoloLens to 2-in-1s and desktop PCs, into the ONNX ecosystem.\\n\\nYesterday we released the Windows 10 RS4 preview to HoloLens so this now allows Data scientists and developers creating AI models will be able to deploy their innovations to this large user base. From an academic perspective I have lots of HoloLens developers wanting to building apps that are use AI models offline to deliver more powerful and engaging experiences.\\n\\n\\n\\nSo what is ONNX?\\n\\nONNX is an open source model representation for interoperability and innovation in the AI ecosystem.\\n\\nMicrosoft actually helped start ONNX in September 2017, and with support from many other companies, ONNX v1 launched in December 2017.\\n\\nThanks to ONNX-ML, Windows supports both classic machine learning and deep learning, enabling a spectrum of AI models and scenarios. Developers can obtain ONNX models to include in their apps in several ways:\\n\\nCreate and train ONNX models using any of the many frameworks that currently support generating ONNX models.\\n\\nConvert models from various sources including SciKit-Learn and CoreML into ONNX models by using ONNXML tools.\\n\\nObtain pre-trained ONNX models from the growing ONNX model zoo.\\n\\nYou can learn more about how to integrate ONNX models into Windows apps here.\\n\\nIf your interested in adding ONNX to your HoloLens Apps see my colleagues Mike Taulty blogs\\n\\nFirst Experiment with Image Classification on Windows ML from UWP\\n\\nSecond Experiment with Image Classification on Windows ML from UWP (on HoloLens)\\n\\nThird Experiment with Image Classification on Windows ML from UWP (on HoloLens in Unity)')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Service Fabric Customer Profile: SiriusIQ', 'SiriusIQ delivers seriously intelligent AI on Azure Service Fabric\\n\\nTo design the next generation of enterprise workflow automation, SiriusIQ built a new development stack and defined their core architecture using Azure Service Fabric, Azure Cognitive Service Language Understanding, and other Azure services.\\n\\nThis article is part of a series on customers who worked closely with Microsoft on Service Fabric over the last year. We look at why they chose Service Fabric, and we take a closer look at the design of their solution.\\n\\nBot-driven intelligence and workflow automation\\n\\nSiriusIQ focuses on next-generation artificial intelligence (AI) and bot technologies with workflow automation. Their cloud-born solutions streamline business processes, conversations, and analytics using a dynamic workflow engine built on the Service Fabric platform.\\n\\nSiriusIQ solutions share a dynamic, intelligent workflow engine at their core. Many of these workflows solve common business issues including automated data migrations, GDPR (General Data Protection Regulation) solutions, AI-built FAQs, automated meeting bookings, and cross-system enterprise workflows involving third-party interfaces such as Salesforce, ServiceNow, and DocuSign.\\n\\nBy combining the power of SiriusIQ’s workflow engine with AI and bot technologies, they developed an innovative cloud-based migration tool that enables their customers to move just about any data from point A to point B. SiriusIQ embraces AI as the UI—their natural language interface makes it easy for customers to interact with the many SiriusIQ services. This interface uses Language Understanding (LUIS) and other services from Azure’s growing library.\\n\\nService Fabric gives us a globally scalable solution that is secure, stays current, and is easily extensible by adding new microservices. It also eased or removed many classic development challenges—OS patching, version testing, security issues, scalability. We can develop and bring new functionality to our global customers in timelines that were not possible before embracing Azure services. —Ken Leach, SiriusIQ Partner\\n\\nMicroservices empower the development team\\n\\nSiriusIQ wanted to avoid the traditional way of developing and solving workflow issues. That model requires distinct solutions, forcing code changes and regression testing for variances in customers’ requirements. Using this classic, monolithic development model, developers are burdened with many challenges. Anyone who has written production code in the last 30 years is familiar with massive code bases, recompilation issues, and the need to redeploy the whole stack while managing the impact on user downtime. Whole app regression testing is challenging enough without also trying to keep up with operating system patches.\\n\\nThis pattern is common in the industry, but SiriusIQ knew they needed to do things differently. They needed a broader architecture than they could create on their own, one that scaled globally, stayed secure, and could be dynamically optimized for performance. They simply could not use the classic development model with its operating system limits and much slower software development life cycle. Azure gave them the platform they needed for delivering a globally balanced solution, and Service Fabric provided a way to get new features into production in just minutes or hours.\\n\\nBy switching to Service Fabric and microservices, the developers at SiriusIQ soon discovered how many of the old development issues they could leave behind. They quickly embraced the new development patterns that enabled them to scale on demand and deploy only what’s changed without downtime. The team didn’t have to worry about operating system patches. Even security was simpler, since many of the classic application security concerns are not relevant in a Service Fabric solution. Microservices also freed developers from the overhead associated with the traditional development model. They can focus on just the microservice they are building, which makes the team more efficient.\\n\\nUsing Service Fabric, the SiriusIQ team could also rethink the way they developed services. They made the most of stateless services for HTTP as well as stateful services for internal processing and actors for processing millions of small transactions—all of which can be scaled independently. This shift in technology led to far more advanced integrations than the team ever imagined and gave rise to QuinnTM, the SiriusIQ intelligent assistant that uses Azure Cognitive Services AI with active learning. Quinn plays many roles at SiriusIQ. It manages the overall flow of data, consumes telemetry, and learns more efficient workflows. The more Quinn is used, the smarter it gets.\\n\\nA new workflow architecture\\n\\nOne of the team’s goals was to create a new workflow engine to speed their customers’ data migrations. SiriusIQ went beyond the classic development model, which runs processes in parallel or uses PowerShell scripts. Instead, they used Service Fabric to build microservices that disconnect reads from writes to create dynamic workflows that adapts to the most efficient path based on their custom AI. The Azure platform provides global scalability and performance.\\n\\nFigure: SiriusIQ dynamic workflow architecture uses separate reader and writer microservices for fast, intelligent data migration.\\n\\nTo achieve the goal of ever smarter, faster services, SiriusIQ’s new workflow architecture uses a collection of AI services and telemetry—much of which is collected with Microsoft Application Insights—to power Quinn, the SiriusIQ AI bot. The team also optimized the new architecture with a continuous deployment model based on Visual Studio Team Services. This continuous integration and deployment environment for Azure allows SiriusIQ to deploy on demand using their AI bot, which avoids the need for downtime.\\n\\nA dynamic service workflow\\n\\nIn the SiriusIQ architecture, dynamic workflows define goals. Those goals are achieved by a collection of microservices that run in Service Fabric and perform a certain task efficiently. Using deep learning and Application Insights telemetry, the AI calculates an efficient path from the starting point to the goal via the correct set of microservices. Intelligent workflow processes consider multiple permutations to ensure the most efficient path. Many SiriusIQ services work in similar fashion to support very complicated workflows such as tracking a contract through signing with HubSpot, ServiceNow, and DocuSign.com.\\n\\nFor data migrations, the AI manages optimal path to reach the goal using SiriusIQ’s growing library of microservices. One migration will easily have millions of Service Fabric requests, instantiating required microservices that securely read data from sources such as email, files, or messaging systems. The reader microservice works independently of the writer microservice that writes to a destination in a similar manner. Massive parallelism ensures extremely fast workflows. The architecture can even read once and write to multiple destination systems if, for instance, a customer wants a copy to go to both OneDrive for Business and Box.com.\\n\\nDuring a file migration, millions of instances may be running, but even if an instance encounters an exception, the overall process does not slow at all. The process with the exception attempts to use a new path based on past telemetry from instances with similar issues where a known path solved the issue.\\n\\nIf a path does not exist, a development “swim lane” is generated with captured telemetry from Application Insights. This detailed information includes the telemetry around the issue as well as the line of code that caused the exception. A developer can quickly find the issue and build a slightly different version of the microservice, then use a continuous deployment pipeline to add it to the possible options—without breaking any of the existing code. The excepted process can then find a new route with the new microservice. If successful, it becomes a model for other processes that may encounter a similar issue. If unsuccessful, the effectiveness rating of the new service drops, and the process is aged off the system if it does not provide any value. In this manner, the development team can dynamically optimize any workflow and take full advantage of Service Fabric and the AI.\\n\\nService Fabric allows SiriusIQ to build a real, managed microservice platform that can be scaled and provisioned easily with high availability both on premises and in the cloud. —Wallace Breza, SiriusIQ Partner\\n\\nService Fabric benefits\\n\\nThe team at SiriusIQ has been working with the latest technology for more than 30 years. Always on the lookout for value and more efficient and effective ways to achieve their goals, the team was impressed by what Service Fabric had to offer. It completely changed the way they developed, tested, deployed, and scaled their core technology. Service Fabric also delivered a valuable model for any development effort going forward, enabling SiriusIQ to bring more value to customers through dynamic services, improved costs and efficiencies, and a better user experience.\\n\\nThe Service Fabric platform provided the following key benefits:\\n\\nFewer dependencies: By breaking up the dynamic workflow process into smaller microservices, the SiriusIQ team removed widespread dependencies. Dependencies still exist but only for a focused microservice instead of a larger dependency matrix typical in monolithic applications. This change alone greatly increased performance and reduced testing and deployment timelines.\\n\\nBy breaking up the dynamic workflow process into smaller microservices, the SiriusIQ team removed widespread dependencies. Dependencies still exist but only for a focused microservice instead of a larger dependency matrix typical in monolithic applications. This change alone greatly increased performance and reduced testing and deployment timelines. Mid-process branching at scale: Using the Service Fabric Actor pattern, SiriusIQ introduced mid-process branching, which opened up many possibilities for future products. For example, an added service is metadata tagging during content migrations. If access is granted by a customer, the contents of the object being moved can be tagged for PII, GDPR, or other custom requirements, all out of band while the payload is in transit. Tagging does not slow down the migration—it occurs without interruption to the dataflow underway.\\n\\nUsing the Service Fabric Actor pattern, SiriusIQ introduced mid-process branching, which opened up many possibilities for future products. For example, an added service is metadata tagging during content migrations. If access is granted by a customer, the contents of the object being moved can be tagged for PII, GDPR, or other custom requirements, all out of band while the payload is in transit. Tagging does not slow down the migration—it occurs without interruption to the dataflow underway. Global scale: The Service Fabric microservice model gave the SiriusIQ team the global scalability and performance they needed. For example, in a simple migration where 1,000 users exchange email, the new platform easily supports more than seventy million messages on Service Fabric. SiriusIQ was also able to comply with data sovereignty laws of Australia while moving sensitive government email. They deployed a complete, dedicated solution in the Azure Australian data center using their AI bot and Azure Resource Manager in under thirty minutes.\\n\\nThe Service Fabric microservice model gave the SiriusIQ team the global scalability and performance they needed. For example, in a simple migration where 1,000 users exchange email, the new platform easily supports more than seventy million messages on Service Fabric. SiriusIQ was also able to comply with data sovereignty laws of Australia while moving sensitive government email. They deployed a complete, dedicated solution in the Azure Australian data center using their AI bot and Azure Resource Manager in under thirty minutes. Streamlined testing: SiriusIQ’s patent-pending workflow exception model allows dynamic workflow updates without code deployments. The new microservice-based process reduces the scope of the testing needed. The overhead of managing patches, scale, and deployments is no longer a burden.\\n\\nSiriusIQ’s patent-pending workflow exception model allows dynamic workflow updates without code deployments. The new microservice-based process reduces the scope of the testing needed. The overhead of managing patches, scale, and deployments is no longer a burden. Self-tuning systems: When a new service is introduced, SiriusIQ’s patent-pending workflow process compares its performance to others that perform a similar function and ranks them dynamically. The best options for a given scenario float to the top; likewise, the slower or non-performing options age off the system. This process is managed by SiriusIQ’s custom AI that considers rank among many other metrics when determining the most efficient path to reach goals.\\n\\nWhen a new service is introduced, SiriusIQ’s patent-pending workflow process compares its performance to others that perform a similar function and ranks them dynamically. The best options for a given scenario float to the top; likewise, the slower or non-performing options age off the system. This process is managed by SiriusIQ’s custom AI that considers rank among many other metrics when determining the most efficient path to reach goals. Security: The compliance certifications of the Azure platform bring added value to SiriusIQ and their customers’ security teams. SiriusIQ performs regular service audits based on the Service Organization Control (SOC) reporting framework. Since SiriusIQ’s complete solution lives in Azure, many of the compliance checkboxes needed are provided to them by Microsoft.\\n\\nOther Azure services\\n\\nSiriusIQ works with a number of other Azure services, including:\\n\\nLUIS: The AI bot can dynamically update the intent model in LUIS, growing as it is used.\\n\\nThe AI bot can dynamically update the intent model in LUIS, growing as it is used. Application Insights: One of the secrets to the success of the best practices model of developing in Service Fabric is to have the best possible telemetry. Application Insights gives the SiriusIQ team a level of visibility into everything that is happening, from one user’s file to billions of messages and migrations happing simultaneously. A small email migration of 1,000 users, for instance, can easily produce over 35 MB a day of telemetry data from Application Insights. SiriusIQ spent a fair amount of time designing how they could not just report status but use their AI and Azure deep learning to make the telemetry actionable.\\n\\nOne of the secrets to the success of the best practices model of developing in Service Fabric is to have the best possible telemetry. Application Insights gives the SiriusIQ team a level of visibility into everything that is happening, from one user’s file to billions of messages and migrations happing simultaneously. A small email migration of 1,000 users, for instance, can easily produce over 35 MB a day of telemetry data from Application Insights. SiriusIQ spent a fair amount of time designing how they could not just report status but use their AI and Azure deep learning to make the telemetry actionable. Azure Redis Cache: The power and speed of Azure Redis Cache combined with the ability to secure the transport brings performance, security, and scalability for the countless services SiriusIQ provides to its customers.\\n\\nThe power and speed of Azure Redis Cache combined with the ability to secure the transport brings performance, security, and scalability for the countless services SiriusIQ provides to its customers. Azure Service Bus: Leveraging Service Bus topics and subscriptions allows the pub/sub model to scale to support thousands of requests per second.\\n\\nLeveraging Service Bus topics and subscriptions allows the pub/sub model to scale to support thousands of requests per second. Azure Cosmos DB: The solution uses Azure Cosmos DB for fast, reliable geo-redundant data storage across document and graph databases.\\n\\nThe solution uses Azure Cosmos DB for fast, reliable geo-redundant data storage across document and graph databases. Microsoft Bot Frameworks: SiriusIQ’s bot communication channels seamlessly scale to the most popular messaging channels available. The framework offers capabilities for building their own as well.\\n\\nSiriusIQ’s bot communication channels seamlessly scale to the most popular messaging channels available. The framework offers capabilities for building their own as well. Azure Key Vault: To keep SiriusIQ’s protected configuration data secure, they use Key Vault.\\n\\nDeciding to build new tech from scratch using Service Fabric at the core of our solution could not have been a better fit for us. We now start projects within a whole ecosystem of technology from within Azure that honestly gets us more than 80 percent of the way to delivery. The more microservices we build and accumulate, the more capabilities we are able to offer to our customers. There was no other alternative on the market that came close to Service Fabric. —Mark Golden, SiriusIQ Partner\\n\\nSummary\\n\\nAs the cloud landscape continues to evolve and customers ask for more services, the Service Fabric platform helps the SiriusIQ team adapt to change quickly. Although the team’s current focus is data migration, GDPR, and healthcare use cases, the company’s core dynamic workflow is very flexible and can be used to solve many business issues. By simply adding new microservices, SiriusIQ can take advantage of new functions in Service Fabric, release new services to its customers, and set the pace of the industry. The natural language capabilities enable users to continue to interact with the process while it runs.\\n\\nSiriusIQ was recently approved as a member of the Co-Sell program of the Microsoft One Commercial Partner ISV program. This exclusive program highlights unique partner solutions that address critical enterprise IT pain points by making the most of Microsoft technologies. Learn more at the SiriusIQ website.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Artificial Intelligence Turns Poachers Into Prey', \"USC's AI technology could help save more rhinos from a fate like this (via tracyhammond/Pixabay)\\n\\nResearchers are using artificial intelligence to turn poachers into prey.\\n\\nThe University of Southern California Center for Artificial Intelligence in Society applied deep learning to spot hunters in near real time.\\n\\nWhat began with impoverished peasants looking for food has grown into a global dilemma, one that poses a serious threat to the survival of many plant and animal populations.\\n\\nUSC has long applied AI to protect wildlife; analysts initially used artificial intelligence and game theory to anticipate poachers’ haunts. Now they have the technology to stop them in their tracks (so to speak).\\n\\nTools like infrared cameras are great for monitoring poachers during nighttime hunts. Except, of course, that the equipment picks up human and animal heat maps, making it difficult to focus on the right mark.\\n\\nSo, a team of USC computer scientists employed their proprietary labeling tool to tag 180,000 people and critters in infrared videos. And with the help of a modified version of the Faster RCNN deep learning algorithm, the team was able to teach a computer to automatically distinguish infrared images of humans from those of animals.\\n\\nThe challenge, then, was to deploy the algorithm to spot poachers, recorded live by drones hovering above.\\n\\nPut to the test at national parks in Zimbabwe and Malawi, the algorithm took 10 seconds to process each image—too long when an elephant, tiger, rhinoceros, or gorilla’s life is on the line.\\n\\nWith a few tweaks (including a move to Microsoft’s Azure cloud computing platform) “SPOT”—the Systematic POacher deTector—now works to detect poachers and animals in about three-tenths of a second.\\n\\n“SPOT will ease the burden on those using drones for anti-poaching by automatically detecting people and animals in infrared imagery, and by providing detections in near real time,” lead author Elizabeth Bondi, a Ph.D. candidate in computer science at USC, said in a statement.\\n\\nThis sounds a lot more high tech than some of the other tactics, like robotic decoy animals, used to lure poachers into a trap.\\n\\nLet us know what you like about Geek by taking our survey.\")\n",
      "('Google AI Looks Deep Into Your Eyes to Predict Heart Disease', 'On the left, an image of the back of the eye. On the right, a retinal image in gray, with the pixels used by the deep learning algorithm to make predictions about the blood pressure highlighted in shades of green (heatmap) (via Google Brain)\\n\\nEyes may be the windows to the soul. But they also offer insight into cardiovascular disease.\\n\\nAccording to a study published this week by the Google Brain team, retinal images can “very accurately” predict indicators of CV health.\\n\\nUsing deep learning algorithms trained on data from more than 284,000 patients, researchers were able to foretell risk factors with “surprisingly high accuracy,” Lily Peng, Google Brain product manager, wrote in a blog announcement.\\n\\n“This discovery is particularly exciting because it suggests we might discover even more ways to diagnose health issues from retinal images,” she said.\\n\\nIt’s a long and winding road to assessing heart health: Doctors must examine various factors—age, sex, smoking habits, blood pressure and cholesterol levels—and take into consideration whether a patient has other diseases, like diabetes, which is associated with high risk of heart attack.\\n\\nGoogle’s algorithm, however, can predict those same factors … and then some.\\n\\nThe deep learning tech, for example, correctly identified the image of a smoker’s eye versus a non-smoker’s eye 71 percent of the time. It can also distinguish between photos of people with severe high blood pressure and those without, and predicts systolic blood pressure within 11 mmHg on average.\\n\\n“Our algorithm was fairly accurate at predicting the risk of a CV event directly,” Peng boasted.\\n\\nAll without drawing blood.\\n\\nLast year, scientists at the University of Nottingham found a new way of anticipating heart failure: machine learning.\\n\\nLed by epidemiologist Stephen Weng, the group trained a neural network to correctly identify 75 percent of fatal and non-fatal cardiovascular events. In fact, the MVP algorithm correctly IDed 355 more patients than human doctors.\\n\\nThe Google Brain team, meanwhile, doesn’t just focus on humans: Peng & Co. also poked around inside the mind of their deep learning system to find out what makes the algorithm tick.\\n\\nThis allows researchers to explain how their technology works, which in turn gives doctors more confidence in the program.\\n\\n“Our approach uses deep learning to draw connections between changes in the human anatomy and disease, akin to how doctors learn to associate signs and symptoms with the diagnosis of a new disease,” Peng said. “This could help scientists generate more targeted hypotheses and drive a wide range of future research.”\\n\\nMoving forward, the team will look at effects of interventions—lifestyle change, medication—on risk predictions, and will continue generating new hypotheses and theories to test.\\n\\nLet us know what you like about Geek by taking our survey.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Google AI Looks Deep Into Your Eyes to Predict Heart Disease', 'On the left, an image of the back of the eye. On the right, a retinal image in gray, with the pixels used by the deep learning algorithm to make predictions about the blood pressure highlighted in shades of green (heatmap) (via Google Brain)\\n\\nEyes may be the windows to the soul. But they also offer insight into cardiovascular disease.\\n\\nAccording to a study published this week by the Google Brain team, retinal images can “very accurately” predict indicators of CV health.\\n\\nUsing deep learning algorithms trained on data from more than 284,000 patients, researchers were able to foretell risk factors with “surprisingly high accuracy,” Lily Peng, Google Brain product manager, wrote in a blog announcement.\\n\\n“This discovery is particularly exciting because it suggests we might discover even more ways to diagnose health issues from retinal images,” she said.\\n\\nIt’s a long and winding road to assessing heart health: Doctors must examine various factors—age, sex, smoking habits, blood pressure and cholesterol levels—and take into consideration whether a patient has other diseases, like diabetes, which is associated with high risk of heart attack.\\n\\nGoogle’s algorithm, however, can predict those same factors … and then some.\\n\\nThe deep learning tech, for example, correctly identified the image of a smoker’s eye versus a non-smoker’s eye 71 percent of the time. It can also distinguish between photos of people with severe high blood pressure and those without, and predicts systolic blood pressure within 11 mmHg on average.\\n\\n“Our algorithm was fairly accurate at predicting the risk of a CV event directly,” Peng boasted.\\n\\nAll without drawing blood.\\n\\nLast year, scientists at the University of Nottingham found a new way of anticipating heart failure: machine learning.\\n\\nLed by epidemiologist Stephen Weng, the group trained a neural network to correctly identify 75 percent of fatal and non-fatal cardiovascular events. In fact, the MVP algorithm correctly IDed 355 more patients than human doctors.\\n\\nThe Google Brain team, meanwhile, doesn’t just focus on humans: Peng & Co. also poked around inside the mind of their deep learning system to find out what makes the algorithm tick.\\n\\nThis allows researchers to explain how their technology works, which in turn gives doctors more confidence in the program.\\n\\n“Our approach uses deep learning to draw connections between changes in the human anatomy and disease, akin to how doctors learn to associate signs and symptoms with the diagnosis of a new disease,” Peng said. “This could help scientists generate more targeted hypotheses and drive a wide range of future research.”\\n\\nMoving forward, the team will look at effects of interventions—lifestyle change, medication—on risk predictions, and will continue generating new hypotheses and theories to test.\\n\\nLet us know what you like about Geek by taking our survey.')\n",
      "('Skip the Lines at Cashier-Less Amazon Go, Now Open in Seattle', 'Amazon Go marks the advent of \"Just Walk Out Shopping\" (via Amazon)\\n\\nAmazon today opened its first cashier-less grocery store in Seattle.\\n\\nAmazon Go invites customers to “just grab and go”—no checkout required.\\n\\nSimply sign into the Amazon Go mobile app to enter the store, shop as usual, and walk out the doors without touching your wallet.\\n\\nRelying on computer vision, sensor fusion, and deep learning (the same functions used in self-driving cars), the company’s so-called “Just Walk Out Technology” automatically detects when a product is removed from or returned to shelves, and keeps track of them in a virtual cart.\\n\\nUpon departure, your Amazon account is charged and a receipt is sent.\\n\\n“We asked ourselves: What if we could create a shopping experience with no lines and no checkout?” the firm wrote online. “Could we push the boundaries of computer vision and machine learning to create a store where customers could simply take what they want and go?\\n\\n“Our answer to those questions is Amazon Go and Just Walk Out Shopping.”\\n\\nPop in for a pre-prepped snack, grocery essentials, or an Amazon Meal Kit, featuring “all the ingredients you need to make a meal for two in about 30 minutes.” The brick-and-mortar retailer, of course, also stocks products from new subsidiary Whole Foods Market.\\n\\nThe OG Go location—in an Amazon office building at 2131 7th Ave. in Seattle—includes a team of associates prepping ingredients, making ready-to-eat food, stocking shelves, and helping customers. The “conveniently compact” shop is open Monday through Friday from 7 a.m. to 9 p.m.\\n\\nYou’ll need to download the Amazon Go app for iOS, Android, or Amazon before visiting.\\n\\nThere is no word on if or when the tech titan will add more venues; according to Reuters, there is no plan to expand the technology to Whole Foods stores.\\n\\nInitially opened to Amazon employees in December 2016, the convenience-style store was expected to go public by early 2017. Technical issues—identifying shoppers with similar body types, children moving items to incorrect places—delayed the grand opening, though.\\n\\n“We have spent a lot of time figuring out how to make our algorithms and our sensors reliable, highly available, and very efficient so that you get things right and we’re very accurate,” Dilip Kumar, Amazon Go’s vice president of technology, told Fast Company.\\n\\nLet us know what you like about Geek by taking our survey.')"
     ]
    }
   ],
   "source": [
    "for doc in zip(db.title,db.text):\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
